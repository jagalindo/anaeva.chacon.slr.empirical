% Encoding: UTF-8

@Article{Tiwari:2013:RRT:2439976.2439982,
  author     = {Tiwari, Rajeev and Goel, Noopur},
  title      = {Reuse: Reducing Test Effort},
  journal    = {SIGSOFT Softw. Eng. Notes},
  year       = {2013},
  volume     = {38},
  number     = {2},
  pages      = {1--11},
  month      = mar,
  issn       = {0163-5948},
  acmid      = {2439982},
  address    = {New York, NY, USA},
  comment    = {11},
  doi        = {10.1145/2439976.2439982},
  issue_date = {March 2013},
  keywords   = {reusable test cases, reuse-oriented test approaches, software product lines, test effort},
  numpages   = {11},
  publisher  = {ACM},
  url        = {http://doi.acm.org/10.1145/2439976.2439982},
}

@Article{Ribeiro:2011:IFD:2189751.2047868,
  author     = {Ribeiro, M\'{a}rcio and Queiroz, Felipe and Borba, Paulo and Tol\^{e}do, T\'{a}rsis and Brabrand, Claus and Soares, S{\'e}rgio},
  title      = {On the Impact of Feature Dependencies when Maintaining Preprocessor-based Software Product Lines},
  journal    = {SIGPLAN Not.},
  year       = {2011},
  volume     = {47},
  number     = {3},
  pages      = {23--32},
  month      = oct,
  issn       = {0362-1340},
  acmid      = {2047868},
  address    = {New York, NY, USA},
  comment    = {10},
  doi        = {10.1145/2189751.2047868},
  issue_date = {March 2012},
  keywords   = {modularity, preprocessors, software product lines},
  numpages   = {10},
  publisher  = {ACM},
  url        = {http://doi.acm.org/10.1145/2189751.2047868},
}

@Article{Kowal:2016:EAF:3093335.2993248,
  author     = {Kowal, Matthias and Ananieva, Sofia and Th\"{u}m, Thomas},
  title      = {Explaining Anomalies in Feature Models},
  journal    = {SIGPLAN Not.},
  year       = {2016},
  volume     = {52},
  number     = {3},
  pages      = {132--143},
  month      = oct,
  issn       = {0362-1340},
  acmid      = {2993248},
  address    = {New York, NY, USA},
  comment    = {12},
  doi        = {10.1145/3093335.2993248},
  issue_date = {March 2017},
  keywords   = {Anomalies, Explanations, Feature Models, Software Product Lines},
  numpages   = {12},
  publisher  = {ACM},
  url        = {http://doi.acm.org/10.1145/3093335.2993248},
}

@Article{Kramer:2013:UDG:2637365.2517214,
  author     = {Kramer, Dean and Oussena, Samia and Komisarczuk, Peter and Clark, Tony},
  title      = {Using Document-oriented GUIs in Dynamic Software Product Lines},
  journal    = {SIGPLAN Not.},
  year       = {2013},
  volume     = {49},
  number     = {3},
  pages      = {85--94},
  month      = oct,
  issn       = {0362-1340},
  acmid      = {2517214},
  address    = {New York, NY, USA},
  comment    = {10},
  doi        = {10.1145/2637365.2517214},
  issue_date = {March 2014},
  keywords   = {dynamic software product lines, graphical user interfaces},
  numpages   = {10},
  publisher  = {ACM},
  url        = {http://doi.acm.org/10.1145/2637365.2517214},
}

@Article{Chimalakonda:2016:ESS:2934240.2934248,
  author     = {Chimalakonda, Sridhar and Lee, Dan Hyung},
  title      = {On the Evolution of Software and Systems Product Line Standards},
  journal    = {SIGSOFT Softw. Eng. Notes},
  year       = {2016},
  volume     = {41},
  number     = {3},
  pages      = {27--30},
  month      = jun,
  issn       = {0163-5948},
  acmid      = {2934248},
  address    = {New York, NY, USA},
  comment    = {3},
  doi        = {10.1145/2934240.2934248},
  issue_date = {May 2016},
  keywords   = {Methods, Patterns, Software Product Lines, Standards, Tools},
  numpages   = {4},
  publisher  = {ACM},
  url        = {http://doi.acm.org/10.1145/2934240.2934248},
}

@Article{Acher:2017:TSP:3155324.3088440,
  author     = {Acher, Mathieu and Lopez-Herrejon, Roberto E. and Rabiser, Rick},
  title      = {Teaching Software Product Lines: A Snapshot of Current Practices and Challenges},
  journal    = {ACM Trans. Comput. Educ.},
  year       = {2017},
  volume     = {18},
  number     = {1},
  pages      = {2:1--2:31},
  month      = oct,
  issn       = {1946-6226},
  acmid      = {3088440},
  address    = {New York, NY, USA},
  articleno  = {2},
  comment    = {31},
  doi        = {10.1145/3088440},
  issue_date = {December 2017},
  keywords   = {Software product lines, software engineering teaching, software product line teaching, variability modeling},
  numpages   = {31},
  publisher  = {ACM},
  url        = {http://doi.acm.org/10.1145/3088440},
}

@Article{Hubaux:2013:SCF:2501654.2501665,
  author     = {Hubaux, Arnaud and Tun, Thein Than and Heymans, Patrick},
  title      = {Separation of Concerns in Feature Diagram Languages: A Systematic Survey},
  journal    = {ACM Comput. Surv.},
  year       = {2013},
  volume     = {45},
  number     = {4},
  pages      = {51:1--51:23},
  month      = aug,
  issn       = {0360-0300},
  acmid      = {2501665},
  address    = {New York, NY, USA},
  articleno  = {51},
  comment    = {23},
  doi        = {10.1145/2501654.2501665},
  issue_date = {August 2013},
  keywords   = {Software product line, feature diagram, separation of concerns, variability},
  numpages   = {23},
  publisher  = {ACM},
  url        = {http://doi.acm.org/10.1145/2501654.2501665},
}

@Article{Seidl:2015:GSP:2936314.2814212,
  author     = {Seidl, Christoph and Schuster, Sven and Schaefer, Ina},
  title      = {Generative Software Product Line Development Using Variability-aware Design Patterns},
  journal    = {SIGPLAN Not.},
  year       = {2015},
  volume     = {51},
  number     = {3},
  pages      = {151--160},
  month      = oct,
  issn       = {0362-1340},
  acmid      = {2814212},
  address    = {New York, NY, USA},
  comment    = {10},
  doi        = {10.1145/2936314.2814212},
  issue_date = {March 2016},
  keywords   = {Design Pattern, Generative Development, Role Modeling, Software Product Line (SPL)},
  numpages   = {10},
  publisher  = {ACM},
  url        = {http://doi.acm.org/10.1145/2936314.2814212},
}

@Article{Rothberg:2016:TSC:3093335.2993252,
  author     = {Rothberg, Valentin and Dietrich, Christian and Ziegler, Andreas and Lohmann, Daniel},
  title      = {Towards Scalable Configuration Testing in Variable Software},
  journal    = {SIGPLAN Not.},
  year       = {2016},
  volume     = {52},
  number     = {3},
  pages      = {156--167},
  month      = oct,
  issn       = {0362-1340},
  acmid      = {2993252},
  address    = {New York, NY, USA},
  comment    = {12},
  doi        = {10.1145/3093335.2993252},
  issue_date = {March 2017},
  keywords   = {Configurability, Linux, Sampling, Software Product Lines, Software Testing},
  numpages   = {12},
  publisher  = {ACM},
  url        = {http://doi.acm.org/10.1145/3093335.2993252},
}

@Article{Linsbauer:2017:CVC:3170492.3136054,
  author     = {Linsbauer, Lukas and Berger, Thorsten and Gr\"{u}nbacher, Paul},
  title      = {A Classification of Variation Control Systems},
  journal    = {SIGPLAN Not.},
  year       = {2017},
  volume     = {52},
  number     = {12},
  pages      = {49--62},
  month      = oct,
  issn       = {0362-1340},
  acmid      = {3136054},
  address    = {New York, NY, USA},
  comment    = {14},
  doi        = {10.1145/3170492.3136054},
  issue_date = {December 2017},
  keywords   = {Variability management, configuration management, software product lines, software repositories},
  numpages   = {14},
  publisher  = {ACM},
  url        = {http://doi.acm.org/10.1145/3170492.3136054},
}

@Article{Ruprecht:2014:AFS:2775053.2658767,
  author     = {Ruprecht, Andreas and Heinloth, Bernhard and Lohmann, Daniel},
  title      = {Automatic Feature Selection in Large-scale System-software Product Lines},
  journal    = {SIGPLAN Not.},
  year       = {2014},
  volume     = {50},
  number     = {3},
  pages      = {39--48},
  month      = sep,
  issn       = {0362-1340},
  acmid      = {2658767},
  address    = {New York, NY, USA},
  comment    = {10},
  doi        = {10.1145/2775053.2658767},
  issue_date = {March 2015},
  keywords   = {Feature Selection, Linux, Software Product Lines, Software Tailoring},
  numpages   = {10},
  publisher  = {ACM},
  url        = {http://doi.acm.org/10.1145/2775053.2658767},
}

@Article{Pereira:2016:FPR:3093335.2993249,
  author     = {Pereira, Juliana Alves and Matuszyk, Pawel and Krieter, Sebastian and Spiliopoulou, Myra and Saake, Gunter},
  title      = {A Feature-based Personalized Recommender System for Product-line Configuration},
  journal    = {SIGPLAN Not.},
  year       = {2016},
  volume     = {52},
  number     = {3},
  pages      = {120--131},
  month      = oct,
  issn       = {0362-1340},
  acmid      = {2993249},
  address    = {New York, NY, USA},
  comment    = {12},
  doi        = {10.1145/3093335.2993249},
  issue_date = {March 2017},
  keywords   = {Personalized Recommendations, Product-Line Configuration, Recommenders, Software Product Lines},
  numpages   = {12},
  publisher  = {ACM},
  url        = {http://doi.acm.org/10.1145/3093335.2993249},
}

@Article{Al-Hajjaji:2016:IEP:3093335.2993253,
  author     = {Al-Hajjaji, Mustafa and Krieter, Sebastian and Th\"{u}m, Thomas and Lochau, Malte and Saake, Gunter},
  title      = {IncLing: Efficient Product-line Testing Using Incremental Pairwise Sampling},
  journal    = {SIGPLAN Not.},
  year       = {2016},
  volume     = {52},
  number     = {3},
  pages      = {144--155},
  month      = oct,
  issn       = {0362-1340},
  acmid      = {2993253},
  address    = {New York, NY, USA},
  comment    = {12},
  doi        = {10.1145/3093335.2993253},
  issue_date = {March 2017},
  keywords   = {Software product lines, combinatorial interaction testing, model-based testing, sampling},
  numpages   = {12},
  publisher  = {ACM},
  url        = {http://doi.acm.org/10.1145/3093335.2993253},
}

@Article{Rosenmuller:2011:TDS:2189751.2047866,
  author     = {Rosenm\"{u}ller, Marko and Siegmund, Norbert and Pukall, Mario and Apel, Sven},
  title      = {Tailoring Dynamic Software Product Lines},
  journal    = {SIGPLAN Not.},
  year       = {2011},
  volume     = {47},
  number     = {3},
  pages      = {3--12},
  month      = oct,
  issn       = {0362-1340},
  acmid      = {2047866},
  address    = {New York, NY, USA},
  comment    = {10},
  doi        = {10.1145/2189751.2047866},
  issue_date = {March 2012},
  keywords   = {dynamic binding, feature-oriented programming, software product lines},
  numpages   = {10},
  publisher  = {ACM},
  url        = {http://doi.acm.org/10.1145/2189751.2047866},
}

@Article{Schulze:2010:CCF:1942788.1868310,
  author     = {Schulze, Sandro and Apel, Sven and K\"{a}stner, Christian},
  title      = {Code Clones in Feature-oriented Software Product Lines},
  journal    = {SIGPLAN Not.},
  year       = {2010},
  volume     = {46},
  number     = {2},
  pages      = {103--112},
  month      = oct,
  issn       = {0362-1340},
  acmid      = {1868310},
  address    = {New York, NY, USA},
  comment    = {10},
  doi        = {10.1145/1942788.1868310},
  issue_date = {Febuary 2011},
  keywords   = {code clones, feature-oriented programming, refactoring, software product lines},
  numpages   = {10},
  publisher  = {ACM},
  url        = {http://doi.acm.org/10.1145/1942788.1868310},
}

@Article{Bashroush:2017:CTS:3058791.3034827,
  author     = {Bashroush, Rabih and Garba, Muhammad and Rabiser, Rick and Groher, Iris and Botterweck, Goetz},
  title      = {CASE Tool Support for Variability Management in Software Product Lines},
  journal    = {ACM Comput. Surv.},
  year       = {2017},
  volume     = {50},
  number     = {1},
  pages      = {14:1--14:45},
  month      = mar,
  issn       = {0360-0300},
  acmid      = {3034827},
  address    = {New York, NY, USA},
  articleno  = {14},
  comment    = {45},
  doi        = {10.1145/3034827},
  issue_date = {April 2017},
  keywords   = {Software engineering, computer-aided software engineering, software variability},
  numpages   = {45},
  publisher  = {ACM},
  url        = {http://doi.acm.org/10.1145/3034827},
}

@Article{El-Sharkawy:2015:AKS:2936314.2814222,
  author     = {El-Sharkawy, Sascha and Krafczyk, Adam and Schmid, Klaus},
  title      = {Analysing the Kconfig Semantics and Its Analysis Tools},
  journal    = {SIGPLAN Not.},
  year       = {2015},
  volume     = {51},
  number     = {3},
  pages      = {45--54},
  month      = oct,
  issn       = {0362-1340},
  acmid      = {2814222},
  address    = {New York, NY, USA},
  comment    = {10},
  doi        = {10.1145/2936314.2814222},
  issue_date = {March 2016},
  keywords   = {Kconfig, Kconfig Reader, LVAT, Linux Kernel Analysis, Linux Variability Analysis Tools, Software Product Lines, Undertaker},
  numpages   = {10},
  publisher  = {ACM},
  url        = {http://doi.acm.org/10.1145/2936314.2814222},
}

@Article{Font:2015:AMR:2936314.2814214,
  author     = {Font, Jaime and Arcega, Lorena and Haugen, \Oystein and Cetina, Carlos},
  title      = {Addressing Metamodel Revisions in Model-based Software Product Lines},
  journal    = {SIGPLAN Not.},
  year       = {2015},
  volume     = {51},
  number     = {3},
  pages      = {161--170},
  month      = oct,
  issn       = {0362-1340},
  acmid      = {2814214},
  address    = {New York, NY, USA},
  comment    = {10},
  doi        = {10.1145/2936314.2814214},
  issue_date = {March 2016},
  keywords   = {Model and Metamodel Co-evolution, Model-based Software Product Lines, Variability Modeling},
  numpages   = {10},
  publisher  = {ACM},
  url        = {http://doi.acm.org/10.1145/2936314.2814214},
}

@Article{Pettersson:2005:IEB:1095430.1081758,
  author     = {Pettersson, Ulf and Jarzabek, Stan},
  title      = {Industrial Experience with Building a Web Portal Product Line Using a Lightweight, Reactive Approach},
  journal    = {SIGSOFT Softw. Eng. Notes},
  year       = {2005},
  volume     = {30},
  number     = {5},
  pages      = {326--335},
  month      = sep,
  issn       = {0163-5948},
  acmid      = {1081758},
  address    = {New York, NY, USA},
  comment    = {10},
  doi        = {10.1145/1095430.1081758},
  issue_date = {September 2005},
  keywords   = {maintenance, program synthesis, reuse, software product lines, static meta-programming, web engineering},
  numpages   = {10},
  publisher  = {ACM},
  url        = {http://doi.acm.org/10.1145/1095430.1081758},
}

@Article{Robinson:2010:SCS:1842713.1842717,
  author     = {Robinson, William N. and Ding, Yi},
  title      = {A Survey of Customization Support in Agent-based Business Process Simulation Tools},
  journal    = {ACM Trans. Model. Comput. Simul.},
  year       = {2010},
  volume     = {20},
  number     = {3},
  pages      = {14:1--14:29},
  month      = oct,
  issn       = {1049-3301},
  acmid      = {1842717},
  address    = {New York, NY, USA},
  articleno  = {14},
  comment    = {29},
  doi        = {10.1145/1842713.1842717},
  issue_date = {September 2010},
  keywords   = {Agent-based modeling, application frameworks, encapsulation, event-driven simulation, modularity, software product line engineering},
  numpages   = {29},
  publisher  = {ACM},
  url        = {http://doi.acm.org/10.1145/1842713.1842717},
}

@Article{Kastner:2009:MRP:1837852.1621632,
  author     = {K\"{a}stner, Christian and Apel, Sven and Kuhlemann, Martin},
  title      = {A Model of Refactoring Physically and Virtually Separated Features},
  journal    = {SIGPLAN Not.},
  year       = {2009},
  volume     = {45},
  number     = {2},
  pages      = {157--166},
  month      = oct,
  issn       = {0362-1340},
  acmid      = {1621632},
  address    = {New York, NY, USA},
  comment    = {10},
  doi        = {10.1145/1837852.1621632},
  issue_date = {February 2010},
  keywords   = {AHEAD, CIDE, FeatureHouse, preprocessor, refinements, separation of concerns, software product lines},
  numpages   = {10},
  publisher  = {ACM},
  url        = {http://doi.acm.org/10.1145/1837852.1621632},
}

@Article{Thum:2012:FDV:2480361.2371404,
  author     = {Th\"{u}m, Thomas and Schaefer, Ina and Apel, Sven and Hentschel, Martin},
  title      = {Family-based Deductive Verification of Software Product Lines},
  journal    = {SIGPLAN Not.},
  year       = {2012},
  volume     = {48},
  number     = {3},
  pages      = {11--20},
  month      = sep,
  issn       = {0362-1340},
  acmid      = {2371404},
  address    = {New York, NY, USA},
  comment    = {10},
  doi        = {10.1145/2480361.2371404},
  issue_date = {March 2013},
  keywords   = {deductive verification, product-line analysis, program families, software product lines, theorem proving},
  numpages   = {10},
  publisher  = {ACM},
  url        = {http://doi.acm.org/10.1145/2480361.2371404},
}

@Article{Sanen:2009:MPS:1837852.1621633,
  author     = {Sanen, Frans and Truyen, Eddy and Joosen, Wouter},
  title      = {Mapping Problem-space to Solution-space Features: A Feature Interaction Approach},
  journal    = {SIGPLAN Not.},
  year       = {2009},
  volume     = {45},
  number     = {2},
  pages      = {167--176},
  month      = oct,
  issn       = {0362-1340},
  acmid      = {1621633},
  address    = {New York, NY, USA},
  comment    = {10},
  doi        = {10.1145/1837852.1621633},
  issue_date = {February 2010},
  keywords   = {DLV, configuration knowledge, default logic, distributed runtime adaptation, problem-solution feature interactions, software product line engineering},
  numpages   = {10},
  publisher  = {ACM},
  url        = {http://doi.acm.org/10.1145/1837852.1621633},
}

@Article{Hess:2014:ALI:2775053.2658772,
  author     = {Hess, Benjamin and Gross, Thomas R. and P\"{u}schel, Markus},
  title      = {Automatic Locality-friendly Interface Extension of Numerical Functions},
  journal    = {SIGPLAN Not.},
  year       = {2014},
  volume     = {50},
  number     = {3},
  pages      = {83--92},
  month      = sep,
  issn       = {0362-1340},
  acmid      = {2658772},
  address    = {New York, NY, USA},
  comment    = {10},
  doi        = {10.1145/2775053.2658772},
  issue_date = {March 2015},
  keywords   = {Libraries, components, interface extension, performance, preprocessors, programming language features interaction, software product lines},
  numpages   = {10},
  publisher  = {ACM},
  url        = {http://doi.acm.org/10.1145/2775053.2658772},
}

@Article{Kastner:2012:TCA:2211616.2211617,
  author     = {K\"{a}stner, Christian and Apel, Sven and Th\"{u}m, Thomas and Saake, Gunter},
  title      = {Type Checking Annotation-based Product Lines},
  journal    = {ACM Trans. Softw. Eng. Methodol.},
  year       = {2012},
  volume     = {21},
  number     = {3},
  pages      = {14:1--14:39},
  month      = jul,
  issn       = {1049-331X},
  acmid      = {2211617},
  address    = {New York, NY, USA},
  articleno  = {14},
  comment    = {39},
  doi        = {10.1145/2211616.2211617},
  issue_date = {June 2012},
  keywords   = {\#ifdef, CFJ, CIDE, Featherweight Java, conditional compilation, software product lines, type system},
  numpages   = {39},
  publisher  = {ACM},
  url        = {http://doi.acm.org/10.1145/2211616.2211617},
}

@Article{Walkingshaw:2014:PEV:2775053.2658766,
  author     = {Walkingshaw, Eric and Ostermann, Klaus},
  title      = {Projectional Editing of Variational Software},
  journal    = {SIGPLAN Not.},
  year       = {2014},
  volume     = {50},
  number     = {3},
  pages      = {29--38},
  month      = sep,
  issn       = {0362-1340},
  acmid      = {2658766},
  address    = {New York, NY, USA},
  comment    = {10},
  doi        = {10.1145/2775053.2658766},
  issue_date = {March 2015},
  keywords   = {bidirectional transformations, projectional editing, software product lines, variation, view-update problem},
  numpages   = {10},
  publisher  = {ACM},
  url        = {http://doi.acm.org/10.1145/2775053.2658766},
}

@Article{Bodden:2013:SLS:2499370.2491976,
  author     = {Bodden, Eric and Tol\^{e}do, T\'{a}rsis and Ribeiro, M\'{a}rcio and Brabrand, Claus and Borba, Paulo and Mezini, Mira},
  title      = {SPLLIFT: Statically Analyzing Software Product Lines in Minutes Instead of Years},
  journal    = {SIGPLAN Not.},
  year       = {2013},
  volume     = {48},
  number     = {6},
  pages      = {355--364},
  month      = jun,
  issn       = {0362-1340},
  acmid      = {2491976},
  address    = {New York, NY, USA},
  comment    = {10},
  doi        = {10.1145/2499370.2491976},
  issue_date = {June 2013},
  keywords   = {context sensitive, flow sensitive, inter-procedural static analysis, software product lines},
  numpages   = {10},
  publisher  = {ACM},
  url        = {http://doi.acm.org/10.1145/2499370.2491976},
}

@Article{Thum:2014:CSA:2620784.2580950,
  author     = {Th\"{u}m, Thomas and Apel, Sven and K\"{a}stner, Christian and Schaefer, Ina and Saake, Gunter},
  title      = {A Classification and Survey of Analysis Strategies for Software Product Lines},
  journal    = {ACM Comput. Surv.},
  year       = {2014},
  volume     = {47},
  number     = {1},
  pages      = {6:1--6:45},
  month      = jun,
  issn       = {0360-0300},
  acmid      = {2580950},
  address    = {New York, NY, USA},
  articleno  = {6},
  comment    = {45},
  doi        = {10.1145/2580950},
  issue_date = {July 2014},
  keywords   = {Product-line analysis, model checking, program family, software analysis, software product line, static analysis, theorem proving, type checking},
  numpages   = {45},
  publisher  = {ACM},
  url        = {http://doi.acm.org/10.1145/2580950},
}

@Article{Fenske:2017:PAA:3170492.3136059,
  author     = {Fenske, Wolfram and Schulze, Sandro and Saake, Gunter},
  title      = {How Preprocessor Annotations (Do Not) Affect Maintainability: A Case Study on Change-proneness},
  journal    = {SIGPLAN Not.},
  year       = {2017},
  volume     = {52},
  number     = {12},
  pages      = {77--90},
  month      = oct,
  issn       = {0362-1340},
  acmid      = {3136059},
  address    = {New York, NY, USA},
  comment    = {14},
  doi        = {10.1145/3170492.3136059},
  issue_date = {December 2017},
  keywords   = {annotations, change-proneness, maintenance, preprocessors, variability},
  numpages   = {14},
  publisher  = {ACM},
  url        = {http://doi.acm.org/10.1145/3170492.3136059},
}

@Article{Jalote:2008:PRG:13487689.13487690,
  author     = {Jalote, Pankaj and Murphy, Brendan and Sharma, Vibhu Saujanya},
  title      = {Post-release Reliability Growth in Software Products},
  journal    = {ACM Trans. Softw. Eng. Methodol.},
  year       = {2008},
  volume     = {17},
  number     = {4},
  pages      = {17:1--17:20},
  month      = aug,
  issn       = {1049-331X},
  acmid      = {13487690},
  address    = {New York, NY, USA},
  articleno  = {17},
  comment    = {20},
  doi        = {10.1145/13487689.13487690},
  issue_date = {August 2008},
  keywords   = {Post-release reliability growth, product stabilization time},
  numpages   = {20},
  publisher  = {ACM},
  url        = {http://doi.acm.org/10.1145/13487689.13487690},
}

@Article{Tan:2009:CDM:1571629.1571630,
  author     = {Tan, Hee Beng Kuan and Zhao, Yuan and Zhang, Hongyu},
  title      = {Conceptual Data Model-based Software Size Estimation for Information Systems},
  journal    = {ACM Trans. Softw. Eng. Methodol.},
  year       = {2009},
  volume     = {19},
  number     = {2},
  pages      = {4:1--4:37},
  month      = oct,
  issn       = {1049-331X},
  acmid      = {1571630},
  address    = {New York, NY, USA},
  articleno  = {4},
  comment    = {37},
  doi        = {10.1145/1571629.1571630},
  issue_date = {October 2009},
  keywords   = {Software sizing, conceptual data model, line of code (LOC), multiple linear regression model},
  numpages   = {37},
  publisher  = {ACM},
  url        = {http://doi.acm.org/10.1145/1571629.1571630},
}

@Article{Nekvi:2014:IRC:2666081.2629432,
  author     = {Nekvi, Md Rashed I. and Madhavji, Nazim H.},
  title      = {Impediments to Regulatory Compliance of Requirements in Contractual Systems Engineering Projects: A Case Study},
  journal    = {ACM Trans. Manage. Inf. Syst.},
  year       = {2014},
  volume     = {5},
  number     = {3},
  pages      = {15:1--15:35},
  month      = dec,
  issn       = {2158-656X},
  acmid      = {2629432},
  address    = {New York, NY, USA},
  articleno  = {15},
  comment    = {35},
  doi        = {10.1145/2629432},
  issue_date = {January 2015},
  keywords   = {Legal requirements elicitation, compliance of requirements, effort estimation, rail infrastructure system},
  numpages   = {35},
  publisher  = {ACM},
  url        = {http://doi.acm.org/10.1145/2629432},
}

@Article{Pritchett:2001:OMS:507546.507604,
  author     = {Pritchett,IV, William W.},
  title      = {An Object-oriented Metrics Suite for Ada 95},
  journal    = {Ada Lett.},
  year       = {2001},
  volume     = {XXI},
  number     = {4},
  pages      = {117--126},
  month      = sep,
  issn       = {1094-3641},
  acmid      = {507604},
  address    = {New York, NY, USA},
  comment    = {10},
  doi        = {10.1145/507546.507604},
  issue_date = {December 2001},
  numpages   = {10},
  publisher  = {ACM},
  url        = {http://doi.acm.org/10.1145/507546.507604},
}

@Article{Strecker:2012:ADC:2211616.2211620,
  author     = {Strecker, Jaymie and Memon, Atif M.},
  title      = {Accounting for Defect Characteristics in Evaluations of Testing Techniques},
  journal    = {ACM Trans. Softw. Eng. Methodol.},
  year       = {2012},
  volume     = {21},
  number     = {3},
  pages      = {17:1--17:43},
  month      = jul,
  issn       = {1049-331X},
  acmid      = {2211620},
  address    = {New York, NY, USA},
  articleno  = {17},
  comment    = {43},
  doi        = {10.1145/2211616.2211620},
  issue_date = {June 2012},
  keywords   = {Defects, GUI testing, faults},
  numpages   = {43},
  publisher  = {ACM},
  url        = {http://doi.acm.org/10.1145/2211616.2211620},
}

@Article{Hierons:2016:SOP:2913009.2897760,
  author     = {Hierons, Robert M. and Li, Miqing and Liu, Xiaohui and Segura, Sergio and Zheng, Wei},
  title      = {SIP: Optimal Product Selection from Feature Models Using Many-Objective Evolutionary Optimization},
  journal    = {ACM Trans. Softw. Eng. Methodol.},
  year       = {2016},
  volume     = {25},
  number     = {2},
  pages      = {17:1--17:39},
  month      = apr,
  issn       = {1049-331X},
  acmid      = {2897760},
  address    = {New York, NY, USA},
  articleno  = {17},
  comment    = {39},
  doi        = {10.1145/2897760},
  issue_date = {May 2016},
  keywords   = {Product selection},
  numpages   = {39},
  publisher  = {ACM},
  url        = {http://doi.acm.org/10.1145/2897760},
}

@Article{Kapfhammer:2003:FTA:949952.940086,
  author     = {Kapfhammer, Gregory M. and Soffa, Mary Lou},
  title      = {A Family of Test Adequacy Criteria for Database-driven Applications},
  journal    = {SIGSOFT Softw. Eng. Notes},
  year       = {2003},
  volume     = {28},
  number     = {5},
  pages      = {98--107},
  month      = sep,
  issn       = {0163-5948},
  acmid      = {940086},
  address    = {New York, NY, USA},
  comment    = {10},
  doi        = {10.1145/949952.940086},
  issue_date = {September 2003},
  keywords   = {database-driven applications, test adequacy criteria},
  numpages   = {10},
  publisher  = {ACM},
  url        = {http://doi.acm.org/10.1145/949952.940086},
}

@Article{Mockus:2002:TCS:567793.567795,
  author     = {Mockus, Audris and Fielding, Roy T. and Herbsleb, James D.},
  title      = {Two Case Studies of Open Source Software Development: Apache and Mozilla},
  journal    = {ACM Trans. Softw. Eng. Methodol.},
  year       = {2002},
  volume     = {11},
  number     = {3},
  pages      = {309--346},
  month      = jul,
  issn       = {1049-331X},
  acmid      = {567795},
  address    = {New York, NY, USA},
  comment    = {38},
  doi        = {10.1145/567793.567795},
  issue_date = {July 2002},
  keywords   = {Apache, Mozilla, Open source software, code ownership, defect density, repair interval},
  numpages   = {38},
  publisher  = {ACM},
  url        = {http://doi.acm.org/10.1145/567793.567795},
}

@article{Baresi:2007:TES:1276933.1276936,
 author = {Baresi, Luciano and Morasca, Sandro},
 title = {Three Empirical Studies on Estimating the Design Effort of Web Applications},
 journal = {ACM Trans. Softw. Eng. Methodol.},
 issue_date = {September 2007},
 volume = {16},
 number = {4},
 month = sep,
 year = {2007},
 issn = {1049-331X},
 articleno = {15},
 url = {http://doi.acm.org/10.1145/1276933.1276936},
 doi = {10.1145/1276933.1276936},
 acmid = {1276936},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {W2000, Web application design, effort estimation, empirical study},
}

@Article{Rhein:2018:VSA:3287303.3280986,
  author     = {Rhein, Alexander Von and Liebig, J\"{o}RG and Janker, Andreas and K\"{a}stner, Christian and Apel, Sven},
  title      = {Variability-Aware Static Analysis at Scale: An Empirical Study},
  journal    = {ACM Trans. Softw. Eng. Methodol.},
  year       = {2018},
  volume     = {27},
  number     = {4},
  pages      = {18:1--18:33},
  month      = nov,
  issn       = {1049-331X},
  acmid      = {3280986},
  address    = {New York, NY, USA},
  articleno  = {18},
  comment    = {33},
  doi        = {10.1145/3280986},
  issue_date = {November 2018},
  keywords   = {Highly configurable systems, TypeChef, configuration sampling, variability-aware analysis},
  numpages   = {33},
  publisher  = {ACM},
  url        = {http://doi.acm.org/10.1145/3280986},
}

@Article{Mohagheghi:2008:EIS:1363102.1363104,
  author     = {Mohagheghi, Parastoo and Conradi, Reidar},
  title      = {An Empirical Investigation of Software Reuse Benefits in a Large Telecom Product},
  journal    = {ACM Trans. Softw. Eng. Methodol.},
  year       = {2008},
  volume     = {17},
  number     = {3},
  pages      = {13:1--13:31},
  month      = jun,
  issn       = {1049-331X},
  acmid      = {1363104},
  address    = {New York, NY, USA},
  articleno  = {13},
  comment    = {31},
  doi        = {10.1145/1363102.1363104},
  issue_date = {June 2008},
  keywords   = {Software reuse, fault density, product family, risks, standardization},
  numpages   = {31},
  publisher  = {ACM},
  url        = {http://doi.acm.org/10.1145/1363102.1363104},
}

@Article{Lazreg:2018:FFA:3284971.3284975,
  author     = {Lazreg, Sami and Collet, Philippe and Mosser, S{\'e}bastien},
  title      = {Functional Feasibility Analysis of Variability-intensive Data Flow-oriented Applications over Highly-configurable Platforms},
  journal    = {SIGAPP Appl. Comput. Rev.},
  year       = {2018},
  volume     = {18},
  number     = {3},
  pages      = {32--48},
  month      = oct,
  issn       = {1559-6915},
  acmid      = {3284975},
  address    = {New York, NY, USA},
  comment    = {17},
  doi        = {10.1145/3284971.3284975},
  issue_date = {September 2018},
  keywords   = {behavioral product lines model checking, embedded system design engineering, feature model, variability modeling},
  numpages   = {17},
  publisher  = {ACM},
  url        = {http://doi.acm.org/10.1145/3284971.3284975},
}

@Article{Zheng:2018:MAC:3234930.3229048,
  author     = {Zheng, Yongjie and Cu, Cuong and Taylor, Richard N.},
  title      = {Maintaining Architecture-Implementation Conformance to Support Architecture Centrality: From Single System to Product Line Development},
  journal    = {ACM Trans. Softw. Eng. Methodol.},
  year       = {2018},
  volume     = {27},
  number     = {2},
  pages      = {8:1--8:52},
  month      = jun,
  issn       = {1049-331X},
  acmid      = {3229048},
  address    = {New York, NY, USA},
  articleno  = {8},
  comment    = {52},
  doi        = {10.1145/3229048},
  issue_date = {July 2018},
  keywords   = {Architecture-implementation mapping, architectural evolution, architecture-centric development, architecture-centric feature traceability, variability conformance},
  numpages   = {52},
  publisher  = {ACM},
  url        = {http://doi.acm.org/10.1145/3229048},
}

@Article{Gencel:2008:FSM:1363102.1363106,
  author     = {Gencel, Cigdem and Demirors, Onur},
  title      = {Functional Size Measurement Revisited},
  journal    = {ACM Trans. Softw. Eng. Methodol.},
  year       = {2008},
  volume     = {17},
  number     = {3},
  pages      = {15:1--15:36},
  month      = jun,
  issn       = {1049-331X},
  acmid      = {1363106},
  address    = {New York, NY, USA},
  articleno  = {15},
  comment    = {36},
  doi        = {10.1145/1363102.1363106},
  issue_date = {June 2008},
  keywords   = {COSMIC FFP, Functional size measurement, MkII FPA, software benchmarking, software estimation},
  numpages   = {36},
  publisher  = {ACM},
  url        = {http://doi.acm.org/10.1145/1363102.1363106},
}

@Article{Purao:2003:PMO:857076.857090,
  author     = {Purao, Sandeep and Vaishnavi, Vijay},
  title      = {Product Metrics for Object-oriented Systems},
  journal    = {ACM Comput. Surv.},
  year       = {2003},
  volume     = {35},
  number     = {2},
  pages      = {191--221},
  month      = jun,
  issn       = {0360-0300},
  acmid      = {857090},
  address    = {New York, NY, USA},
  comment    = {31},
  doi        = {10.1145/857076.857090},
  issue_date = {June 2003},
  keywords   = {Software metrics, measurement theory, object-oriented metrics, object-oriented product metrics, object-oriented systems},
  numpages   = {31},
  publisher  = {ACM},
  url        = {http://doi.acm.org/10.1145/857076.857090},
}

@Article{Coarfa:2006:PAT:1124153.1124155,
  author     = {Coarfa, Cristian and Druschel, Peter and Wallach, Dan S.},
  title      = {Performance Analysis of TLS Web Servers},
  journal    = {ACM Trans. Comput. Syst.},
  year       = {2006},
  volume     = {24},
  number     = {1},
  pages      = {39--69},
  month      = feb,
  issn       = {0734-2071},
  acmid      = {1124155},
  address    = {New York, NY, USA},
  comment    = {31},
  doi        = {10.1145/1124153.1124155},
  issue_date = {February 2006},
  keywords   = {Internet, RSA accelerator, TLS, e-commerce, secure Web servers},
  numpages   = {31},
  publisher  = {ACM},
  url        = {http://doi.acm.org/10.1145/1124153.1124155},
}

@Article{Kolesnikov:2013:CPF:2637365.2517213,
  author     = {Kolesnikov, Sergiy and von Rhein, Alexander and Hunsen, Claus and Apel, Sven},
  title      = {A Comparison of Product-based, Feature-based, and Family-based Type Checking},
  journal    = {SIGPLAN Not.},
  year       = {2013},
  volume     = {49},
  number     = {3},
  pages      = {115--124},
  month      = oct,
  issn       = {0362-1340},
  acmid      = {2517213},
  address    = {New York, NY, USA},
  comment    = {10},
  doi        = {10.1145/2637365.2517213},
  issue_date = {March 2014},
  keywords   = {feature-oriented programming, fuji, product-line analysis, type checking},
  numpages   = {10},
  publisher  = {ACM},
  url        = {http://doi.acm.org/10.1145/2637365.2517213},
}

@Article{Chen:2012:ETS:2398856.2364535,
  author     = {Chen, Sheng and Erwig, Martin and Walkingshaw, Eric},
  title      = {An Error-tolerant Type System for Variational Lambda Calculus},
  journal    = {SIGPLAN Not.},
  year       = {2012},
  volume     = {47},
  number     = {9},
  pages      = {29--40},
  month      = sep,
  issn       = {0362-1340},
  acmid      = {2364535},
  address    = {New York, NY, USA},
  comment    = {12},
  doi        = {10.1145/2398856.2364535},
  issue_date = {September 2012},
  keywords   = {error-tolerant type systems, variational lambda calculus, variational type inference, variational types},
  numpages   = {12},
  publisher  = {ACM},
  url        = {http://doi.acm.org/10.1145/2398856.2364535},
}

@Article{Henard2014,
  author   = {C. Henard and M. Papadakis and G. Perrouin and J. Klein and P. Heymans and Y. Le Traon},
  title    = {Bypassing the Combinatorial Explosion: Using Similarity to Generate and Prioritize T-Wise Test Configurations for Software Product Lines},
  journal  = {IEEE Transactions on Software Engineering},
  year     = {2014},
  volume   = {40},
  number   = {7},
  pages    = {650-670},
  month    = {July},
  issn     = {0098-5589},
  abstract = {Large Software Product Lines (SPLs) are common in industry, thus introducing the need of practical solutions to test them. To this end, t-wise can help to drastically reduce the number of product configurations to test. Current t-wise approaches for SPLs are restricted to small values of t. In addition, these techniques fail at providing means to finely control the configuration process. In view of this, means for automatically generating and prioritizing product configurations for large SPLs are required. This paper proposes (a) a search-based approach capable of generating product configurations for large SPLs, forming a scalable and flexible alternative to current techniques and (b) prioritization algorithms for any set of product configurations. Both these techniques employ a similarity heuristic. The ability of the proposed techniques is assessed in an empirical study through a comparison with state of the art tools. The comparison focuses on both the product configuration generation and the prioritization aspects. The results demonstrate that existing t-wise tools and prioritization techniques fail to handle large SPLs. On the contrary, the proposed techniques are both effective and scalable. Additionally, the experiments show that the similarity heuristic can be used as a viable alternative to t-wise.},
  comment  = {21},
  doi      = {10.1109/TSE.2014.2327020},
  keywords = {combinatorial mathematics;program testing;software product lines;combinatorial explosion;test configurations;software product lines;SPL;product configurations;configuration process;search based approach;similarity heuristic;product configuration generation;Testing;Frequency modulation;Context;Scalability;Software;Linux;Arrays;Software product lines;testing;T-wise Interactions;search-based approaches;prioritization;similarity},
}

@Article{Itzik2016,
  author   = {N. Itzik and I. Reinhartz-Berger and Y. Wand},
  title    = {Variability Analysis of Requirements: Considering Behavioral Differences and Reflecting Stakeholdersâ€™ Perspectives},
  journal  = {IEEE Transactions on Software Engineering},
  year     = {2016},
  volume   = {42},
  number   = {7},
  pages    = {687-706},
  month    = {July},
  issn     = {0098-5589},
  abstract = {Adoption of Software Product Line Engineering (SPLE) to support systematic reuse of software-related artifacts within product families is challenging, time-consuming and error-prone. Analyzing the variability of existing artifacts needs to reflect different perspectives and preferences of stakeholders in order to facilitate decisions in SPLE adoption. Considering that requirements drive many development methods and activities, we introduce an approach to analyze variability of behaviors as presented in functional requirements. The approach, called semantic and ontological variability analysis (SOVA), uses ontological and semantic considerations to automatically analyze differences between initial states (preconditions), external events (triggers) that act on the system, and final states (post-conditions) of behaviors. The approach generates feature diagrams typically used in SPLE to model variability. Those diagrams are organized according to perspective profiles, reflecting the needs and preferences of the potential stakeholders for given tasks. We conducted an empirical study to examine the usefulness of the approach by comparing it to an existing tool which is mainly based on a latent semantic analysis measurement. SOVA appears to create outputs that are more comprehensible in significantly shorter times. These results demonstrate SOVA's potential to allow for flexible, behavior-oriented variability analysis.},
  comment  = {20},
  doi      = {10.1109/TSE.2015.2512599},
  keywords = {formal specification;ontologies (artificial intelligence);software product lines;behavior-oriented variability analysis;latent semantic analysis measurement;model variability;feature diagrams;SOVA;semantic-and-ontological variability analysis;SPLE adoption;software product line engineering;requirements variability analysis;Stakeholders;Software;Semantics;Feature extraction;Software product lines;Systematics;Software product line engineering;variability analysis;feature diagrams;requirements specifications;ontology},
}

@Article{Riehle2016,
  author   = {D. Riehle and M. Capraro and D. Kips and L. Horn},
  title    = {Inner Source in Platform-Based Product Engineering},
  journal  = {IEEE Transactions on Software Engineering},
  year     = {2016},
  volume   = {42},
  number   = {12},
  pages    = {1162-1177},
  month    = {Dec},
  issn     = {0098-5589},
  abstract = {Inner source is an approach to collaboration across intra-organizational boundaries for the creation of shared reusable assets. Prior project reports on inner source suggest improved code reuse and better knowledge sharing. Using a multiple-case case study research approach, we analyze the problems that three major software development organizations were facing in their product line engineering efforts. We find that a root cause, the separation of product units as profit centers from a platform organization as a cost center, leads to delayed deliveries, increased defect rates, and redundant software components. All three organizations assume that inner source can help solve these problems. The article analyzes the expectations that these companies were having towards inner source and the problems they were experiencing in its adoption. Finally, the article presents our conclusions on how these organizations should adapt their existing engineering efforts.},
  comment  = {16},
  doi      = {10.1109/TSE.2016.2554553},
  keywords = {asset management;public domain software;software product lines;inner source;platform-based product line engineering;shared reusable asset creation;software development organization;product unit separation;profit center;Collaboration;Product design;Software product lines;Best practices;Open source software;Inner source;product line engineering;product families;platform-based product engineering;open source;open collaboration;case study research},
}

@Article{Lim2015,
  author   = {S. L. Lim and P. J. Bentley and N. Kanakam and F. Ishikawa and S. Honiden},
  title    = {Investigating Country Differences in Mobile App User Behavior and Challenges for Software Engineering},
  journal  = {IEEE Transactions on Software Engineering},
  year     = {2015},
  volume   = {41},
  number   = {1},
  pages    = {40-64},
  month    = {Jan},
  issn     = {0098-5589},
  abstract = {Mobile applications (apps) are software developed for use on mobile devices and made available through app stores. App stores are highly competitive markets where developers need to cater to a large number of users spanning multiple countries. This work hypothesizes that there exist country differences in mobile app user behavior and conducts one of the largest surveys to date of app users across the world, in order to identify the precise nature of those differences. The survey investigated user adoption of the app store concept, app needs, and rationale for selecting or abandoning an app. We collected data from more than 15 countries, including USA, China, Japan, Germany, France, Brazil, United Kingdom, Italy, Russia, India, Canada, Spain, Australia, Mexico, and South Korea. Analysis of data provided by 4,824 participants showed significant differences in app user behaviors across countries, for example users from USA are more likely to download medical apps, users from the United Kingdom and Canada are more likely to be influenced by price, users from Japan and Australia are less likely to rate apps. Analysis of the results revealed new challenges to market-driven software engineering related to packaging requirements, feature space, quality expectations, app store dependency, price sensitivity, and ecosystem effect.},
  comment  = {25},
  doi      = {10.1109/TSE.2014.2360674},
  keywords = {consumer behaviour;mobile computing;smart phones;software engineering;market-driven software engineering;medical applications;data analysis;South Korea;South;Mexico;Australia;Spain;Canada;India;Russia;Italy;United Kingdom;Brazil;France;Germany;Japan;China;USA;applications stores;mobile devices;user behavior;mobile application;Mobile communication;Software;Smart phones;Software engineering;Data mining;Educational institutions;Requirements/specifications;market-driven software engineering;mobile application development;user requirements;survey research;app user behavior;software product lines;software ecosystems;Requirements/specifications;market-driven software engineering;mobile application development;user requirements;survey research;app user behavior;software product lines;software ecosystems},
}

@Article{Feigenspan2012,
  author   = {J. Feigenspan and M. Schulze and M. Papendieck and C. Kastner and R. Dachselt and V. Koppen and M. Frisch and G. Saake},
  title    = {Supporting program comprehension in large preprocessor-based software product lines},
  journal  = {IET Software},
  year     = {2012},
  volume   = {6},
  number   = {6},
  pages    = {488-501},
  month    = {Dec},
  issn     = {1751-8806},
  abstract = {Software product line (SPL) engineering provides an effective mechanism to implement variable software. However, using preprocessors to realise variability, which is typical in industry, is heavily criticised, because it often leads to obfuscated code. Using background colours to highlight code annotated with preprocessor statements to support comprehensibility has proved to be effective, however, scalability to large SPLs is questionable. The authors' aim is to implement and evaluate scalable usage of background colours for industrial-sized SPLs. They designed and implemented scalable concepts in a tool called FeatureCommander. To evaluate its effectiveness, the authors conducted a controlled experiment with a large real-world SPL with over 99'000 lines of code and 340 features. They used a within-subjects design with treatment colours and no colours. They compared correctness and response time of tasks for both treatments. For certain kinds of tasks, background colours improve program comprehension. Furthermore, the subjects generally favour background colours compared with no background colours. In addition, the subjects who worked with background colours had to use the search functions less frequently. The authors show that background colours can improve program comprehension in large SPLs. Based on these encouraging results, they continue their work on improving program comprehension in large SPLs.},
  comment  = {14},
  doi      = {10.1049/iet-sen.2011.0172},
  keywords = {reverse engineering;software engineering;program comprehension support;preprocessor-based software product lines;SPL engineering;variable software implementation;background colours;code annotation;industrial-sized SPL;FeatureCommander tool;within-subjects design;treatment colours;no colours},
}

@Article{Zhang2016,
  author   = {H. Zhang and F. Wang and Y. Zhang and J. Xu},
  title    = {STPSO: Optimal configuration for cloud environments},
  journal  = {China Communications},
  year     = {2016},
  volume   = {13},
  number   = {10},
  pages    = {198-208},
  month    = {Oct},
  issn     = {1673-5447},
  abstract = {With the increasing number of resources provided by cloud environments, identifying which types of resources should be rent when deploying an application is often a difficult and error-prone process. Currently, most cloud environments offer a wide range of configurable resources, which can be combined in many different ways. Finding an appropriate configuration under cost constraints while meeting requirements is still a challenge. In this paper, software product line engineering is introduced to describe cloud environments, and configurable resources are abstracted as features with attributes. Then, a Self-Tuning Particle Swarm Optimization approach (called STPSO) is proposed to configure the cloud environment. STPSO can automatically adjust the arbitrary configuration to a valid configuration. To evaluate the performance of the proposed approach, we conduct a series of comprehensive experiments. The empirical experiment shows that our approach reduces time and provides a reliable way to find a correct and suitable cloud configuration when dealing with a significant number of resources.},
  comment  = {11},
  doi      = {10.1109/CC.2016.7733044},
  keywords = {cloud computing;particle swarm optimisation;STPSO;cloud environments;self-tuning particle swarm optimization approach;cloud configuration;Cloud computing;Optimization;Computational modeling;Particle swarm optimization;Frequency modulation;Software product lines;cloud environment;resource configuration;software product line;particle swarm algorithm},
}

@Article{Thurimella2017,
  author   = {A. K. Thurimella and B. Bruegge and D. Janzen},
  title    = {Variability Plug-Ins for Requirements Tools: A Case-Based Theory Building Approach},
  journal  = {IEEE Systems Journal},
  year     = {2017},
  volume   = {11},
  number   = {4},
  pages    = {1935-1946},
  month    = {Dec},
  issn     = {1932-8184},
  abstract = {Product line engineering is a promising discipline for developing a family of systems based on a reusable asset base by systematically managing variability. Requirements tools that deal with variability are a key in the development of product lines. Because of the weak tool support to reuse system requirements and heterogeneity in the representations and processes, we identify a need for guidance and empirical evidence in order to extend a requirements tool with variability. We believe that existing requirement tools can be reused for product line requirements engineering (RE) by adding plug-ins. In this paper, we report a retrospective multiple case study with two diverse cases on developing and deploying variability plug-ins. Our study is based on the theory building process from social sciences and organizational theory. We show how our results can be used by practitioners to extend an RE tool with minimal implementation effort to manage variability.},
  comment  = {12},
  doi      = {10.1109/JSYST.2015.2418290},
  keywords = {formal specification;software development management;software product lines;software tools;systems analysis;requirements tool;product line engineering;reusable asset base;weak tool support;system requirements;product line requirements engineering;developing deploying variability plug-ins;theory building process;case-based theory building approach;systematic variability management;product line development;social sciences;organizational theory;Modeling;Data models;Requirements engineering;Unified modeling language;Product design;Extensibility;modeling;product lines;requirements engineering (RE);tools},
}

@Article{Berger2013,
  author   = {T. Berger and S. She and R. Lotufo and A. Wasowski and K. Czarnecki},
  title    = {A Study of Variability Models and Languages in the Systems Software Domain},
  journal  = {IEEE Transactions on Software Engineering},
  year     = {2013},
  volume   = {39},
  number   = {12},
  pages    = {1611-1640},
  month    = {Dec},
  issn     = {0098-5589},
  abstract = {Variability models represent the common and variable features of products in a product line. Since the introduction of FODA in 1990, several variability modeling languages have been proposed in academia and industry, followed by hundreds of research papers on variability models and modeling. However, little is known about the practical use of such languages. We study the constructs, semantics, usage, and associated tools of two variability modeling languages, Kconfig and CDL, which are independently developed outside academia and used in large and significant software projects. We analyze 128 variability models found in 12 open--source projects using these languages. Our study 1) supports variability modeling research with empirical data on the real-world use of its flagship concepts. However, we 2) also provide requirements for concepts and mechanisms that are not commonly considered in academic techniques, and 3) challenge assumptions about size and complexity of variability models made in academic papers. These results are of interest to researchers working on variability modeling and analysis techniques and to designers of tools, such as feature dependency checkers and interactive product configurators.},
  comment  = {30},
  doi      = {10.1109/TSE.2013.34},
  keywords = {public domain software;simulation languages;software engineering;interactive product configurators;feature dependency checkers;variability analysis techniques;open-source projects;software projects;associated language tools;language usage;language semantics;language constructs;CDL language;Kconfig language;variability modeling languages;FODA;systems software domain;variability models;Biological system modeling;Software products;Product line;Analytical models;Computational modeling;Semantics;Computer architecture;Empirical software engineering;software product lines;variability modeling;feature modeling;configuration;open source},
}

@Article{Niu2014,
  author   = {N. Niu and J. Savolainen and Z. Niu and M. Jin and J. C. Cheng},
  title    = {A Systems Approach to Product Line Requirements Reuse},
  journal  = {IEEE Systems Journal},
  year     = {2014},
  volume   = {8},
  number   = {3},
  pages    = {827-836},
  month    = {Sept},
  issn     = {1932-8184},
  abstract = {Product line engineering has become the main method for achieving systematic software reuse. Embracing requirements in a product line's asset base enhances the effectiveness of reuse as engineers can work on the abstractions closer to the domain's initial concepts. Conventional proactive approaches to product line engineering cause excessive overhead when codifying the assets. In this paper, we propose a systems-oriented approach to extracting functional requirements profiles. The validated extraction constructs are amenable to semantic case analysis and orthogonal variability modeling, so as to uncover the variation structure and constraints. To evaluate our approach, we present an experiment to quantify the extraction overhead and effectiveness and a case study to assess our approach's usefulness. The results show that our automatic support offers an order-of-magnitude saving over the manual extraction effort without significantly compromising quality and that our approach receives a positive adoption rate by systems engineers.},
  comment  = {10},
  doi      = {10.1109/JSYST.2013.2260092},
  keywords = {formal specification;software product lines;software quality;product line requirement reuse;software reuse;product line asset base;reuse effectiveness enhancement;product line engineering;system-oriented approach;functional requirements profile extraction;extraction constructs;semantic case analysis;orthogonal variability modeling;variation structure;variation constraints;extraction overhead quantification;effectiveness quantification;order-of-magnitude saving;positive adoption rate;Product line engineering;requirements engineering;reuse in systems engineering;software reuse;Product line engineering;requirements engineering;reuse in systems engineering;software reuse},
}

@Article{Moon2005,
  author   = {Mikyeong Moon and Keunhyuk Yeom and Heung Seok Chae},
  title    = {An approach to developing domain requirements as a core asset based on commonality and variability analysis in a product line},
  journal  = {IEEE Transactions on Software Engineering},
  year     = {2005},
  volume   = {31},
  number   = {7},
  pages    = {551-569},
  month    = {July},
  issn     = {0098-5589},
  abstract = {The methodologies of product line engineering emphasize proactive reuse to construct high-quality products more quickly that are less costly. Requirements engineering for software product families differs significantly from requirements engineering for single software products. The requirements for a product line are written for the group of systems as a whole, with requirements for individual systems specified by a delta or an increment to the generic set. Therefore, it is necessary to identify and explicitly denote the regions of commonality and points of variation at the requirements level. In this paper, we suggest a method of producing requirements that will be a core asset in the product line. We describe a process for developing domain requirements where commonality and variability in a domain are explicitly considered. A CASE environment, named DREAM, for managing commonality and variability analysis of domain requirements is also described. We also describe a case study for an e-travel system domain where we found that our approach to developing domain requirements based on commonality and variability analysis helped to produce domain requirements as a core asset for product lines.},
  comment  = {19},
  doi      = {10.1109/TSE.2005.76},
  keywords = {computer aided software engineering;software reusability;software quality;software architecture;formal specification;travel industry;product line engineering;software reuse;variability analysis;commonality analysis;high-quality software product;requirements engineering;software product family;CASE environment;DREAM;e-travel system domain;Design engineering;Computer architecture;Moon;Computer Society;Computer aided software engineering;Environmental management;Software systems;Control systems;Testing;Costs;Index Terms- Requirement engineering;product-line;core asset;commonality;variability;domain analysis;reuse.},
}

@Article{Apel2008,
  author   = {S. Apel and T. Leich and G. Saake},
  title    = {Aspectual Feature Modules},
  journal  = {IEEE Transactions on Software Engineering},
  year     = {2008},
  volume   = {34},
  number   = {2},
  pages    = {162-180},
  month    = {March},
  issn     = {0098-5589},
  abstract = {Two programming paradigms are gaining attention in the overlapping fields of software product lines (SPLs) and incremental software development (ISD). Feature-oriented programming (FOP) aims at large-scale compositional programming and feature modularity in SPLs using ISD. Aspect-oriented programming (AOP) focuses on the modularization of crosscutting concerns in complex software. Although feature modules, the main abstraction mechanisms of FOP, perform well in implementing large-scale software building blocks, they are incapable of modularizing certain kinds of crosscutting concerns. This weakness is exactly the strength of aspects, the main abstraction mechanisms of AOP. We contribute a systematic evaluation and comparison of FOP and AOP. It reveals that aspects and feature modules are complementary techniques. Consequently, we propose the symbiosis of FOP and AOP and aspectual feature modules (AFMs), a programming technique that integrates feature modules and aspects. We provide a set of tools that support implementing AFMs on top of Java and C++. We apply AFMs to a nontrivial case study demonstrating their practical applicability and to justify our design choices.},
  comment  = {19},
  doi      = {10.1109/TSE.2007.70770},
  keywords = {object-oriented programming;software reusability;software product line;incremental software development;feature-oriented programming;large-scale compositional programming;aspect-oriented programming;crosscutting concern;large-scale software building block;systematic evaluation;aspectual feature module;Java;C++;Programming;Large-scale systems;Collaborative software;Computer languages;Computer Society;Software performance;Symbiosis;Java;Software design;Software engineering;Design Tools and Techniques;Design;Language Constructs and Features;Design Tools and Techniques;Design;Language Constructs and Features},
}

@Article{Bu2019,
  author        = {Bu, C. and Wang, X. and Cheng, H. and Huang, M. and Li, K.},
  title         = {Routing as a service (RaaS): An open framework for customizing routing services},
  journal       = {Journal of Network and Computer Applications},
  year          = {2019},
  volume        = {125},
  pages         = {130-145},
  note          = {cited By 0},
  __markedentry = {[mac:]},
  abstract      = {With the emergence of various types of network applications, the user communication requirements for them are becoming more and more diversified and personalized. In order to accommodate the user frequently changing demands for different network applications, the Internet Service Provider (ISP) traditionally purchases and operates new dedicated network equipment, which always incurs high capital expense (CAPEX) and operating expense (OPEX) from the economic viewpoint and also burdens network management. Inspired by the ideas of Software Defined Networking (SDN) and Network Function Virtualization (NFV), we consider dealing with the above challenge by reusing virtualized network functions and selecting appropriate ones to compose the customized routing services on the routing paths for different applications. In this paper, based on SDN and NFV, we propose Routing as a Service (RaaS) as an open framework to customize the specific routing services for applications. Then, we present Routing Service Product Line (RSPL) by introducing Dynamic Software Product Line (DSPL) into the proposed RaaS, so as to rapidly customize a large number of routing services with different characteristics. In addition, according to the proposed framework, we also carry out a case study to customizing routing services with benefits of both the user and the ISP considered. Simulation results show that the proposed RaaS is feasible and efficient. Â© 2018 Elsevier Ltd},
  comment       = {16},
  document_type = {Article},
  doi           = {10.1016/j.jnca.2018.10.010},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056187673&doi=10.1016%2fj.jnca.2018.10.010&partnerID=40&md5=461981270098fdfa938721ecd3ca9471},
}

@Article{Horcas2019,
  author        = {Horcas, J.-M. and Pinto, M. and Fuentes, L.},
  title         = {Context-Aware Energy-Efficient Applications for Cyber-Physical Systems},
  journal       = {Ad Hoc Networks},
  year          = {2019},
  volume        = {82},
  pages         = {15-30},
  note          = {cited By 0},
  __markedentry = {[mac:]},
  abstract      = {Software systems have a strong impact on the energy consumption of the hardware they use. This is especially important for cyber-physical systems where power consumption strongly influences the battery life. For this reason, software developers should be more aware of the energy consumed by their systems. Moreover, software systems should be developed to adapt their behavior to minimize the energy consumed during their execution. This can be done by monitoring the usage context of the system and having runtime support to react to those changes that impact the energy footprint negatively. Although both the hardware and the software parts of cyber-physical systems can be adapted to reduce its energy consumption, this paper focuses on software adaptation. Concretely, the paper illustrates how to address the problem of developing context-aware energy-efficient applications using a Green Eco-Assistant that makes use of advanced software engineering methods, such as Dynamic Software Product Lines and Separation of Concerns. The main steps of our approach are illustrated by applying them to a cyber-physical system case study. Â© 2018 Elsevier B.V.},
  comment       = {16},
  document_type = {Article},
  doi           = {10.1016/j.adhoc.2018.08.004},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051643345&doi=10.1016%2fj.adhoc.2018.08.004&partnerID=40&md5=d7dfa3119ecae9d24de0bc4a4401a92a},
}

@Article{Lienhardt2018,
  author        = {Lienhardt, M. and Damiani, F. and Testa, L. and Turin, G.},
  title         = {On checking delta-oriented product lines of statecharts},
  journal       = {Science of Computer Programming},
  year          = {2018},
  volume        = {166},
  pages         = {3-34},
  note          = {cited By 0},
  __markedentry = {[mac:]},
  abstract      = {A Software Product Line (SPL) is a set of programs, called variants, which are generated from a common artifact base. Delta-Oriented Programming (DOP) is a flexible approach to implement SPLs. In this article, we provide a foundation for rigorous development of delta-oriented product lines of statecharts. We introduce a core language for statecharts, we define DOP on top of it, we present an analysis ensuring that a product line is well-formed (i.e., all variants can be generated and are well-formed statecharts), and we illustrate how an implementation of the analysis has been applied to an industrial case study. Â© 2018 Elsevier B.V.},
  comment       = {32},
  document_type = {Article},
  doi           = {10.1016/j.scico.2018.05.007},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047936031&doi=10.1016%2fj.scico.2018.05.007&partnerID=40&md5=71a5ae273d36841b172898f5b5f457c8},
}

@Article{Barros-Justo2018,
  author        = {Barros-Justo, J.L. and Pinciroli, F. and Matalonga, S. and MartÃ­nez-Araujo, N.},
  title         = {What software reuse benefits have been transferred to the industry? A systematic mapping study},
  journal       = {Information and Software Technology},
  year          = {2018},
  volume        = {103},
  pages         = {1-21},
  note          = {cited By 0},
  __markedentry = {[mac:]},
  abstract      = {Context: The term software reuse was first used in 1968 at the NATO conference. Since then, work in the scientific literature has stated that the application of software reuse offers benefits such as increase in quality and productivity. Nonetheless, in spite of many publications reporting software reuse experiences, evidence that such benefits having reached industrial settings is scarce. Objective: To identify and classify the benefits transferred to real-world settings by the application of software reuse strategies. Method: We conducted a systematic mapping study (SMS). Our search strategies retrieved a set of 2,413 papers out of which 49 were selected as primary studies. We defined five facets to classify these studies: (a) the type of benefit, (b) the reuse process, (c) the industry's domain, (d) the type of reuse and (e) the type of research reported. Results: Quality increase (28 papers) and Productivity increase (25 papers) were the two most mentioned benefits. Component-Based Development (CBD) was the most reported reuse strategy (41%), followed by Software Product Lines (SPL, 30%). The selected papers mentioned fourteen industrial domains, of which four stand out: aerospace and defense, telecommunications, electronics and IT services. The application of systematic reuse was reported in 78% of the papers. Regarding the research type, 50% use evaluation research as the investigation method. Finally, 13 papers (27%) reported validity threats for the research method applied. Conclusions: The literature analyzed presents a lack of empirical data, making it difficult to evaluate the effective transfer of benefits to the industry. This work did not find any relationship between the reported benefits and the reuse strategy applied by the industry or the industry domain. Although the most reported research method was industrial case studies (25 works), half of these works (12) did not report threats to validity. Â© 2018 Elsevier B.V.},
  comment       = {21},
  document_type = {Article},
  doi           = {10.1016/j.infsof.2018.06.003},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049539211&doi=10.1016%2fj.infsof.2018.06.003&partnerID=40&md5=78f2c4abb85946cc56a3b5b29e716faf},
}

@Article{Shi2018,
  author        = {Shi, K. and Yu, H. and Guo, J. and Fan, G. and Yang, X.},
  title         = {A parallel portfolio approach to configuration optimization for large software product lines},
  journal       = {Software - Practice and Experience},
  year          = {2018},
  volume        = {48},
  number        = {9},
  pages         = {1588-1606},
  note          = {cited By 0},
  __markedentry = {[mac:]},
  abstract      = {Software product line (SPL) engineering demands for optimal or near-optimal products that balance multiple often competing and conflicting objectives. A major challenge for large SPLs is to efficiently explore a huge space of various products and satisfy a large number of predefined constraints simultaneously. To improve the optimality and convergence speed, we propose a parallel portfolio approach, called IBEAPORT, which designs three algorithm variants by incorporating constraint solving into the indicator-based evolutionary algorithm in different ways and performs these variants by utilizing parallelization techniques. Our approach utilizes the exploration capabilities of different algorithms and improves optimality as far as possible within a limited time budget. We evaluate our approach on five large-scale real-world SPLs. Empirical results demonstrate that our approach significantly outperforms the state of the art for all five SPLs on a quality indicator and a diversity indicator. Moreover, IBEAPORT quickly converges to a relatively stable hypervolume value even for the largest SPL with 6888 features. Â© 2018 John Wiley & Sons, Ltd.},
  comment       = {19},
  document_type = {Article},
  doi           = {10.1002/spe.2594},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051181003&doi=10.1002%2fspe.2594&partnerID=40&md5=ee488ce5f83aa22f026d84e38547ee7b},
}

@Article{Wang2018,
  author        = {Wang, L. and Li, S. and Wei, O. and Huang, M. and Hu, J.},
  title         = {An Automated Fault Tree Generation Approach with Fault Configuration Based on Model Checking},
  journal       = {IEEE Access},
  year          = {2018},
  volume        = {6},
  pages         = {46900-46914},
  note          = {cited By 0},
  __markedentry = {[mac:]},
  abstract      = {Fault tree generation technology is a key issue for safety analysis of large complex systems. Traditional safety analysis methods usually describe the origin, propagation, or concrete behavior of the fault and do not portray the constraints between faults. However, these constraints are the system's characteristics, and a lack of expression of these constraints will make the fault model defective, thereby resulting in a fault tree that will reduce the accuracy of the safety analysis. To improve the efficiency and accuracy of safety analysis, this paper proposes a fault tree generation method that is based on fault configuration and introduces the variability management of software product lines to model system faults and perform the formal analysis. First, the fault feature diagram is defined to describe the constraint relationships between system faults, and the fault-labeled transition system is defined based on the Kripke structure to describe the system behavior. Then, based on the model semantics, the procedure for generating fault trees by model checking is established. Finally, using temporal logic to describe the system safety attributes, we adopt the model checking tool SNIP to verify the safety attributes and generate the fault tree automatically. The fault modeling method that is proposed in this paper includes the inherent constraints between faults, which makes the system fault model more realistic and accurate. A case study demonstrates the effectiveness of the proposed method. Â© 2013 IEEE.},
  art_number    = {8425974},
  comment       = {15},
  document_type = {Article},
  doi           = {10.1109/ACCESS.2018.2863696},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051402145&doi=10.1109%2fACCESS.2018.2863696&partnerID=40&md5=f477d533948f5e123c9bf96cdae679f7},
}

@Article{Krueger2018c,
  author        = {KrÃ¼ger, J. and Pinnecke, M. and Kenner, A. and Kruczek, C. and Benduhn, F. and Leich, T. and Saake, G.},
  title         = {Composing annotations without regret? Practical experiences using FeatureC},
  journal       = {Software - Practice and Experience},
  year          = {2018},
  volume        = {48},
  number        = {3},
  pages         = {402-427},
  note          = {cited By 3},
  __markedentry = {[mac:]},
  abstract      = {Software product lines enable developers to derive similar products from a common code base. Existing implementation techniques can be categorized as composition-based and annotation-based approaches, with both approaches promising complementary benefits. However, annotation-based approaches are commonly used in practice despite composition allowing physical separation of features and, thus, improving traceability and maintenance. A main hindrance to migrate annotated systems toward a composition-based product line is the challenging and time-consuming transformation task. For a company, it is difficult to predict the corresponding costs, and a successful outcome is uncertain. To overcome such problems, a solution proposed by the previous work is to use a hybrid approach, utilizing composition and annotation simultaneously. Based on this idea, we introduce a stepwise migration process from annotation-based toward composition-based approaches to lower the adoption barrier of composition. This process itself is independent of used implementation techniques and enables developers to incrementally migrate toward composition. We support our approach with detailed examples by partially migrating a real-world system. In detail, we describe the following: (1) our migration process, (2) its application on a real-world system, and (3) discuss practical challenges we face. We implemented the proposed approach and show that appropriate tool support helps to migrate toward composition-based product lines. Based on the case study, we show that the hybrid product lines work correctly and can compete with the performance of the original annotated system. However, the results also illustrate open issues that have to be solved to apply such migrations in practice. Copyright Â© 2017 John Wiley & Sons, Ltd.},
  comment       = {26},
  document_type = {Article},
  doi           = {10.1002/spe.2525},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029425150&doi=10.1002%2fspe.2525&partnerID=40&md5=3822d0535163ebd0fd64133619fb2359},
}

@Article{Brugali2018,
  author        = {Brugali, D. and Hochgeschwender, N.},
  title         = {Software product line engineering for robotic perception systems},
  journal       = {International Journal of Semantic Computing},
  year          = {2018},
  volume        = {12},
  number        = {1},
  pages         = {89-107},
  note          = {cited By 0},
  __markedentry = {[mac:]},
  abstract      = {Control systems for autonomous robots are concurrent, distributed, embedded, real-time and data intensive software systems. A real-world robot control system is composed of tens of software components. For each component providing robotic functionality, tens of different implementations may be available. The difficult challenge in robotic system engineering consists in selecting a coherent set of components, which provide the functionality required by the application requirements, taking into account their mutual dependencies. This challenge is exacerbated by the fact that robotics system integrators and application developers are usually not specifically trained in software engineering. In various application domains, software product line (SPL) development has proven to be the most effective approach to face this kind of challenges. In a previous paper [D. Brugali and N. Hochgeschwender, Managing the functional variability of robotic perception systems, in First IEEE Int. Conf. Robotic Computing, 2017, pp. 277-283.] we have presented a model-based approach to the development of SPL for robotic perception systems, which integrates two modeling technologies developed by the authors: The HyperFlex toolkit [L. Gherardi and D. Brugali, Modeling and reusing robotic software architectures: The HyperFlex toolchain, in IEEE Int. Conf. Robotics and Automation, 2014, pp. 6414-6420.] and the Robot Perception Specification Language (RPSL) [N. Hochgeschwender, S. Schneider, H. Voos and G. K. Kraetzschmar, Declarative specification of robot perception architectures, in 4th Int. Conf. Simulation, Modeling, and Programming for Autonomous Robots, 2014, pp. 291-302.]. This paper extends our previous work by illustrating the entire development process of an SPL for robot perception systems with a real case study. Â© 2018 World Scientific Publishing Company.},
  comment       = {19},
  document_type = {Conference Paper},
  doi           = {10.1142/S1793351X18400056},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051567683&doi=10.1142%2fS1793351X18400056&partnerID=40&md5=3f5695b83cad5bd31b567996e47a51b2},
}

@Article{Horcas2018a,
  author        = {Horcas, J.-M. and Pinto, M. and Fuentes, L.},
  title         = {Variability models for generating efficient configurations of functional quality attributes},
  journal       = {Information and Software Technology},
  year          = {2018},
  volume        = {95},
  pages         = {147-164},
  note          = {cited By 2},
  __markedentry = {[mac:]},
  abstract      = {Context: Quality attributes play a critical role in the architecture elicitation phase. Software Sustainability and energy efficiency is becoming a critical quality attribute that can be used as a selection criteria to choose from among different design or implementation alternatives. Energy efficiency usually competes with other non-functional requirements, like for instance, performance. Objective: This paper presents a process that helps developers to automatically generate optimum configurations of functional quality attributes in terms of energy efficiency and performance. Functional quality attributes refer to the behavioral properties that need to be incorporated inside a software architecture to fulfill a particular quality attribute (e.g., encryption and authentication for the security quality attribute, logging for the usability quality attribute). Method: Quality attributes are characterized to identify their design and implementation variants and how the different configurations influence both energy efficiency and performance. A usage model for each characterized quality attribute is defined. The variability of quality attributes, as well as the energy efficiency and performance experiment results, are represented as a constraint satisfaction problem with the goal of formally reasoning about it. Then, a configuration of the selected functional quality attributes is automatically generated, which is optimum with respect to a selected objective function. Results: Software developers can improve the energy efficiency and/or performance of their applications by using our approach to perform a richer analysis of the energy consumption and performance of different alternatives for functional quality attributes. We show quantitative values of the benefits of using our approach and discuss the threats to validity. Conclusions: The process presented in this paper will help software developers to build more energy efficient software, whilst also being aware of how their decisions affect other quality attributes, such as performance. Â© 2017 The Authors},
  comment       = {18},
  document_type = {Article},
  doi           = {10.1016/j.infsof.2017.10.018},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032894007&doi=10.1016%2fj.infsof.2017.10.018&partnerID=40&md5=20a198c05b43337b0a1f3933d5b3d88c},
}

@Article{Lopez-Herrejon2018,
  author        = {Lopez-Herrejon, R.E. and Illescas, S. and Egyed, A.},
  title         = {A systematic mapping study of information visualization for software product line engineering},
  journal       = {Journal of Software: Evolution and Process},
  year          = {2018},
  volume        = {30},
  number        = {2},
  note          = {cited By 1},
  __markedentry = {[mac:]},
  abstract      = {Software product lines (SPLs) are families of related systems whose members are distinguished by the set of features they provide. Over 2 decades of research and practice can attest to the substantial benefits of applying SPL practices such as better customization, improved software reuse, and faster time to market. Software product line engineering (SPLE) refers to the paradigm of developing SPLs. Typical SPLE efforts involve a large number of features that are combined to form also large numbers of products, implemented using multiple and different types of software artifacts. Because of the sheer amount of information and its complexity, visualization techniques have been used for different SPLE activities. In this paper, we present an extended systematic mapping study on this subject. Our research questions aim to gather information regarding the techniques that have been applied, at what SPLE activities, how they were implemented, the publication fora used, the methods of empirical evaluation, and the provenance of the evaluation examples. Our driving goal is to identify common trends, gaps, and opportunities for further research and application. Copyright Â© 2017 John Wiley & Sons, Ltd.},
  art_number    = {e1912},
  document_type = {Conference Paper},
  doi           = {10.1002/smr.1912},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033242057&doi=10.1002%2fsmr.1912&partnerID=40&md5=5524b7459098a9b4caf8751ee5786628},
}

@Article{Lanna2018,
  author        = {Lanna, A. and Castro, T. and Alves, V. and Rodrigues, G. and Schobbens, P.-Y. and Apel, S.},
  title         = {Feature-family-based reliability analysis of software product lines},
  journal       = {Information and Software Technology},
  year          = {2018},
  volume        = {94},
  pages         = {59-81},
  note          = {cited By 0},
  __markedentry = {[mac:]},
  abstract      = {Context Verification techniques are being applied to ensure that software systems achieve desired quality levels and fulfill functional and non-functional requirements. However, applying these techniques to software product lines is challenging, given the exponential blowup of the number of products. Current product-line verification techniques leverage symbolic model checking and variability information to optimize the analysis, but still face limitations that make them costly or infeasible. In particular, state-of-the-art verification techniques for product-line reliability analysis are enumerative which hinders their applicability, given the latent exponential blowup of the configuration space. Objective The objectives of this paper are the following: (a) we present a method to efficiently compute the reliability of all configurations of a compositional or annotation-based software product line from its UML behavioral models, (b) we provide a tool that implements the proposed method, and (c) we report on an empirical study comparing the performance of different reliability analysis strategies for software product lines. Method We present a novel feature-family-based analysis strategy to compute the reliability of all products of a (compositional or annotation-based) software product line. The feature-based step of our strategy divides the behavioral models into smaller units that can be analyzed more efficiently. The family-based step performs the reliability computation for all configurations at once by evaluating reliability expressions in terms of a suitable variational data structure. Results Our empirical results show that our feature-family-based strategy for reliability analysis outperforms, in terms of time and space, four state-of-the-art strategies (product-based, family-based, feature-product-based, and family-product-based) for the same property. It is the only one that could be scaled to a 220-fold increase in the size of the configuration space. Conclusion Our feature-family-based strategy leverages both feature- and family-based strategies by taming the size of the models to be analyzed and by avoiding the products enumeration inherent to some state-of-the-art analysis methods. Â© 2017 Elsevier B.V.},
  comment       = {23},
  document_type = {Article},
  doi           = {10.1016/j.infsof.2017.10.001},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031715221&doi=10.1016%2fj.infsof.2017.10.001&partnerID=40&md5=95561d47e8bbc8279031b08250c08189},
}

@Article{Giron2018,
  author        = {Giron, A.A. and Gimenes, I.M.S. and Oliveira, E., Jr.},
  title         = {Evaluation of test case generation based on a Software Product Line for model transformation},
  journal       = {Journal of Computer Science},
  year          = {2018},
  volume        = {14},
  number        = {1},
  pages         = {108-121},
  note          = {cited By 0},
  __markedentry = {[mac:]},
  abstract      = {Model-Driven Engineering (MDE) supports model evolution and refinement by means of model transformations at several abstraction levels. Validating these transformations is essential to ensure the quality and correctness of such models. However, MDE transformations become more complex to validate, for example, when they are implemented in different languages. One particular example is the transformation of the SyMPLES approach. SyMPLES is a development approach for embedded systems, which is based on concepts of both Software Product Lines (SPL) and MDE. SyMPLES has a model transformation process which creates Simulink models from SysML models. This paper presents a case study which applies test case generation based on SPL to validate this model transformation. An SPL was used to generate a set of test cases based on coverage criteria. The results showed that the test cases generated uncovered errors in the transformation of SyMPLES. In addition, a comparison with the test case generation based on metamodel is presented, in order to analyze the effectiveness of the techniques. The coverage criteria made it possible to reduce the number of test cases generated, thus minimizing test effort and time. Â© 2018 Alexandre Augusto Giron, Itana Maria de Souza Gimenes and Edson OliveiraJr.},
  comment       = {14},
  document_type = {Article},
  doi           = {10.3844/jcssp.2018.108.121},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041612613&doi=10.3844%2fjcssp.2018.108.121&partnerID=40&md5=ec87bb404092cb7ed4c40ad5e01f7d57},
}

@Article{Rincon2018,
  author        = {RincÃ³n, L. and MartÃ­nez, J.-C. and PabÃ³n, M.C. and MogollÃ³n, J. and Caballero, A.},
  title         = {Creating a software product line of mini-games to support language therapy},
  journal       = {Communications in Computer and Information Science},
  year          = {2018},
  volume        = {885},
  pages         = {418-431},
  note          = {cited By 0; Conference of 13th Colombian Conference on Computing, CCC 2018 ; Conference Date: 26 September 2018 Through 28 September 2018; Conference Code:218769},
  __markedentry = {[mac:]},
  abstract      = {During the therapy for the treatment of language disorders, a technology-based material approach, such as mini-games, could be introduced to motivate and interact with the patients, especially when they are children. However, adapting the therapy to the needs of each patient is essential to maintain the patientâ€™s progress and interest, which makes it difficult to use just any kind of mini-game. This paper describes the creation of a Software Product Line (SPL) to produce customized mini-games that children could use during their therapy sessions. Our experience shows that software product lines are feasible to provide the customization required in the therapy. Â© Springer Nature Switzerland AG 2018.},
  comment       = {14},
  document_type = {Conference Paper},
  doi           = {10.1007/978-3-319-98998-3_32},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054344870&doi=10.1007%2f978-3-319-98998-3_32&partnerID=40&md5=b5291225399bcf4464923817650d41ca},
}

@Article{Pesce2018,
  author        = {Pesce, F. and Caballero, S. and Buccella, A. and Cechich, A.},
  title         = {Reusing a geographic software product line platform: A case study in the paleontological sub-domain},
  journal       = {Communications in Computer and Information Science},
  year          = {2018},
  volume        = {790},
  pages         = {145-154},
  note          = {cited By 0; Conference of 23rd Argentine Congress of Computer Science, CACIC 2017 ; Conference Date: 9 October 2017 Through 13 October 2017; Conference Code:210229},
  __markedentry = {[mac:]},
  abstract      = {Developing Software Product Lines (SPLs) is a paradigm oriented to reusing software within particular domains. Key aspects within this paradigm are the inherent particularities of these domains and the techniques applied to systematize the ways to maximize reuse. In this article, we describe a process for creating SPLs by reusing through domain hierarchies, starting from the geographical domain and going deeper into the paleontological sub-domain. In particular, our process is based on standardizations and previous techniques already applied to another geographical sub-domain, which is marine ecology. Here we show how these techniques are applied in the paleontological sub-domain, improving the systematic reuse of software artifacts. Â© Springer International Publishing AG, part of Springer Nature 2018.},
  comment       = {10},
  document_type = {Conference Paper},
  doi           = {10.1007/978-3-319-75214-3_14},
  source        = {Scopus},
  sponsors      = {Engineering Academy of Buenos Aires; et al.; Ministry of Science, Technology and Productive Innovation (MinCyT); National University of La Plata (UNLP); Network of Universities with Careers in Computer Science (RedUNCI); Secretary of University Politics (SPU); Trust Fund for the Promotion of the Software Industry (FONSOFT)},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041843530&doi=10.1007%2f978-3-319-75214-3_14&partnerID=40&md5=24673d7c3f1e6f7248334eb6ffd27af5},
}

@Article{Barve2018,
  author        = {Barve, Y.D. and Patil, P. and Bhattacharjee, A. and Gokhale, A.},
  title         = {PADS: Design and Implementation of a Cloud-Based, Immersive Learning Environment for Distributed Systems Algorithms},
  journal       = {IEEE Transactions on Emerging Topics in Computing},
  year          = {2018},
  volume        = {6},
  number        = {1},
  pages         = {20-31},
  note          = {cited By 0},
  __markedentry = {[mac:]},
  abstract      = {As distributed systems become more complex, understanding the underlying algorithms that make these systems work becomes even harder. Traditional learning modalities based on didactic teaching and theoretical proofs alone are no longer sufficient for a holistic understanding of these algorithms. Instead, an environment that promotes an immersive, hands-on learning of distributed systems algorithms is needed to complement existing teaching modalities. Such an environment must be flexible to support the learning of a variety of algorithms. The environment should also support extensibility and reuse since many of these algorithms share several common traits with each other while differing only in some aspects. Finally, it must also allow students to experiment with large-scale deployments in a variety of operating environments. To address these concerns, we use the principles of software product lines and model-driven engineering, and adopt the cloud platform to design an immersive learning environment called the Playground of Algorithms for Distributed Systems (PADS). A prototype implementation of PADS is described to showcase use cases involving BitTorrent Peer-to-Peer file sharing, ZooKeeper-based coordination, and Paxos-based consensus, which show the benefits of rapid deployment of the distributed systems algorithms. Results from a preliminary user study are also presented. Â© 2013 IEEE.},
  comment       = {12},
  document_type = {Article},
  doi           = {10.1109/TETC.2017.2731984},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029188711&doi=10.1109%2fTETC.2017.2731984&partnerID=40&md5=cac99429f55ae0fd634190e6dc7b3ab7},
}

@Article{Soujanya2018,
  author        = {Soujanya, K.L.S.},
  title         = {Ontology based variability management for dynamic reconfiguration of software product lines},
  journal       = {Journal of Advanced Research in Dynamical and Control Systems},
  year          = {2018},
  volume        = {9},
  number        = {Special Issue 18},
  pages         = {2361-2375},
  note          = {cited By 0},
  __markedentry = {[mac:]},
  abstract      = {Software Product Line (SPL) a collection of software products that have certain common features and variable features for customization to satisfy the needs of target customers. Usually core assets and custom assets required by SPLS are built at the development time. However there are some software systems that need automatic and dynamic reconfiguration. To accommodate this, the SPL should have mechanisms to deal with it. Feature model is an important constituent of SPL. The feature model can represent similarities and variabilityâ€™s of software besides supporting quality product derivation. However, this model has drawbacks in usage for dynamic reconfiguration. Therefore it is essential to represent features using a different model for monitoring, retrieving and modifying automatically. Ontology is one such proven model that can formally represent well the features and their relationships in machine processable fashion. In the existing work, ontology was not implemented in the proposed frame work. The present work aims at implementing ontology based solution for automatic reconfiguration of SPL and product derivation. The empirical study with the improved prototype reveals that there is significant performance advantage when ontology is used for SPL configuration management. Â© 2018, Institute of Advanced Scientific Research, Inc. All rights reserved.},
  comment       = {15},
  document_type = {Article},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055963348&partnerID=40&md5=8b1a3c6bb9e6fe4cb9359f45e22219bc},
}

@Article{Maazoun2018,
  author        = {Maazoun, J. and Bouassida, N. and Ben-Abdallah, H.},
  title         = {Evaluating SPL Quality with Metrics},
  journal       = {Advances in Intelligent Systems and Computing},
  year          = {2018},
  volume        = {736},
  pages         = {42-51},
  note          = {cited By 0; Conference of 17th International Conference on Intelligent Systems Design and Applications, ISDA 2017 ; Conference Date: 14 December 2017 Through 16 December 2017; Conference Code:212209},
  __markedentry = {[mac:]},
  abstract      = {A Software Product Line (SPL) is a set of systems that share a group of manageable features and satisfy the specific needs of a particular domain. The features of an SPL can be used in variable combinations to derive product variants in the SPL domain. Because SPLs promote product development through reuse, it is vital to have a means to measure their quality in terms of quality attributes like complexity, reusability,â€¦ In this paper, we propose a set of metrics to evaluate the quality of an SPL at three levels: the feature model, design and code. We adapted a set of metrics for software quality and defined new metrics to deal with the inherent characteristics of SPLs, specifically the feature model and the traceability between features, design and code. Furthermore, to assist in interpreting the quality of a given SPL, we conducted an empirical study over ten open source SPLs to identify thresholds for the proposed metrics. Â© 2018, Springer International Publishing AG, part of Springer Nature.},
  comment       = {10},
  document_type = {Conference Paper},
  doi           = {10.1007/978-3-319-76348-4_5},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044475814&doi=10.1007%2f978-3-319-76348-4_5&partnerID=40&md5=e2e162871deb7bcb48c7baca54c9b48e},
}

@Article{Sboui2017,
  author        = {Sboui, T. and Ben Ayed, M. and Alimi, A.M.},
  title         = {A UI-DSPL Approach for the Development of Context-Adaptable User Interfaces},
  journal       = {IEEE Access},
  year          = {2017},
  volume        = {6},
  pages         = {7066-7081},
  note          = {cited By 0},
  __markedentry = {[mac:]},
  abstract      = {Unlike adaptive interfaces which use sensors to adapt themselves, adaptable interfaces need the intervention of end users to adapt their different aspects according to user requirements. These requirements are commonly expressed according to the context of use. This latter was defined by the triplet <platform, environment, user> where the platform refers to the physical device and the device software, the environment refers to the physical environment in which the application is used and the user element refers to the user preferences and user profile. In this paper, we define a dynamic software product line (DSPL) approach for the development of a family of context-adaptable user interfaces. The DSPL paradigm exploits the knowledge acquired in software product line engineering to develop systems that can be context-aware, or runtime adaptable. Our approach satisfies a set of contributions which will be validated by implementing and evaluating them according to an illustrative case study. Â© 2013 IEEE.},
  comment       = {16},
  document_type = {Article},
  doi           = {10.1109/ACCESS.2017.2782880},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038856795&doi=10.1109%2fACCESS.2017.2782880&partnerID=40&md5=8f91eea93d07df30a585882acfb2ccd6},
}

@Article{Assuncao2017,
  author        = {AssunÃ§Ã£o, W.K.G. and Lopez-Herrejon, R.E. and Linsbauer, L. and Vergilio, S.R. and Egyed, A.},
  title         = {Reengineering legacy applications into software product lines: a systematic mapping},
  journal       = {Empirical Software Engineering},
  year          = {2017},
  volume        = {22},
  number        = {6},
  pages         = {2972-3016},
  note          = {cited By 10},
  __markedentry = {[mac:]},
  abstract      = {Software Product Lines (SPLs) are families of systems that share common assets allowing a disciplined reuse. Rarely SPLs start from scratch, instead they usually start from a set of existing systems that undergo a reengineering process. Many approaches to conduct the reengineering process have been proposed and documented in research literature. This scenario is a clear testament to the interest in this research area. We conducted a systematic mapping study to provide an overview of the current research on reengineering of existing systems to SPLs, identify the community activity in regarding of venues and frequency of publications in this field, and point out trends and open issues that could serve as references for future research. This study identified 119 relevant publications. These primary sources were classified in six different dimensions related to reengineering phases, strategies applied, types of systems used in the evaluation, input artefacts, output artefacts, and tool support. The analysis of the results points out the existence of a consolidate community on this topic and a wide range of strategies to deal with different phases and tasks of the reengineering process, besides the availability of some tools. We identify some open issues and areas for future research such as the implementation of automation and tool support, the use of different sources of information, need for improvements in the feature management, the definition of ways to combine different strategies and methods, lack of sophisticated refactoring, need for new metrics and measures and more robust empirical evaluation. Reengineering of existing systems into SPLs is an active research topic with real benefits in practice. This mapping study motivates new research in this field as well as the adoption of systematic reuse in software companies. Â© 2017, Springer Science+Business Media New York.},
  comment       = {45},
  document_type = {Article},
  doi           = {10.1007/s10664-017-9499-z},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011923741&doi=10.1007%2fs10664-017-9499-z&partnerID=40&md5=cdb40d653362e54b58f0cb7eb7331cbe},
}

@Article{Pereira2017a,
  author        = {Pereira, J.A. and Maciel, L. and Noronha, T.F. and Figueiredo, E.},
  title         = {Heuristic and exact algorithms for product configuration in software product lines},
  journal       = {International Transactions in Operational Research},
  year          = {2017},
  volume        = {24},
  number        = {6},
  pages         = {1285-1306},
  note          = {cited By 4},
  __markedentry = {[mac:]},
  abstract      = {Software product line (SPL) is a set of software applications that share a common set of features satisfying the specific needs of a particular market segment. SPL engineering is a paradigm to develop software applications that commonly use a feature model to capture and document common and variable features, and their relationships. A big challenge is to derive one product among all possible products in the SPL, which satisfies the business and customer requirements. This task is known as product configuration. Although product configuration has been extensively investigated in the literature, customer's preferences are frequently neglected. In this paper, we propose a novel approach to configure a product that considers both qualitative and quantitative feature properties. We model the product configuration task as a combinatorial optimization problem, and heuristic and exact algorithms are proposed. As far as we are concerned, this proposal is the first work in the literature that considers feature properties in both leaf and nonleaf features. Computational experiments showed that the best of our heuristics found optimal solutions for all instances where those are known. For the instances where optimal solutions are not known, our heuristic outperformed the best solution obtained by a one-hour run of the exact algorithm by up to 67.89%. Â© 2017 The Authors. International Transactions in Operational Research Â© 2017 International Federation of Operational Research Societies Published by John Wiley & Sons Ltd, 9600 Garsington Road, Oxford OX4 2DQ, UK and 350 Main St, Malden, MA02148, USA.},
  comment       = {22},
  document_type = {Article},
  doi           = {10.1111/itor.12414},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019597391&doi=10.1111%2fitor.12414&partnerID=40&md5=f8e76b01c059843b03e2442ba4c00dfa},
}

@Article{Acher2017,
  author        = {Acher, M. and Lopez-Herrejon, R.E. and Rabiser, R.},
  title         = {Teaching software product lines: A snapshot of current practices and challenges},
  journal       = {ACM Transactions on Computing Education},
  year          = {2017},
  volume        = {18},
  number        = {1},
  note          = {cited By 5},
  __markedentry = {[mac:]},
  abstract      = {Software Product Line (SPL) engineering has emerged to provide the means to efficiently model, produce, and maintain multiple similar software variants, exploiting their common properties, and managing their variabilities (differences). With over two decades of existence, the community of SPL researchers and practitioners is thriving, as can be attested by the extensive research output and the numerous successful industrial projects. Education has a key role to support the next generation of practitioners to build highly complex, variability-intensive systems. Yet, it is unclear how the concepts of variability and SPLs are taught, what are the possible missing gaps and difficulties faced, what are the benefits, and what is the material available. Also, it remains unclear whether scholars teach what is actually needed by industry. In this article, we report on three initiatives we have conducted with scholars, educators, industry practitioners, and students to further understand the connection between SPLs and education, that is, an online survey on teaching SPLs we performed with 35 scholars, another survey on learning SPLs we conducted with 25 students, as well as two workshops held at the International Software Product Line Conference in 2014 and 2015 with both researchers and industry practitioners participating. We build upon the two surveys and the workshops to derive recommendations for educators to continue improving the state of practice of teaching SPLs, aimed at both individual educators as well as the wider community. Â© 2017 ACM.},
  art_number    = {2},
  document_type = {Article},
  doi           = {10.1145/3088440},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033217521&doi=10.1145%2f3088440&partnerID=40&md5=093e3203852b30f86d9374363e700dab},
}

@Article{Linsbauer2017,
  author        = {Linsbauer, L. and Lopez-Herrejon, R.E. and Egyed, A.},
  title         = {Variability extraction and modeling for product variants},
  journal       = {Software and Systems Modeling},
  year          = {2017},
  volume        = {16},
  number        = {4},
  pages         = {1179-1199},
  note          = {cited By 4},
  __markedentry = {[mac:]},
  abstract      = {Fast-changing hardware and software technologies in addition to larger and more specialized customer bases demand software tailored to meet very diverse requirements. Software development approaches that aim at capturing this diversity on a single consolidated platform often require large upfront investments, e.g., time or budget. Alternatively, companies resort to developing one variant of a software product at a time by reusing as much as possible from already-existing product variants. However, identifying and extracting the parts to reuse is an error-prone and inefficient task compounded by the typically large number of product variants. Hence, more disciplined and systematic approaches are needed to cope with the complexity of developing and maintaining sets of product variants. Such approaches require detailed information about the product variants, the features they provide and their relations. In this paper, we present an approach to extract such variability information from product variants. It identifies traces from features and feature interactions to their implementation artifacts, and computes their dependencies. This work can be useful in many scenarios ranging from ad hoc development approaches such as clone-and-own to systematic reuse approaches such as software product lines. We applied our variability extraction approach to six case studies and provide a detailed evaluation. The results show that the extracted variability information is consistent with the variability in our six case study systems given by their variability models and available product variants. Â© 2016, The Author(s).},
  comment       = {21},
  document_type = {Article},
  doi           = {10.1007/s10270-015-0512-y},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84955565242&doi=10.1007%2fs10270-015-0512-y&partnerID=40&md5=42f24866de27f621c0ed588f40db1a32},
}

@Article{Bezerra2017,
  author        = {Bezerra, C.I.M. and Andrade, R.M.C. and Monteiro, J.M.},
  title         = {Exploring quality measures for the evaluation of feature models: a case study},
  journal       = {Journal of Systems and Software},
  year          = {2017},
  volume        = {131},
  pages         = {366-385},
  note          = {cited By 4},
  __markedentry = {[mac:]},
  abstract      = {Evaluating the quality of a feature model is essential to ensure that errors in the early stages do not spread throughout the Software Product Line (SPL). One way to evaluate the feature model is to use measures that could be associated with the feature model quality characteristics and their quality attributes. In this paper, we aim at investigating how measures can be applied to the quality assessment of SPL feature models. We performed an exploratory case study using the COfFEE maintainability measures catalog and the S.P.L.O.T. feature models repository. In order to support this case study, we built a dataset (denoted by MAcchiATO) containing the values of 32 measures from COfFEE for 218 software feature models, extracted from S.P.L.O.T. This research approach allowed us to explore three different data analysis techniques. First, we applied the Spearman's rank correlation coefficient in order to identify relationships between the measures. This analysis showed that not all 32 measures in COfFEE are necessary to reveal the quality of a feature model and just 15 measures could be used. Next, the 32 measures in COfFEE were grouped by applying the Principal Component Analysis and a set of 9 new grouped measures were defined. Finally, we used the Tolerance Interval technique to define statistical thresholds for these 9 new grouped measures. So, our findings suggest that measures can be effectively used to support the quality evaluation of SPL feature models. Â© 2016 Elsevier Inc.},
  comment       = {20},
  document_type = {Article},
  doi           = {10.1016/j.jss.2016.07.040},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997701970&doi=10.1016%2fj.jss.2016.07.040&partnerID=40&md5=01aaca45d5a909e5b66a04df062a8689},
}

@Article{Oliveira2017,
  author        = {Oliveira, R.P.D. and Santos, A.R. and Almeida, E.S.D. and Gomes, G.S.D.S.},
  title         = {Evaluating Lehman's Laws of software evolution within software product lines industrial projects},
  journal       = {Journal of Systems and Software},
  year          = {2017},
  volume        = {131},
  pages         = {347-365},
  note          = {cited By 1},
  __markedentry = {[mac:]},
  abstract      = {The evolution of a single system is a task where we deal with the modification of a single product. Lehman's Laws of software evolution were broadly evaluated within this type of system and the results shown that these single systems evolve according to his stated laws over time. However, considering Software Product Lines (SPL), we need to deal with the modification of several products which include common, variable, and product specific assets. Because of the several assets within SPL, each stated law may have a different behavior for each asset kind. Nonetheless, we do not know if all of the stated laws are still valid for SPL since they were partially evaluated in this context. Thus, this paper details an empirical investigation where Lehman's Laws (LL) of Software Evolution were used in two SPL industrial projects to understand how the SPL assets evolve over time. These projects are related to an application in the medical domain and another in the financial domain, developed by medium-size companies in Brazil. They contain a total of 71 modules and a total of 71.442 bug requests in their tracking system, gathered along the total of more than 10 years. We employed two techniques - the KPSS Test and linear regression analysis, to assess the relationship between LL and SPL assets. Results showed that one law was completely supported (conservation of organizational stability) for all assets within both empirical studies. Two laws were partially supported for both studies depending on the asset type (continuous growth and conservation of familiarity). Finally, the remaining laws had differences among their results for all assets (continuous change, increasing complexity, and declining quality). Â© 2016 Elsevier Inc.},
  comment       = {19},
  document_type = {Article},
  doi           = {10.1016/j.jss.2016.07.038},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84999663630&doi=10.1016%2fj.jss.2016.07.038&partnerID=40&md5=a699a65ad2018e0c1d7b7aaaf7b96821},
}

@Article{Lucas2017,
  author        = {Lucas, E.M. and Oliveira, T.C. and Farias, K. and Alencar, P.S.C.},
  title         = {CollabRDL: A language to coordinate collaborative reuse},
  journal       = {Journal of Systems and Software},
  year          = {2017},
  volume        = {131},
  pages         = {505-527},
  note          = {cited By 4},
  __markedentry = {[mac:]},
  abstract      = {Coordinating software reuse activities is a complex problem when considering collaborative software development. This is mainly motivated due to the difficulty in specifying how the artifacts and the knowledge produced in previous projects can be applied in future ones. In addition, modern software systems are developed in group working in separate geographical locations. Therefore, techniques to enrich collaboration on software development are important to improve quality and reduce costs. Unfortunately, the current literature fails to address this problem by overlooking existing reuse techniques. There are many reuse approaches proposed in academia and industry, including Framework Instantiation, Software Product Line, Transformation Chains, and Staged Configuration. But, the current approaches do not support the representation and implementation of collaborative instantiations that involve individual and group roles, the simultaneous performance of multiple activities, restrictions related to concurrency and synchronization of activities, and allocation of activities to reuse actors as a coordination mechanism. These limitations are the main reasons why the Reuse Description Language (RDL) is unable to promote collaborative reuse, i.e., those related to reuse activities in collaborative software development. To overcome these shortcomings, this work, therefore, proposes CollabRDL, a language to coordinate collaborative reuse by providing essential concepts and constructs for allowing group-based reuse activities. For this purpose, we extend RDL by introducing three new commands, including role, parallel, and doparallel. To evaluate CollabRDL we have conducted a case study in which developer groups performed reuse activities collaboratively to instantiate a mainstream Java framework. The results indicated that CollabRDL was able to represent critical workflow patterns, including parallel split pattern, synchronization pattern, multiple-choice pattern, role-based distribution pattern, and multiple instances with decision at runtime. Overall, we believe that the provision of a new language that supports group-based activities in framework instantiation can help enable software organizations to document their coordinated efforts and achieve the benefits of software mass customization with significantly less development time and effort. Â© 2017 Elsevier Inc.},
  comment       = {23},
  document_type = {Article},
  doi           = {10.1016/j.jss.2017.01.031},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85012878938&doi=10.1016%2fj.jss.2017.01.031&partnerID=40&md5=fd8149148ac606464439485f96779a16},
}

@Article{Assuncao2017a,
  author        = {AssunÃ§Ã£o, W.K.G. and Lopez-Herrejon, R.E. and Linsbauer, L. and Vergilio, S.R. and Egyed, A.},
  title         = {Multi-objective reverse engineering of variability-safe feature models based on code dependencies of system variants},
  journal       = {Empirical Software Engineering},
  year          = {2017},
  volume        = {22},
  number        = {4},
  pages         = {1763-1794},
  note          = {cited By 5},
  __markedentry = {[mac:]},
  abstract      = {Maintenance of many variants of a software system, developed to supply a wide range of customer-specific demands, is a complex endeavour. The consolidation of such variants into a Software Product Line is a way to effectively cope with this problem. A crucial step for this consolidation is to reverse engineer feature models that represent the desired combinations of features of all the available variants. Many approaches have been proposed for this reverse engineering task but they present two shortcomings. First, they use a single-objective perspective that does not allow software engineers to consider design trade-offs. Second, they do not exploit knowledge from implementation artifacts. To address these limitations, our work takes a multi-objective perspective and uses knowledge from source code dependencies to obtain feature models that not only represent the desired feature combinations but that also check that those combinations are indeed well-formed, i.e. variability safe. We performed an evaluation of our approach with twelve case studies using NSGA-II and SPEA2, and a single-objective algorithm. Our results indicate that the performance of the multi-objective algorithms is similar in most cases and that both clearly outperform the single-objective algorithm. Our work also unveils several avenues for further research. Â© 2016, Springer Science+Business Media New York.},
  comment       = {32},
  document_type = {Article},
  doi           = {10.1007/s10664-016-9462-4},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991821880&doi=10.1007%2fs10664-016-9462-4&partnerID=40&md5=8e45fa95048b0edd14d9f3e88ad41f1a},
}

@Article{Font2017,
  author        = {Font, J. and Arcega, L. and Haugen, Ã˜. and Cetina, C.},
  title         = {Leveraging variability modeling to address metamodel revisions in Model-based Software Product Lines},
  journal       = {Computer Languages, Systems and Structures},
  year          = {2017},
  volume        = {48},
  pages         = {20-38},
  note          = {cited By 3},
  __markedentry = {[mac:]},
  abstract      = {Metamodels evolve over time, which can break the conformance between the models and the metamodel. Model migration strategies aim to co-evolve models and metamodels together, but their application is currently not fully automatizable and is thus cumbersome and error prone. We introduce the Variable MetaModel (VMM) strategy to address the evolution of the reusable model assets of a model-based Software Product Line. The VMM strategy applies variability modeling ideas to express the evolution of the metamodel in terms of commonalities and variabilities. When the metamodel evolves, changes are automatically formalized into the VMM and models that conform to previous versions of the metamodel continue to conform to the VMM, thus eliminating the need for migration. We have applied both the traditional migration strategy and the VMM strategy to a retrospective case study that includes 13 years of evolution of our industrial partner, an induction hobs manufacturer. The comparison between the two strategies shows better results for the VMM strategy in terms of model indirection, automation, and trust leak. Â© 2016 Elsevier Ltd},
  comment       = {19},
  document_type = {Article},
  doi           = {10.1016/j.cl.2016.08.003},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994193695&doi=10.1016%2fj.cl.2016.08.003&partnerID=40&md5=86f0c11fe9f791dbe01ed189f402cde1},
}

@Article{Seidl2017,
  author        = {Seidl, C. and Schuster, S. and Schaefer, I.},
  title         = {Generative software product line development using variability-aware design patterns},
  journal       = {Computer Languages, Systems and Structures},
  year          = {2017},
  volume        = {48},
  pages         = {89-111},
  note          = {cited By 2},
  __markedentry = {[mac:]},
  abstract      = {Software Product Lines (SPLs) are an approach to reuse in-the-large that models a set of closely related software systems in terms of commonalities and variabilities. Design patterns are best practices for addressing recurring design problems in object-oriented source code. In the practice of implementing SPL, instances of certain design patterns are employed to handle variability, which makes these â€œvariability-aware design patternsâ€ a best practice for SPL design. However, currently there is no dedicated method for proactively developing SPLs using design patterns suitable for realizing variable functionality. In this paper, we present a method to perform generative SPL development with design patterns. We use role models to capture design patterns and their relation to a variability model. We further allow mapping of individual design pattern roles to (parts of) implementation elements to be generated (e.g., classes, methods) and check the conformance of the realization with the specification of the pattern. We provide definitions for the variability-aware versions of the design patterns Observer, Strategy, Template Method and Composite. Furthermore, we support generation of realizations in Java, C++ and UML class diagrams utilizing annotative, compositional and transformational variability realization mechanisms. Hence, we support proactive development of SPLs using design patterns to apply best practices for the realization of variability. We realize our concepts within the Eclipse IDE and demonstrate them within a case study. Â© 2016 Elsevier Ltd},
  comment       = {23},
  document_type = {Article},
  doi           = {10.1016/j.cl.2016.08.006},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995752793&doi=10.1016%2fj.cl.2016.08.006&partnerID=40&md5=85cb107ea1b92dcce82154a31efd6979},
}

@Article{Hoda2017,
  author        = {Hoda, R. and Salleh, N. and Grundy, J. and Tee, H.M.},
  title         = {Systematic literature reviews in agile software development: A tertiary study},
  journal       = {Information and Software Technology},
  year          = {2017},
  volume        = {85},
  pages         = {1339-1351},
  note          = {cited By 17},
  __markedentry = {[mac:]},
  abstract      = {Context A number of systematic literature reviews and mapping studies (SLRs) covering numerous primary research studies on various aspects of agile software development (ASD) exist. Objective The aim of this paper is to provide an overview of the SLRs on ASD research topics for software engineering researchers and practitioners. Method We followed the tertiary study guidelines by Kitchenham et al. to find SLRs published between late 1990s to December 2015. Results We found 28 SLRs focusing on ten different ASD research areas: adoption, methods, practices, human and social aspects, CMMI, usability, global software engineering (GSE), organizational agility, embedded systems, and software product line engineering. The number of SLRs on ASD topics, similar to those on software engineering (SE) topics in general, is on the rise. A majority of the SLRs applied standardized guidelines and the quality of these SLRs on ASD topics was found to be slightly higher for journal publications than for conferences. While some individuals and institutions seem to lead this area, the spread of authors and institutions is wide. With respect to prior review recommendations, significant progress was noticed in the area of connecting agile to established domains such as usability, CMMI, and GSE; and considerable progress was observed in focusing on management-oriented approaches as Scrum and sustaining ASD in different contexts such as embedded systems. Conclusion SLRs of ASD studies are on the rise and cover a variety of ASD aspects, ranging from early adoption issues to newer applications of ASD such as in product line engineering. ASD research can benefit from further primary and secondary studies on evaluating benefits and challenges of ASD methods, agile hybrids in large-scale setups, sustainability, motivation, teamwork, and project management; as well as a fresh review of empirical studies in ASD to cover the period post 2008. Â© 2017 Elsevier B.V.},
  comment       = {13},
  document_type = {Article},
  doi           = {10.1016/j.infsof.2017.01.007},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009961943&doi=10.1016%2fj.infsof.2017.01.007&partnerID=40&md5=6a85ea7243f2d03424972d48f898f921},
}

@Article{Bashari2017,
  author        = {Bashari, M. and Bagheri, E. and Du, W.},
  title         = {Dynamic Software Product Line Engineering: A Reference Framework},
  journal       = {International Journal of Software Engineering and Knowledge Engineering},
  year          = {2017},
  volume        = {27},
  number        = {2},
  pages         = {191-234},
  note          = {cited By 9},
  __markedentry = {[mac:]},
  abstract      = {Runtime adaptive systems are able to dynamically transform their internal structure, and hence their behavior, in response to internal or external changes. Such transformations provide the basis for new functionalities or improvements of the non-functional properties that match operational requirements and standards. Software Product Line Engineering (SPLE) has introduced several models and mechanisms for variability modeling and management. Dynamic software product lines (DSPL) engineering exploits the knowledge acquired in SPLE to develop systems that can be context-aware, post-deployment reconfigurable, or runtime adaptive. This paper focuses on DSPL engineering approaches for developing runtime adaptive systems and proposes a framework for classifying and comparing these approaches from two distinct perspectives: adaptation properties and adaptation realization. These two perspectives are linked together by a series of guidelines that help to select a suitable adaptation realization approach based on desired adaptation types. Â© 2017 World Scientific Publishing Company.},
  comment       = {44},
  document_type = {Article},
  doi           = {10.1142/S0218194017500085},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016321904&doi=10.1142%2fS0218194017500085&partnerID=40&md5=9e3707cf66288cf1fe8eb164fb6575b5},
}

@Article{Bashroush2017,
  author        = {Bashroush, R. and Garba, M. and Rabiser, R. and Groher, I. and Botterweck, G.},
  title         = {CASE Tool support for variability management in software product lines},
  journal       = {ACM Computing Surveys},
  year          = {2017},
  volume        = {50},
  number        = {1},
  note          = {cited By 5},
  __markedentry = {[mac:]},
  abstract      = {Software product lines (SPL) aim at reducing time-to-market and increasing software quality through extensive, planned reuse of artifacts. An essential activity in SPL is variability management, i.e., defining and managing commonality and variability among member products. Due to the large scale and complexity of today's software-intensive systems, variability management has become increasingly complex to conduct. Accordingly, tool support for variability management has been gathering increasing momentum over the last few years and can be considered a key success factor for developing and maintaining SPLs. While several studies have already been conducted on variability management, none of these analyzed the available tool support in detail. In this work, we report on a survey in which we analyzed 37 existing variability management tools identified using a systematic literature review to understand the tools' characteristics, maturity, and the challenges in the field. We conclude that while most studies on variability management tools provide a good motivation and description of the research context and challenges, they often lack empirical data to support their claims and findings. It was also found that quality attributes important for the practical use of tools such as usability, integration, scalability, and performance were out of scope for most studies. Â© 2017 ACM.},
  art_number    = {14},
  document_type = {Article},
  doi           = {10.1145/3034827},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017139105&doi=10.1145%2f3034827&partnerID=40&md5=5825b1032c45237740832e0b7457a506},
}

@Article{Marinho2017,
  author        = {Marinho, A. and de Oliveira, D. and Ogasawara, E. and Silva, V. and OcaÃ±a, K. and Murta, L. and Braganholo, V. and Mattoso, M.},
  title         = {Deriving scientific workflows from algebraic experiment lines: A practical approach},
  journal       = {Future Generation Computer Systems},
  year          = {2017},
  volume        = {68},
  pages         = {111-127},
  note          = {cited By 2},
  __markedentry = {[mac:]},
  abstract      = {The exploratory nature of a scientific computational experiment involves executing variations of the same workflow with different approaches, programs, and parameters. However, current approaches do not systematize the derivation process from the experiment definition to the concrete workflows and do not track the experiment provenance down to the workflow executions. Therefore, the composition, execution, and analysis for the entire experiment become a complex task. To address this issue, we propose the Algebraic Experiment Line (AEL). AEL uses a data-centric workflow algebra, which enriches the experiment representation by introducing a uniform data model and its corresponding operators. This representation and the AEL provenance model map concepts from the workflow execution data to the AEL derived workflows with their corresponding experiment abstract definitions. We show how AEL has improved the understanding of a real experiment in the bioinformatics area. By combining provenance data from the experiment and its corresponding executions, AEL provenance queries navigate from experiment concepts defined at high abstraction level to derived workflows and their execution data. It also shows a direct way of querying results from different trials involving activity variations and optionalities, only present at the experiment level of abstraction. Â© 2016 Elsevier B.V.},
  comment       = {17},
  document_type = {Article},
  doi           = {10.1016/j.future.2016.08.016},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84989866090&doi=10.1016%2fj.future.2016.08.016&partnerID=40&md5=b9f956816a38755d563acf6b407b90f6},
}

@Article{Chitchyan2017,
  author        = {Chitchyan, R. and Groher, I. and Noppen, J.},
  title         = {Uncovering sustainability concerns in software product lines},
  journal       = {Journal of Software: Evolution and Process},
  year          = {2017},
  volume        = {29},
  number        = {2},
  note          = {cited By 3},
  __markedentry = {[mac:]},
  abstract      = {Sustainable living, ie, living within the bounds of the available environmental, social, and economic resources, is the focus of many present-day social and scientific discussions. But what does sustainability mean within the context of software engineering? In this paper, we undertake a comprehensive analysis of 8 case studies to address this question within the context of a specific software engineering approach, software product line engineering (SPLE). We identify the sustainability-related characteristics that arise in present-day studies that apply SPLE. We conclude that technical and economic sustainability are in prime focus on the present SPLE practice, with social sustainability issues, where they relate to organisations, also addressed to a good degree. On the other hand, the issues related to the personal sustainability are less prominent, and environmental considerations are nearly completely amiss. We present feature models and cross-relations that result from our analysis as a starting point for sustainability engineering through SPLE, suggesting that any new development should consider how these models would be instantiated and expanded for the intended sociotechnical system. The good representation of sustainability features in these models is also validated with 2 additional case studies. Copyright Â© 2017 John Wiley & Sons, Ltd.},
  art_number    = {e1853},
  document_type = {Conference Paper},
  doi           = {10.1002/smr.1853},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013469369&doi=10.1002%2fsmr.1853&partnerID=40&md5=795a2abe870c297214d65f4db6c81319},
}

@Article{Devroey2017,
  author        = {Devroey, X. and Perrouin, G. and Cordy, M. and Samih, H. and Legay, A. and Schobbens, P.-Y. and Heymans, P.},
  title         = {Statistical prioritization for software product line testing: an experience report},
  journal       = {Software and Systems Modeling},
  year          = {2017},
  volume        = {16},
  number        = {1},
  pages         = {153-171},
  note          = {cited By 1},
  __markedentry = {[mac:]},
  abstract      = {Software product lines (SPLs) are families of software systems sharing common assets and exhibiting variabilities specific to each product member of the family. Commonalities and variabilities are often represented as features organized in a feature model. Due to combinatorial explosion of the number of products induced by possible features combinations, exhaustive testing of SPLs is intractable. Therefore, sampling and prioritization techniques have been proposed to generate sorted lists of products based on coverage criteria or weights assigned to features. Solely based on the feature model, these techniques do not take into account behavioural usage of such products as a source of prioritization. In this paper, we assess the feasibility of integrating usage models into the testing process to derive statistical testing approaches for SPLs. Usage models are given as Markov chains, enabling prioritization of probable/rare behaviours. We used featured transition systems, compactly modelling variability and behaviour for SPLs, to determine which products are realizing prioritized behaviours. Statistical prioritization can achieve a significant reduction in the state space, and modelling efforts can be rewarded by better automation. In particular, we used MaTeLo, a statistical test cases generation suite developed at ALL4TEC. We assess feasibility criteria on two systems: Claroline, a configurable course management system, and Sferionâ„¢, an embedded system providing helicopter landing assistance. Â© 2015, Springer-Verlag Berlin Heidelberg.},
  comment       = {19},
  document_type = {Article},
  doi           = {10.1007/s10270-015-0479-8},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937933560&doi=10.1007%2fs10270-015-0479-8&partnerID=40&md5=05e5d292352e727f469b0cc635961e21},
}

@Article{Bonifacio2017,
  author        = {BonifÃ¡cio, R. and Borba, P. and Ferraz, C. and Accioly, P.},
  title         = {Empirical assessment of two approaches for specifying software product line use case scenarios},
  journal       = {Software and Systems Modeling},
  year          = {2017},
  volume        = {16},
  number        = {1},
  pages         = {97-123},
  note          = {cited By 0},
  __markedentry = {[mac:]},
  abstract      = {Modularity benefits, including the independent maintenance and comprehension of individual modules, have been widely advocated. However, empirical assessments to investigate those benefits have mostly focused on source code, and thus, the relevance of modularity to earlier artifacts is still not so clear (such as requirements and design models). In this paper, we use a multimethod technique, including designed experiments, to empirically evaluate the benefits of modularity in the context of two approaches for specifying product line use case scenarios: PLUSS and MSVCM. The first uses an annotative approach for specifying variability, whereas the second relies on aspect-oriented constructs for separating common and variant scenario specifications. After evaluating these approaches through the specifications of several systems, we find out that MSVCM reduces feature scattering and improves scenario cohesion. These results suggest that evolving a product line specification using MSVCM requires only localized changes. On the other hand, the results of six experiments reveal that MSVCM requires more time to derive the product line specifications and, contrasting with the modularity results, reduces the time to evolve a product line specification only when the subjects have been well trained and are used to the task of evolving product line specifications. Â© 2015, Springer-Verlag Berlin Heidelberg.},
  comment       = {27},
  document_type = {Article},
  doi           = {10.1007/s10270-015-0471-3},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929900234&doi=10.1007%2fs10270-015-0471-3&partnerID=40&md5=f2635739cdc8744117607019ed6aa88a},
}

@Article{Lochau2017,
  author        = {Lochau, M. and BÃ¼rdek, J. and HÃ¶lzle, S. and SchÃ¼rr, A.},
  title         = {Specification and automated validation of staged reconfiguration processes for dynamic software product lines},
  journal       = {Software and Systems Modeling},
  year          = {2017},
  volume        = {16},
  number        = {1},
  pages         = {125-152},
  note          = {cited By 5},
  __markedentry = {[mac:]},
  abstract      = {Dynamic software product lines (DSPLs) propose elaborated design and implementation principles for engineering highly configurable runtime-adaptive systems in a sustainable and feature-oriented way. For this, DSPLs add to classical software product lines (SPL) the notions of (1) staged (pre-)configurations with dedicated binding times for each individual feature, and (2) continuous runtime reconfigurations of dynamic features throughout the entire product life cycle. Especially in the context of safety- and mission-critical systems, the design of reliable DSPLs requires capabilities for accurately specifying and validating arbitrary complex constraints among configuration parameters and/or respective reconfiguration options. Compared to classical SPL domain analysis which is usually based on Boolean constraint solving, DSPL validation, therefore, further requires capabilities for checking temporal properties of reconfiguration processes. In this article, we present a comprehensive approach for modeling and automatically verifying essential validity properties of staged reconfiguration processes with complex binding time constraints during DSPL domain engineering. The novel modeling concepts introduced are motivated by (re-)configuration constraints apparent in a real-world industrial case study from the automation engineering domain, which are not properly expressible and analyzable using state-of-the-art SPL domain modeling approaches. We present a prototypical tool implementation based on the model checker SPIN and present evaluation results obtained from our industrial case study, demonstrating the applicability of the approach. Â© 2015, Springer-Verlag Berlin Heidelberg.},
  comment       = {28},
  document_type = {Article},
  doi           = {10.1007/s10270-015-0470-4},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929120403&doi=10.1007%2fs10270-015-0470-4&partnerID=40&md5=deb247d3adad24816cc4021911e09fe4},
}

@Article{Nasr2017,
  author        = {Nasr, S.B. and BÃ©can, G. and Acher, M. and Ferreira Filho, J.B. and Sannier, N. and Baudry, B. and Davril, J.-M.},
  title         = {Automated extraction of product comparison matrices from informal product descriptions},
  journal       = {Journal of Systems and Software},
  year          = {2017},
  volume        = {124},
  pages         = {82-103},
  note          = {cited By 9},
  __markedentry = {[mac:]},
  abstract      = {Domain analysts, product managers, or customers aim to capture the important features and differences among a set of related products. A case-by-case reviewing of each product description is a laborious and time-consuming task that fails to deliver a condense view of a family of product. In this article, we investigate the use of automated techniques for synthesizing a product comparison matrix (PCM) from a set of product descriptions written in natural language. We describe a tool-supported process, based on term recognition, information extraction, clustering, and similarities, capable of identifying and organizing features and values in a PCM â€“ despite the informality and absence of structure in the textual descriptions of products. We evaluate our proposal against numerous categories of products mined from BestBuy. Our empirical results show that the synthesized PCMs exhibit numerous quantitative, comparable information that can potentially complement or even refine technical descriptions of products. The user study shows that our automatic approach is capable of extracting a significant portion of correct features and correct values. This approach has been implemented in MatrixMiner a web environment with an interactive support for automatically synthesizing PCMs from informal product descriptions. MatrixMiner also maintains traceability with the original descriptions and the technical specifications for further refinement or maintenance by users. Â© 2016 Elsevier Inc.},
  comment       = {22},
  document_type = {Article},
  doi           = {10.1016/j.jss.2016.11.018},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84996593342&doi=10.1016%2fj.jss.2016.11.018&partnerID=40&md5=6c44aaadfbc3fb4cbc5292ddb3503d98},
}

@Article{Marcolino2017,
  author        = {Marcolino, A.S. and Oliveira, E., Jr.},
  title         = {Comparing SMarty and PLUS for variability identification and representation at product-line UML class level: A controlled quasi-experiment},
  journal       = {Journal of Computer Science},
  year          = {2017},
  volume        = {13},
  number        = {11},
  pages         = {617-632},
  note          = {cited By 0},
  __markedentry = {[mac:]},
  abstract      = {Although variability management is one of the main activities of software product lines, current literature provides almost no empirical evaluations on variability management approaches based on UML. This paper aims at experimentally comparing two approaches and picks SMarty and PLUS as representative examples. Such comparison takes into account their effectiveness of expressing correctly and incorrectly variabilities in UML class diagrams. We used a 2Ã—2 factorial design for this study. We calculated and analyzed data from participants using the T-Test. The Spearman technique supported correlation of the effectiveness of the approaches and the participants prior variability knowledge. In general, PLUS was more effective than SMarty. Generalization of results is not possible as this is an incipient evidence of PLUS and SMarty effectiveness based on graduate students and lecturers. However, counting on students and lecturers provides several contributions as we discuss in this paper. Â© 2017 Anderson S. Marcolino and Edson OliveiraJr.},
  comment       = {16},
  document_type = {Article},
  doi           = {10.3844/jcssp.2017.617.632},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037031346&doi=10.3844%2fjcssp.2017.617.632&partnerID=40&md5=bedec9dff77e55b39451bebcba04df39},
}

@Article{Usman2017,
  author        = {Usman, M. and Iqbal, M.Z. and Khan, M.U.},
  title         = {A product-line model-driven engineering approach for generating feature-based mobile applications},
  journal       = {Journal of Systems and Software},
  year          = {2017},
  volume        = {123},
  pages         = {1-32},
  note          = {cited By 3},
  __markedentry = {[mac:]},
  abstract      = {A significant challenge faced by the mobile application industry is developing and maintaining multiple native variants of mobile applications to support different mobile operating systems, devices and varying application functional requirements. The current industrial practice is to develop and maintain these variants separately. Any potential change has to be applied across variants manually, which is neither efficient nor scalable. We consider the problem of supporting multiple platforms as a â€˜software product-line engineeringâ€™ problem. The paper proposes a novel application of product-line model-driven engineering to mobile application development and addresses the key challenges of feature-based native mobile application variants for multiple platforms. Specifically, we deal with three types of variations in mobile applications: variation due to operation systems and their versions, software and hardware capabilities of mobile devices, and functionalities offered by the mobile application. We develop a tool MOPPET that automates the proposed approach. Finally, the results of applying the approach on two industrial case studies show that the proposed approach is applicable to industrial mobile applications and have potential to significantly reduce the development effort and time. Â© 2016 Elsevier Inc.},
  comment       = {32},
  document_type = {Article},
  doi           = {10.1016/j.jss.2016.09.049},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991221546&doi=10.1016%2fj.jss.2016.09.049&partnerID=40&md5=ed8bb39053d6229343132f57b665dd57},
}

@Article{Buerdek2016,
  author        = {BÃ¼rdek, J. and Kehrer, T. and Lochau, M. and Reuling, D. and Kelter, U. and SchÃ¼rr, A.},
  title         = {Reasoning about product-line evolution using complex feature model differences},
  journal       = {Automated Software Engineering},
  year          = {2016},
  volume        = {23},
  number        = {4},
  pages         = {687-733},
  note          = {cited By 14},
  __markedentry = {[mac:]},
  abstract      = {Features define common and variable parts of the members of a (software) product line. Feature models are used to specify the set of all valid feature combinations. Feature models not only enjoy an intuitive tree-like graphical syntax, but also a precise formal semantics, which can be denoted as propositional formulae over Boolean feature variables. A product line usually constitutes a long-term investment and, therefore, has to undergo continuous evolution to meet ever-changing requirements. First of all, product-line evolution leads to changes of the feature model due to its central role in the product-line paradigm. As a result, product-line engineers are often faced with the problems that (1) feature models are changed in an ad-hoc manner without proper documentation, and (2) the semantic impact of feature diagram changes is unclear. In this article, we propose a comprehensive approach to tackle both challenges. For (1), our approach compares the old and new version of the diagram representation of a feature model and specifies the changes using complex edit operations on feature diagrams. In this way, feature model changes are automatically detected and formally documented. For (2), we propose an approach for reasoning about the semantic impact of diagram changes. We present a set of edit operations on feature diagrams, where complex operations are primarily derived from evolution scenarios observed in a real-world case study, i.e., a product line from the automation engineering domain. We evaluated our approach to demonstrate its applicability with respect to the case study, as well as its scalability concerning experimental data sets. Â© 2015, Springer Science+Business Media New York.},
  comment       = {47},
  document_type = {Article},
  doi           = {10.1007/s10515-015-0185-3},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944615031&doi=10.1007%2fs10515-015-0185-3&partnerID=40&md5=dedba32d55a76719cbfda76e77866829},
}

@Article{Xue2016,
  author        = {Xue, Y. and Zhong, J. and Tan, T.H. and Liu, Y. and Cai, W. and Chen, M. and Sun, J.},
  title         = {IBED: Combining IBEA and DE for optimal feature selection in software product line engineering},
  journal       = {Applied Soft Computing Journal},
  year          = {2016},
  volume        = {49},
  pages         = {1215-1231},
  note          = {cited By 9},
  __markedentry = {[mac:]},
  abstract      = {Software configuration, which aims to customize the software for different users (e.g., Linux kernel configuration), is an important and complicated task. In software product line engineering (SPLE), feature oriented domain analysis is adopted and feature model is used to guide the configuration of new product variants. In SPLE, product configuration is an optimal feature selection problem, which needs to find a set of features that have no conflicts and meanwhile achieve multiple design objectives (e.g., minimizing cost and maximizing the number of features). In previous studies, several multi-objective evolutionary algorithms (MOEAs) were used for the optimal feature selection problem and indicator-based evolutionary algorithm (IBEA) was proven to be the best MOEA for this problem. However, IBEA still suffers from the issues of correctness and diversity of found solutions. In this paper, we propose a dual-population evolutionary algorithm, named IBED, to achieve both correctness and diversity of solutions. In IBED, two populations are individually evolved with two different types of evolutionary operators, i.e., IBEA operators and differential evolution (DE) operators. Furthermore, we propose two enhancement techniques for existing MOEAs, namely the feedback-directed mechanism to fast find the correct solutions (e.g., solutions that satisfy the feature model constraints) and the preprocessing method to reduce the search space. Our empirical results have shown that IBED with the enhancement techniques can outperform several state-of-the-art MOEAs on most case studies in terms of correctness and diversity of found solutions. Â© 2016 Elsevier B.V.},
  comment       = {17},
  document_type = {Article},
  doi           = {10.1016/j.asoc.2016.07.040},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997496779&doi=10.1016%2fj.asoc.2016.07.040&partnerID=40&md5=3b3bec8813e78da624b567c95244b62e},
}

@Article{Grbac2016,
  author        = {Grbac, T.G. and Runeson, P. and HuljeniÄ‡, D.},
  title         = {A quantitative analysis of the unit verification perspective on fault distributions in complex software systems: an operational replication},
  journal       = {Software Quality Journal},
  year          = {2016},
  volume        = {24},
  number        = {4},
  pages         = {967-995},
  note          = {cited By 3},
  __markedentry = {[mac:]},
  abstract      = {Unit verification, including software inspections and unit tests, is usually the first code verification phase in the software development process. However, principles of unit verification are weakly explored, mostly due to the lack of data, since unit verification data are rarely systematically collected and only a few studies have been published with such data from industry. Therefore, we explore the theory of fault distributions, originating in the quantitative analysis by Fenton and Ohlsson, in the weakly explored context of unit verification in large-scale software development. We conduct a quantitative case study on a sequence of four development projects on consecutive releases of the same complex software product line system for telecommunication exchanges. We replicate the operationalization from earlier studies, analyzed hypotheses related to the Pareto principle of fault distribution, persistence of faults, effects of module size, and quality in terms of fault densities, however, now from the perspective of unit verification. The patterns in unit verification results resemble those of later verification phases, e.g., regarding the Pareto principle, and may thus be used for prediction and planning purposes. Using unit verification results as predictors may improve the quality and efficiency of software verification. Â© 2015, Springer Science+Business Media New York.},
  comment       = {29},
  document_type = {Article},
  doi           = {10.1007/s11219-015-9273-7},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84925610743&doi=10.1007%2fs11219-015-9273-7&partnerID=40&md5=06a3c6fc1f779d20074176c121d7df1a},
}

@Article{Bakar2016,
  author        = {Bakar, N.H. and Kasirun, Z.M. and Salleh, N. and Jalab, H.A.},
  title         = {Extracting features from online software reviews to aid requirements reuse},
  journal       = {Applied Soft Computing Journal},
  year          = {2016},
  volume        = {49},
  pages         = {1297-1315},
  note          = {cited By 6},
  __markedentry = {[mac:]},
  abstract      = {Sets of common features are essential assets to be reused in fulfilling specific needs in software product line methodology. In Requirements Reuse (RR), the extraction of software features from Software Requirement Specifications (SRS) is viable only to practitioners who have access to these software artefacts. Due to organisational privacy, SRS are always kept confidential and not easily available to the public. As alternatives, researchers opted to use the publicly available software descriptions such as product brochures and online software descriptions to identify potential software features to initiate the RR process. The aim of this paper is to propose a semi-automated approach, known as Feature Extraction for Reuse of Natural Language requirements (FENL), to extract phrases that can represent software features from software reviews in the absence of SRS as a way to initiate the RR process. FENL is composed of four stages, which depend on keyword occurrences from several combinations of nouns, verbs, and/or adjectives. In the experiment conducted, phrases that could reflect software features, which reside within online software reviews were extracted by utilising the techniques from information retrieval (IR) area. As a way to demonstrate the feature groupings phase, a semi-automated approach to group the extracted features were then conducted with the assistance of a modified word overlap algorithm. As for the evaluation, the proposed extraction approach is evaluated through experiments against the truth data set created manually. The performance results obtained from the feature extraction phase indicates that the proposed approach performed comparably with related works in terms of recall, precision, and F-Measure. Â© 2016 Elsevier B.V.},
  comment       = {19},
  document_type = {Article},
  doi           = {10.1016/j.asoc.2016.07.048},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997124422&doi=10.1016%2fj.asoc.2016.07.048&partnerID=40&md5=e7c61585c2eae8eaf3f9c8cca858e5a2},
}

@Article{Zhang2016a,
  author        = {Zhang, H. and Wang, F. and Zhang, Y. and Xu, J.},
  title         = {STPSO: Optimal configuration for cloud environments},
  journal       = {China Communications},
  year          = {2016},
  volume        = {13},
  number        = {10},
  pages         = {198-208},
  note          = {cited By 0},
  __markedentry = {[mac:]},
  abstract      = {With the increasing number of resources provided by cloud environments, identifying which types of resources should be rent when deploying an application is often a difficult and error-prone process. Currently, most cloud environments offer a wide range of configurable resources, which can be combined in many different ways. Finding an appropriate configuration under cost constraints while meeting requirements is still a challenge. In this paper, software product line engineering is introduced to describe cloud environments, and configurable resources are abstracted as features with attributes. Then, a Self-Tuning Particle Swarm Optimization approach (called STPSO) is proposed to configure the cloud environment. STPSO can automatically adjust the arbitrary configuration to a valid configuration. To evaluate the performance of the proposed approach, we conduct a series of comprehensive experiments. The empirical experiment shows that our approach reduces time and provides a reliable way to find a correct and suitable cloud configuration when dealing with a significant number of resources. Â© 2013 IEEE.},
  art_number    = {7733044},
  comment       = {11},
  document_type = {Article},
  doi           = {10.1109/CC.2016.7733044},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995662372&doi=10.1109%2fCC.2016.7733044&partnerID=40&md5=dbae8d6835b5ccf5da51d7cf422b3d6a},
}

@Article{Tanhaei2016,
  author        = {Tanhaei, M. and Habibi, J. and Mirian-Hosseinabadi, S.-H.},
  title         = {A Feature Model Based Framework for Refactoring Software Product Line Architecture},
  journal       = {Journal of Computer Science and Technology},
  year          = {2016},
  volume        = {31},
  number        = {5},
  pages         = {951-986},
  note          = {cited By 4},
  __markedentry = {[mac:]},
  abstract      = {Software product line (SPL) is an approach used to develop a range of software products with a high degree of similarity. In this approach, a feature model is usually used to keep track of similarities and differences. Over time, as modifications are made to the SPL, inconsistencies with the feature model could arise. The first approach to dealing with these inconsistencies is refactoring. Refactoring consists of small steps which, when accumulated, may lead to large-scale changes in the SPL, resulting in features being added to or eliminated from the SPL. In this paper, we propose a framework for refactoring SPLs, which helps keep SPLs consistent with the feature model. After some introductory remarks, we describe a formal model for representing the feature model. We express various refactoring patterns applicable to the feature model and the SPL formally, and then introduce an algorithm for finding them in the SPL. In the end, we use a real-world case study of an SPL to illustrate the applicability of the framework introduced in the paper. Â© 2016, Springer Science+Business Media New York.},
  comment       = {36},
  document_type = {Article},
  doi           = {10.1007/s11390-016-1674-y},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84984908557&doi=10.1007%2fs11390-016-1674-y&partnerID=40&md5=bb87ce0a2218e447cdac76a5b15fa444},
}

@Article{Myllaerniemi2016a,
  author        = {MyllÃ¤rniemi, V. and Savolainen, J. and Raatikainen, M. and MÃ¤nnistÃ¶, T.},
  title         = {Performance variability in software product lines: proposing theories from a case study},
  journal       = {Empirical Software Engineering},
  year          = {2016},
  volume        = {21},
  number        = {4},
  pages         = {1623-1669},
  note          = {cited By 4},
  __markedentry = {[mac:]},
  abstract      = {In the software product line research, product variants typically differ by their functionality and quality attributes are not purposefully varied. The goal is to study purposeful performance variability in software product lines, in particular, the motivation to vary performance, and the strategy for realizing performance variability in the product line architecture. The research method was a theory-building case study that was augmented with a systematic literature review. The case was a mobile network base station product line with capacity variability. The data collection, analysis and theorizing were conducted in several stages: the initial case study results were augmented with accounts from the literature. We constructed three theoretical models to explain and characterize performance variability in software product lines: the models aim to be generalizable beyond the single case. The results describe capacity variability in a base station product line. Thereafter, theoretical models of performance variability in software product lines in general are proposed. Performance variability is motivated by customer needs and characteristics, by trade-offs and by varying operating environment constraints. Performance variability can be realized by hardware or software means; moreover, the software can either realize performance differences in an emergent way through impacts from other variability or by utilizing purposeful varying design tactics. The results point out two differences compared with the prevailing literature. Firstly, when the customer needs and characteristics enable price differentiation, performance may be varied even with no trade-offs or production cost differences involved. Secondly, due to the dominance of feature modeling, the literature focuses on the impact management realization. However, performance variability can be realized through purposeful design tactics to downgrade the available software resources and by having more efficient hardware. Â© 2015, Springer Science+Business Media New York.},
  comment       = {47},
  document_type = {Article},
  doi           = {10.1007/s10664-014-9359-z},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923360642&doi=10.1007%2fs10664-014-9359-z&partnerID=40&md5=f7cd2d20470c075b0a65d80d8de67744},
}

@Article{Sobernig2016,
  author        = {Sobernig, S. and Apel, S. and Kolesnikov, S. and Siegmund, N.},
  title         = {Quantifying structural attributes of system decompositions in 28 feature-oriented software product lines: An exploratory study},
  journal       = {Empirical Software Engineering},
  year          = {2016},
  volume        = {21},
  number        = {4},
  pages         = {1670-1705},
  note          = {cited By 1},
  __markedentry = {[mac:]},
  abstract      = {A key idea of feature orientation is to decompose a software product line along the features it provides. Feature decomposition is orthogonal to object-oriented decompositionâ€”it crosscuts the underlying package and class structure. It has been argued often that feature decomposition improves system structure by reducing coupling and by increasing cohesion. However, recent empirical findings suggest that this is not necessarily the case. In this exploratory, observational study, we investigate the decompositions of 28 feature-oriented software product lines into classes, features, and feature-specific class fragments. The product lines under investigation are implemented using the feature-oriented programming language Fuji. In particular, we quantify and compare the internal attributes import coupling and cohesion of the different product-line decompositions in a systematic, reproducible manner. For this purpose, we adopt three established software measures (e.g., coupling between units, CBU; internal-ratio unit dependency, IUD) as well as standard concentration statistics (e.g., Gini coefficient). In our study, we found that feature decomposition can be associated with higher levels of structural coupling in a product line than a decomposition into classes. Although coupling can be concentrated in very few features in most feature decompositions, there are not necessarily hot-spot features in all product lines. Interestingly, feature cohesion is not necessarily higher than class cohesion, whereas features are more equal in serving dependencies internally than classes of a product line. Our empirical study raises critical questions about alleged advantages of feature decomposition. At the same time, we demonstrate how our measurement approach of coupling and cohesion has potential to support static and dynamic analyses of software product lines (i.e., type checking and feature-interaction detection) by facilitating product sampling. Â© 2014, Springer Science+Business Media New York.},
  comment       = {36},
  document_type = {Article},
  doi           = {10.1007/s10664-014-9336-6},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908355432&doi=10.1007%2fs10664-014-9336-6&partnerID=40&md5=cc1eb006ed351a5be2ec4619fb3e256f},
}

@Article{Becan2016,
  author        = {BÃ©can, G. and Acher, M. and Baudry, B. and Nasr, S.B.},
  title         = {Breathing ontological knowledge into feature model synthesis: an empirical study},
  journal       = {Empirical Software Engineering},
  year          = {2016},
  volume        = {21},
  number        = {4},
  pages         = {1794-1841},
  note          = {cited By 9},
  __markedentry = {[mac:]},
  abstract      = {Feature Models (FMs) are a popular formalism for modeling and reasoning about the configurations of a software product line. As the manual construction of an FM is time-consuming and error-prone, management operations have been developed for reverse engineering, merging, slicing, or refactoring FMs from a set of configurations/dependencies. Yet the synthesis of meaningless ontological relations in the FM â€“ as defined by its feature hierarchy and feature groups â€“ may arise and cause severe difficulties when reading, maintaining or exploiting it. Numerous synthesis techniques and tools have been proposed, but only a few consider both configuration and ontological semantics of an FM. There are also few empirical studies investigating ontological aspects when synthesizing FMs. In this article, we define a generic, ontologic-aware synthesis procedure that computes the likely siblings or parent candidates for a given feature. We develop six heuristics for clustering and weighting the logical, syntactical and semantical relationships between feature names. We then perform an empirical evaluation on hundreds of FMs, coming from the SPLOT repository and Wikipedia. We provide evidence that a fully automated synthesis (i.e., without any user intervention) is likely to produce FMs far from the ground truths. As the role of the user is crucial, we empirically analyze the strengths and weaknesses of heuristics for computing ranking lists and different kinds of clusters. We show that a hybrid approach mixing logical and ontological techniques outperforms state-of-the-art solutions. We believe our approach, environment, and empirical results support researchers and practitioners working on reverse engineering and management of FMs. Â© 2015, Springer Science+Business Media New York.},
  comment       = {48},
  document_type = {Article},
  doi           = {10.1007/s10664-014-9357-1},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924266514&doi=10.1007%2fs10664-014-9357-1&partnerID=40&md5=a8bb299ac1e292ee13e93d3b3aaf87ad},
}

@Article{Asadi2016,
  author        = {Asadi, M. and Soltani, S. and GaÅ¡eviÄ‡, D. and Hatala, M.},
  title         = {The effects of visualization and interaction techniques on feature model configuration},
  journal       = {Empirical Software Engineering},
  year          = {2016},
  volume        = {21},
  number        = {4},
  pages         = {1706-1743},
  note          = {cited By 2},
  __markedentry = {[mac:]},
  abstract      = {A Software Product Line is a set of software systems of a domain, which share some common features but also have significant variability. A feature model is a variability modeling artifact which represents differences among software products with respect to variability relationships among their features. Having a feature model along with a reference model developed in the domain engineering lifecycle, a concrete product of the family is derived by selecting features in the feature model (referred to as the configuration process) and by instantiating the reference model. However, feature model configuration can be a cumbersome task because: 1) feature models may consist of a large number of features, which are hard to comprehend and maintain; and 2) many factors including technical limitations, implementation costs, stakeholdersâ€™ requirements and expectations must be considered in the configuration process. Recognizing these issues, a significant amount of research efforts has been dedicated to different aspects of feature model configuration such as automating the configuration process. Several approaches have been proposed to alleviate the feature model configuration challenges through applying visualization and interaction techniques. However, there have been limited empirical insights available into the impact of visualization and interaction techniques on the feature model configuration process. In this paper, we present a set of visualization and interaction interventions for representing and configuring feature models, which are then empirically validated to measure the impact of the proposed interventions. An empirical study was conducted by following the principles of control experiments in software engineering and by applying the well-known software quality standard ISO 9126 to operationalize the variables investigated in the experiment. The results of the empirical study revealed that the employed visualization and interaction interventions significantly improved completion time of comprehension and changing of the feature model configuration. Additionally, according to results, the proposed interventions are easy-to-use and easy-to-learn for the participants. Â© 2015, Springer Science+Business Media New York.},
  comment       = {38},
  document_type = {Article},
  doi           = {10.1007/s10664-014-9353-5},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84925275030&doi=10.1007%2fs10664-014-9353-5&partnerID=40&md5=985f277899e4fd1ea4af6e2b70d33f32},
}

@Article{Itzik2016a,
  author        = {Itzik, N. and Reinhartz-Berger, I. and Wand, Y.},
  title         = {Variability Analysis of Requirements: Considering Behavioral Differences and Reflecting Stakeholders' Perspectives},
  journal       = {IEEE Transactions on Software Engineering},
  year          = {2016},
  volume        = {42},
  number        = {7},
  pages         = {687-706},
  note          = {cited By 15},
  __markedentry = {[mac:]},
  abstract      = {Adoption of Software Product Line Engineering (SPLE) to support systematic reuse of software-related artifacts within product families is challenging, time-consuming and error-prone. Analyzing the variability of existing artifacts needs to reflect different perspectives and preferences of stakeholders in order to facilitate decisions in SPLE adoption. Considering that requirements drive many development methods and activities, we introduce an approach to analyze variability of behaviors as presented in functional requirements. The approach, called semantic and ontological variability analysis (SOVA), uses ontological and semantic considerations to automatically analyze differences between initial states (preconditions), external events (triggers) that act on the system, and final states (post-conditions) of behaviors. The approach generates feature diagrams typically used in SPLE to model variability. Those diagrams are organized according to perspective profiles, reflecting the needs and preferences of the potential stakeholders for given tasks. We conducted an empirical study to examine the usefulness of the approach by comparing it to an existing tool which is mainly based on a latent semantic analysis measurement. SOVA appears to create outputs that are more comprehensible in significantly shorter times. These results demonstrate SOVA's potential to allow for flexible, behavior-oriented variability analysis. Â© 2016 IEEE.},
  art_number    = {7366597},
  comment       = {20},
  document_type = {Article},
  doi           = {10.1109/TSE.2015.2512599},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979523673&doi=10.1109%2fTSE.2015.2512599&partnerID=40&md5=ee5f562b77b3b04d1927f702abbf9ebb},
}

@Article{Mariani2016,
  author        = {Mariani, T. and Elita Colanzi, T. and Regina Vergilio, S.},
  title         = {Preserving architectural styles in the search based design of software product line architectures},
  journal       = {Journal of Systems and Software},
  year          = {2016},
  volume        = {115},
  pages         = {157-173},
  note          = {cited By 1},
  __markedentry = {[mac:]},
  abstract      = {Architectural styles help to improve the Product Line Architecture (PLA) design by providing a better organization of its elements, which results in some benefits, like flexibility, extensibility and maintainability. The PLA design can also be improved by using a search based optimization approach, taking into account different metrics, such as cohesion, coupling and feature modularization. However, the application of search operators changes the PLA organization, and consequently may violate the architectural styles rules, impacting negatively in the architecture understanding. To overcome such limitation, this work introduces a set of search operators to be used in the search based design with the goal of preserving the architectural styles during the optimization process. Such operators consider rules of the layered and client/server architectural styles, generally used in the search based design of conventional architectures and PLAs. The operators are implemented and evaluated in the context of MOA4PLA, a Multi-objective Optimization Approach for PLA Design. Results from an empirical evaluation show that the proposed operators contribute to obtain better solutions, preserving the adopted style and also improving some software metric values. Â© 2016 Elsevier Inc. All rights reserved.},
  comment       = {17},
  document_type = {Article},
  doi           = {10.1016/j.jss.2016.01.039},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959319935&doi=10.1016%2fj.jss.2016.01.039&partnerID=40&md5=26a38167189dc84b18847bdde4d16b12},
}

@Article{Farahani2016,
  author        = {Farahani, E.D. and Habibi, J.},
  title         = {Configuration Management Model in Evolutionary Software Product Line},
  journal       = {International Journal of Software Engineering and Knowledge Engineering},
  year          = {2016},
  volume        = {26},
  number        = {3},
  pages         = {433-455},
  note          = {cited By 0},
  __markedentry = {[mac:]},
  abstract      = {In Software Product Line (SPL), Configuration Management (CM) is a multi-dimensional problem. On the one hand, the Core Assets that constitute a configuration need to be managed, and on the other hand, each product in the product line that is built using a configuration must be managed, and furthermore, the management of all these configurations must be coordinated under a single process. Therefore, CM for product lines is more complex than for single systems. The CM of any software system involves four closely related activities: Change Management (ChM), Version Management (VM), System Building (SB) and Release Management (RM) [I. Sommerville, Software Engineering, 9th edn. (Addison-Wesley, 2010)]. The aim of this paper is to provide ChM and VM models for evolutionary-based SPL system development and maintenance. The proposed models support any level of aggregation in SPLs and have been applied to Mobile SPL as a case study. Â© 2016 World Scientific Publishing Company.},
  comment       = {23},
  document_type = {Article},
  doi           = {10.1142/S0218194016500182},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84968548424&doi=10.1142%2fS0218194016500182&partnerID=40&md5=04b82211594b36b88b4f29690caba20f},
}

@Article{Koziolek2016,
  author        = {Koziolek, H. and Goldschmidt, T. and de Gooijer, T. and Domis, D. and Sehestedt, S. and Gamer, T. and Aleksy, M.},
  title         = {Assessing software product line potential: an exploratory industrial case study},
  journal       = {Empirical Software Engineering},
  year          = {2016},
  volume        = {21},
  number        = {2},
  pages         = {411-448},
  note          = {cited By 4},
  __markedentry = {[mac:]},
  abstract      = {Corporate organizations sometimes offer similar software products in certain domains due to former company mergers or due to the complexity of the organization. The functional overlap of such products is an opportunity for future systematic reuse to reduce software development and maintenance costs. Therefore, we have tailored existing domain analysis methods to our organization to identify commonalities and variabilities among such products and to assess the potential for software product line (SPL) approaches. As an exploratory case study, we report on our experiences and lessons learned from conducting the domain analysis in four application cases with large-scale software products. We learned that the outcome of a domain analysis was often a smaller integration scenario instead of an SPL and that business case calculations were less relevant for the stakeholders and managers from the business units during this phase. We also learned that architecture reconstruction using a simple block diagram notation aids domain analysis and that large parts of our approach were reusable across application cases. Â© 2015, Springer Science+Business Media New York.},
  comment       = {38},
  document_type = {Article},
  doi           = {10.1007/s10664-014-9358-0},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84922424792&doi=10.1007%2fs10664-014-9358-0&partnerID=40&md5=d52527a44c67d5bcbcaab488aa82cc99},
}

@Article{Hunsen2016,
  author        = {Hunsen, C. and Zhang, B. and Siegmund, J. and KÃ¤stner, C. and LeÃŸenich, O. and Becker, M. and Apel, S.},
  title         = {Preprocessor-based variability in open-source and industrial software systems: An empirical study},
  journal       = {Empirical Software Engineering},
  year          = {2016},
  volume        = {21},
  number        = {2},
  pages         = {449-482},
  note          = {cited By 14},
  __markedentry = {[mac:]},
  abstract      = {Almost every sufficiently complex software system today is configurable. Conditional compilation is a simple variability-implementation mechanism that is widely used in open-source projects and industry. Especially, the C preprocessor (CPP) is very popular in practice, but it is also gaining (again) interest in academia. Although there have been several attempts to understand and improve CPP, there is a lack of understanding of how it is used in open-source and industrial systems and whether different usage patterns have emerged. The background is that much research on configurable systems and product lines concentrates on open-source systems, simply because they are available for study in the first place. This leads to the potentially problematic situation that it is unclear whether the results obtained from these studies are transferable to industrial systems. We aim at lowering this gap by comparing the use of CPP in open-source projects and industryâ€”especially from the embedded-systems domainâ€”based on a substantial set of subject systems and well-known variability metrics, including size, scattering, and tangling metrics. A key result of our empirical study is that, regarding almost all aspects we studied, the analyzed open-source systems and the considered embedded systems from industry are similar regarding most metrics, including systems that have been developed in industry and made open source at some point. So, our study indicates that, regarding CPP as variability-implementation mechanism, insights, methods, and tools developed based on studies of open-source systems are transferable to industrial systemsâ€”at least, with respect to the metrics we considered. Â© 2015, Springer Science+Business Media New York.},
  comment       = {34},
  document_type = {Article},
  doi           = {10.1007/s10664-015-9360-1},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84927927173&doi=10.1007%2fs10664-015-9360-1&partnerID=40&md5=ccef9e8933e9d1d78ec680cab9cf1b1c},
}

@Article{Saeed2016,
  author        = {Saeed, M. and Saleh, F. and Al-Insaif, S. and El-Attar, M.},
  title         = {Empirical validating the cognitive effectiveness of a new feature diagrams visual syntax},
  journal       = {Information and Software Technology},
  year          = {2016},
  volume        = {71},
  pages         = {1-26},
  note          = {cited By 3},
  __markedentry = {[mac:]},
  abstract      = {Context Feature models are commonly used to capture and communicate the commonality and variability of features in a Software Product Line. The core component of Feature models is feature diagrams, which graphically depict features in a hierarchical form. In previous work we have proposed a new notation that aims to improve the cognitive effectiveness of feature diagrams. Objective The objective of this paper is to empirically validate the cognitive effectiveness of the new feature diagrams notation in comparison to its original form. Methods We use two distinct empirical user-studies to validate the new notation. The first empirical study uses the survey approach while the second study is a subject-based experiment. The survey study investigates the semantic transparency of the new notation while the second study investigates the speed and accuracy of reading the notation. Results The results of the studies indicate that the proposed changes have significantly improved its cognitive effectiveness. Conclusions The cognitive effectiveness of feature diagrams has been improved, however there remains further research for full acceptance of the new notation by its potential user community.},
  comment       = {26},
  document_type = {Article},
  doi           = {10.1016/j.infsof.2015.10.012},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84952767067&doi=10.1016%2fj.infsof.2015.10.012&partnerID=40&md5=b8c024993f085b6cbd539a0319569487},
}

@Article{Asadi2016a,
  author        = {Asadi, M. and GrÃ¶ner, G. and Mohabbati, B. and GaÅ¡eviÄ‡, D.},
  title         = {Goal-oriented modeling and verification of feature-oriented product lines},
  journal       = {Software and Systems Modeling},
  year          = {2016},
  volume        = {15},
  number        = {1},
  pages         = {257-279},
  note          = {cited By 6},
  __markedentry = {[mac:]},
  abstract      = {Goal models represent requirements and intentions of a software system. They play an important role in the development life cycle of software product lines (SPLs). In the domain engineering phase, goal models guide the development of variability in SPLs by providing the rationale for the variability, while they are used for the configuration of SPLs in the application engineering phase. However, variability in SPLs, which is represented by feature models, usually has design and implementation-induced constraints. When those constraints are not aligned with variability in goal models, the configuration with goal models becomes error prone. To remedy this problem, we propose a description logic (DL)-based approach to represent both models and their relations in a common DL knowledge base. Moreover, we apply reasoning to detect inconsistencies in the variability of goal and feature models. A formal proof is provided to demonstrate the correctness of the reasoning approach. An empirical evaluation shows computational tractability of the inconsistency detection. Â© 2014, Springer-Verlag Berlin Heidelberg.},
  comment       = {23},
  document_type = {Article},
  doi           = {10.1007/s10270-014-0402-8},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84956612126&doi=10.1007%2fs10270-014-0402-8&partnerID=40&md5=f9e90df907b717cae8579d320572f8ae},
}

@Article{Sepulveda2016,
  author        = {SepÃºlveda, S. and Cravero, A. and Cachero, C.},
  title         = {Requirements modeling languages for software product lines: A systematic literature review},
  journal       = {Information and Software Technology},
  year          = {2016},
  volume        = {69},
  pages         = {16-36},
  note          = {cited By 10},
  __markedentry = {[mac:]},
  abstract      = {Context: Software product lines (SPLs) have reached a considerable level of adoption in the software industry, having demonstrated their cost-effectiveness for developing higher quality products with lower costs. For this reason, in the last years the requirements engineering community has devoted much effort to the development of a myriad of requirements modelling languages for SPLs. Objective: In this paper, we review and synthesize the current state of research of requirements modelling languages used in SPLs with respect to their degree of empirical validation, origin and context of use, level of expressiveness, maturity, and industry adoption. Method: We have conducted a systematic literature review with six research questions that cover the main objective. It includes 54 studies, published from 2000 to 2013. Results: The mean level of maturity of the modelling languages is 2.59 over 5, with 46% of them falling within level 2 or below -no implemented abstract syntax reported-. They show a level of expressiveness of 0.7 over 1.0. Some constructs (feature, mandatory, optional, alternative, exclude and require) are present in all the languages, while others (cardinality, attribute, constraint and label) are less common. Only 6% of the languages have been empirically validated, 41% report some kind of industry adoption and 71% of the languages are independent from any development process. Last but not least, 57% of the languages have been proposed by the academia, while 43% have been the result of a joint effort between academia and industry. Conclusions: Research on requirements modeling languages for SPLs has generated a myriad of languages that differ in the set of constructs provided to express SPL requirements. Their general lack of empirical validation and adoption in industry, together with their differences in maturity, draws the picture of a discipline that still needs to evolve. Â© 2015 Elsevier B.V. All rights reserved.},
  comment       = {21},
  document_type = {Article},
  doi           = {10.1016/j.infsof.2015.08.007},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946595413&doi=10.1016%2fj.infsof.2015.08.007&partnerID=40&md5=718ae3d53a66e2e0acba045e425380dd},
}

@Article{Pereira2016,
  author        = {Pereira, J.A. and Constantino, K. and Figueiredo, E. and Saake, G.},
  title         = {Quantitative and qualitative empirical analysis of three feature modeling tools},
  journal       = {Communications in Computer and Information Science},
  year          = {2016},
  volume        = {703},
  pages         = {66-88},
  note          = {cited By 3; Conference of 11th International Conference on Evaluation of Novel Approaches to Software Engineering, ENASE 2016 ; Conference Date: 27 April 2016 Through 28 April 2016; Conference Code:191529},
  __markedentry = {[mac:]},
  abstract      = {During the last couple of decades, feature modeling tools have played a significant role in the improvement of software productivity and quality by assisting tasks in software product line (SPL). SPL decomposes a large-scale software system in terms of their functionalities. The goal of the decomposition is to create well-structured individual software systems that can meet different usersâ€™ requirements. Thus, feature modeling tools provides means to manage the inter-dependencies among reusable common and variable functionalities, called features. There are several tools to support variability management by modeling features in SPL. The variety of tools in the current literature makes it difficult to understand what kinds of tasks are supported and how much effort can be reduced by using these tools. In this paper, we present the results of an empirical study aiming to support SPL engineers choosing the feature modeling tool that best fits their needs. This empirical study compares and analyzes three tools, namely SPLOT, FeatureIDE, and pure::variants. These tools are analyzed based on data from 119 participants. Each participant used one tool for typical feature modeling tasks, such as create a model, update a model, automated analysis of the model, and product configuration. Finally, analysis concerning the perceived ease of use, usefulness, effectiveness, and efficiency are presented. Â© Springer International Publishing AG 2016.},
  comment       = {23},
  document_type = {Conference Paper},
  doi           = {10.1007/978-3-319-56390-9_4},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019123685&doi=10.1007%2f978-3-319-56390-9_4&partnerID=40&md5=4640e9e6abae4822bfe3de6c9a50a71d},
}

@Article{Zanardini2016,
  author        = {Zanardini, D. and Albert, E. and Villela, K.},
  title         = {Resource-usage-aware configuration in software product lines},
  journal       = {Journal of Logical and Algebraic Methods in Programming},
  year          = {2016},
  volume        = {85},
  number        = {1},
  pages         = {173-199},
  note          = {cited By 3},
  __markedentry = {[mac:]},
  abstract      = {Deriving concrete products from a product-line infrastructure requires resolving the variability captured in the product line, based on the company market strategy or requirements from specific customers. Selecting the most appropriate set of features for a product is a complex task, especially if quality requirements have to be considered. Resource-usage-aware configuration aims at providing awareness of resource-usage properties of artifacts throughout the configuration process. This article envisages several strategies for resource-usage-aware configuration which feature different performance and efficiency trade-offs. The common idea in all strategies is the use of resource-usage estimates obtained by an off-the-shelf static resource-usage analyzer as a heuristic for choosing among different candidate configurations. We report on a prototype implementation of the most practical strategies for resource-usage-aware configuration and apply it on an industrial case study. Â© 2015 Elsevier Inc. All rights reserved.},
  comment       = {27},
  document_type = {Article},
  doi           = {10.1016/j.jlamp.2015.08.003},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961584133&doi=10.1016%2fj.jlamp.2015.08.003&partnerID=40&md5=9c1a0f4a5f3569addc00d91678064641},
}

@Article{Schwaegerl2016a,
  author        = {SchwÃ¤gerl, F. and Buchmann, T. and Westfechtel, B.},
  title         = {Filtered model-driven product line engineering with superMod: The home automation case},
  journal       = {Communications in Computer and Information Science},
  year          = {2016},
  volume        = {586},
  pages         = {19-41},
  note          = {cited By 3; Conference of 10th International Joint Conference on Software Technologies, ICSOFT 2015 ; Conference Date: 20 July 2015 Through 22 July 2015; Conference Code:167379},
  __markedentry = {[mac:]},
  abstract      = {Software Product Line Engineering promises to increase the productivity of software development. In the literature, a plan-driven process has been established that is divided up into domain and application engineering. We argue that the strictly sequential order of its process activities implies several disadvantages such as increased complexity, late customer feedback, and duplicate maintenance. SuperMod is a novel model-driven tool based upon a filtered editing model oriented towards version control. The tool provides integrated support for domain and application engineering, offering an iterative and incremental style of development. In this paper, we apply SuperMod to a well-known case study, the Home Automation System product line. We learn that the tool supports a broad variety of iterative and incremental development processes, ranging from phase-structured to feature-driven. Furthermore, it can mitigate the disadvantages of the traditional software product line development process. Â© Springer International Publishing Switzerland 2016.},
  comment       = {23},
  document_type = {Conference Paper},
  doi           = {10.1007/978-3-319-30142-6_2},
  source        = {Scopus},
  sponsors      = {Institute for Systems and Technologies of Information, Control and Communication (INSTICC)},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960436440&doi=10.1007%2f978-3-319-30142-6_2&partnerID=40&md5=b27a8d001069fd67759b48dbcc44f29b},
}

@Article{Carromeu2016,
  author        = {Carromeu, C. and Paiva, D.B. and Cagnin, M.I.},
  title         = {From e-Gov Web SPL to e-Gov Mobile SPL},
  journal       = {International Journal of Web Information Systems},
  year          = {2016},
  volume        = {12},
  number        = {1},
  pages         = {39-61},
  note          = {cited By 0},
  __markedentry = {[mac:]},
  abstract      = {Purpose-This paper aims to discuss the motivation and present the evolution from a Software Product Line (SPL) in the e-Gov Web (e-Gov Web SPL) domain to a SPL in the mobile domain (e-Gov Mobile SPL). Design/methodology/approach-The evolution was supported by the Product Line UML-Based Software Engineering approach and the feature model. Findings-The authors were able to observe that it is feasible to evolve from a SPL for the Web platform to a SPL for the mobile platform, with the intent to port existing Web applications to mobile platforms such that users can have access to the main information and are able to interact with the most important functionalities of Web applications in a mobile device. Research limitations/implications-As for the main limitations, the authors can point out the small number of instantiations performed until the moment with the support of the e-Gov Mobile SPL, what prevented the conduction of an empirical study. Practical implications-Using e-Gov Mobile SPL, it is possible to reduce development time and cost. Originality/value-The existing SPLs do not worry about supporting the development of mobile applications corresponding to existing Web applications, as it is desirable to have access to the information and main features of these applications in mobile devices. We obtained some e-Gov Mobile SPL instantiations corresponding to e-Gov Web SPL instantiations to attend the demands of the Brazilian Agricultural Research Corporation Unit situated at Campo Grande, MS, Brazil. Â© Emerald Group Publishing Limited.},
  comment       = {23},
  document_type = {Conference Paper},
  doi           = {10.1108/IJWIS-10-2015-0036},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84971351807&doi=10.1108%2fIJWIS-10-2015-0036&partnerID=40&md5=fd88b8ca73aeeaa55fe4c41f927b8a9e},
}

@Article{Quinton2016,
  author        = {Quinton, C. and Romero, D. and Duchien, L.},
  title         = {SALOON: A platform for selecting and configuring cloud environments},
  journal       = {Software - Practice and Experience},
  year          = {2016},
  volume        = {46},
  number        = {1},
  pages         = {55-78},
  note          = {cited By 18},
  __markedentry = {[mac:]},
  abstract      = {Migrating legacy systems or deploying a new application to a cloud environment has recently become very trendy, because the number of cloud providers available is still increasing. These cloud environments provide a wide range of resources at different levels of functionality, which must be appropriately configured by stakeholders for the application to run properly. Handling this variability during the configuration and deployment stages is known as a complex and error-prone process, usually made in an ad hoc manner. In this paper, we propose SALOON, a software product lines-based platform to face these issues. We describe the architecture of the SALOON platform, which relies on feature models combined with a domain model used to select among cloud environments a well-suited one. SALOON supports stakeholders while configuring the selected cloud environment in a consistent way and automates the deployment of such configurations through the generation of executable configuration scripts. This paper also reports on some experiments, showing that using SALOON significantly reduces time to configure a cloud environment compared with a manual approach and provides a reliable way to find a correct and suitable configuration. Moreover, our empirical evaluation shows that our approach is effective and scalable to properly deal with a significant number of cloud environments. Â© 2015 John Wiley & Sons, Ltd.},
  comment       = {24},
  document_type = {Article},
  doi           = {10.1002/spe.2311},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84955684730&doi=10.1002%2fspe.2311&partnerID=40&md5=89bb5c31cca858f908e56667d4988d6a},
}

@Article{Farahani2016a,
  author        = {Farahani, E.D. and Habibi, J.},
  title         = {Comprehensive configuration management model for software product line},
  journal       = {International Journal of Control Theory and Applications},
  year          = {2016},
  volume        = {9},
  number        = {25},
  pages         = {301-322},
  note          = {cited By 0},
  __markedentry = {[mac:]},
  abstract      = {In Software Product Line (SPL), Configuration Management (CM) is a multi-dimensional problem. On the one hand, the Core Assets that constitute a configuration need to be managed, and on the other hand, each product in the product line that is built using a configuration must be managed, and furthermore, the management of all these configurations must be coordinated under a single process. Therefore, CM for product lines is more complex than for single systems. The CM of any software system involves four closely related activities: Change Management (ChM), Version Management (VM), Build Management (BM) and Release Management (RM). The aim of this paper is to provide a comprehensive CM model comprising four main sub-models for all CM-related activities required for evolutionary based SPL system development and maintenance. The proposed models support any level of aggregation in SPLs and have been applied to Mobile SPL as a case study. Â© International Science Press.},
  comment       = {22},
  document_type = {Article},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007028727&partnerID=40&md5=3164e6050cad5ccf1db519d1ff64c1c8},
}

@Article{Martini2016b,
  author        = {Martini, A. and Pareto, L. and Bosch, J.},
  title         = {A multiple case study on the inter-group interaction speed in large, embedded software companies employing agile},
  journal       = {Journal of Software: Evolution and Process},
  year          = {2016},
  volume        = {28},
  number        = {1},
  pages         = {4-26},
  note          = {cited By 3},
  __markedentry = {[mac:]},
  abstract      = {The adoption of Agile Software Development in large companies is a recent phenomenon of great interest both for researchers and practitioners. Although intra-team interaction is well supported by established agile practices, the critical interaction between the agile team and other parts of the organization is still unexplored in literature. Such interactions slow down the development, hindering the achievement of business goals based on speed: short time to market, quick replication of products of a product-line, and reaction time for product evolution. We have employed a two-year long multiple-case case-study, collecting data through interviews and a survey in three large companies developing embedded software. Through a combination of qualitative and quantitative analysis, we have found strong evidence that interaction challenges between the development team and other groups in the organization hinder speed and are widespread in the organizations. This paper also identifies current practices in use at the studied companies and provides detailed guidelines for novel solutions in the investigated domain. Such practices are called boundary-spanning activities in information system research and coordination theory. We present a comparison between large embedded software companies employing agile and developing a line of products based on reused assets and agile companies developing pure software. We highlight specific contextual factors and areas where novel spanning activities are needed for mitigating the interaction challenges hindering speed. Copyright Â© 2015 John Wiley & Sons, Ltd.},
  comment       = {23},
  document_type = {Article},
  doi           = {10.1002/smr.1757},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84956572746&doi=10.1002%2fsmr.1757&partnerID=40&md5=47ef8f873f968c785386cd31a24d639f},
}

@Article{Rubin2015,
  author        = {Rubin, J. and Czarnecki, K. and Chechik, M.},
  title         = {Cloned product variants: from ad-hoc to managed software product lines},
  journal       = {International Journal on Software Tools for Technology Transfer},
  year          = {2015},
  volume        = {17},
  number        = {5},
  pages         = {627-646},
  note          = {cited By 8},
  __markedentry = {[mac:]},
  abstract      = {We focus on the problem of managing a collection of related software product variants realized via cloning. By analyzing three industrial case studies of organizations with cloned product lines, we conclude that an efficient management of clones relies on both refactoring cloned variants into a single-copy product line representation and improving development experience when maintaining existing clones. We propose a framework that consists of seven conceptual operators for cloned product line management and show that these operators are adequate to realize development activities we observed in the analyzed case studies. We discuss options for implementing the operators and benefits of the operator-based view. Â© 2014, Springer-Verlag Berlin Heidelberg.},
  comment       = {20},
  document_type = {Article},
  doi           = {10.1007/s10009-014-0347-9},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941423205&doi=10.1007%2fs10009-014-0347-9&partnerID=40&md5=7472e5524c685cc539f316498ae8f0c1},
}

@Article{Haber2015,
  author        = {Haber, A. and HÃ¶lldobler, K. and Kolassa, C. and Look, M. and MÃ¼ller, K. and Rumpe, B. and Schaefer, I. and Schulze, C.},
  title         = {Systematic synthesis of delta modeling languages},
  journal       = {International Journal on Software Tools for Technology Transfer},
  year          = {2015},
  volume        = {17},
  number        = {5},
  pages         = {601-626},
  note          = {cited By 4},
  __markedentry = {[mac:]},
  abstract      = {Delta modeling is a modular, yet flexible approach to capture variability by explicitly representing differences between system variants or versions. The conceptual idea of delta modeling is language-independent. But, to apply delta modeling to a concrete language, either a generic transformation language has to be used or the corresponding delta language has to be manually developed for each considered base language. Generic languages and their tool support often lack readability and specific context condition checking, since they are unrelated to the base language. In this paper, we present a process that allows synthesizing a delta language from the grammar of a given base language. Our method relies on an automatically generated language extension that can be manually adapted to meet domain-specific needs. We illustrate our method using delta modeling on a textual variant of architecture diagrams. Furthermore, we evaluate our method using a comparative case study. This case study covers an architectural, a structural, and a behavioral language and compares the preexisting handwritten grammars to the generated grammars as well as the manually tailored grammars. This paper is an extension of Haber et al. (Proceedings of the 17th international software product line conference (SPLCâ€™13), pp 22â€“31, 2013). Â© 2015, Springer-Verlag Berlin Heidelberg.},
  comment       = {26},
  document_type = {Article},
  doi           = {10.1007/s10009-015-0387-9},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941426977&doi=10.1007%2fs10009-015-0387-9&partnerID=40&md5=b20a162f1709303e9a6be78314750243},
}

@Article{Diaz2015,
  author        = {DÃ­az, J. and PÃ©rez, J. and Garbajosa, J.},
  title         = {A model for tracing variability from features to product-line architectures: A case study in smart grids},
  journal       = {Requirements Engineering},
  year          = {2015},
  volume        = {20},
  number        = {3},
  pages         = {323-343},
  note          = {cited By 6},
  __markedentry = {[mac:]},
  abstract      = {In current software systems with highly volatile requirements, traceability plays a key role to maintain the consistency between requirements and code. Traceability between artifacts involved in the development of software product line (SPL) is still more critical because it is necessary to guarantee that the selection of variants that realize the different SPL products meet the requirements. Current SPL traceability mechanisms trace from variability in features to variations in the configuration of product-line architecture (PLA) in terms of adding and removing components. However, it is not always possible to materialize the variable features of a SPL through adding or removing components, since sometimes they are materialized inside components, i.e., in part of their functionality: a class, a service, and/or an interface. Additionally, variations that happen inside components may crosscut several components of architecture. These kinds of variations are still challenging and their traceability is not currently well supported. Therefore, it is not possible to guarantee that those SPL products with these kinds of vriations meet the requirements. This paper presents a solution for tracing variability from features to PLA by taking these kinds of variations into account. This solution is based on models and traceability between models in order to automate SPL configuration by selecting the variants and realizing the product application. The FPLA modeling framework supports this solution which has been deployed in a software factory. Validation has consisted in putting the solution into practice to develop a product line of power metering management applications for smart grids. Â© Springer-Verlag London 2014.},
  art_number    = {A003},
  comment       = {21},
  document_type = {Article},
  doi           = {10.1007/s00766-014-0203-1},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84943353769&doi=10.1007%2fs00766-014-0203-1&partnerID=40&md5=2b9700810f69105f1098e29cc8ca542c},
}

@Article{Lung2015,
  author        = {Lung, C.-H. and Balasubramaniam, B. and Selvarajah, K. and Elankeswaran, P. and Gopalasundaram, U.},
  title         = {On building architecture-centric product line architecture},
  journal       = {Requirements Engineering},
  year          = {2015},
  volume        = {20},
  number        = {3},
  pages         = {301-321},
  note          = {cited By 2},
  __markedentry = {[mac:]},
  abstract      = {Software architects typically spend a great deal of time and effort exploring uncertainties, evaluating alternatives, and balancing the concerns of stakeholders. Selecting the best architecture to meet both the functional and non-functional requirements is a critical but difficult task, especially at the early stage of software development when there may be many uncertainties. For example, how will a technology match the operational or performance expectations in reality? This paper presents an approach to building architecture-centric product line. The main objective of the proposed approach is to support effective requirements validation and architectural prototyping for the application-level software. Architectural prototyping is practically essential to architecture design and evaluation. However, architectural prototyping practiced in the field mostly is not used to explore alternatives. Effective construction and evaluation of multiple architecture alternatives is one of the critically challenging tasks. The product line architecture advocated in this paper consists of multiple software architecture alternatives, from which the architect can select and rapidly generate a working application prototype. The paper presents a case study of developing a framework that is primarily built with robust architecture patterns in distributed and concurrent computing and includes variation mechanisms to support various applications even in different domains. The development process of the framework is an application of software product line engineering with an aim to effectively facilitate upfront requirements analysis for an application and rapid architectural prototyping to explore and evaluate architecture alternatives. Â© Springer-Verlag London 2014.},
  art_number    = {A002},
  comment       = {21},
  document_type = {Article},
  doi           = {10.1007/s00766-014-0201-3},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84943348957&doi=10.1007%2fs00766-014-0201-3&partnerID=40&md5=5934c044b3b58dcc93f783735a84538c},
}

@Article{Dermeval2015,
  author        = {Dermeval, D. and TenÃ³rio, T. and Bittencourt, I.I. and Silva, A. and Isotani, S. and Ribeiro, M.},
  title         = {Ontology-based feature modeling: An empirical study in changing scenarios},
  journal       = {Expert Systems with Applications},
  year          = {2015},
  volume        = {42},
  number        = {11},
  pages         = {4950-4964},
  note          = {cited By 9},
  __markedentry = {[mac:]},
  abstract      = {A software product line (SPL) is a set of software systems that have a particular set of common features and that satisfy the needs of a particular market segment or mission. Feature modeling is one of the key activities involved in the design of SPLs. The feature diagram produced in this activity captures the commonalities and variabilities of SPLs. In some complex domains (e.g.; ubiquitous computing, autonomic systems and context-aware computing), it is difficult to foresee all functionalities and variabilities a specific SPL may require. Thus, Dynamic Software Product Lines (DSPLs) bind variation points at runtime to adapt to fluctuations in user needs as well as to adapt to changes in the environment. In this context, relying on formal representations of feature models is important to allow them to be automatically analyzed during system execution. Among the mechanisms used for representing and analyzing feature models, description logic (DL) based approaches demand to be better investigated in DSPLs since it provides capabilities, such as automated inconsistency detection, reasoning efficiency, scalability and expressivity. Ontology is the most common way to represent feature models knowledge based on DL reasoners. Previous works conceived ontologies for feature modeling either based on OWL classes and properties or based on OWL individuals. However, considering change or evolution scenarios of feature models, we need to compare whether a class-based or an individual-based feature modeling style is recommended to describe feature models to support SPLs, and especially its capabilities to deal with changes in feature models, as required by DSPLs. In this paper, we conduct a controlled experiment to empirically compare two approaches based on each one of these modeling styles in several changing scenarios (e.g.; add/remove mandatory feature, add/remove optional feature and so on). We measure time to perform changes, structural impact of changes (flexibility) and correctness for performing changes in our experiment. Our results indicate that using OWL individuals requires less time to change and is more flexible than using OWL classes and properties. These results provide insightful assumptions towards the definition of an approach relying on reasoning capabilities of ontologies that can effectively support products reconfiguration in the context of DSPL. Â© 2015 Elsevier Ltd. All rights reserved.},
  comment       = {15},
  document_type = {Article},
  doi           = {10.1016/j.eswa.2015.02.020},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924777581&doi=10.1016%2fj.eswa.2015.02.020&partnerID=40&md5=2fefeaf0ff5c921b30d1c9a4d9f75aa9},
}

@Article{Benlarabi2015b,
  author        = {Benlarabi, A. and Khtira, A. and El Asri, B.},
  title         = {Analyzing trends in software product lines evolution using a cladistics based approach},
  journal       = {Information (Switzerland)},
  year          = {2015},
  volume        = {6},
  number        = {3},
  pages         = {550-563},
  note          = {cited By 2},
  __markedentry = {[mac:]},
  abstract      = {Abstract: A software product line is a complex system the aim of which is to provide a platform dedicated to large reuse. It necessitates a great investment. Thus, its ability to cope with customers' ever-changing requirements is among its key success factors. Great effort has been made to deal with the software product line evolution. In our previous works, we carried out a classification of these works to provide an overview of the used techniques. We also identified the following key challenges of software product lines evolution: the ability to predict future changes, the ability to define the impact of a change easily and the improvement in understanding the change. We have already tackled the second and the third challenges. The objective of this paper is to deal with the first challenge. We use the cladistics classification which was used in biology to understand the evolution of organisms sharing the same ancestor and their process of descent at the aim of predicting their future changes. By analogy, we consider a population of applications for media management on mobile devices derived from the same platform and we use cladistics to construct their evolutionary tree. We conducted an analysis to show how to identify the evolution trends of the case study products and to predict future changes. Â© 2015 by the authors.},
  comment       = {13},
  document_type = {Article},
  doi           = {10.3390/info6030550},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84943561816&doi=10.3390%2finfo6030550&partnerID=40&md5=892c17009c76fbae807d486836450566},
}

@Article{DaSilva2015,
  author        = {Da Silva, I.F. and Da Mota Silveira Neto, P.A. and O'Leary, P. and De Almeida, E.S. and De Lemos Meira, S.R.},
  title         = {Using a multi-method approach to understand agile software product lines},
  journal       = {Information and Software Technology},
  year          = {2015},
  volume        = {57},
  number        = {1},
  pages         = {527-542},
  note          = {cited By 2},
  __markedentry = {[mac:]},
  abstract      = {Context: Software product lines (SPLs) and Agile are approaches that share similar objectives. The main difference is the way in which these objectives are met. Typically evidence on what activities of Agile and SPL can be combined and how they can be integrated stems from different research methods performed separately. The generalizability of this evidence is low, as the research topic is still relatively new and previous studies have been conducted using only one research method. Objective: This study aims to increase understanding of Agile SPL and improve the generalizability of the identified evidence through the use of a multi-method approach. Method: Our multi-method research combines three complementary methods (Mapping Study, Case Study and Expert Opinion) to consolidate the evidence. Results: This combination results in 23 findings that provide evidence on how Agile and SPL could be combined. Conclusion: Although multi-method research is time consuming and requires a high degree of effort to plan, design, and perform, it helps to increase the understanding on Agile SPL and leads to more generalizable evidence. The findings confirm a synergy between Agile and SPL and serve to improve the body of evidence in Agile SPL. When researchers and practitioners develop new Agile SPL approaches, it will be important to consider these synergies. Â© 2014 Elsevier B.V. All rights reserved.},
  comment       = {16},
  document_type = {Conference Paper},
  doi           = {10.1016/j.infsof.2014.06.004},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84940244978&doi=10.1016%2fj.infsof.2014.06.004&partnerID=40&md5=c5776a865b7685023c2704b245211ad2},
}

@Article{Tuezuen2015,
  author        = {TÃ¼zÃ¼n, E. and Tekinerdogan, B. and Kalender, M.E. and Bilgen, S.},
  title         = {Empirical evaluation of a decision support model for adopting software product line engineering},
  journal       = {Information and Software Technology},
  year          = {2015},
  volume        = {60},
  pages         = {77-101},
  note          = {cited By 9},
  __markedentry = {[mac:]},
  abstract      = {Context The software product line engineering (SPLE) community has provided several different approaches for assessing the feasibility of SPLE adoption and selecting transition strategies. These approaches usually include many rules and guidelines which are very often implicit or scattered over different publications. Hence, for the practitioners it is not always easy to select and use these rules to support the decision making process. Even in case the rules are known, the lack of automated support for storing and executing the rules seriously impedes the decision making process. Objective We aim to evaluate the impact of a decision support system (DSS) on decision-making in SPLE adoption. In alignment with this goal, we provide a decision support model (DSM) and the corresponding DSS. Method First, we apply a systematic literature review (SLR) on the existing primary studies that discuss and present approaches for analyzing the feasibility of SPLE adoption and transition strategies. Second, based on the data extraction and synthesis activities of the SLR, the required questions and rules are derived and implemented in the DSS. Third, for validation of the approach we conduct multiple case studies. Results In the course of the SLR, 31 primary studies were identified from which we could construct 25 aspects, 39 questions and 312 rules. We have developed the DSS tool Transit-PL that embodies these elements. Conclusions The multiple case study validation showed that the adoption of the developed DSS tool is justified to support the decision making process in SPLE adoption. Â©2015 Elsevier B.V. All rights reserved.},
  comment       = {25},
  document_type = {Article},
  doi           = {10.1016/j.infsof.2014.12.007},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921772938&doi=10.1016%2fj.infsof.2014.12.007&partnerID=40&md5=8aa699041661142533209386bfcf15cd},
}

@Article{DeSouza2015,
  author        = {De Souza, L.O. and O'Leary, P. and De Almeida, E.S. and De Lemos Meira, S.R.},
  title         = {Product derivation in practice},
  journal       = {Information and Software Technology},
  year          = {2015},
  volume        = {58},
  pages         = {319-337},
  note          = {cited By 1},
  __markedentry = {[mac:]},
  abstract      = {Context: The process of constructing a product from a product line of software assets is known product derivation. An effective product derivation process is important in order to ensure that the efforts required to develop these shared assets is lower than the benefits achieved through their use. Despite its importance, relatively little work has been dedicated to the product derivation process and the strategies applied in practice. Additionally, there is a lack of empirical reports describing product derivation in industrial settings, and, in general, where these reports are available, they have been conducted as informal studies. Objective: Our aim is to investigate how product derivation is performed in practice. Method: We apply a multi-case study design to two different industrial software product line projects with the goal of investigating how they derive their products in practice. The findings from our studies were individually analyzed using the Constant Comparison technique. In order to identify patterns across these studies, the findings were compared using a Cross-case analysis approach. Results: The research approach allowed us to examine the case study outcomes from different perspectives, capturing similarities and differences. From the cases, we identified context specific strategies for product derivation which are easier for practitioners to contextualise and implement. Conclusions: The case studies provide method-in-action insights into concepts explored in the literature, such as: iterative and incremental product derivation, instantiation and integration of platform components and derivation of product databases. Practitioners can use this work as a basis for defining, adapting or evaluating their own product derivation approaches. While researchers can use this work as a starting point for new industrial reports, presenting their experiences with product derivation. Â© 2014 Elsevier B.V. All rights reserved.},
  comment       = {19},
  document_type = {Article},
  doi           = {10.1016/j.infsof.2014.07.004},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84914144344&doi=10.1016%2fj.infsof.2014.07.004&partnerID=40&md5=161e2856a77bae190e2fc6e05556a2fe},
}

@Article{Zdravkovic2015,
  author        = {Zdravkovic, J. and Svee, E.-O. and Giannoulis, C.},
  title         = {Capturing consumer preferences as requirements for software product lines},
  journal       = {Requirements Engineering},
  year          = {2015},
  volume        = {20},
  number        = {1},
  pages         = {71-90},
  note          = {cited By 6},
  __markedentry = {[mac:]},
  abstract      = {Delivering great consumer experiences in competitive market conditions requires software vendors to move away from traditional modes of thinking to an outside-in perspective, one that shifts their business to becoming consumer-centric. Requirements engineers operating in these conditions thus need new means to both capture real preferences of consumers and then relate them to requirements for software customized in different ways to fit anyone. Additionally, because system development models require inputs that are more concrete than abstract, the indistinct values of consumers need to be classified and formalized. To address this challenge, this study aims to establish a conceptual link between preferences of consumers and system requirements, using software product line (SPL) as a means for systematically accommodating the variations within the preferences. The novelty of this study is a conceptual model of consumer preference, which integrates generic value frameworks from both psychology and marketing, and a method for its transformation to requirements for SPL using a goal-oriented RE framework as the mediator. The presented artifacts are grounded in an empirical study related to the development of a system for online education. Â© 2013, The Author(s).},
  comment       = {20},
  document_type = {Article},
  doi           = {10.1007/s00766-013-0187-2},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924223166&doi=10.1007%2fs00766-013-0187-2&partnerID=40&md5=427f356ae79e2831846fe0a519b4f852},
}

@Article{Beloglazov2015,
  author        = {Beloglazov, A. and Banerjee, D. and Hartman, A. and Buyya, R.},
  title         = {Improving Productivity in Design and Development of Information Technology (IT) Service Delivery Simulation Models},
  journal       = {Journal of Service Research},
  year          = {2015},
  volume        = {18},
  number        = {1},
  pages         = {75-89},
  note          = {cited By 6},
  __markedentry = {[mac:]},
  abstract      = {The unprecedented scale of Information Technology (IT) service delivery requires careful analysis and optimization of service systems. The simulation is an efficient way to handle the complexity of modeling and optimization of real-world service delivery systems. However, typically developed custom simulation models lack standard architectures and limit the reuse of design and implementation artifacts across multiple models. In this work, following the design science research methodology, based on a formal model of service delivery systems and applying an adapted software product line (SPL) approach, we create a design artifact for building product lines of IT service delivery simulation models, which vastly simplify and reduce the cost of simulation model design and development. We evaluate the design artifact by constructing a product line of simulation models for a set of IBMâ€™s IT service delivery systems. We validate the proposed approach by comparing the simulation results obtained using our models with the results from the corresponding custom simulation models. The case study demonstrates that the proposed approach leads to 5â€“8 times reductions in the time required to design and develop related simulation models. The potential implications of the application of the proposed approach within an organization are quicker responses to changes in the business environment, more information to assist in managerial decisions, and reduced workload on the process reengineering specialists. Â© The Author(s) 2014.},
  comment       = {15},
  document_type = {Article},
  doi           = {10.1177/1094670514541002},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921056881&doi=10.1177%2f1094670514541002&partnerID=40&md5=f275d49946d43f6e2a61aadd9364b21b},
}

@Article{Lim2015a,
  author        = {Lim, S.L. and Bentley, P.J. and Kanakam, N. and Ishikawa, F. and Honiden, S.},
  title         = {Investigating country differences in mobile app user behavior and challenges for software engineering},
  journal       = {IEEE Transactions on Software Engineering},
  year          = {2015},
  volume        = {41},
  number        = {1},
  pages         = {40-64},
  note          = {cited By 32},
  __markedentry = {[mac:]},
  abstract      = {Mobile applications (apps) are software developed for use on mobile devices and made available through app stores. App stores are highly competitive markets where developers need to cater to a large number of users spanning multiple countries. This work hypothesizes that there exist country differences in mobile app user behavior and conducts one of the largest surveys to date of app users across the world, in order to identify the precise nature of those differences. The survey investigated user adoption of the app store concept, app needs, and rationale for selecting or abandoning an app. We collected data from more than 15 countries, including USA, China, Japan, Germany, France, Brazil, United Kingdom, Italy, Russia, India, Canada, Spain, Australia, Mexico, and South Korea. Analysis of data provided by 4,824 participants showed significant differences in app user behaviors across countries, for example users from USA are more likely to download medical apps, users from the United Kingdom and Canada are more likely to be influenced by price, users from Japan and Australia are less likely to rate apps. Analysis of the results revealed new challenges to market-driven software engineering related to packaging requirements, feature space, quality expectations, app store dependency, price sensitivity, and ecosystem effect. Â© 1976-2012 IEEE.},
  art_number    = {6913003},
  comment       = {25},
  document_type = {Article},
  doi           = {10.1109/TSE.2014.2360674},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921368982&doi=10.1109%2fTSE.2014.2360674&partnerID=40&md5=f2277b95563f18dad360bb633d21ced4},
}

@Article{Wnuk2015,
  author        = {Wnuk, K. and Kabbedijk, J. and Brinkkemper, S. and Regnell, B. and Callele, D.},
  title         = {Exploring factors affecting decision outcome and lead time in large-scale requirements engineering},
  journal       = {Journal of Software: Evolution and Process},
  year          = {2015},
  volume        = {27},
  number        = {9},
  pages         = {647-673},
  note          = {cited By 2},
  __markedentry = {[mac:]},
  abstract      = {Optimizing decision lead time and outcome is important for successful product management. This work identifies decision lead time and outcome factors in large-scale requirements engineering. Our investigation brings supporting evidence that complex changes have longer lead time and that important customers more likely get what they request. The results provide input into the discussion of whether a large company should focus on only a few of its large customers and disregard its significantly larger group of small customers.
Lead time, defined as the duration between the moment a request was filed and the moment the decision was made, is an important aspect of decision making in market-driven requirements engineering. Minimizing lead time allows software companies to focus their resources on the most profitable functionality and enables them to remain competitive within the quickly changing software market. Achieving and sustaining low decision lead time and the resulting high decision efficiency require a better understanding of factors that may affect both decision lead time and outcome. In order to identify possible factors, we conducted an exploratory two-stage case study that combines the statistical analysis of seven possible relationships among decision characteristics at a large company with a survey of industry participants. Our results show that the number of products affected by a decision increases the time needed to make a decision. Practitioners should take this aspect into consideration when planning for efficient decision making and possibly reducing the complexity of decisions. Our results also show that when a change request originates from an important customer, the request is more often accepted. The results provide input into the discussion of whether a large company should focus on only a few of its large customers and disregard its significantly larger group of small customers. The results provide valuable insights for researchers, who can use them to plan research of decision-making processes and methods, and for practitioners, who can use them to optimize their decision-making processes. In future work, we plan to investigate other decision characteristics, such as the number of stakeholders involved in the discussion about the potential change or the number of dependencies between software components. Â© 2015 John Wiley & Sons, Ltd.},
  comment       = {27},
  document_type = {Article},
  doi           = {10.1002/smr.1721},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941654310&doi=10.1002%2fsmr.1721&partnerID=40&md5=499f3f223a5e1a9a599d1570af30de82},
}

@Article{Jamshidi2015,
  author        = {Jamshidi, P. and Pahl, C.},
  title         = {Orthogonal to support multi-cloud application configuration},
  journal       = {Communications in Computer and Information Science},
  year          = {2015},
  volume        = {508},
  pages         = {249-261},
  note          = {cited By 1; Conference of Workshops of European Conference on Service-Oriented and Cloud Computing, ESOCC 2014 ; Conference Date: 2 September 2014 Through 4 September 2014; Conference Code:114149},
  __markedentry = {[mac:]},
  abstract      = {Cloud service providers benefit from a vast majority of customers due to variability and making profit from commonalities between the cloud services that they provide. Recently, application configuration dimensions has been increased dramatically due to multi-tenant, multi-device and multi-cloud paradigm. This challenges the configuration and customization of cloud-based software that are typically offered as a service due to the intrinsic variability. In this paper, we present a model-driven approach based on variability models originating from the software product line community to handle such multi-dimensional variability in the cloud. We exploit orthogonal variability models to systematically manage and create tenant-specific configuration and customizations. We also demonstrate how such variability models can be utilized to take into account the already deployed application parts to enable harmonized deployments for new tenants in a multi-cloud setting. The approach considers application functional and non-functional requirements to provide a set of valid multi-cloud configurations. We illustrate our approach through a case study. Â© Springer International Publishing Switzerland 2015.},
  comment       = {13},
  document_type = {Conference Paper},
  doi           = {10.1007/978-3-319-14886-1_23},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924168003&doi=10.1007%2f978-3-319-14886-1_23&partnerID=40&md5=a1e8c513d863386eefdc8b6c17e84678},
}

@Article{Alferez2014,
  author        = {AlfÃ©rez, M. and BonifÃ¡cio, R. and Teixeira, L. and Accioly, P. and Kulesza, U. and Moreira, A. and AraÃºjo, J. and Borba, P.},
  title         = {Evaluating scenario-based SPL requirements approaches: the case for modularity, stability and expressiveness},
  journal       = {Requirements Engineering},
  year          = {2014},
  volume        = {19},
  number        = {4},
  pages         = {355-376},
  note          = {cited By 6},
  __markedentry = {[mac:]},
  abstract      = {Software product lines (SPL) provide support for productivity gains through systematic reuse. Among the various quality attributes supporting these goals, modularity, stability and expressiveness of feature specifications, their composition and configuration knowledge emerge as strategic values in modern software development paradigms. This paper presents a metric-based evaluation aiming at assessing how well the chosen qualities are supported by scenario-based SPL requirementsapproaches. The selected approaches for this study span from type of notation (textual or graphical based), style to support variability (annotation or composition based), and specification expressiveness. They are compared using the metrics developed in a set of releases from an exemplar case study. Our major findings indicate that composition-based approaches have greater potential to support modularity and stability, and that quantification mechanisms simplify and increase expressiveness of configuration knowledge and composition specifications. Â© 2013, Springer-Verlag London.},
  comment       = {22},
  document_type = {Article},
  doi           = {10.1007/s00766-013-0184-5},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84920253202&doi=10.1007%2fs00766-013-0184-5&partnerID=40&md5=26bd7eec7ee2caf73ab4fc950066efee},
}

@Article{Derakhshanmanesh2014,
  author        = {Derakhshanmanesh, M. and Fox, J. and Ebert, J.},
  title         = {Requirements-driven incremental adoption of variability management techniques and tools: an industrial experience report},
  journal       = {Requirements Engineering},
  year          = {2014},
  volume        = {19},
  number        = {4},
  pages         = {333-354},
  note          = {cited By 4},
  __markedentry = {[mac:]},
  abstract      = {In theory, software product line engineering has reached a mature state. In practice though, implementing a variability management approach remains a tough case-by-case challenge for any organization. To tame the complexity of this undertaking, it is inevitable to handle variability from multiple perspectives and to manage variability consistently across artifacts, tools, and workflows. Especially, a solid understanding and management of the requirements to be met by the products is an inevitable prerequisite. In this article, we share experiences from the ongoing incremental adoption of explicit variability management at TRW Automotiveâ€™s department for automotive slip control systemsâ€”located in Koblenz, Germany. On the technical side, the three key drivers of this adoption effort are (a) domain modeling and scoping, (b) handling of variability in requirements and (c) tighter integration of software engineering focus areas (e.g., domain modeling, requirements engineering, architectural modeling) to make use of variability-related data. In addition to implementation challenges with using and integrating concrete third-party tools, social and workflow-related issues are covered as well. The lessons learned are presented, discussed, and thoroughly compared with the state of the art in research. Â© 2013, Springer-Verlag London.},
  comment       = {22},
  document_type = {Article},
  doi           = {10.1007/s00766-013-0185-4},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84920258824&doi=10.1007%2fs00766-013-0185-4&partnerID=40&md5=87e9e513d18e8be3cc6ae95b7eccfec8},
}

@Article{Zhang2014,
  author        = {Zhang, G. and Ye, H. and Lin, Y.},
  title         = {Quality attribute modeling and quality aware product configuration in software product lines},
  journal       = {Software Quality Journal},
  year          = {2014},
  volume        = {22},
  number        = {3},
  pages         = {365-401},
  note          = {cited By 14},
  __markedentry = {[mac:]},
  abstract      = {In software product line engineering, the customers mostly concentrate on the functionalities of the target product during product configuration. The quality attributes of a target product, such as security and performance, are often assessed until the final product is generated. However, it might be very costly to fix the problem if it is found that the generated product cannot satisfy the customersâ€™ quality requirements. Although the quality of a generated product will be affected by all the life cycles of product development, feature-based product configuration is the first stage where the estimation or prediction of the quality attributes should be considered. As we know, the key issue of predicting the quality attributes for a product configured from feature models is to measure the interdependencies between functional features and quality attributes. The current existing approaches have several limitations on this issue, such as requiring real products for the measurement or involving domain expertsâ€™ efforts. To overcome these limitations, we propose a systematic approach of modeling quality attributes in feature models based on domain expertsâ€™ judgments using the analytic hierarchical process (AHP) and conducting quality aware product configuration based on the captured quality knowledge. Domain expertsâ€™ judgments are adapted to avoid generating the real products for quality evaluation, and AHP is used to reduce domain expertsâ€™ efforts involved in the judgments. A prototype tool is developed to implement the concepts of the proposed approach, and a formal evaluation is carried out based on a large-scale case study. Â© 2013, Springer Science+Business Media New York.},
  comment       = {37},
  document_type = {Article},
  doi           = {10.1007/s11219-013-9197-z},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875049200&doi=10.1007%2fs11219-013-9197-z&partnerID=40&md5=ad822ef7f6b285eb153f76658a503708},
}

@Article{Henard2014a,
  author        = {Henard, C. and Papadakis, M. and Perrouin, G. and Klein, J. and Heymans, P. and Traon, Y.L.},
  title         = {Bypassing the combinatorial explosion: Using similarity to generate and prioritize t-wise test configurations for software product lines},
  journal       = {IEEE Transactions on Software Engineering},
  year          = {2014},
  volume        = {40},
  number        = {7},
  pages         = {650-670},
  note          = {cited By 72},
  __markedentry = {[mac:]},
  abstract      = {Large Software Product Lines (SPLs) are common in industry, thus introducing the need of practical solutions to test them. To this end, t-wise can help to drastically reduce the number of product configurations to test. Current t-wise approaches for SPLs are restricted to small values of t. In addition, these techniques fail at providing means to finely control the configuration process. In view of this, means for automatically generating and prioritizing product configurations for large SPLs are required. This paper proposes (a) a search-based approach capable of generating product configurations for large SPLs, forming a scalable and flexible alternative to current techniques and (b) prioritization algorithms for any set of product configurations. Both these techniques employ a similarity heuristic. The ability of the proposed techniques is assessed in an empirical study through a comparison with state of the art tools. The comparison focuses on both the product configuration generation and the prioritization aspects. The results demonstrate that existing t-wise tools and prioritization techniques fail to handle large SPLs. On the contrary, the proposed techniques are both effective and scalable. Additionally, the experiments show that the similarity heuristic can be used as a viable alternative to t-wise. Â© 1976-2012 IEEE.},
  art_number    = {6823132},
  comment       = {21},
  document_type = {Article},
  doi           = {10.1109/TSE.2014.2327020},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904278397&doi=10.1109%2fTSE.2014.2327020&partnerID=40&md5=c2caa0658f61d9491bb12cd2beff08ba},
}

@Article{Rincon2014,
  author        = {RincÃ³n, L.F. and Giraldo, G.L. and Mazo, R. and Salinesi, C.},
  title         = {An ontological rule-based approach for analyzing dead and false optional features in feature models},
  journal       = {Electronic Notes in Theoretical Computer Science},
  year          = {2014},
  volume        = {302},
  pages         = {111-132},
  note          = {cited By 20},
  __markedentry = {[mac:]},
  abstract      = {Feature models are a common way to represent variability requirements of software product lines by expressing the set of feature combinations that software products can have. Assuring quality of feature models is thus of paramount importance for assuring quality in software product line engineering. However, feature models can have several types of defects that disminish benefits of software product line engineering.Two of such defects are dead features and false optional features. Several state-of-the-art techniques identify these defects, but only few of them tackle the problem of identifying their causes. Besides, the explanations they provide are cumbersome and hard to understand by humans. In this paper, we propose an ontological rule-based approach to: (a) identify dead and false optional features; (b)identify certain causes of these defects; and (c) explain these causes in natural language helping modelers to correct found defects. We represent our approach with a feature model taken from literature. A preliminary empirical evaluation of our approach over 31 FMs shows that our proposal is effective, accurate and scalable to 150 features. Â© 2014 Elsevier B.V.},
  comment       = {22},
  document_type = {Article},
  doi           = {10.1016/j.entcs.2014.01.023},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894169420&doi=10.1016%2fj.entcs.2014.01.023&partnerID=40&md5=c8eb87067b63d1f08d18a4f24d963b13},
}

@Article{SousaFerreira2014,
  author        = {Sousa Ferreira, G.C. and Gaia, F.N. and Figueiredo, E. and De Almeida Maia, M.},
  title         = {On the use of feature-oriented programming for evolving software product lines - A comparative study},
  journal       = {Science of Computer Programming},
  year          = {2014},
  volume        = {93},
  number        = {PART A},
  pages         = {65-85},
  note          = {cited By 17},
  __markedentry = {[mac:]},
  abstract      = {Feature-oriented programming (FOP) is a programming technique based on composition mechanisms, called refinements. It is often assumed that feature-oriented programming is more suitable than other variability mechanisms for implementing Software Product Lines (SPLs). However, there is no empirical evidence to support this claim. In fact, recent research work found out that some composition mechanisms might degenerate the SPL modularity and stability. However, there is no study investigating these properties focusing on the FOP composition mechanisms. This paper presents quantitative and qualitative analysis of how feature modularity and change propagation behave in the context of two evolving SPLs, namely WebStore and MobileMedia. Quantitative data have been collected from the SPLs developed in three different variability mechanisms: FOP refinements, conditional compilation, and object-oriented design patterns. Our results suggest that FOP requires few changes in source code and a balanced number of added modules, providing better support than other techniques for non-intrusive insertions. Therefore, it adheres closer to the Open-Closed principle. Additionally, FOP seems to be more effective tackling modularity degeneration, by avoiding feature tangling and scattering in source code, than conditional compilation and design patterns. These results are based not only on the variability mechanism itself, but also on careful SPL design. However, the aforementioned results are weaker when the design needs to cope with crosscutting and fine-grained features. Â© 2013 Elsevier B.V. All rights reserved.},
  comment       = {21},
  document_type = {Conference Paper},
  doi           = {10.1016/j.scico.2013.10.010},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905459866&doi=10.1016%2fj.scico.2013.10.010&partnerID=40&md5=b909f275cd0c20a8c0c799592c8c29de},
}

@Article{Alsawalqah2014,
  author        = {Alsawalqah, H.I. and Kang, S. and Lee, J.},
  title         = {A method to optimize the scope of a software product platform based on end-user features},
  journal       = {Journal of Systems and Software},
  year          = {2014},
  volume        = {98},
  pages         = {79-106},
  note          = {cited By 8},
  __markedentry = {[mac:]},
  abstract      = {Context: Due to increased competition and the advent of mass customization, many software firms are utilizing product families - groups of related products derived from a product platform - to provide product variety in a cost-effective manner. The key to designing a successful software product family is the product platform, so it is important to determine the most appropriate product platform scope related to business objectives, for product line development.
Aim: This paper proposes a novel method to find the optimized scope of a software product platform based on end-user features.
Method: The proposed method, PPSMS (Product Platform Scoping Method for Software Product Lines), mathematically formulates the product platform scope selection as an optimization problem. The problem formulation targets identification of an optimized product platform scope that will maximize life cycle cost savings and the amount of commonality, while meeting the goals and needs of the envisioned customers' segments. A simulated annealing based algorithm that can solve problems heuristically is then used to help the decision maker in selecting a scope for the product platform, by performing tradeoff analysis of the commonality and cost savings objectives.
Results In a case study, PPSMS helped in identifying 5 non-dominated solutions considered to be of highest preference for decision making, taking into account both cost savings and commonality objectives. A quantitative and qualitative analysis indicated that human experts perceived value in adopting the method in practice, and that it was effective in identifying appropriate product platform scope. Â© 2014 Elsevier Inc. All rights reserved.},
  comment       = {28},
  document_type = {Article},
  doi           = {10.1016/j.jss.2014.08.034},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908291258&doi=10.1016%2fj.jss.2014.08.034&partnerID=40&md5=b78a56f37674d45f046b08e09e784143},
}

@Article{Rosko2014a,
  author        = {RoÅ¡ko, Z. and Strahonja, V.},
  title         = {A case study of software product line for business applications changeability prediction},
  journal       = {Journal of Information and Organizational Sciences},
  year          = {2014},
  volume        = {38},
  number        = {2},
  pages         = {145-160},
  note          = {cited By 0},
  __markedentry = {[mac:]},
  abstract      = {The changeability, a sub-characteristic of maintainability, refers to the level of effort which is required to do modifications to a software product line (SPL) application component. Assuming dependencies between SPL application components and reference architecture implementation (a platform), this paper empirically investigates the relationship between 7 design metrics and changeability of 46 server components of a product line for business applications. In addition, we investigated the usefulness of Platform Responsibility (PR) metric as an indicator of product line component changeability. The results show that most of the design metrics are strongly related to the changeability of server component and also indicate statistically significant correlation between Maintainability Index (MI) and PR metric. The assessment is based on a case study of the implementation of the product line for business applications in a financial institution. The results show that PR metric can be used as good predictor of changeability in the software product line environment. Â© 2014, University of Zagreb. All rights reserved.},
  comment       = {16},
  document_type = {Article},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84918540616&partnerID=40&md5=5c9f9c6b3eeb9a82151b8ed9178db021},
}

@Article{DaSilva2014,
  author        = {Da Silva, I.F. and Da Mota Silveira Neto, P.A. and O'Leary, P. and De Almeida, E.S. and Meira, S.R.D.L.},
  title         = {Software product line scoping and requirements engineering in a small and medium-sized enterprise: An industrial case study},
  journal       = {Journal of Systems and Software},
  year          = {2014},
  volume        = {88},
  number        = {1},
  pages         = {189-206},
  note          = {cited By 9},
  __markedentry = {[mac:]},
  abstract      = {Software product line (SPL) engineering has been applied in several domains, especially in large-scale software development. Given the benefits experienced and reported, SPL engineering has increasingly garnered interest from small to medium-sized companies. It is possible to find a wide range of studies reporting on the challenges of running a SPL project in large companies. However, very little reports exist that consider the situation for small to medium-sized enterprises and these studies try develop universal truths for SPL without lessons learned from empirical evidence need to be contextualized. This study is a step towards bridging this gap in contextual evidence by characterizing the weaknesses discovered in the scoping (SC) and requirements (RE) disciplines of SPL. Moreover, in this study we conducted a case study in a small to medium sized enterprises (SMEs) to justify the use of agile methods when introducing the SPL SC and RE disciplines through the characterization of their bottlenecks. The results of the characterization indicated that ineffective communication and collaboration, long iteration cycles, and the absence of adaptability and flexibility can increase the effort and reduce motivation during project development. These issues can be mitigated by agile methods. Â© 2013 Elsevier Inc. All rights reserved.},
  comment       = {18},
  document_type = {Article},
  doi           = {10.1016/j.jss.2013.10.040},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891625335&doi=10.1016%2fj.jss.2013.10.040&partnerID=40&md5=dd69d4becd8b94d2bc063e89827148e2},
}

@Article{Reinhartz-Berger2014,
  author        = {Reinhartz-Berger, I. and Sturm, A.},
  title         = {Comprehensibility of UML-based software product line specifications A controlled experiment},
  journal       = {Empirical Software Engineering},
  year          = {2014},
  volume        = {19},
  number        = {3},
  pages         = {678-713},
  note          = {cited By 6},
  __markedentry = {[mac:]},
  abstract      = {Software Product Line Engineering (SPLE) deals with developing artifacts that capture the common and variable aspects of software product families. Domain models are one kind of such artifacts. Being developed in early stages, domain models need to specify commonality and variability and guide the reuse of the artifacts in particular software products. Although different modeling methods have been proposed to manage and support these activities, the assessment of these methods is still in an inceptive stage. In this work, we examined the comprehensibility of domain models specified in ADOM, a UML-based SPLE method. In particular, we conducted a controlled experiment in which 116 undergraduate students were required to answer comprehension questions regarding a domain model that was equipped with explicit reuse guidance and/or variability specification. We found that explicit specification of reuse guidance within the domain model helped understand the model, whereas explicit specification of variability increased comprehensibility only to a limited extent. Explicit specification of both reuse guidance and variability often provided intermediate results, namely, results that were better than specification of variability without reuse guidance, but worse than specification of reuse guidance without variability. All these results were perceived in different UML diagram types, namely, use case, class, and sequence diagrams and for different commonality-, variability-, and reuse-related aspects. Â© 2012 Springer Science+Business Media, LLC.},
  comment       = {36},
  document_type = {Article},
  doi           = {10.1007/s10664-012-9234-8},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899943828&doi=10.1007%2fs10664-012-9234-8&partnerID=40&md5=8c6715fe527f714fb9a98aa9dd230798},
}

@Article{Saeed2014,
  author        = {Saeed, M. and Saleh, F. and Al-Insaif, S. and El-Attar, M.},
  title         = {Evaluating the Cognitive Effectiveness of the Visual Syntax of Feature Diagrams},
  journal       = {Communications in Computer and Information Science},
  year          = {2014},
  volume        = {432 CCIS},
  pages         = {180-194},
  note          = {cited By 1; Conference of 1st Asia Pacific Requirements Engineering Symposium, APRES 2014 ; Conference Date: 28 April 2014 Through 29 April 2014; Conference Code:106565},
  __markedentry = {[mac:]},
  abstract      = {[Context and Motivation] Feature models are widely used in the Software Product Line (SPL) domain to capture and communicate the commonality and variability of features in a product line. Feature models contain feature diagrams that graphically depict features in a hierarchical form. [Problem/Question] Many research works have been devoted to enriching the visual syntax of feature diagrams to extend its expressiveness to capture additional types of semantics, however, there is a lack of research that evaluates the visual perception of feature models by its readers. Models serve a dual purpose: to brainstorm and communicate. A very sophisticated yet unreadable model is arguably useless. To date, there has not been a scientific evaluation of the cognitive effectiveness of the visual syntax of feature diagrams. [Principle Ideas] This paper presents a scientific evaluation of the cognitive effectiveness of feature diagrams. The evaluation approach is based on theory and empirical evidence mainly from the cognitive science field. [Contribution] The evaluation reveals drawbacks in the visual notation of feature diagrams. The paper concludes with some recommendations for improvement to remedy the identified flaws. Â© Springer-Verlag Berlin Heidelberg 2014.},
  address       = {Auckland},
  comment       = {15},
  document_type = {Conference Paper},
  doi           = {10.1007/978-3-662-43610-3_14},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904750779&doi=10.1007%2f978-3-662-43610-3_14&partnerID=40&md5=9604755a9b0ee53a178a597d51bd687a},
}

@Article{Bagheri2014,
  author        = {Bagheri, E. and Ensan, F.},
  title         = {Dynamic decision models for staged software product line configuration},
  journal       = {Requirements Engineering},
  year          = {2014},
  volume        = {19},
  number        = {2},
  pages         = {187-212},
  note          = {cited By 14},
  __markedentry = {[mac:]},
  abstract      = {Software product line engineering practices offer desirable characteristics such as rapid product development, reduced time-to-market, and more affordable development costs as a result of systematic representation of the variabilities of a domain of discourse that leads to methodical reuse of software assets. The development lifecycle of a product line consists of two main phases: domain engineering, which deals with the understanding and formally modeling of the target domain, and application engineering that is concerned with the configuration of a product line into one concrete product based on the preferences and requirements of the stakeholders. The work presented in this paper focuses on the application engineering phase and builds both the theoretical and technological tools to assist the stakeholders in (a) understanding the complex interactions of the features of a product line; (b) eliciting the utility of each feature for the stakeholders and hence exposing the stakeholders' otherwise implicit preferences in a way that they can more easily make decisions; and (c) dynamically building a decision model through interaction with the stakeholders and by considering the structural characteristics of software product line feature models, which will guide the stakeholders through the product configuration process. Initial exploratory empirical experiments that we have performed show that our proposed approach for helping stakeholders understand their feature preferences and its associated staged feature model configuration process is able to positively impact the quality of the end results of the application engineering process within the context of the limited number of participants. In addition, it has been observed that the offered tooling support is able to ease the staged feature model configuration process. Â© 2013 Springer-Verlag London.},
  comment       = {26},
  document_type = {Article},
  doi           = {10.1007/s00766-013-0165-8},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84900872964&doi=10.1007%2fs00766-013-0165-8&partnerID=40&md5=8dc990fc51cc488a06456ca6e52fbe4a},
}

@Article{Berger2014b,
  author        = {Berger, T. and Pfeiffer, R.-H. and Tartler, R. and Dienst, S. and Czarnecki, K. and WaÌ§sowski, A. and She, S.},
  title         = {Variability mechanisms in software ecosystems},
  journal       = {Information and Software Technology},
  year          = {2014},
  volume        = {56},
  number        = {11},
  pages         = {1520-1535},
  note          = {cited By 25},
  __markedentry = {[mac:]},
  abstract      = {Context Software ecosystems are increasingly popular for their economic, strategic, and technical advantages. Application platforms such as Android or iOS allow users to highly customize a system by selecting desired functionality from a large variety of assets. This customization is achieved using variability mechanisms. Objective Variability mechanisms are well-researched in the context of software product lines. Although software ecosystems are often seen as conceptual successors, the technology that sustains their success and growth is much less understood. Our objective is to improve empirical understanding of variability mechanisms used in successful software ecosystems. Method We analyze five ecosystems, ranging from the Linux kernel through Eclipse to Android. A qualitative analysis identifies and characterizes variability mechanisms together with their organizational context. This analysis leads to a conceptual framework that unifies ecosystem-specific aspects using a common terminology. A quantitative analysis investigates scales, growth rates, and - most importantly - dependency structures of the ecosystems. Results In all the studied ecosystems, we identify rich dependency languages and variability descriptions that declare many direct and indirect dependencies. Indirect dependencies to abstract capabilities, as opposed to concrete variability units, are used predominantly in fast-growing ecosystems. We also find that variability models - while providing system-wide abstractions over code - work best in centralized variability management and are, thus, absent in ecosystems with large free markets. These latter ecosystems tend to emphasize maintaining capabilities and common vocabularies, dynamic discovery, and binding with strong encapsulation of contributions, together with uniform distribution channels. Conclusion The use of specialized mechanisms in software ecosystems with large free markets, as opposed to software product lines, calls for recognition of a new discipline - variability encouragement. Â© 2014 Elsevier B.V. All rights reserved.},
  comment       = {16},
  document_type = {Article},
  doi           = {10.1016/j.infsof.2014.05.005},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905441720&doi=10.1016%2fj.infsof.2014.05.005&partnerID=40&md5=be0733bc2c3d76e97ea11b75ee18fb46},
}

@Article{Biffl2014,
  author        = {Biffl, S. and Kalinowski, M. and Rabiser, R. and Ekaputra, F. and Winkler, D.},
  title         = {Systematic knowledge engineering: Building bodies of knowledge from published research},
  journal       = {International Journal of Software Engineering and Knowledge Engineering},
  year          = {2014},
  volume        = {24},
  number        = {10},
  pages         = {1533-1571},
  note          = {cited By 4},
  __markedentry = {[mac:]},
  abstract      = {Context. Software engineering researchers conduct systematic literature reviews (SLRs) to build bodies of knowledge (BoKs). Unfortunately, relevant knowledge collected in the SLR process is not publicly available, which considerably slows down building BoKs incrementally. Objective. We present and evaluate the Systematic Knowledge Engineering (SKE) process to support efficiently building BoKs from published research. Method. SKE is based on the SLR process and on Knowledge Engineering practices to build a Knowledge Base (KB) by reusing intermediate data extraction results from SLRs. We evaluated the feasibility of applying SKE by building a Software Inspection BoK KB from published experiments and a Software Product Line BoK KB from published experience reports. We compared the effort, benefits, and risks of building BoK KBs regarding the SKE and the traditional SLR processes. Results. The application of SKE for incrementally collecting and organizing knowledge in the context of a BoK was feasible for different domains and different types of evidence. While the efforts for conducting the SKE and traditional SLR processes are comparable, SKE provides significant benefits for building BoKs. Conclusions. SKE enables researchers in a scientific community to reuse and incrementally build knowledge in a BoK. SKE is ready to be evaluated in other software engineering domains. Â© 2014 World Scientific Publishing Company.},
  comment       = {39},
  document_type = {Article},
  doi           = {10.1142/S021819401440018X},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84928562233&doi=10.1142%2fS021819401440018X&partnerID=40&md5=25738e99352373633d4b6ba562f575c9},
}

@Article{Kim2014,
  author        = {Kim, J. and Kang, S. and Lee, J.},
  title         = {A comparison of software product line traceability approaches from end-to-end traceability perspectives},
  journal       = {International Journal of Software Engineering and Knowledge Engineering},
  year          = {2014},
  volume        = {24},
  number        = {4},
  pages         = {677-714},
  note          = {cited By 4},
  __markedentry = {[mac:]},
  abstract      = {Software traceability is the ability to provide trace information on requirements, design, and implementation of a system. It helps stakeholders understand the many associations of software artifacts created during a software development project. End-to-end traceability refers to linkage of all artifacts in the entire lifecycle of a software development project. Its goal is to provide stakeholders of the software development with trace information in order to analyze impacts due to changes in a software system. Compared to that of a single product, the end-to-end traceability of software product line is more complicated because Software Product Line Development (SPLD) requires two separate but intimately related phases of domain engineering and application engineering. Various SPLD traceability approaches have been proposed in the past. However, thus far no research work on SPLD traceability has focused on SPLD end-to-end traceability. This paper defines SPLD end-to-end traceability and evaluates the existing SPLD traceability approaches from SPLD end-to-end traceability perspectives. We surveyed studies on SPLD traceability methods, traceability mechanisms used in major SPLD approaches, and software traceability survey papers. We compared the existing SPLD traceability approaches based on Systematic Literature Review (SLR). Through the survey, we found that none of the SPLD traceability studies fully supports SPLD end-to-end traceability, and there are unexplored research areas of SPLD end-to-end traceability in the existing SPLD traceability studies. The contribution of this paper is that it presents future research directions that give research guidelines for each unexplored research area in SPLD end-to-end traceability. Finally, based on the research directions, this paper suggests future research opportunities for SPLD end-to-end traceability. Â© 2014 World Scientific Publishing Company.},
  comment       = {38},
  document_type = {Article},
  doi           = {10.1142/S0218194014500260},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929319373&doi=10.1142%2fS0218194014500260&partnerID=40&md5=1954a03cd457a16de3963c2bd0040fe0},
}

@Article{Oliveira2014a,
  author        = {de Oliveira, R.P. and Blanes, D. and Gonzalez-Huerta, J. and Insfran, E. and AbrahÃ£o, S. and Cohen, S. and de Almeida, E.S.},
  title         = {Defining and validating a feature-driven requirements engineering approach},
  journal       = {Journal of Universal Computer Science},
  year          = {2014},
  volume        = {20},
  number        = {5},
  pages         = {666-691},
  note          = {cited By 1},
  __markedentry = {[mac:]},
  abstract      = {The specification of requirements is a key activity for achieving the goals of any software project and it has long been established and recognized by researchers and practitioners. Within Software Product Lines (SPL), this activity is even more critical owing to the need to deal with common, variable, and product-specific requirements, not only for a single product but for the whole set of products. In this paper, we present a Feature-Driven Requirements Engineering approach (FeDRE) that provides support to the requirements specification of SPL. The approach realizes features into functional requirements by considering the variability captured in a feature model. It also provides detailed guidelines on how to associate chunks of features from a feature model and to consider them as the context for the Use Case specification. The evaluation of the approach is illustrated in a case study for developing an SPL of mobile applications for emergency notifications. This case study was applied within 14 subjects, 8 subjects from Universitat PolitÃ¨cnica de ValÃ¨ncia and 6 subjects from Federal University of Bahia. Evaluations concerning the perceived ease of use, perceived usefulness, effectiveness and efficiency as regards requirements analysts using the approach are also presented. The results show that FeDRE was perceived as easy to learn and useful by the participants. Â© J.UCS.},
  comment       = {26},
  document_type = {Article},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904769020&partnerID=40&md5=31d21d19a0249965fa4b5f0742277f51},
}

@Article{Accioly2014,
  author        = {Accioly, P. and Borba, P. and BonifÃ¡cio, R.},
  title         = {Controlled experiments comparing black-box testing strategies for software product lines},
  journal       = {Journal of Universal Computer Science},
  year          = {2014},
  volume        = {20},
  number        = {5},
  pages         = {615-639},
  note          = {cited By 1},
  __markedentry = {[mac:]},
  abstract      = {SPL testing has been considered a challenging task, mainly due to the diversity of products that might be generated from an SPL. To deal with this problem, techniques for specifying and deriving product specific functional test cases have been proposed. However, there is little empirical evidence of the benefits and drawbacks of these techniques. To provide this kind of evidence, in a previous work we conducted an empirical study that compared two design techniques for black-box manual testing, a generic technique that we have observed in an industrial test execution environment, and a product specific technique whose functional test cases could be derived using any SPL approach that considers variations in functional tests. Besides revisiting the first study, here we present a second study that reinforce our findings and brings new insights to our investigation. Both studies indicate that specific test cases improve test execution productivity and quality. Â© J.UCS.},
  comment       = {25},
  document_type = {Article},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904764309&partnerID=40&md5=112aba25b5f4f038fd0f7ce17b051afe},
}

@Article{White2014,
  author        = {White, J. and Galindo, J.A. and Saxena, T. and Dougherty, B. and Benavides, D. and Schmidt, D.C.},
  title         = {Evolving feature model configurations in software product lines},
  journal       = {Journal of Systems and Software},
  year          = {2014},
  volume        = {87},
  number        = {1},
  pages         = {119-136},
  note          = {cited By 26},
  __markedentry = {[mac:]},
  abstract      = {The increasing complexity and cost of software-intensive systems has led developers to seek ways of reusing software components across development projects. One approach to increasing software reusability is to develop a software product-line (SPL), which is a software architecture that can be reconfigured and reused across projects. Rather than developing software from scratch for a new project, a new configuration of the SPL is produced. It is hard, however, to find a configuration of an SPL that meets an arbitrary requirement set and does not violate any configuration constraints in the SPL. Existing research has focused on techniques that produce a configuration of an SPL in a single step. Budgetary constraints or other restrictions, however, may require multi-step configuration processes. For example, an aircraft manufacturer may want to produce a series of configurations of a plane over a span of years without exceeding a yearly budget to add features. This paper provides three contributions to the study of multi-step configuration for SPLs. First, we present a formal model of multi-step SPL configuration and map this model to constraint satisfaction problems (CSPs). Second, we show how solutions to these SPL configuration problems can be automatically derived with a constraint solver by mapping them to CSPs. Moreover, we show how feature model changes can be mapped to our approach in a multi-step scenario by using feature model drift. Third, we present empirical results demonstrating that our CSP-based reasoning technique can scale to SPL models with hundreds of features and multiple configuration steps. Â© 2013 Elsevier Inc.},
  comment       = {18},
  document_type = {Article},
  doi           = {10.1016/j.jss.2013.10.010},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84888645293&doi=10.1016%2fj.jss.2013.10.010&partnerID=40&md5=8ba32b418ee9714829c54449d3e18c41},
}

@Article{Farahani2014,
  author        = {Farahani, F.F. and Ramsin, R.},
  title         = {Methodologies for agile product line engineering: A survey and evaluation},
  journal       = {Frontiers in Artificial Intelligence and Applications},
  year          = {2014},
  volume        = {265},
  pages         = {545-564},
  note          = {cited By 3; Conference of 13th International Conference on New Trends in Intelligent Software Methodology Tools, and Techniques, SoMeT 2014 ; Conference Date: 22 September 2014 Through 24 September 2014; Conference Code:116901},
  __markedentry = {[mac:]},
  abstract      = {Agile Product Line Engineering (APLE) is a relatively new approach which has emerged as the result of combining two successful approaches: Software Product Line Engineering and Agile Software Development. The goal of this combined approach is to cover the weaknesses of each of the two approaches while maximizing the advantages of both. Several methodologies exist which provide a practical process for applying APLE. In this paper, a select set of these methodologies are evaluated using a criteria-based approach, the results of which highlight each methodology's strengths and weaknesses. The evaluation framework and the results can be helpful in selecting, comparing, and modifying APLE methodologies; they can also be used for developing bespoke APLE methodologies, tailored to fit the specific needs of organizations and projects. Â© 2014 The authors and IOS Press. All rights reserved.},
  comment       = {20},
  document_type = {Conference Paper},
  doi           = {10.3233/978-1-61499-434-3-545},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84948778166&doi=10.3233%2f978-1-61499-434-3-545&partnerID=40&md5=92a9ac3a5646d4296c1c8c4d4631fbe1},
}

@Article{Diaz2014,
  author        = {DÃ­az, J. and PÃ©rez, J. and Garbajosa, J.},
  title         = {Agile product-line architecting in practice: A case study in smart grids},
  journal       = {Information and Software Technology},
  year          = {2014},
  volume        = {56},
  number        = {7},
  pages         = {727-748},
  note          = {cited By 14},
  __markedentry = {[mac:]},
  abstract      = {Context Software Product Line Engineering implies the upfront design of a Product-Line Architecture (PLA) from which individual product applications can be engineered. The big upfront design associated with PLAs is in conflict with the current need of "being open to change". To make the development of product-lines more flexible and adaptable to changes, several companies are adopting Agile Product Line Engineering. However, to put Agile Product Line Engineering into practice it is still necessary to make mechanisms available to assist and guide the agile construction and evolution of PLAs. Objective This paper presents the validation of a process for "the agile construction and evolution of product-line architectures", called Agile Product-Line Architecting (APLA). The contribution of the APLA process is the integration of a set of models for describing, documenting, and tracing PLAs, as well as an algorithm for guiding the change decision-making process of PLAs. The APLA process is assessed to prove that assists Agile Product Line Engineering practitioners in the construction and evolution of PLAs. Method Validation is performed through a case study by using both quantitative and qualitative analysis. Quantitative analysis was performed using statistics, whereas qualitative analysis was performed through interviews using constant comparison, triangulation, and supporting tools. This case study was conducted according to the guidelines of Runeson and HÃ¶st in a software factory where three projects in the domain of Smart Grids were involved. Results APLA is deployed through the Flexible-PLA modeling framework. This framework supported the successful development and evolution of the PLA of a family of power metering management applications for Smart Grids. Conclusions APLA is a well-supported solution for the agile construction and evolution of PLAs. This case study illustrates that the proposed solution for the agile construction of PLAs is viable in an industry project on Smart Grids. Â© 2014 Elsevier B.V. All rights reserved.},
  comment       = {22},
  document_type = {Article},
  doi           = {10.1016/j.infsof.2014.01.014},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84898810060&doi=10.1016%2fj.infsof.2014.01.014&partnerID=40&md5=79cd1ad172a29071b1a41d63ef7c884c},
}

@Article{Cetina2013,
  author        = {Cetina, C. and Giner, P. and Fons, J. and Pelechano, V.},
  title         = {Prototyping Dynamic Software Product Lines to evaluate run-time reconfigurations},
  journal       = {Science of Computer Programming},
  year          = {2013},
  volume        = {78},
  number        = {12},
  pages         = {2399-2413},
  note          = {cited By 8},
  __markedentry = {[mac:]},
  abstract      = {Dynamic Software Product Lines (DSPL) encompass systems that are capable of modifying their own behavior with respect to changes in their operating environment by using run-time reconfigurations. A failure in these reconfigurations can directly impact the user experience since the reconfigurations are performed when the system is already under the users control. In this work, we prototype a Smart Hotel DSPL to evaluate the reliability-based risk of the DSPL reconfigurations, specifically, the probability of malfunctioning (Availability) and the consequences of malfunctioning (Severity). This DSPL prototype was performed with the participation of human subjects by means of a Smart Hotel case study which was deployed with real devices. Moreover, we successfully identified and addressed two challenges associated with the involvement of human subjects in DSPL prototyping: enabling participants to (1) trigger the run-time reconfigurations and to (2) understand the effects of the reconfigurations. The evaluation of the case study reveals positive results regarding both Availability and Severity. However, the participant feedback highlights issues with recovering from a failed reconfiguration or a reconfiguration triggered by mistake. To address these issues, we discuss some guidelines learned in the case study. Finally, although the results achieved by the DSPL may be considered satisfactory for its particular domain, DSPL engineers must provide users with more control over the reconfigurations or the users will not be comfortable with DSPLs. Â© 2012 Elsevier B.V. All rights reserved.},
  comment       = {15},
  document_type = {Conference Paper},
  doi           = {10.1016/j.scico.2012.06.007},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884637085&doi=10.1016%2fj.scico.2012.06.007&partnerID=40&md5=9983ebc28ad5519b3283a208a72f7271},
}

@Article{Anjorin2013,
  author        = {Anjorin, A. and Saller, K. and Reimund, I. and Oster, S. and Zorcic, I. and SchÃ¼rr, A.},
  title         = {Model-driven rapid prototyping with programmed graph transformations},
  journal       = {Journal of Visual Languages and Computing},
  year          = {2013},
  volume        = {24},
  number        = {6},
  pages         = {441-462},
  note          = {cited By 1},
  __markedentry = {[mac:]},
  abstract      = {Modern software systems are constantly increasing in complexity and supporting the rapid prototyping of such systems has become crucial to check the feasibility of extensions and optimizations, thereby reducing risks and, consequently, the cost of development. As modern software systems are also expected to be reused, extended, and adapted over a much longer lifetime than ever before, ensuring the maintainability of such systems is equally gaining relevance. In this paper, we present the development, optimization and maintenance of MoSo-PoLiTe, a framework for Software Product Line (SPL) testing, as a novel case study for rapid prototyping via metamodelling and programmed graph transformations. The first part of the case study evaluates the use of programmed graph transformations for optimizing an existing, hand-written system (MoSo-PoLiTe) via rapid prototyping of various strategies. In the second part, we present a complete re-engineering of the hand-written system with programmed graph transformations and provide a critical comparison of both implementations.Our results and conclusions indicate that metamodelling and programmed graph transformation are not only suitable techniques for rapid prototyping, but also lead to more maintainable systems. Â© 2013 Elsevier Ltd.},
  comment       = {22},
  document_type = {Article},
  doi           = {10.1016/j.jvlc.2013.08.001},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84888819910&doi=10.1016%2fj.jvlc.2013.08.001&partnerID=40&md5=67b44619de5e40fa303fed2e7e82fe10},
}

@Article{Hartmann2013,
  author        = {Hartmann, H. and Keren, M. and Matsinger, A. and Rubin, J. and Trew, T. and Yatzkar-Haham, T.},
  title         = {Using MDA for integration of heterogeneous components in software supply chains},
  journal       = {Science of Computer Programming},
  year          = {2013},
  volume        = {78},
  number        = {12},
  pages         = {2313-2330},
  note          = {cited By 6},
  __markedentry = {[mac:]},
  abstract      = {Software product lines are increasingly built using components from specialized suppliers. A company that is in the middle of a supply chain has to integrate components from its suppliers and offer (partially configured) products to its customers. To satisfy both the variability required by each customer and the variability required to satisfy different customers' needs, it may be necessary for such a company to use components from different suppliers, partly offering the same feature set. This leads to a product line with alternative components, possibly using different mechanisms for interfacing, binding and variability, which commonly occurs in embedded software development. In this paper, we describe the limitations of the current practice of combining heterogeneous components in a product line and describe the challenges that arise from software supply chains. We introduce a model-driven approach for automating the integration between components that can generate a partially or fully configured variant, including glue between mismatched components. We analyze the consequences of using this approach in an industrial context, using a case study derived from an existing supply chain and describe the process and roles associated with this approach. Â© 2012 Elsevier B.V. All rights reserved.},
  comment       = {18},
  document_type = {Conference Paper},
  doi           = {10.1016/j.scico.2012.04.004},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884673619&doi=10.1016%2fj.scico.2012.04.004&partnerID=40&md5=84e11f2c4726419a91a5e5660508d0e6},
}

@Article{Berger2013a,
  author        = {Berger, T. and She, S. and Lotufo, R. and Wasowski, A. and Czarnecki, K.},
  title         = {A study of variability models and languages in the systems software domain},
  journal       = {IEEE Transactions on Software Engineering},
  year          = {2013},
  volume        = {39},
  number        = {12},
  pages         = {1611-1640},
  note          = {cited By 53},
  __markedentry = {[mac:]},
  abstract      = {Variability models represent the common and variable features of products in a product line. Since the introduction of FODA in 1990, several variability modeling languages have been proposed in academia and industry, followed by hundreds of research papers on variability models and modeling. However, little is known about the practical use of such languages. We study the constructs, semantics, usage, and associated tools of two variability modeling languages, Kconfig and CDL, which are independently developed outside academia and used in large and significant software projects. We analyze 128 variability models found in 12 open - source projects using these languages. Our study 1) supports variability modeling research with empirical data on the real-world use of its flagship concepts. However, we 2) also provide requirements for concepts and mechanisms that are not commonly considered in academic techniques, and 3) challenge assumptions about size and complexity of variability models made in academic papers. These results are of interest to researchers working on variability modeling and analysis techniques and to designers of tools, such as feature dependency checkers and interactive product configurators. Â© 1976-2012 IEEE.},
  art_number    = {6572787},
  comment       = {30},
  document_type = {Article},
  doi           = {10.1109/TSE.2013.34},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890065386&doi=10.1109%2fTSE.2013.34&partnerID=40&md5=3856bb83fd01545006d2a5389c1e3129},
}

@Article{Wohlin2013,
  author        = {Wohlin, C. and Runeson, P. and Da Mota Silveira Neto, P.A. and EngstrÃ¶m, E. and Do Carmo Machado, I. and De Almeida, E.S.},
  title         = {On the reliability of mapping studies in software engineering},
  journal       = {Journal of Systems and Software},
  year          = {2013},
  volume        = {86},
  number        = {10},
  pages         = {2594-2610},
  note          = {cited By 52},
  __markedentry = {[mac:]},
  abstract      = {Background Systematic literature reviews and systematic mapping studies are becoming increasingly common in software engineering, and hence it becomes even more important to better understand the reliability of such studies. Objective This paper presents a study of two systematic mapping studies to evaluate the reliability of mapping studies and point out some challenges related to this type of study in software engineering. Method The research is based on an in-depth case study of two published mapping studies on software product line testing. Results We found that despite the fact that the two studies are addressing the same topic, there are quite a number of differences when it comes to papers included and in terms of classification of the papers included in the two mapping studies. Conclusions From this we conclude that although mapping studies are important, their reliability cannot simply be taken for granted. Based on the findings we also provide four conjectures that further research has to address to make secondary studies (systematic mapping studies and systematic literature reviews) even more valuable to both researchers and practitioners. Â© 2013 Elsevier Inc.},
  comment       = {17},
  document_type = {Article},
  doi           = {10.1016/j.jss.2013.04.076},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84882695716&doi=10.1016%2fj.jss.2013.04.076&partnerID=40&md5=097cdd6ea83b702f2c96adbfb2e4edda},
}

@Article{Feigenspan2013,
  author        = {Feigenspan, J. and KÃ¤stner, C. and Apel, S. and Liebig, J. and Schulze, M. and Dachselt, R. and Papendieck, M. and Leich, T. and Saake, G.},
  title         = {Do background colors improve program comprehension in the #ifdef hell?},
  journal       = {Empirical Software Engineering},
  year          = {2013},
  volume        = {18},
  number        = {4},
  pages         = {699-745},
  note          = {cited By 35},
  __markedentry = {[mac:]},
  abstract      = {Software-product-line engineering aims at the development of variable and reusable software systems. In practice, software product lines are often implemented with preprocessors. Preprocessor directives are easy to use, and many mature tools are available for practitioners. However, preprocessor directives have been heavily criticized in academia and even referred to as "#ifdef hell", because they introduce threats to program comprehension and correctness. There are many voices that suggest to use other implementation techniques instead, but these voices ignore the fact that a transition from preprocessors to other languages and tools is tedious, erroneous, and expensive in practice. Instead, we and others propose to increase the readability of preprocessor directives by using background colors to highlight source code annotated with ifdef directives. In three controlled experiments with over 70 subjects in total, we evaluate whether and how background colors improve program comprehension in preprocessor-based implementations. Our results demonstrate that background colors have the potential to improve program comprehension, independently of size and programming language of the underlying product. Additionally, we found that subjects generally favor background colors. We integrate these and other findings in a tool called FeatureCommander, which facilitates program comprehension in practice and which can serve as a basis for further research. Â© 2012 Springer Science+Business Media, LLC.},
  comment       = {47},
  document_type = {Article},
  doi           = {10.1007/s10664-012-9208-x},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879846095&doi=10.1007%2fs10664-012-9208-x&partnerID=40&md5=76165de9ec8580474e8390c79a7ad728},
}

@Article{Hubaux2013,
  author        = {Hubaux, A. and Tun, T.T. and Heymans, P.},
  title         = {Separation of concerns in feature diagram languages: A systematic survey},
  journal       = {ACM Computing Surveys},
  year          = {2013},
  volume        = {45},
  number        = {4},
  note          = {cited By 15},
  __markedentry = {[mac:]},
  abstract      = {The need for flexible customization of large feature-rich software systems, according to requirements of various stakeholders, has become an important problem in software development. Among the many software engineering approaches dealing with variability management, the notion of Software Product Line (SPL) has emerged as a major unifying concept. Drawing from established disciplines of manufacturing, SPL approaches aim to design repertoires of software artifacts, from which customized software systems for specific stakeholder requirements can be developed. A major difficulty SPL approaches attempt to address is the modularization of software artifacts, which reconciles the user's needs for certain features and the development and technical constraints. Towards this end, many SPL approaches use feature diagrams to describe possible configurations of a feature set. There have been several proposals for feature diagram languages with varying degrees of expressiveness, intuitiveness, and precision. However, these feature diagram languages have limited scalability when applied to realistic software systems. This article provides a systematic survey of various concerns of feature diagrams and ways in which concerns have been separated. The survey shows how the uncertainty in the purpose of feature diagram languages creates both conceptual and practical limitations to scalability of those languages. Â© 2013 ACM.},
  art_number    = {2501665},
  document_type = {Article},
  doi           = {10.1145/2501654.2501665},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885220291&doi=10.1145%2f2501654.2501665&partnerID=40&md5=558c02ce543378cade65bfeae79734d6},
}

@Article{Laguna2013,
  author        = {Laguna, M.A. and Crespo, Y.},
  title         = {A systematic mapping study on software product line evolution: From legacy system reengineering to product line refactoring},
  journal       = {Science of Computer Programming},
  year          = {2013},
  volume        = {78},
  number        = {8},
  pages         = {1010-1034},
  note          = {cited By 43},
  __markedentry = {[mac:]},
  abstract      = {Software product lines (SPLs) are used in industry to develop families of similar software systems. Legacy systems, either highly configurable or with a story of versions and local variations, are potential candidates for reconfiguration as SPLs using reengineering techniques. Existing SPLs can also be restructured using specific refactorings to improve their internal quality. Although many contributions (including industrial experiences) can be found in the literature, we lack a global vision covering the whole life cycle of an evolving product line. This study aims to survey existing research on the reengineering of legacy systems into SPLs and the refactoring of existing SPLs in order to identify proven approaches and pending challenges for future research in both subfields. We launched a systematic mapping study to find as much literature as possible, covering the diverse terms involved in the search string (restructuring, refactoring, reengineering, etc. always connected with SPLs) and filtering the papers using relevance criteria. The 74 papers selected were classified with respect to several dimensions: main focus, research and contribution type, academic or industrial validation if included, etc. We classified the research approaches and analyzed their feasibility for use in industry. The results of the study indicate that the initial works focused on the adaptation of generic reengineering processes to SPL extraction. Starting from that foundation, several trends have been detected in recent research: the integrated or guided reengineering of (typically object-oriented) legacy code and requirements; specific aspect-oriented or feature-oriented refactoring into SPLs, and more recently, refactoring for the evolution of existing product lines. A majority of papers include academic or industrial case studies, though only a few are based on quantitative data. The degree of maturity of both subfields is different: Industry examples for the reengineering of the legacy system subfield are abundant, although more evaluation research is needed to provide better evidence for adoption in industry. Product line evolution through refactoring is an emerging topic with some pending challenges. Although it has recently received some attention, the theoretical foundation is rather limited in this subfield and should be addressed in the near future. To sum up, the main contributions of this work are the classification of research approaches as well as the analysis of remaining challenges, open issues, and research opportunities. Â© 2012 Elsevier B.V. All rights reserved.},
  comment       = {25},
  document_type = {Conference Paper},
  doi           = {10.1016/j.scico.2012.05.003},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878225914&doi=10.1016%2fj.scico.2012.05.003&partnerID=40&md5=b8fba00f0623f75c93648c14304019ce},
}

@Article{Souza2013a,
  author        = {Souza, I.S. and Da Silva Gomes, G.S. and Da Mota Silveira Neto, P.A. and Do Carmo Machado, I. and De Almeida, E.S. and De Lemos Meira, S.R.},
  title         = {Evidence of software inspection on feature specification for software product lines},
  journal       = {Journal of Systems and Software},
  year          = {2013},
  volume        = {86},
  number        = {5},
  pages         = {1172-1190},
  note          = {cited By 4},
  __markedentry = {[mac:]},
  abstract      = {In software product lines (SPL), scoping is a phase responsible for capturing, specifying and modeling features, and also their constraints, interactions and variations. The feature specification task, performed in this phase, is usually based on natural language, which may lead to lack of clarity, non-conformities and defects. Consequently, scoping analysts may introduce ambiguity, inconsistency, omissions and non-conformities. In this sense, this paper aims at gathering evidence about the effects of applying an inspection approach to feature specification for SPL. Data from a SPL reengineering project were analyzed in this work and the analysis indicated that the correction activity demanded more effort. Also, Pareto's principle showed that incompleteness and ambiguity reported higher non-conformity occurrences. Finally, the Poisson regression analysis showed that sub-domain risk information can be a good indicator for prioritization of sub-domains in the inspection activity.Â© 2012 Elsevier Inc. All rights reserved.},
  comment       = {19},
  document_type = {Article},
  doi           = {10.1016/j.jss.2012.11.044},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875245673&doi=10.1016%2fj.jss.2012.11.044&partnerID=40&md5=625c750c31f8fc3e8ca34cbc8fdb2dea},
}

@Article{Lobato2013,
  author        = {Lobato, L.L. and Bittar, T.J. and Neto, P.A.D.M.S. and MacHado, I.D.C. and De Almeida, E.S. and Meira, S.R.D.L.},
  title         = {Risk management in software product line engineering: A mapping study},
  journal       = {International Journal of Software Engineering and Knowledge Engineering},
  year          = {2013},
  volume        = {23},
  number        = {4},
  pages         = {523-558},
  note          = {cited By 2},
  __markedentry = {[mac:]},
  abstract      = {Software Product Line (SPL) Engineering focuses on systematic software reuse, which has benefits such as reductions in time-to-market and effort, and improvements in the quality of products. However, establishing a SPL is not a simple matter, and can affect all aspects of the organization, since the approach is complex and involves major investment and considerable risk. These risks can have a negative impact on the expected ROI for an organization, if SPL is not sufficiently managed. This paper presents a mapping study of Risk Management (RM) in SPL Engineering. We analyzed a set of thirty studies in the field. The results points out the need for risk management practices in SPL, due to the little research on RM practices in SPL and the importance of identifying insight on RM in SPL. Most studies simply mention the importance of RM, however the steps for managing risk are not clearly specified. Our findings suggest that greater attention should be given, through the use of industrial case studies and experiments, to improve SPL productivity and ensure its success. This research is a first attempt within the SPL community to identify, classify, and manage risks, and establish mitigation strategies. Â© 2013 World Scientific Publishing Company.},
  comment       = {36},
  document_type = {Conference Paper},
  doi           = {10.1142/S0218194013500150},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84881014807&doi=10.1142%2fS0218194013500150&partnerID=40&md5=c6c53641edd6d02b759a1afa12f08755},
}

@Article{Hurtado2013,
  author        = {Hurtado, J.A. and Bastarrica, M.C. and Ochoa, S.F. and Simmonds, J.},
  title         = {MDE software process lines in small companies},
  journal       = {Journal of Systems and Software},
  year          = {2013},
  volume        = {86},
  number        = {5},
  pages         = {1153-1171},
  note          = {cited By 10},
  __markedentry = {[mac:]},
  abstract      = {Software organizations specify their software processes so that process knowledge can be systematically reused across projects. However, different projects may require different processes. Defining a separate process for each potential project context is expensive and error-prone, since these processes must simultaneously evolve in a consistent manner. Moreover, an organization cannot envision all possible project contexts in advance because several variables may be involved, and these may also be combined in different ways. This problem is even worse in small companies since they usually cannot afford to define more than one process. Software process lines are a specific type of software product lines, in the software process domain. A benefit of software process lines is that they allow software process customization with respect to a context. In this article we propose a model-driven approach for software process lines specification and configuration. The article also presents two industrial case studies carried out at two small Chilean software development companies. Both companies have benefited from applying our approach to their processes: new projects are now developed using custom processes, process knowledge is systematically reused, and the total time required to customize a process is much shorter than before.Â© 2012 Elsevier Inc. All rights reserved.},
  comment       = {19},
  document_type = {Article},
  doi           = {10.1016/j.jss.2012.09.033},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875270026&doi=10.1016%2fj.jss.2012.09.033&partnerID=40&md5=b4bd0bb9378eaba09a42ff8306d01824},
}

@Article{Thuem2013,
  author        = {ThÃ¼m, T. and Schaefer, I. and Apel, S. and Hentschel, M.},
  title         = {Family-based deductive verification of software product lines},
  journal       = {ACM SIGPLAN Notices},
  year          = {2013},
  volume        = {48},
  number        = {3},
  pages         = {11-20},
  note          = {cited By 7},
  __markedentry = {[mac:]},
  abstract      = {A software product line is a set of similar software products that share a common code base. While software product lines can be implemented efficiently using feature-oriented programming, verifying each product individually does not scale, especially if human effort is required (e.g., as in interactive theorem proving). We present a family-based approach of deductive verification to prove the correctness of a software product line efficiently. We illustrate and evaluate our approach for software product lines written in a feature-oriented dialect of Java and specified using the Java Modeling Language. We show that the theorem prover KeY can be used off-the-shelf for this task, without any modifications. Compared to the individual verification of each product, our approach reduces the verification time needed for our case study by more than 85 %. Copyright 2012 ACM.},
  comment       = {10},
  document_type = {Conference Paper},
  doi           = {10.1145/2480361.2371404},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877914932&doi=10.1145%2f2480361.2371404&partnerID=40&md5=040800419faa1099e7b9d3d297b62583},
}

@Article{Buccella2013,
  author        = {Buccella, A. and Cechich, A. and Arias, M. and Pol'la, M. and Doldan, M.D.S. and Morsan, E.},
  title         = {Towards systematic software reuse of GIS: Insights from a case study},
  journal       = {Computers and Geosciences},
  year          = {2013},
  volume        = {54},
  pages         = {9-20},
  note          = {cited By 12},
  __markedentry = {[mac:]},
  abstract      = {With the development and adoption of geographic information systems, there is an increasingly amount of software resources being stored or recorded as products to be reused. At the same time, complexity of geographic services is addressed through standardization, which allows developers reaching higher quality levels. In this paper, we introduce our domain-oriented approach to developing geographic software product lines focusing on the experiences collected from a case study. It was developed in the Marine Ecology Domain (Patagonia, Argentina) and illustrates insights of the process. Â© 2013.},
  comment       = {12},
  document_type = {Article},
  doi           = {10.1016/j.cageo.2012.11.014},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874512859&doi=10.1016%2fj.cageo.2012.11.014&partnerID=40&md5=0c1cf8fbaa740dfc5873d8f7e95167cc},
}

@Article{Siegmund2013,
  author        = {Siegmund, N. and RosenmÃ¼ller, M. and KÃ¤stner, C. and Giarrusso, P.G. and Apel, S. and Kolesnikov, S.S.},
  title         = {Scalable prediction of non-functional properties in software product lines: Footprint and memory consumption},
  journal       = {Information and Software Technology},
  year          = {2013},
  volume        = {55},
  number        = {3},
  pages         = {491-507},
  note          = {cited By 34},
  __markedentry = {[mac:]},
  abstract      = {Context: A software product line is a family of related software products, typically created from a set of common assets. Users select features to derive a product that fulfills their needs. Users often expect a product to have specific non-functional properties, such as a small footprint or a bounded response time. Because a product line may have an exponential number of products with respect to its features, it is usually not feasible to generate and measure non-functional properties for each possible product. Objective: Our overall goal is to derive optimal products with respect to non-functional requirements by showing customers which features must be selected. Method: We propose an approach to predict a product's non-functional properties based on the product's feature selection. We aggregate the influence of each selected feature on a non-functional property to predict a product's properties. We generate and measure a small set of products and, by comparing measurements, we approximate each feature's influence on the non-functional property in question. As a research method, we conducted controlled experiments and evaluated prediction accuracy for the non-functional properties footprint and main-memory consumption. But, in principle, our approach is applicable for all quantifiable non-functional properties. Results: With nine software product lines, we demonstrate that our approach predicts the footprint with an average accuracy of 94%, and an accuracy of over 99% on average if feature interactions are known. In a further series of experiments, we predicted main memory consumption of six customizable programs and achieved an accuracy of 89% on average. Conclusion: Our experiments suggest that, with only few measurements, it is possible to accurately predict non-functional properties of products of a product line. Furthermore, we show how already little domain knowledge can improve predictions and discuss trade-offs between accuracy and required number of measurements. With this technique, we provide a basis for many reasoning and product-derivation approaches. Â© 2012 Elsevier B.V. All rights reserved.},
  comment       = {17},
  document_type = {Conference Paper},
  doi           = {10.1016/j.infsof.2012.07.020},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872961131&doi=10.1016%2fj.infsof.2012.07.020&partnerID=40&md5=6b156f0c57fa225ac4233c5604b091d6},
}

@Article{Guana2013,
  author        = {Guana, V. and Correal, D.},
  title         = {Improving software product line configuration: A quality attribute-driven approach},
  journal       = {Information and Software Technology},
  year          = {2013},
  volume        = {55},
  number        = {3},
  pages         = {541-562},
  note          = {cited By 2},
  __markedentry = {[mac:]},
  abstract      = {Context: During the definition of software product lines (SPLs) it is necessary to choose the components that appropriately fulfil a product's intended functionalities, including its quality requirements (i.e., security, performance, scalability). The selection of the appropriate set of assets from many possible combinations is usually done manually, turning this process into a complex, time-consuming, and error-prone task. Objective: Our main objective is to determine whether, with the use of modeling tools, we can simplify and automate the definition process of a SPL, improving the selection process of reusable assets. Method: We developed a model-driven strategy based on the identification of critical points (sensitivity points) inside the SPL architecture. This strategy automatically selects the components that appropriately match the product's functional and quality requirements. We validated our approach experimenting with different real configuration and derivation scenarios in a mobile healthcare SPL where we have worked during the last three years. Results: Through our SPL experiment, we established that our approach improved in nearly 98% the selection of reusable assets when compared with the unassisted analysis selection. However, using our approach there is an increment in the time required for the configuration corresponding to the learning curve of the proposed tools. Conclusion: We can conclude that our domain-specific modeling approach significantly improves the software architect's decision making when selecting the most suitable combinations of reusable components in the context of a SPL. Â© 2012 Elsevier B.V. All rights reserved.},
  comment       = {22},
  document_type = {Conference Paper},
  doi           = {10.1016/j.infsof.2012.09.007},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872930912&doi=10.1016%2fj.infsof.2012.09.007&partnerID=40&md5=aabb6af7e8b142cc656943bcfcc95fc2},
}

@Article{Engstroem2013,
  author        = {EngstrÃ¶m, E. and Runeson, P.},
  title         = {Test overlay in an emerging software product line-An industrial case study},
  journal       = {Information and Software Technology},
  year          = {2013},
  volume        = {55},
  number        = {3},
  pages         = {581-594},
  note          = {cited By 6},
  __markedentry = {[mac:]},
  abstract      = {Context: In large software organizations with a product line development approach, system test planning and scope selection is a complex task. Due to repeated testing: across different testing levels, over time (test for regression) as well as of different variants, the risk of redundant testing is large as well as the risk of overlooking important tests, hidden by the huge amount of possible tests. Aims: This study assesses the amount and type of overlaid manual testing across feature, integration and system test in such context, it explores the causes of potential redundancy and elaborates on how to provide decision support in terms of visualization for the purpose of avoiding redundancy. Method: An in-depth case study was launched including both qualitative and quantitative observations. Results: A high degree of test overlay is identified originating from distributed test responsibilities, poor documentation and structure of test cases, parallel work and insufficient delta analysis. The amount of test overlay depends on which level of abstraction is studied. Conclusions: Avoiding redundancy requires tool support, e.g. visualization of test design coverage, test execution progress, priorities of coverage items as well as visualized priorities of variants to support test case selection. Â© 2012 Elsevier B.V. All rights reserved.},
  comment       = {14},
  document_type = {Conference Paper},
  doi           = {10.1016/j.infsof.2012.04.009},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872969667&doi=10.1016%2fj.infsof.2012.04.009&partnerID=40&md5=f0761683155fbeab04ba6c98af239ed2},
}

@Article{Wang2013a,
  author        = {Wang, Y. and Kobsa, A.},
  title         = {A PLA-based privacy-enhancing user modeling framework and its evaluation},
  journal       = {User Modeling and User-Adapted Interaction},
  year          = {2013},
  volume        = {23},
  number        = {1},
  pages         = {41-82},
  note          = {cited By 6},
  __markedentry = {[mac:]},
  abstract      = {Reconciling personalization with privacy has been a continuing interest in user modeling research. This aim has computational, legal and behavioral/attitudinal ramifications. We present a dynamic privacy-enhancing user modeling framework that supports compliance with users' individual privacy preferences and with the privacy laws and regulations that apply to each user. The framework is based on a software product line architecture. It dynamically selects personalization methods during runtime that meet the current privacy constraints. Since dynamic architectural reconfiguration is typically resource-intensive, we conducted a performance evaluation with four implementations of our system that vary two factors. The results demonstrate that at least one implementation of our approach is technically feasible with comparatively modest additional resources, even for websites with the highest traffic today. To gauge user reactions to privacy controls that our framework enables, we also conducted a controlled experiment that allowed one group of users to specify privacy preferences and view the resulting effects on employed personalization methods. We found that users in this treatment group utilized this feature, deemed it useful, and had fewer privacy concerns as measured by higher disclosure of their personal data. Â© 2012 Springer Science+Business Media B.V.},
  comment       = {42},
  document_type = {Article},
  doi           = {10.1007/s11257-011-9114-8},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872381970&doi=10.1007%2fs11257-011-9114-8&partnerID=40&md5=1a3a96d04f816a8a746cd93b932acbf8},
}

@Article{Thurimella2013,
  author        = {Thurimella, A.K. and BrÃ¼gge, B.},
  title         = {A mixed-method approach for the empirical evaluation of the issue-based variability modeling},
  journal       = {Journal of Systems and Software},
  year          = {2013},
  volume        = {86},
  number        = {7},
  pages         = {1831-1849},
  note          = {cited By 8},
  __markedentry = {[mac:]},
  abstract      = {Background: Variability management is the fundamental part of software product line engineering, which deals with customization and reuse of artifacts for developing a family of systems. Rationale approaches structure decision-making by managing the tacit-knowledge behind decisions. This paper reports a quasi-experiment for evaluating a rationale enriched collaborative variability management methodology called issue-based variability modeling. Objective: We studied the interaction of stakeholders with issue-based modeling to evaluate its applicability in requirements engineering teams. Furthermore, we evaluated the reuse of rationale while instantiating and changing variability. Approach: We enriched a quasi-experimental design with a variety of methods found in case study research. A sample of 258 students was employed with data collection and analysis based on a mix of qualitative and quantitative methods. Our study was performed in two phases: the first phase focused on variability identification and instantiation, while the second phase included tasks on variability evolution. Results: We obtained strong empirical evidence on reuse patterns for rationale during instantiation and evolution of variability. The tabular representations used by rationale modeling are learnable and usable in teams of diverse backgrounds. Â© 2013 Elsevier Inc. All rights reserved.},
  comment       = {19},
  document_type = {Article},
  doi           = {10.1016/j.jss.2013.01.038},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879943433&doi=10.1016%2fj.jss.2013.01.038&partnerID=40&md5=2fc1982c04931e26e6518b14ddf38d75},
}

@Article{Feigenspan2012a,
  author        = {Feigenspan, J. and Schulze, M. and Papendieck, M. and KÃ¤stner, C. and Dachselt, R. and KÃ¶ppen, V. and Frisch, M. and Saake, G.},
  title         = {Supporting program comprehension in large preprocessor-based software product lines},
  journal       = {IET Software},
  year          = {2012},
  volume        = {6},
  number        = {6},
  pages         = {488-501},
  note          = {cited By 5},
  __markedentry = {[mac:]},
  abstract      = {Software product line (SPL) engineering provides an effective mechanism to implement variable software. However, using preprocessors to realise variability, which is typical in industry, is heavily criticised, because it often leads to obfuscated code. Using background colours to highlight code annotated with preprocessor statements to support comprehensibility has proved to be effective, however, scalability to large SPLs is questionable. The authors' aim is to implement and evaluate scalable usage of background colours for industrial-sized SPLs. They designed and implemented scalable concepts in a tool called FeatureCommander. To evaluate its effectiveness, the authors conducted a controlled experiment with a large real-world SPL with over 99 000 lines of code and 340 features. They used a within-subjects design with treatment colours and no colours. They compared correctness and response time of tasks for both treatments. For certain kinds of tasks, background colours improve program comprehension. Furthermore, the subjects generally favour background colours compared with no background colours. In addition, the subjects who worked with background colours had to use the search functions less frequently. The authors show that background colours can improve program comprehension in large SPLs. Based on these encouraging results, they continue their work on improving program comprehension in large SPLs. Â© 2012 The Institution of Engineering and Technology.},
  comment       = {14},
  document_type = {Article},
  doi           = {10.1049/iet-sen.2011.0172},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870419356&doi=10.1049%2fiet-sen.2011.0172&partnerID=40&md5=4b531e8f5d4a9751fbc0361641fbb8d4},
}

@Article{Wong2012,
  author        = {Wong, P.Y.H. and Albert, E. and Muschevici, R. and ProenÃ§a, J. and SchÃ¤fer, J. and Schlatte, R.},
  title         = {The ABS tool suite: Modelling, executing and analysing distributed adaptable object-oriented systems},
  journal       = {International Journal on Software Tools for Technology Transfer},
  year          = {2012},
  volume        = {14},
  number        = {5},
  pages         = {567-588},
  note          = {cited By 28},
  __markedentry = {[mac:]},
  abstract      = {Modern software systems must support a high degree of variability to accommodate a wide range of requirements and operating conditions. This paper introduces the Abstract Behavioural Specification (ABS) language and tool suite, a comprehensive platform for developing and analysing highly adaptable distributed concurrent software systems. The ABS language has a hybrid functional and object- oriented core, and comes with extensions that support the development of systems that are adaptable to diversified requirements, yet capable to maintain a high level of trustworthiness. Using ABS, system variability is consistently traceable from the level of requirements engineering down to object behaviour. This facilitates temporal evolution, as changes to the required set of features of a system are automatically reflected by functional adaptation of the system's behaviour. The analysis capabilities of ABS stretch from debugging, observing and simulating to resource analysis of ABS models and help ensure that a system will remain dependable throughout its evolutionary lifetime. We report on the experience of using the ABS language and the ABS tool suite in an industrial case study. Â© 2012 Springer-Verlag.},
  comment       = {22},
  document_type = {Article},
  doi           = {10.1007/s10009-012-0250-1},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866277516&doi=10.1007%2fs10009-012-0250-1&partnerID=40&md5=b359c6bba10bf37ad3cd65f572643aba},
}

@Article{Chen2012a,
  author        = {Chen, S. and Erwig, M. and Walkingshaw, E.},
  title         = {An error-tolerant type system for variational lambda calculus},
  journal       = {ACM SIGPLAN Notices},
  year          = {2012},
  volume        = {47},
  number        = {9},
  pages         = {29-40},
  note          = {cited By 12},
  __markedentry = {[mac:]},
  abstract      = {Conditional compilation and software product line technologies make it possible to generate a huge number of different programs from a single software project. Typing each of these programs individually is usually impossible due to the sheer number of possible variants. Our previous work has addressed this problem with a type system for variational lambda calculus (VLC), an extension of lambda calculus with basic constructs for introducing and organizing variation. Although our type inference algorithm is more efficient than the brute-force strategy of inferring the types of each variant individually, it is less robust since type inference will fail for the entire variational expression if any one variant contains a type error. In this work, we extend our type system to operate on VLC expressions containing type errors. This extension directly supports locating ill-typed variants and the incremental development of variational programs. It also has many subtle implications for the unification of variational types. We show that our extended type system possesses a principal typing property and that the underlying unification problem is unitary. Our unification algorithm computes partial unifiers that lead to result types that (1) contain errors in as few variants as possible and (2) are most general. Finally, we perform an empirical evaluation to determine the overhead of this extension compared to our previous work, to demonstrate the improvements over the brute-force approach, and to explore the effects of various error distributions on the inference process. Copyright Â© 2012 ACM.},
  comment       = {12},
  document_type = {Conference Paper},
  doi           = {10.1145/2398856.2364535},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870459025&doi=10.1145%2f2398856.2364535&partnerID=40&md5=1ca5927e4faa07d14bc25caf52a09a4a},
}

@Article{Joerges2012,
  author        = {JÃ¶rges, S. and Lamprecht, A.-L. and Margaria, T. and Schaefer, I. and Steffen, B.},
  title         = {A constraint-based variability modeling framework},
  journal       = {International Journal on Software Tools for Technology Transfer},
  year          = {2012},
  volume        = {14},
  number        = {5},
  pages         = {511-530},
  note          = {cited By 26},
  __markedentry = {[mac:]},
  abstract      = {Constraint-based variability modeling is a flexible, declarative approach to managing solution-space variability. Product variants are defined in a top-down manner by successively restricting the admissible combinations of product artifacts until a specific product variant is determined. In this paper, we illustrate the range of constraint-based variability modeling by discussing two of its extreme flavors: constraint-guarded variability modeling and constraint-driven variability modeling. The former applies model checking to establish the global consistency of product variants which are built by manual specification of variations points, whereas the latter uses synthesis technology to fully automatically generate product variants that satisfy all given constraints. Each flavor is illustrated by means of a concrete case study. Â© 2012 Springer-Verlag.},
  comment       = {20},
  document_type = {Article},
  doi           = {10.1007/s10009-012-0254-x},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866284416&doi=10.1007%2fs10009-012-0254-x&partnerID=40&md5=63ea47588f2d468bea6a411704f58378},
}

@Article{Hanssen2012,
  author        = {Hanssen, G.K.},
  title         = {A longitudinal case study of an emerging software ecosystem: Implications for practice and theory},
  journal       = {Journal of Systems and Software},
  year          = {2012},
  volume        = {85},
  number        = {7},
  pages         = {1455-1466},
  note          = {cited By 58},
  __markedentry = {[mac:]},
  abstract      = {Software ecosystems is an emerging trend within the software industry, implying a shift from closed organizations and processes towards open structures, where actors external to the software development organization are becoming increasingly involved in development. This forms an ecosystem of organizations that are related through the shared interest in a software product, leading to new opportunities and new challenges to the industry and its organizational environment. To understand why and how this change occurs, we have followed the development of a software product line organization for a period of approximately five years. We have studied their change from a waterfall-like approach, via agile software product line engineering, towards an emerging software ecosystem. We discuss implications for practice, and propose a nascent theory on software ecosystems. We conclude that the observed change has led to an increase in collaboration across (previously closed) organizational borders, and to the development of a shared value consisting of two components: the technology (the product line, as an extensible platform), and the business domain it supports. Opening up both the technical interface of the product and the organizational interfaces are key enablers of such a change. Â© 2012 Elsevier Inc. All rights reserved.},
  comment       = {12},
  document_type = {Article},
  doi           = {10.1016/j.jss.2011.04.020},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-79958283622&doi=10.1016%2fj.jss.2011.04.020&partnerID=40&md5=8fb17817169ad361e9012b12f779eaba},
}

@Article{Stallinger2012,
  author        = {Stallinger, F. and Neumann, R.},
  title         = {Extending ISO/IEC 12207 with software product management: A process reference model proposal},
  journal       = {Communications in Computer and Information Science},
  year          = {2012},
  volume        = {290 CCIS},
  pages         = {93-106},
  note          = {cited By 7; Conference of 12th International Conference on Software Process Improvement and Capability Determination, SPICE 2012 ; Conference Date: 29 May 2012 Through 31 May 2012; Conference Code:90125},
  __markedentry = {[mac:]},
  abstract      = {Software product management is generally expected to link and integrate business and product related goals with core software engineering and software life cycle activities. Empirical research demonstrates the positive effect of mature software product management practices on key software development performance indicators. Nevertheless, the various frameworks available for software product management have distinct and diverse focus points, are often linked or incorporated with specific development paradigms, or lack integration with or addressing of core software engineering activities. On the other hand, traditional software process improvement approaches generally lack the provision of explicit or detailed software product management activities. - In this paper we build on the results of preceding research on identifying a lack of software product management practices within ISO/IEC 12207 and on deriving key outcomes of software product management activities from selected software product management frameworks. Based on these results we propose a process reference model for software product management that can be integrated with the process reference model as defined in ISO/IEC 12207 for software life cycle processes. Â© 2012 Springer-Verlag.},
  address       = {Palma},
  comment       = {14},
  document_type = {Conference Paper},
  doi           = {10.1007/978-3-642-30439-2_9},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862188967&doi=10.1007%2f978-3-642-30439-2_9&partnerID=40&md5=bc10ccce40d79f40f8b241c020b2da7c},
}

@Article{Rosenmueller2012,
  author        = {RosenmÃ¼ller, M. and Siegmund, N. and Pukall, M. and Apel, S.},
  title         = {Multilingual component programming in racket},
  journal       = {ACM SIGPLAN Notices},
  year          = {2012},
  volume        = {47},
  number        = {3},
  pages         = {3-12},
  note          = {cited By 1},
  __markedentry = {[mac:]},
  abstract      = {Software product lines (SPLs) and adaptive systems aim at variability to cope with changing requirements. Variability can be described in terms of features, which are central for development and configuration of SPLs. In traditional SPLs, features are bound statically before runtime. By contrast, adaptive systems support feature binding at runtime and are sometimes called dynamic SPLs (DSPLs). DSPLs are usually built from coarse-grained components, which reduces the number of possible application scenarios. To overcome this limitation, we closely integrate static binding of traditional SPLs and runtime adaptation of DSPLs. We achieve this integration by statically generating a tailor-made DSPL from a highly customizable SPL. The generated DSPL provides only the runtime variability required by a particular application scenario and the execution environment. The DSPL supports self-configuration based on coarse-grained modules. We provide a feature-based adaptation mechanism that reduces the effort of computing an optimal configuration at runtime. In a case study, we demonstrate the practicability of our approach and show that a seamless integration of static binding and runtime adaptation reduces the complexity of the adaptation process. Â© 2011 ACM.},
  comment       = {10},
  document_type = {Conference Paper},
  doi           = {10.1145/2189751.2047864},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867552032&doi=10.1145%2f2189751.2047864&partnerID=40&md5=ef3e6bbbdf9ec375dcbc008e3e2e2345},
}

@Article{Conejero2012,
  author        = {Conejero, J.M. and Figueiredo, E. and Garcia, A. and HernÃ¡ndez, J. and Jurado, E.},
  title         = {On the relationship of concern metrics and requirements maintainability},
  journal       = {Information and Software Technology},
  year          = {2012},
  volume        = {54},
  number        = {2},
  pages         = {212-238},
  note          = {cited By 10},
  __markedentry = {[mac:]},
  abstract      = {Context: Maintainability has become one of the most essential attributes of software quality, as software maintenance has shown to be one of the most costly and time-consuming tasks of software development. Many studies reveal that maintainability is not often a major consideration in requirements and design stages, and software maintenance costs may be reduced by a more controlled design early in the software life cycle. Several problem factors have been identified as harmful for software maintainability, such as lack of upfront consideration of proper modularity choices. In that sense, the presence of crosscutting concerns is one of such modularity anomalies that possibly exert negative effects on software maintainability. However, to the date there is little or no knowledge about how characteristics of crosscutting concerns, observable in early artefacts, are correlated with maintainability. Objective: In this setting, this paper introduces an empirical analysis where the correlation between crosscutting properties and two ISO/IEC 9126 maintainability attributes, namely changeability and stability, is presented. Method: This correlation is based on the utilization of a set of concern metrics that allows the quantification of crosscutting, scattering and tangling. Results: Our study confirms that a change in a crosscutting concern is more difficult to be accomplished and that artefacts addressing crosscutting concerns are found to be less stable later as the system evolves. Moreover, our empirical analysis reveals that crosscutting properties introduce non-syntactic dependencies between software artefacts, thereby decreasing the quality of software in terms of changeability and stability as well. These subtle dependencies cannot be easily detected without the use of concern metrics. Conclusion: The correlation provides evidence that the presence of certain crosscutting properties negatively affects to changeability and stability. The whole analysis is performed using as target cases three software product lines, where maintainability properties are of upmost importance not only for individual products but also for the core architecture of the product line. Â© 2011 Elsevier B.V. All rights reserved.},
  comment       = {27},
  document_type = {Conference Paper},
  doi           = {10.1016/j.infsof.2011.09.003},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-81055140252&doi=10.1016%2fj.infsof.2011.09.003&partnerID=40&md5=2eb396552b5d1d2a125d858d5e0c5f44},
}

@Article{OLeary2012b,
  author        = {O'Leary, P. and McCaffery, F. and Thiel, S. and Richardson, I.},
  title         = {An Agile process model for product derivation in software product line engineering},
  journal       = {Journal of software: Evolution and Process},
  year          = {2012},
  volume        = {24},
  number        = {5},
  pages         = {561-571},
  note          = {cited By 13},
  __markedentry = {[mac:]},
  abstract      = {Software product lines (SPL) and Agile practices have emerged as new paradigms for developing software. Both approaches share common goals; such as improving productivity, reducing time to market, decreasing development costs and increasing customer satisfaction. These common goals provide the motivation for this research. We believe that integrating Agile practices into SPL can bring a balance between agility and formalism. However, there has been a little research on such integration. We have been researching the potential of integrating Agile approaches in one of the key SPL process areas, product derivation (PD). In this paper, we present an outline of our Agile process model for PD that was developed through industry-based case study research. Copyright Â© 2010 John Wiley & Sons, Ltd.},
  comment       = {11},
  document_type = {Conference Paper},
  doi           = {10.1002/smr.498},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866382322&doi=10.1002%2fsmr.498&partnerID=40&md5=4bd09b3f1a387b08742f9e9c9f7442be},
}

@Article{Anjorin2012,
  author        = {Anjorin, A. and Oster, S. and Zorcic, I. and SchÃ¼rr, A.},
  title         = {Optimizing model-based software product line testing with graph transformations},
  journal       = {Electronic Communications of the EASST},
  year          = {2012},
  volume        = {47},
  note          = {cited By 1},
  __markedentry = {[mac:]},
  abstract      = {Software Product Lines (SPLs) are increasing in relevance and importance as various domains strive to cope with the challenges of supporting a high degree of variability in modern software systems. Especially the systematic testing of SPLs is nontrivial as a high degree of variability implies a vast number of possible products. As testing every valid product individually quickly becomes infeasible, heuristics are often used to choose a representative subset of products to be tested. MoSo-PoLiTe (Model-Based Software Product Line Testing) is a framework for SPL testing that combines and applies combinatorial (in particular pairwise) and model-based testing to SPL feature models. In this paper, we (1) present MoSo-PoLiTe as a novel case study for graph transformations in general and Story Driven Modelling (SDM) in particular, (2) show why we consider SDMs to be ideal for rapid prototyping optimization strategies in this context, and (3) evaluate our implemented optimizations and quantify the realized improvements for MoSo-PoLiTe. Â© Graph Transformation and Visual Modeling Techniques 2012.},
  document_type = {Conference Paper},
  doi           = {10.14279/tuj.eceasst.47.724.729},
  page_count    = {14},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884331701&doi=10.14279%2ftuj.eceasst.47.724.729&partnerID=40&md5=da0433108614c9fb260635e6361c0adf},
}

@Article{Montagud2012,
  author        = {Montagud, S. and AbrahÃ£o, S. and Insfran, E.},
  title         = {A systematic review of quality attributes and measures for software product lines},
  journal       = {Software Quality Journal},
  year          = {2012},
  volume        = {20},
  number        = {3-4},
  pages         = {425-486},
  note          = {cited By 33},
  __markedentry = {[mac:]},
  abstract      = {It is widely accepted that software measures provide an appropriate mechanism for understanding, monitoring, controlling, and predicting the quality of software development projects. In software product lines (SPL), quality is even more important than in a single software product since, owing to systematic reuse, a fault or an inadequate design decision could be propagated to several products in the family. Over the last few years, a great number of quality attributes and measures for assessing the quality of SPL have been reported in literature. However, no studies summarizing the current knowledge about them exist. This paper presents a systematic literature review with the objective of identifying and interpreting all the available studies from 1996 to 2010 that present quality attributes and/or measures for SPL. These attributes and measures have been classified using a set of criteria that includes the life cycle phase in which the measures are applied; the corresponding quality characteristics; their support for specific SPL characteristics (e. g., variability, compositionality); the procedure used to validate the measures, etc. We found 165 measures related to 97 different quality attributes. The results of the review indicated that 92% of the measures evaluate attributes that are related to maintainability. In addition, 67% of the measures are used during the design phase of Domain Engineering, and 56% are applied to evaluate the product line architecture. However, only 25% of them have been empirically validated. In conclusion, the results provide a global vision of the state of the research within this area in order to help researchers in detecting weaknesses, directing research efforts, and identifying new research lines. In particular, there is a need for new measures with which to evaluate both the quality of the artifacts produced during the entire SPL life cycle and other quality characteristics. There is also a need for more validation (both theoretical and empirical) of existing measures. In addition, our results may be useful as a reference guide for practitioners to assist them in the selection or the adaptation of existing measures for evaluating their software product lines. Â© 2011 Springer Science+Business Media, LLC.},
  comment       = {62},
  document_type = {Article},
  doi           = {10.1007/s11219-011-9146-7},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865626740&doi=10.1007%2fs11219-011-9146-7&partnerID=40&md5=33b25cd0a25fc09685e06cbc9c988f14},
}

@Article{Lochau2012a,
  author        = {Lochau, M. and Oster, S. and Goltz, U. and SchÃ¼rr, A.},
  title         = {Model-based pairwise testing for feature interaction coverage in software product line engineering},
  journal       = {Software Quality Journal},
  year          = {2012},
  volume        = {20},
  number        = {3-4},
  pages         = {567-604},
  note          = {cited By 24},
  __markedentry = {[mac:]},
  abstract      = {Testing software product lines (SPLs) is very challenging due to a high degree of variability leading to an enormous number of possible products. The vast majority of today's testing approaches for SPLs validate products individually using different kinds of reuse techniques for testing. Because of their reusability and adaptability capabilities, model-based approaches are suitable to describe variability and are therefore frequently used for implementation and testing purposes of SPLs. Due to the enormous number of possible products, individual product testing becomes more and more infeasible. Pairwise testing offers one possibility to test a subset of all possible products. However, according to the best of our knowledge, there is no contribution discussing and rating this approach in the SPL context. In this contribution, we provide a mapping between feature models describing the common and variable parts of an SPL and a reusable test model in the form of statecharts. Thereby, we interrelate feature model-based coverage criteria and test model-based coverage criteria such as control and data flow coverage and are therefore able to discuss the potentials and limitations of pairwise testing. We pay particular attention to test requirements for feature interactions constituting a major challenge in SPL engineering. We give a concise definition of feature dependencies and feature interactions from a testing point of view, and we discuss adequacy criteria for SPL coverage under pairwise feature interaction testing and give a generalization to the T-wise case. The concept and implementation of our approach are evaluated by means of a case study from the automotive domain. Â© 2011 Springer Science+Business Media, LLC.},
  comment       = {38},
  document_type = {Article},
  doi           = {10.1007/s11219-011-9165-4},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865660809&doi=10.1007%2fs11219-011-9165-4&partnerID=40&md5=ba9f3f3ec3827bc0c43273d7364916b1},
}

@Article{Thurimella2012,
  author        = {Thurimella, A.K. and Bruegge, B.},
  title         = {Issue-based variability management},
  journal       = {Information and Software Technology},
  year          = {2012},
  volume        = {54},
  number        = {9},
  pages         = {933-950},
  note          = {cited By 21},
  __markedentry = {[mac:]},
  abstract      = {Context: Variability management is a key activity in software product line engineering. This paper focuses on managing rationale information during the decision-making activities that arise during variability management. By decision-making we refer to systematic problem solving by considering and evaluating various alternatives. Rationale management is a branch of science that enables decision-making based on the argumentation of stakeholders while capturing the reasons and justifications behind these decisions. Objective: Decision-making should be supported to identify variability in domain engineering and to resolve variation points in application engineering. We capture the rationale behind variability management decisions. The captured rationale information is useful to evaluate future changes of variability models as well as to handle future instantiations of variation points. We claim that maintaining rationale will enhance the longevity of variability models. Furthermore, decisions should be performed using a formal communication between domain engineering and application engineering. Method: We initiate the novel area of issue-based variability management (IVM) by extending variability management with rationale management. The key contributions of this paper are: (i) an issue-based variability management methodology (IVMM), which combines questions, options and criteria (QOC) and a specific variability approach; (ii) a meta-model for IVMM and a process for variability management and (iii) a tool for the methodology, which was developed by extending an open source rationale management tool. Results: Rationale approaches (e.g. questions, options and criteria) guide distributed stakeholders when selecting choices for instantiating variation points. Similarly, rationale approaches also aid the elicitation of variability and the evaluation of changes. The rationale captured within the decision-making process can be reused to perform future decisions on variability. Conclusion: IVMM was evaluated comparatively based on an experimental survey, which provided evidence that IVMM is more effective than a variability modeling approach that does not use issues. Â© 2012 Elsevier B.V. All rights reserved.},
  comment       = {18},
  document_type = {Article},
  doi           = {10.1016/j.infsof.2012.02.005},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861953812&doi=10.1016%2fj.infsof.2012.02.005&partnerID=40&md5=3f2ec22a5e8fc4126819f77a5b6012e2},
}

@Article{Alonso2012,
  author        = {Alonso, D. and Pastor, J.A. and SÃ¡nchez, P. and Ãlvarez, B. and Vicente-Chicote, C.},
  title         = {Automatic code generation for real-time systems: A development approach based on components, models, and frameworks [GeneraciÃ³n automÃ¡tica de software para sistemas de tiempo real: Un enfoque basado en componentes, modelos y frameworks]},
  journal       = {RIAI - Revista Iberoamericana de Automatica e Informatica Industrial},
  year          = {2012},
  volume        = {9},
  number        = {2},
  pages         = {170-181},
  note          = {cited By 6},
  __markedentry = {[mac:]},
  abstract      = {Real-Time Systems have some characteristics that make them particularly sensitive to architectural decisions. The use of Frameworks and Components has proven effective in improving productivity and software quality, especially when combined with Software Product Line approaches. However, the results in terms of software reuse and standardization make the lack of portability of both the design and componentbased implementations clear. This article, based on the Model- Driven Software Development paradigm, presents an approach that separates the component-based description of real-time applications from their possible implementations on different platforms. This separation is supported by the automatic integration of the code obtained from the input models into object-oriented frameworks. The article also details the architectural decisions taken in the implementation of one of such frameworks, which is used as a case study to illustrate the proposed approach. Finally, a comparison with other alternative approaches is made in terms of development cost. Â© 2012 CEA. Publicado por Elsevier EspaÃ±a, S.L.},
  comment       = {12},
  document_type = {Article},
  doi           = {10.1016/j.riai.2012.02.010},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863736214&doi=10.1016%2fj.riai.2012.02.010&partnerID=40&md5=fd2181decbe551cbf2c5afe5f1455f04},
}

@Article{Amalio2011,
  author        = {AmÃ¡lio, N. and Glodt, C. and Pinto, F. and Kelsen, P.},
  title         = {Platform-variant applications from platform-independent models via templates},
  journal       = {Electronic Notes in Theoretical Computer Science},
  year          = {2011},
  volume        = {279},
  number        = {3},
  pages         = {3-25},
  note          = {cited By 0},
  __markedentry = {[mac:]},
  abstract      = {By raising the level of abstraction from code to models, model-driven development (MDD) emphasises design rather than implementation and platform-specificity. This paper presents an experiment with a MDD approach, which takes platform-independent models and generates code for various platforms from them. The platform code is generated from templates. Our approach is based on EP, a formal executable modelling language, supplemented with OCL, and FTL, a formal language of templates. The paper's experiment generates code for the mobile platforms Android and iPhone from the same abstract functional model of a case study. The experiment shows the feasibility of MDD to tackle present day problems, highlighting many benefits of the MDD approach and opportunities for improvement. Â© 2011 Elsevier B.V. All rights reserved.},
  comment       = {23},
  document_type = {Conference Paper},
  doi           = {10.1016/j.entcs.2011.11.035},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855696791&doi=10.1016%2fj.entcs.2011.11.035&partnerID=40&md5=0f6226b96c53c65c7ed20c03c28ac926},
}

@Article{Lamancha2011,
  author        = {Lamancha, B.P. and Polo, M. and Piattini, M.},
  title         = {Systematic review on Software Product Line Testing},
  journal       = {Communications in Computer and Information Science},
  year          = {2011},
  volume        = {170},
  pages         = {58-71},
  note          = {cited By 5; Conference of 5th International Conference on Software and Data Technologies, ICSOFT 2010 ; Conference Date: 22 July 2010 Through 24 July 2010; Conference Code:98838},
  __markedentry = {[mac:]},
  abstract      = {This article presents a systematic review of the literature about Testing in Software Product Lines. The objective is to analyze the existing approaches to testing in software product lines, discussing the significant issues related to this area of knowledge and providing an up-to-date state of the art, which can serve as a basis for innovative research activities. The paper includes an analysis on how SPL research can contribute to dynamize the research in software testing. Â© Springer-Verlag Berlin Heidelberg 2011.},
  address       = {Athens},
  comment       = {14},
  document_type = {Conference Paper},
  doi           = {10.1007/978-3-642-29578-2},
  source        = {Scopus},
  sponsors      = {Inst. for Syst. and Technol. of; Inf. Control and Commun. (INSTICC)},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879472096&doi=10.1007%2f978-3-642-29578-2&partnerID=40&md5=c62c91ce78008def01df926a79a396c5},
}

@Article{Guo2011,
  author        = {Guo, J. and White, J. and Wang, G. and Li, J. and Wang, Y.},
  title         = {A genetic algorithm for optimized feature selection with resource constraints in software product lines},
  journal       = {Journal of Systems and Software},
  year          = {2011},
  volume        = {84},
  number        = {12},
  pages         = {2208-2221},
  note          = {cited By 93},
  __markedentry = {[mac:]},
  abstract      = {Software product line (SPL) engineering is a software engineering approach to building configurable software systems. SPLs commonly use a feature model to capture and document the commonalities and variabilities of the underlying software system. A key challenge when using a feature model to derive a new SPL configuration is determining how to find an optimized feature selection that minimizes or maximizes an objective function, such as total cost, subject to resource constraints. To help address the challenges of optimizing feature selection in the face of resource constraints, this paper presents an approach that uses G enetic A lgorithms for optimized FE ature S election (GAFES) in SPLs. Our empirical results show that GAFES can produce solutions with 86-97% of the optimality of other automated feature selection algorithms and in 45-99% less time than existing exact and heuristic feature selection techniques. Â© 2011 Elsevier Inc. All rights reserved.},
  comment       = {14},
  document_type = {Article},
  doi           = {10.1016/j.jss.2011.06.026},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053576823&doi=10.1016%2fj.jss.2011.06.026&partnerID=40&md5=30f35b5b93f00b26fa9f0a96ec2e89a2},
}

@Article{Huysegoms2011,
  author        = {Huysegoms, T. and Snoeck, M. and Dedene, G. and Goderis, A.},
  title         = {Requirements for successful software development with variability: A case study},
  journal       = {Communications in Computer and Information Science},
  year          = {2011},
  volume        = {219 CCIS},
  number        = {PART 1},
  pages         = {238-247},
  note          = {cited By 1; Conference of International Conference on Enterprise Information Systems, CENTERIS 2011 ; Conference Date: 5 October 2011 Through 7 October 2011; Conference Code:86917},
  __markedentry = {[mac:]},
  abstract      = {According to state of the art literature, software product lines are an effective way to achieve economies of scale through reusability while coping with the problem of variability in related software systems. Fundamentals of variability management and product lines have been available in the software engineering research field for several decades. Nevertheless, projects to cope with variability in practice tend to fall short of target. The reason for this gap between sound theories and poor practice, common in multiple software engineering subfields, remains unclear. Therefore, an empirical study was conducted in a large-scale software dependent multinational. The results of this case study show a number of factors that impact successful variability practice. These factors can be abstracted into general hypotheses useful for bridging the gap between theory and practice. Based on the sources of discrepancy, this research suggests a practical way to overcome the obstacles on the road towards successful variability management. Â© 2011 Springer-Verlag.},
  address       = {Vilamoura},
  comment       = {10},
  document_type = {Conference Paper},
  doi           = {10.1007/978-3-642-24358-5_24},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-80054063237&doi=10.1007%2f978-3-642-24358-5_24&partnerID=40&md5=041136ec5e283f157e5b134bff7c0340},
}

@Article{Wu2011,
  author        = {Wu, Z. and Tang, J. and Kwong, C.K. and Chan, C.Y.},
  title         = {An optimization model for reuse scenario selection considering reliability and cost in software product line development},
  journal       = {International Journal of Information Technology and Decision Making},
  year          = {2011},
  volume        = {10},
  number        = {5},
  pages         = {811-841},
  note          = {cited By 7},
  __markedentry = {[mac:]},
  abstract      = {In this paper, a model that assists developers to evaluate and compare alternative reuse scenarios in software product line (SPL) development systematically in proposed. The model can identify basic activities (abstracted as operations) and precisely relate cost and reliability with each basic operation. A typical reuse mode is described from the perspectives of application and domain engineering. According to this scheme, six reuse modes are identified, and alternative industry reuse scenarios can be derived from these modes. A bi-objective 0-1 integer programming model is developed to help decision makers select reuse scenarios when they develop a SPL to minimize cost and maximize reliability while satisfying system requirements to a certain degree. This model is called the cost and reliability optimization under constraint satisfaction (CROS). To design the model efficiently, a three-phase algorithm for finding all efficient solutions is developed, where the first two phases can obtain an efficient solution, and the last phase can generate a nonsupported efficient solution. Two practical methods are presented to facilitate decision making on selecting from the entire range of efficient solutions in light of the decision-maker's preference for mancomputer interaction. An application of the CROS model in a mail server system development is presented as a case study. Â© 2011 World Scientific Publishing Company.},
  comment       = {31},
  document_type = {Article},
  doi           = {10.1142/S0219622011004580},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052562807&doi=10.1142%2fS0219622011004580&partnerID=40&md5=a0262f562a9e74e50aa90712dde54c2d},
}

@Article{E-Amin2011,
  author        = {Fazal-E-Amin and Mahmood, A.K. and Oxley, A.},
  title         = {Metrics based variability assessment of code assets},
  journal       = {Communications in Computer and Information Science},
  year          = {2011},
  volume        = {181 CCIS},
  number        = {PART 3},
  pages         = {66-75},
  note          = {cited By 0; Conference of 2nd International Conference on Software Engineering and Computer Systems, ICSECS 2011 ; Conference Date: 27 June 2011 Through 29 June 2011; Conference Code:85603},
  __markedentry = {[mac:]},
  abstract      = {The potential benefits of software reuse motivate the use of component based software development and software product lines. In these software development methodologies software assets are being reused. Variability management is a tenet of software reuse. Variability is the capacity of software to satisfy variant requirements. Variability, being the central player in reuse and an important characteristic of reusable components, needs to be measured. In this paper we acknowledge this need and identify measures of variability. Variability implementation mechanisms are analyzed followed by metrics. The metrics are applied on open source component code and the results are validated by an experiment carried out with human subjects. Â© 2011 Springer-Verlag.},
  address       = {Kuantan},
  comment       = {10},
  document_type = {Conference Paper},
  doi           = {10.1007/978-3-642-22203-0_6},
  source        = {Scopus},
  sponsors      = {Springer},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960364100&doi=10.1007%2f978-3-642-22203-0_6&partnerID=40&md5=c113445b648d7d8aff676a5bf242dd11},
}

@Article{Bosch2011b,
  author        = {Bosch, J. and Bosch-Sijtsema, P.M.},
  title         = {Introducing agile customer-centered development in a legacy software product line},
  journal       = {Software - Practice and Experience},
  year          = {2011},
  volume        = {41},
  number        = {8},
  pages         = {871-882},
  note          = {cited By 20},
  __markedentry = {[mac:]},
  abstract      = {The ability to rapidly respond to customer interest and to effectively prioritize development effort has been a long-standing challenge for mass-market software intensive products. This problem is exacerbated in the context of software product lines as functionality may easily fall over software asset and organizational boundaries with consequent losses in efficiency and nimbleness. Some companies facing these problems in their product line respond with a new development process. In this paper we discuss the developments within a single case study, Intuit's Quickbooks product line that combined agile software development, design thinking and self-organizing teams in a successful approach, which provided a significant improvement in terms of responsiveness and accuracy of building customer value. Copyright Â© 2011 John Wiley & Sons, Ltd.},
  comment       = {12},
  document_type = {Article},
  doi           = {10.1002/spe.1063},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-79958247118&doi=10.1002%2fspe.1063&partnerID=40&md5=b2bfc7317e1d83ae546384531cf74963},
}

@Article{Hanssen2011,
  author        = {Hanssen, G.K.},
  title         = {Agile software product line engineering: Enabling factors},
  journal       = {Software - Practice and Experience},
  year          = {2011},
  volume        = {41},
  number        = {8},
  pages         = {883-897},
  note          = {cited By 5},
  __markedentry = {[mac:]},
  abstract      = {This paper reports on a study of a software product line organization that has adopted agile software development to address process rigidity and slowing performance. Experience has showed that despite some impediments, this has become a valuable change to both the organization and its development process. The aim of this study is to identify and understand enabling factors of a combined process, and to understand their subsequent effects. Qualitative data are summarized and analyzed, giving insight into the actions taken, their effects that have emerged over time, and the enabling and contextual factors. The study concludes that a combined process is feasible, that the simplified approach makes the organization more flexible and thus capable of serving a volatile market with fast-changing technologies. This has also enabled the organization to collaborate better with external actors. Copyright Â© 2011 John Wiley & Sons, Ltd.},
  comment       = {15},
  document_type = {Article},
  doi           = {10.1002/spe.1064},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-79958281206&doi=10.1002%2fspe.1064&partnerID=40&md5=c50cda5a08c3f2e8eb05f37a2cdc5aa3},
}

@Article{DaMotaSilveiraNeto2011,
  author        = {Da Mota Silveira Neto, P.A. and Carmo MacHado, I.D. and McGregor, J.D. and De Almeida, E.S. and De Lemos Meira, S.R.},
  title         = {A systematic mapping study of software product lines testing},
  journal       = {Information and Software Technology},
  year          = {2011},
  volume        = {53},
  number        = {5},
  pages         = {407-423},
  note          = {cited By 122},
  __markedentry = {[mac:]},
  abstract      = {Context: In software development, Testing is an important mechanism both to identify defects and assure that completed products work as specified. This is a common practice in single-system development, and continues to hold in Software Product Lines (SPL). Even though extensive research has been done in the SPL Testing field, it is necessary to assess the current state of research and practice, in order to provide practitioners with evidence that enable fostering its further development. Objective: This paper focuses on Testing in SPL and has the following goals: investigate state-of-the-art testing practices, synthesize available evidence, and identify gaps between required techniques and existing approaches, available in the literature. Method: A systematic mapping study was conducted with a set of nine research questions, in which 120 studies, dated from 1993 to 2009, were evaluated. Results: Although several aspects regarding testing have been covered by single-system development approaches, many cannot be directly applied in the SPL context due to specific issues. In addition, particular aspects regarding SPL are not covered by the existing SPL approaches, and when the aspects are covered, the literature just gives brief overviews. This scenario indicates that additional investigation, empirical and practical, should be performed. Conclusion: The results can help to understand the needs in SPL Testing, by identifying points that still require additional investigation, since important aspects regarding particular points of software product lines have not been addressed yet. Â© 2010 Elsevier B.V. All rights reserved.},
  comment       = {17},
  document_type = {Conference Paper},
  doi           = {10.1016/j.infsof.2010.12.003},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952451558&doi=10.1016%2fj.infsof.2010.12.003&partnerID=40&md5=48146e279174abef9ae7749f41cec642},
}

@Article{Bagheri2011,
  author        = {Bagheri, E. and Ensan, F. and GaÅ¡evic, D. and BoÅ¡kovic, M.},
  title         = {Modular feature models: Representation and configuration},
  journal       = {Journal of Research and Practice in Information Technology},
  year          = {2011},
  volume        = {43},
  number        = {2},
  pages         = {109-140},
  note          = {cited By 6},
  __markedentry = {[mac:]},
  abstract      = {Within the realm of software product line engineering, feature modeling is one of the widely used techniques for modeling commonality as well as variability. Feature models incorporate the entire domain application configuration space, and are therefore developed collectively by teams of domain experts. In large scale industrial domains, feature models become too complex both in terms of maintenance and configuration. In order to make the maintenance and configuration of feature models feasible, we propose to modularize feature models based on the well-established Distributed Description Logics formalism. Modular feature models provide for an enhanced collaborative/ distributed feature model design, more efficient feature model evolution and better reusability of feature model structure. We also develop methods for the configuration and configuration verification of a modular feature model based on standard inference mechanisms. We describe and evaluate our proposed approach through a case study on an online electronic store application domain. Â© 2011, Australian Computer Society Inc.},
  comment       = {32},
  document_type = {Article},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860433186&partnerID=40&md5=05447b5ad7d0b50d1200456b1f58c390},
}

@Article{Chen2011,
  author        = {Chen, L. and Ali Babar, M.},
  title         = {A systematic review of evaluation of variability management approaches in software product lines},
  journal       = {Information and Software Technology},
  year          = {2011},
  volume        = {53},
  number        = {4},
  pages         = {344-362},
  note          = {cited By 118},
  __markedentry = {[mac:]},
  abstract      = {Context: Variability management (VM) is one of the most important activities of software product-line engineering (SPLE), which intends to develop software-intensive systems using platforms and mass customization. VM encompasses the activities of eliciting and representing variability in software artefacts, establishing and managing dependencies among different variabilities, and supporting the exploitation of the variabilities for building and evolving a family of software systems. Software product line (SPL) community has allocated huge amount of effort to develop various approaches to dealing with variability related challenges during the last two decade. Several dozens of VM approaches have been reported. However, there has been no systematic effort to study how the reported VM approaches have been evaluated. Objective: The objectives of this research are to review the status of evaluation of reported VM approaches and to synthesize the available evidence about the effects of the reported approaches. Method: We carried out a systematic literature review of the VM approaches in SPLE reported from 1990s until December 2007. Results: We selected 97 papers according to our inclusion and exclusion criteria. The selected papers appeared in 56 publication venues. We found that only a small number of the reviewed approaches had been evaluated using rigorous scientific methods. A detailed investigation of the reviewed studies employing empirical research methods revealed significant quality deficiencies in various aspects of the used quality assessment criteria. The synthesis of the available evidence showed that all studies, except one, reported only positive effects. Conclusion: The findings from this systematic review show that a large majority of the reported VM approaches have not been sufficiently evaluated using scientifically rigorous methods. The available evidence is sparse and the quality of the presented evidence is quite low. The findings highlight the areas in need of improvement, i.e., rigorous evaluation of VM approaches. However, the reported evidence is quite consistent across different studies. That means the proposed approaches may be very beneficial when they are applied properly in appropriate situations. Hence, it can be concluded that further investigations need to pay more attention to the contexts under which different approaches can be more beneficial. Â© 2010 Elsevier B.V. All rights reserved.},
  comment       = {19},
  document_type = {Article},
  doi           = {10.1016/j.infsof.2010.12.006},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-79951813796&doi=10.1016%2fj.infsof.2010.12.006&partnerID=40&md5=30a609a648e0a56c33c9feb550e3df7d},
}

@Article{Schulze2011,
  author        = {Schulze, S. and Apel, S. and KÃ¤stner, C.},
  title         = {Code clones in feature-oriented software product lines},
  journal       = {ACM SIGPLAN Notices},
  year          = {2011},
  volume        = {46},
  number        = {2},
  pages         = {103-112},
  note          = {cited By 2},
  __markedentry = {[mac:]},
  abstract      = {Some limitations of object-oriented mechanisms are known to cause code clones (e.g., extension using inheritance). Novel programming paradigms such as feature-oriented programming (FOP) aim at alleviating these limitations. However, it is an open issue whether FOP is really able to avoid code clones or whether it even facilitates (FOP-related) clones. To address this issue, we conduct an empirical analysis on ten feature-oriented software product lines with respect to code cloning. We found that there is a considerable number of clones in feature-oriented software product lines and that a large fraction of these clones is FOP-related (i.e., caused by limitations of feature-oriented mechanisms). Based on our results, we initiate a discussion on the reasons for FOP-related clones and on how to cope with them. We show by means of examples how such clones can be removed by applying refactorings. Copyright Â© 2010 ACM.},
  comment       = {10},
  document_type = {Article},
  doi           = {10.1145/1942788.1868310},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-79951743431&doi=10.1145%2f1942788.1868310&partnerID=40&md5=f151bb4f4d206bf539e21a1b3fc92e58},
}

@Article{Bagheri2011a,
  author        = {Bagheri, E. and Gasevic, D.},
  title         = {Assessing the maintainability of software product line feature models using structural metrics},
  journal       = {Software Quality Journal},
  year          = {2011},
  volume        = {19},
  number        = {3},
  pages         = {579-612},
  note          = {cited By 73},
  __markedentry = {[mac:]},
  abstract      = {A software product line is a unified representation of a set of conceptually similar software systems that share many common features and satisfy the requirements of a particular domain. Within the context of software product lines, feature models are tree-like structures that are widely used for modeling and representing the inherent commonality and variability of software product lines. Given the fact that many different software systems can be spawned from a single software product line, it can be anticipated that a low-quality design can ripple through to many spawned software systems. Therefore, the need for early indicators of external quality attributes is recognized in order to avoid the implications of defective and low-quality design during the late stages of production. In this paper, we propose a set of structural metrics for software product line feature models and theoretically validate them using valid measurement-theoretic principles. Further, we investigate through controlled experimentation whether these structural metrics can be good predictors (early indicators) of the three main subcharacteristics of maintainability: analyzability, changeability, and understandability. More specifically, a four-step analysis is conducted: (1) investigating whether feature model structural metrics are correlated with feature model maintainability through the employment of classical statistical correlation techniques; (2) understanding how well each of the structural metrics can serve as discriminatory references for maintainability; (3) identifying the sufficient set of structural metrics for evaluating each of the subcharacteristics of maintainability; and (4) evaluating how well different prediction models based on the proposed structural metrics can perform in indicating the maintainability of a feature model. Results obtained from the controlled experiment support the idea that useful prediction models can be built for the purpose of evaluating feature model maintainability using early structural metrics. Some of the structural metrics show significant correlation with the subjective perception of the subjects about the maintainability of the feature models. Â© 2010 Springer Science+Business Media, LLC.},
  comment       = {34},
  document_type = {Article},
  doi           = {10.1007/s11219-010-9127-2},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-79958248566&doi=10.1007%2fs11219-010-9127-2&partnerID=40&md5=e0e2572c8e9a517f7df77ecfd32da223},
}

@Article{Peng2011,
  author        = {Peng, X. and Yu, Y. and Zhao, W.},
  title         = {Analyzing evolution of variability in a software product line: From contexts and requirements to features},
  journal       = {Information and Software Technology},
  year          = {2011},
  volume        = {53},
  number        = {7},
  pages         = {707-721},
  note          = {cited By 10},
  __markedentry = {[mac:]},
  abstract      = {Context: In the long run, features of a software product line (SPL) evolve with respect to changes in stakeholder requirements and system contexts. Neither domain engineering nor requirements engineering handles such co-evolution of requirements and contexts explicitly, making it especially hard to reason about the impact of co-changes in complex scenarios. Objective: In this paper, we propose a problem-oriented and value-based analysis method for variability evolution analysis. The method takes into account both kinds of changes (requirements and contexts) during the life of an evolving software product line. Method: The proposed method extends the core requirements engineering ontology with the notions to represent variability-intensive problem decomposition and evolution. On the basis of problemorientation, the analysis method identifies candidate changes, detects influenced features, and evaluates their contributions to the value of the SPL. Results and Conclusion: The process of applying the analysis method is illustrated using a concrete case study of an evolving enterprise software system, which has confirmed that tracing back to requirements and contextual changes is an effective way to understand the evolution of variability in the software product line. Â© 2011 Elsevier B.V. All rights reserved.},
  comment       = {15},
  document_type = {Article},
  doi           = {10.1016/j.infsof.2011.01.001},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-79955070030&doi=10.1016%2fj.infsof.2011.01.001&partnerID=40&md5=dba8bf360fcd61e03ea82701cae2869b},
}

@Article{Laguna2010,
  author        = {Laguna, M.A. and MarquÃ©s, J.M.},
  title         = {UML support for designing software product lines: The package merge mechanism},
  journal       = {Journal of Universal Computer Science},
  year          = {2010},
  volume        = {16},
  number        = {17},
  pages         = {2313-2332},
  note          = {cited By 7},
  __markedentry = {[mac:]},
  abstract      = {Software product lines have become a successful but challenging approach to software reuse. Some of the problems that hinder the adoption of this development paradigm are the conceptual gap between the variability and design models, as well as the complexity of the traceability management between them. Most current development methods use UML stereotypes or modify UML to face variability and traceability issues. Commercial tools focus mainly on code management, at a fine-grained level. However, the use of specialized techniques and tools represent additional barriers for the widespread introduction of product lines in software companies. In this paper, we propose an alternative based on the UML package merge mechanisms to reflect the structure of the variability models in product line package architecture, thus making the traceability of the configuration decisions straightforward. This package architecture and the configuration of the concrete products are automatically generated (using Model Driven Engineering techniques) from the variability models. As an additional advantage, the package merge mechanism can be directly implemented at code level using partial classes (present in languages such as C#). To support the proposal, we have developed a tool incorporated into MS Visual Studio. This tool permits the product line variability to be modeled and the required transformations to be automated, including the final compilation of concrete products. A case study of a successful experience is described in the article as an example of applying these techniques and tools. The proposed approach, a combination of UML techniques and conventional IDE tools, can make the development of product lines easier for an organization as it removes the need for specialized tools and personnel. Â© J.UCS.},
  comment       = {20},
  document_type = {Article},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650294973&partnerID=40&md5=10049679947c53bf4832416b3646fedf},
}

@Article{Marchione2010,
  author        = {Marchione, F.G. and Fantinato, M. and De Toledo, M.B.F. and De Souza Gimenes, I.M.},
  title         = {E-contracting with price configuration for Web services and QoS},
  journal       = {International Journal of Web and Grid Services},
  year          = {2010},
  volume        = {6},
  number        = {4},
  pages         = {357-384},
  note          = {cited By 3},
  __markedentry = {[mac:]},
  abstract      = {The large amount of information in electronic contracts hampers their establishment due to high complexity. An approach inspired in Software Product Line (PL) and based on feature modelling was proposed to make this process more systematic through information reuse and structuring. By assessing the feature-based approach in relation to a proposed set of requirements, it was showed that the approach does not allow the price of services and of Quality of Services (QoS) attributes to be considered in the negotiation and included in the electronic contract. Thus, this paper also presents an extension of such approach in which prices and price types associated to Web services and QoS levels are applied. An extended toolkit prototype is also presented as well as an experiment example of the proposed approach. Copyright Â© 2010 Inderscience Enterprises Ltd.},
  comment       = {28},
  document_type = {Article},
  doi           = {10.1504/IJWGS.2010.036403},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-78049489331&doi=10.1504%2fIJWGS.2010.036403&partnerID=40&md5=dc25e161202646c1cab1d910e99b89f6},
}

@Article{Benavides2010,
  author        = {Benavides, D. and Segura, S. and Ruiz-CortÃ©s, A.},
  title         = {Automated analysis of feature models 20 years later: A literature review},
  journal       = {Information Systems},
  year          = {2010},
  volume        = {35},
  number        = {6},
  pages         = {615-636},
  note          = {cited By 630},
  __markedentry = {[mac:]},
  abstract      = {Software product line engineering is about producing a set of related products that share more commonalities than variabilities. Feature models are widely used for variability and commonality management in software product lines. Feature models are information models where a set of products are represented as a set of features in a single model. The automated analysis of feature models deals with the computer-aided extraction of information from feature models. The literature on this topic has contributed with a set of operations, techniques, tools and empirical results which have not been surveyed until now. This paper provides a comprehensive literature review on the automated analysis of feature models 20 years after of their invention. This paper contributes by bringing together previously disparate streams of work to help shed light on this thriving area. We also present a conceptual framework to understand the different proposals as well as categorise future contributions. We finally discuss the different studies and propose some challenges to be faced in the future. Â© 2010 Elsevier B.V. All rights reserved.},
  comment       = {22},
  document_type = {Article},
  doi           = {10.1016/j.is.2010.01.001},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955227439&doi=10.1016%2fj.is.2010.01.001&partnerID=40&md5=3f1015b79ab4c771fb5528d80b3d98d7},
}

@Article{Robinson2010,
  author        = {Robinson, W.N. and Ding, Y.},
  title         = {A survey of customization support in agent-based business process simulation tools},
  journal       = {ACM Transactions on Modeling and Computer Simulation},
  year          = {2010},
  volume        = {20},
  number        = {3},
  note          = {cited By 5},
  __markedentry = {[mac:]},
  abstract      = {Agent-based business process simulation has grown in popularity, in part because of its analysis capabilities. The analyses depend on the kinds of simulations that can be built, adapted, and extended, which in turn depend on the underlying simulation framework.We report the results of our analysis of 19 agent-based process simulation tools and their simulation frameworks. We conclude that a growing number of simulation tools are using component-based software techniques. Nevertheless, most simulation tools do not directly support requirements models, their transformation into executable simulations, or the management of model variants over time. Such practices are becoming more widely applied in software engineering under the term software product line engineering (SPLE). Based on our analysis, agent-based process simulation tools may improve their customization capacity by: (1) supporting object modeling more completely and (2) supporting software product line engineering issues. Â© 2010 ACM.},
  art_number    = {14},
  document_type = {Article},
  doi           = {10.1145/1842713.1842717},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-77958034463&doi=10.1145%2f1842713.1842717&partnerID=40&md5=a599f7f7e7dd669d996527d65c486bae},
}

@Article{Thoern2010,
  author        = {ThÃ¶rn, C.},
  title         = {Current state and potential of variability management practices in software-intensive SMEs: Results from a regional industrial survey},
  journal       = {Information and Software Technology},
  year          = {2010},
  volume        = {52},
  number        = {4},
  pages         = {411-421},
  note          = {cited By 10},
  __markedentry = {[mac:]},
  abstract      = {Context: More and more, small and medium-sized enterprises (SMEs) are using software to augment the functionality of their products and offerings. Variability management of software is becoming an interesting topic for SMEs with expanding portfolios and increasingly complex product structures. While the use of software product lines to resolve high variability is well known in larger organizations, there is less known about the practices in SMEs. Objective: This paper presents results from a survey of software developing SMEs. The purpose of the paper is to provide a snapshot of the current awareness and practices of variability modeling in organizations that are developing software with the constraints present in SMEs. Method: A survey with questions regarding the variability practices was distributed to software developing organizations in a region of Sweden that has many SMEs. The response rate was 13% and 25 responses are used in this analysis. Results: We find that, although there are SMEs that develop implicit software product lines and have relatively sophisticated variability structures for their solution space, the structures of the problem space and the product space have room for improvement. Conclusions: The answers in the survey indicate that SMEs are in situations where they can benefit from more structured variability management, but the awareness need to be raised. Even though the problem space similarity is high, there is little reuse and traceability activities performed. The existence of SMEs with qualified variability management and product line practices indicates that small organizations are capable to apply such practices. Â© 2009 Elsevier B.V. All rights reserved.},
  comment       = {11},
  document_type = {Article},
  doi           = {10.1016/j.infsof.2009.10.009},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-77049124496&doi=10.1016%2fj.infsof.2009.10.009&partnerID=40&md5=c6c79551ab7aa0aaf04e6ead9774ed78},
}

@Article{Rabiser2010,
  author        = {Rabiser, R. and GrÃ¼nbacher, P. and Dhungana, D.},
  title         = {Requirements for product derivation support: Results from a systematic literature review and an expert survey},
  journal       = {Information and Software Technology},
  year          = {2010},
  volume        = {52},
  number        = {3},
  pages         = {324-346},
  note          = {cited By 70},
  __markedentry = {[mac:]},
  abstract      = {Context: An increasing number of publications in product line engineering address product derivation, i.e., the process of building products from reusable assets. Despite its importance, there is still no consensus regarding the requirements for product derivation support. Objective: Our aim is to identify and validate requirements for tool-supported product derivation. Method: We identify the requirements through a systematic literature review and validate them with an expert survey. Results: We discuss the resulting requirements and provide implementation examples from existing product derivation approaches. Conclusions: We conclude that key requirements are emerging in the research literature and are also considered relevant by experts in the field. Â© 2009 Elsevier B.V. All rights reserved.},
  comment       = {23},
  document_type = {Article},
  doi           = {10.1016/j.infsof.2009.11.001},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-75449091148&doi=10.1016%2fj.infsof.2009.11.001&partnerID=40&md5=dd02663c17d0c088444aea71f6824956},
}

@Article{Sanen2010,
  author        = {Sanen, F. and Truyen, E. and Joosen, W.},
  title         = {Mapping problem-space to solution-space features: A feature interaction approach},
  journal       = {ACM SIGPLAN Notices},
  year          = {2010},
  volume        = {45},
  number        = {2},
  pages         = {167-176},
  note          = {cited By 1},
  __markedentry = {[mac:]},
  abstract      = {Mapping problem-space features into solution-space features is a fundamental configuration problem in software product line engineering. A configuration problem is defined as generating the most optimal combination of software features given a requirements specification and given a set of configuration rules. Current approaches however provide little support for expressing complex configuration rules between problem and solution space that support incomplete requirements specifications. In this paper, we propose an approach to model complex configuration rules based on a generalization of the concept of problem-solution feature interactions. These are interactions between solution-space features that only arise in specific problem contexts. The use of an existing tool to support our approach is also discussed: we use the DLV answer set solver to express a particular configuration problem as a logic program whose answer set corresponds to the optimal combinations of solution-space features. We motivate and illustrate our approach with a case study in the field of managing dynamic adaptations in distributed software, where the goal is to generate an optimal protocol for accommodating a given adaptation. Copyright Â© 2009 ACM.},
  comment       = {10},
  document_type = {Conference Paper},
  doi           = {10.1145/1837852.1621633},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-77957578691&doi=10.1145%2f1837852.1621633&partnerID=40&md5=cb98c2a724d5c725309d676a4c180ff3},
}

@Article{Bosch2010,
  author        = {Bosch, J. and Bosch-Sijtsema, P.},
  title         = {From integration to composition: On the impact of software product lines, global development and ecosystems},
  journal       = {Journal of Systems and Software},
  year          = {2010},
  volume        = {83},
  number        = {1},
  pages         = {67-76},
  note          = {cited By 133},
  __markedentry = {[mac:]},
  abstract      = {Three trends accelerate the increase in complexity of large-scale software development, i.e. software product lines, global development and software ecosystems. For the case study companies we studied, these trends caused several problems, which are organized around architecture, process and organization, and the problems are related to the efficiency and effectiveness of software development as these companies used too integration-centric approaches. We present five approaches to software development, organized from integration-centric to composition-oriented and describe the areas of applicability. Â© 2009 Elsevier Inc. All rights reserved.},
  comment       = {10},
  document_type = {Article},
  doi           = {10.1016/j.jss.2009.06.051},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-71649103965&doi=10.1016%2fj.jss.2009.06.051&partnerID=40&md5=e61ab6fe4b0836bab041045ed02ed7bc},
}

@Article{Freeman2010,
  author        = {Freeman, G. and Batory, D. and Lavender, G. and Sarvela, J.N.},
  title         = {Lifting transformational models of product lines: A case study},
  journal       = {Software and Systems Modeling},
  year          = {2010},
  volume        = {9},
  number        = {3},
  pages         = {359-373},
  note          = {cited By 3},
  __markedentry = {[mac:]},
  abstract      = {Model driven engineering (MDE) of software product lines (SPLs) merges two increasing important paradigms that synthesize programs by transformation. MDE creates programs by transforming models, and SPLs elaborate programs by applying transformations called features. In this paper, we present the design and implementation of a transformational model of a product line of scalar vector graphics and JavaScript applications. We explain how we simplified our implementation by lifting selected features and their compositions from our original product line (whose implementations were complex) to features and their compositions of another product line (whose specifications were simple). We used operators to map higher-level features and their compositions to their lower-level counterparts. Doing so exposed commuting relationships among feature compositions in both product lines that helped validate our model and implementation. Â© 2009 Springer-Verlag.},
  comment       = {15},
  document_type = {Article},
  doi           = {10.1007/s10270-009-0131-6},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-77953539738&doi=10.1007%2fs10270-009-0131-6&partnerID=40&md5=fde2b5867d497d79784a24a757a1badb},
}

@Article{Khurum2009,
  author        = {Khurum, M. and Gorschek, T.},
  title         = {A systematic review of domain analysis solutions for product lines},
  journal       = {Journal of Systems and Software},
  year          = {2009},
  volume        = {82},
  number        = {12},
  pages         = {1982-2003},
  note          = {cited By 40},
  __markedentry = {[mac:]},
  abstract      = {Domain analysis is crucial and central to software product line engineering (SPLE) as it is one of the main instruments to decide what to include in a product and how it should fit in to the overall software product line. For this reason many domain analysis solutions have been proposed both by researchers and industry practitioners. Domain analysis comprises various modeling and scoping activities. This paper presents a systematic review of all the domain analysis solutions presented until 2007. The goal of the review is to analyze the level of industrial application and/or empirical validation of the proposed solutions with the purpose of mapping maturity in terms of industrial application, as well as to what extent proposed solutions might have been evaluated in terms of usability and usefulness. The finding of this review indicates that, although many new domain analysis solutions for software product lines have been proposed over the years, the absence of qualitative and quantitative results from empirical application and/or validation makes it hard to evaluate the potential of proposed solutions with respect to their usability and/or usefulness for industry adoption. The detailed results of the systematic review can be used by individual researchers to see large gaps in research that give opportunities for future work, and from a general research perspective lessons can be learned from the absence of validation as well as from good examples presented. From an industry practitioner view, the results can be used to gauge as to what extent solutions have been applied and/or validated and in what manner, both valuable as input prior to industry adoption of a domain analysis solution. Â© 2009 Elsevier Inc. All rights reserved.},
  comment       = {22},
  document_type = {Article},
  doi           = {10.1016/j.jss.2009.06.048},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-71749101571&doi=10.1016%2fj.jss.2009.06.048&partnerID=40&md5=918b8e050edb9174e0efeaeb1f2290ee},
}

@Article{Nunes2009a,
  author        = {Nunes, C. and Kulesza, U. and Sant'Anna, C. and Nunes, I. and Garcia, A. and Lucena, C.},
  title         = {Assessment of the design modularity and stability of multi-agent system product lines},
  journal       = {Journal of Universal Computer Science},
  year          = {2009},
  volume        = {15},
  number        = {11},
  pages         = {2254-2283},
  note          = {cited By 7},
  __markedentry = {[mac:]},
  abstract      = {A multi-agent system product line (MAS-PL) defines an architecture whose design and implementation is accomplished using software agents to address its common and variable features. MAS-PL promotes the large-scale reuse of common and variable agency features across multiple MAS applications. The development of MAS-PLs can be achieved through MAS-specific platforms and implementation techniques, such as conditional compilation and aspect-oriented programming (AOP). However, there is not much evidence on how these techniques provide better modularity, allowing the conception of stable MAS-PL designs. This paper presents a quantitative study on the design modularity and stability of an evolving MAS-PL. The MAS-PL was built following the reactive product line adoption approach. The product line was developed and evolved based on several versions of a conference management web-based system, named Expert Committee (EC). Our evaluation is made through a series of change scenarios related to new agency features, which are agent characteristics that enhance the system with autonomous behavior. The quantitative study consists of a systematic comparison between two different versions of the EC MAS-PL based on a MAS-specific platform, called JADE. One version was implemented with object-oriented and conditional compilation techniques. The other one relied on AOP. Our analysis was driven by well-known modularity and change impact metrics.},
  comment       = {30},
  document_type = {Article},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-70350176494&partnerID=40&md5=4c31d9416f2203717f92a4ce9001eca2},
}

@Article{Eriksson2009,
  author        = {Eriksson, M. and BÃ¶rstler, J. and Borg, K.},
  title         = {Managing requirements specifications for product lines - An approach and industry case study},
  journal       = {Journal of Systems and Software},
  year          = {2009},
  volume        = {82},
  number        = {3},
  pages         = {435-447},
  note          = {cited By 33},
  __markedentry = {[mac:]},
  abstract      = {Software product line development has emerged as a leading approach for software reuse. This paper describes an approach to manage natural-language requirements specifications in a software product line context. Variability in such product line specifications is modeled and managed using a feature model. The proposed approach has been introduced in the Swedish defense industry. We present a multiple-case study covering two different product lines with in total eight product instances. These were compared to experiences from previous projects in the organization employing clone-and-own reuse. We conclude that the proposed product line approach performs better than clone-and-own reuse of requirements specifications in this particular industrial context. Â© 2008 Elsevier Inc. All rights reserved.},
  comment       = {13},
  document_type = {Article},
  doi           = {10.1016/j.jss.2008.07.046},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-61349133483&doi=10.1016%2fj.jss.2008.07.046&partnerID=40&md5=0a3ae64dcba52802d30b67dc41cef4f1},
}

@Article{Kofron2009,
  author        = {KofroÅˆ, J. and PlÃ¡Å¡il, F. and Å erÃ½, O.},
  title         = {Modes in component behavior specification via EBP and their application in product lines},
  journal       = {Information and Software Technology},
  year          = {2009},
  volume        = {51},
  number        = {1},
  pages         = {31-41},
  note          = {cited By 5},
  __markedentry = {[mac:]},
  abstract      = {The concept of software product lines (SPL) is a modern approach to software development simplifying construction of related variants of a product thus lowering development costs and shortening time-to-market. In SPL, software components play an important role. In this paper, we show how the original idea of component mode can be captured and further developed in behavior specification via the formalism of extended behavior protocols (EBP). Moreover, we demonstrate how the modes in behavior specification can be used for modeling behavior of an entire product line. The main benefits include (i) the existence of a single behavior specification capturing the behavior of all product variants, and (ii) automatic verification of absence of communication errors among the cooperating components taking the variability into account. These benefits are demonstrated on a part of a non-trivial case study. Â© 2008 Elsevier B.V. All rights reserved.},
  comment       = {11},
  document_type = {Article},
  doi           = {10.1016/j.infsof.2008.09.011},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-56649114354&doi=10.1016%2fj.infsof.2008.09.011&partnerID=40&md5=985fa698b6816ece343555191796cd75},
}

@Article{Burgareli2008,
  author        = {Burgareli, L.A. and Melnikoff, S.S.S. and Ferreira, M.G.V.},
  title         = {A variability management strategy for software product lines of Brazilian satellite luncher vehicles},
  journal       = {Studies in Computational Intelligence},
  year          = {2008},
  volume        = {150},
  pages         = {1-14},
  note          = {cited By 1},
  __markedentry = {[mac:]},
  abstract      = {The Product Line approach offers to the software development benefits such as savings, large-scale productivity and increased product quality. The management of variability is a key and challenging issue in the development of the software product line and product derivation. This work presents a strategy for the variability management for software product line of Brazilian Satellite Launcher Vehicles. After modeling the variability, extracting them from use case diagrams and features, the proposed strategy uses a variation mechanism based on a set of Adaptive Design Patterns as support in the creation of variants. The proposed strategy uses as case study the software system of an existing specific vehicle, the Brazilian Satellite Launcher (BSL). Â© Springer-Verlag Berlin Heidelberg 2008.},
  comment       = {14},
  document_type = {Conference Paper},
  doi           = {10.1007/978-3-540-70561-1_1},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-59549088931&doi=10.1007%2f978-3-540-70561-1_1&partnerID=40&md5=48b5f80ab238e0fb937c5c2af0267953},
}

@Article{Batory2008,
  author        = {Batory, D. and BÃ¶rger, E.},
  title         = {Modularizing theorems for software product lines: The jbook case study},
  journal       = {Journal of Universal Computer Science},
  year          = {2008},
  volume        = {14},
  number        = {12},
  pages         = {2059-2082},
  note          = {cited By 25},
  __markedentry = {[mac:]},
  abstract      = {A gvvoal of software product lines is the economical assembly of programs in a family of programs. In this paper, we explore how theorems about program properties may be integrated into feature-based development of software product lines. As a case study, we analyze an existing Java/JVM compilation correctness proof for defining, interpreting, compiling, and executing bytecode for the Java language. We show how features modularize program source, theorem statements and their proofs. By composing features, the source code, theorem statements and proofs for a program are assembled. The investigation in this paper reveals a striking similarity of the refinement concepts used in Abstract State Machines (ASM) based system development and Feature-Oriented Programming (FOP) of software product lines. We suggest to exploit this observation for a fruitful interaction of researchers in the two communities. Â© J.UCS.},
  comment       = {24},
  document_type = {Article},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-55249084004&partnerID=40&md5=05f8438cdd2224729c25937b88e8b009},
}

@Article{Ahmed2008d,
  author        = {Ahmed, F. and Capretz, L.F.},
  title         = {The software product line architecture: An empirical investigation of key process activities},
  journal       = {Information and Software Technology},
  year          = {2008},
  volume        = {50},
  number        = {11},
  pages         = {1098-1113},
  note          = {cited By 20},
  __markedentry = {[mac:]},
  abstract      = {Software architecture has been a key area of concern in software industry due to its profound impact on the productivity and quality of software products. This is even more crucial in case of software product line, because it deals with the development of a line of products sharing common architecture and having controlled variability. The main contributions of this paper is to increase the understanding of the influence of key software product line architecture process activities on the overall performance of software product line by conducting a comprehensive empirical investigation covering a broad range of organizations currently involved in the business of software product lines. This is the first study to empirically investigate and demonstrate the relationships between some of the software product line architecture process activities and the overall software product line performance of an organization at the best of our knowledge. The results of this investigation provide empirical evidence that software product line architecture process activities play a significant role in successfully developing and managing a software product line. Â© 2007 Elsevier B.V. All rights reserved.},
  comment       = {16},
  document_type = {Article},
  doi           = {10.1016/j.infsof.2007.10.013},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-49549123847&doi=10.1016%2fj.infsof.2007.10.013&partnerID=40&md5=63a5afabfc5f33af279aa8e53b82a03c},
}

@Article{Fantinato2008,
  author        = {Fantinato, M. and De Toledo, M.B.F. and De Souza Gimenes, I.M.},
  title         = {WS-contract establishment with QOS:},
  journal       = {International Journal of Cooperative Information Systems},
  year          = {2008},
  volume        = {17},
  number        = {3},
  pages         = {373-407},
  note          = {cited By 23},
  __markedentry = {[mac:]},
  abstract      = {Electronic contracts describe inter-organizational business processes in terms of supply and consumption of electronic services (commonly Web services). The establishment of e-contracts in a particular business domain usually involves a set of well-defined common and variable properties. These properties are not fully exploited by the existing e-contract establishment approaches. Feature modeling is a software engineering technique that has been widely used for capturing and managing commonalities and variabilities of product families in the context of software product line. This paper presents a feature-based approach to support Web services e-contract (WS-contract) establishment. The approach aims at improving the information structure and reuse of WS-contracts, including the QoS attributes. Features are used to represent possible WS-contract elements in order to drive WS-contract template instantiation, thus acting as a configuration space manager. A toolkit named FeatureContract was developed to automatically support the proposed approach. A case study was carried out within the telecom context to show the approach feasibility. Â© 2008 World Scientific Publishing Company.},
  comment       = {35},
  document_type = {Conference Paper},
  doi           = {10.1142/S0218843008001889},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-50949123633&doi=10.1142%2fS0218843008001889&partnerID=40&md5=5b4ae3194ddd2d62e7c64661813e9a4f},
}

@Article{Sellier2008,
  author        = {Sellier, D. and Mannion, M. and Mansell, J.X.},
  title         = {Managing requirements inter-dependency for software product line derivation},
  journal       = {Requirements Engineering},
  year          = {2008},
  volume        = {13},
  number        = {4},
  pages         = {299-313},
  note          = {cited By 6},
  __markedentry = {[mac:]},
  abstract      = {Software Product Line Engineering (SPLE) can reduce software development costs, reduce time to market and improve product quality. A software product line is a set of software products sharing a set of common features but containing variation points. Successful SPLE requires making selection decisions at variation points effectively and efficiently. A significant challenge is how to identify, represent and manage the inter-dependency of selection decisions for requirements. We developed the concept of a meta-model for requirement decision models to bring formalism and consistency to the structure and to model inter-dependencies between requirement selection decisions. Here we present a meta-model for requirement selection decisions that includes inter-dependencies and we use a mobile phone worked example to illustrate our approach. To support our method, we developed two separate tools, V-Define (for domain decision model construction) and V-Resolve (for new product derivation). Finally the results of a metal processing product line case study using the tools are described. Â© Springer-Verlag London Limited 2008.},
  comment       = {15},
  document_type = {Article},
  doi           = {10.1007/s00766-008-0066-4},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-54249094126&doi=10.1007%2fs00766-008-0066-4&partnerID=40&md5=32aa8c667a85021263ab3ed83b09389b},
}

@Article{Mellado2008d,
  author        = {Mellado, D. and FernÃ¡ndez-Medina, E. and Piattini, M.},
  title         = {Towards security requirements management for software product lines: A security domain requirements engineering process},
  journal       = {Computer Standards and Interfaces},
  year          = {2008},
  volume        = {30},
  number        = {6},
  pages         = {361-371},
  note          = {cited By 25},
  __markedentry = {[mac:]},
  abstract      = {Security and requirements engineering are one of the most important factors of success in the development of a software product line due to the complexity and extensive nature of them, given that a weakness in security can cause problems throughout the products of a product line. The main contribution of this work is that of providing a security standard-based process for software product line development, which is an add-in of activities in the domain engineering. This process deals with security requirements from the early stages of the product line lifecycle in a systematic and intuitive way especially adapted for product line based development. It is based on the use of the latest security requirements techniques, together with the integration of the Common Criteria (ISO/IEC 15408) and the ISO/IEC 17799 controls into the product line lifecycle. Additionally, it deals with security artefacts variability and traceability, providing us with a Security Core Assets Repository. Moreover, it facilitates the conformance to the most relevant security standards with regard to the management of security requirements, such as ISO/IEC 27001 and ISO/IEC 17799. Finally, we will illustrate our proposed process by describing part of a real case study, as a preliminary validation of it. Â© 2008 Elsevier B.V. All rights reserved.},
  comment       = {11},
  document_type = {Article},
  doi           = {10.1016/j.csi.2008.03.004},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-44949241673&doi=10.1016%2fj.csi.2008.03.004&partnerID=40&md5=ec78211e6b7f4d0935b70d5d7c220c10},
}

@Article{Ramos2008,
  author        = {Ramos, M.A. and Penteado, R.A.D.},
  title         = {Embedded software revitalization through component mining and software product line techniques},
  journal       = {Journal of Universal Computer Science},
  year          = {2008},
  volume        = {14},
  number        = {8},
  pages         = {1207-1227},
  note          = {cited By 4},
  __markedentry = {[mac:]},
  abstract      = {The mining of generic software components from legacy systems can be used as an auxiliary technique to revitalize systems. This paper presents a software maintenance approach that uses such technique to revitalize one or more embedded legacy systems simultaneously and, in addition, create a core of reusable assets that can be used to support the development of new similar products. Software Product Line techniques are used to support the tasks of domain modelling and software component development. A real case study in the domain of Point of Sale (POS) terminals is presented and it illustrates the use of the proposed approach to revitalize three similar embedded legacy systems, simultaneously. It also shows how it is possible, through the created core of reusable assets, to deliver variations of these systems to meet the requirements of a wide family of POS terminals with different hardware configurations. Â© J.UCS.},
  comment       = {21},
  document_type = {Conference Paper},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-46049088149&partnerID=40&md5=3196ea29d6331ce88e4ea332ebd0a55a},
}

@Article{Hanssen2008,
  author        = {Hanssen, G.K. and FÃ¦gri, T.E.},
  title         = {Process fusion: An industrial case study on agile software product line engineering},
  journal       = {Journal of Systems and Software},
  year          = {2008},
  volume        = {81},
  number        = {6},
  pages         = {843-854},
  note          = {cited By 49},
  __markedentry = {[mac:]},
  abstract      = {This paper presents a case study of a software product company that has successfully integrated practices from software product line engineering and agile software development. We show how practices from the two fields support the company's strategic and tactical ambitions, respectively. We also discuss how the company integrates strategic, tactical and operational processes to optimize collaboration and consequently improve its ability to meet market needs, opportunities and challenges. The findings from this study are relevant to software product companies seeking ways to balance agility and product management. The findings also contribute to research on industrializing software engineering. Â© 2007 Elsevier Inc. All rights reserved.},
  comment       = {12},
  document_type = {Article},
  doi           = {10.1016/j.jss.2007.10.025},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-42049088680&doi=10.1016%2fj.jss.2007.10.025&partnerID=40&md5=70bc5b3b44f009b164aebff9608f2e82},
}

@Article{Trinidad2008,
  author        = {Trinidad, P. and Benavides, D. and DurÃ¡n, A. and Ruiz-CortÃ©s, A. and Toro, M.},
  title         = {Automated error analysis for the agilization of feature modeling},
  journal       = {Journal of Systems and Software},
  year          = {2008},
  volume        = {81},
  number        = {6},
  pages         = {883-896},
  note          = {cited By 70},
  __markedentry = {[mac:]},
  abstract      = {Software Product Lines (SPL) and agile methods share the common goal of rapidly developing high-quality software. Although they follow different approaches to achieve it, some synergies can be found between them by (i) applying agile techniques to SPL activities so SPL development becomes more agile; and (ii) tailoring agile methodologies to support the development of SPL. Both options require an intensive use of feature models, which are usually strongly affected by changes on requirements. Changing large-scale feature models as a consequence of changes on requirements is a well-known error-prone activity. Since one of the objectives of agile methods is a rapid response to changes in requirements, it is essential an automated error analysis support in order to make SPL development more agile and to produce error-free feature models. As a contribution to find the intended synergies, this article sets the basis to provide an automated support to feature model error analysis by means of a framework which is organized in three levels: a feature model level, where the problem of error treatment is described; a diagnosis level, where an abstract solution that relies on Reiter's theory of diagnosis is proposed; and an implementation level, where the abstract solution is implemented by using Constraint Satisfaction Problems (CSP). To show an application of our proposal, a real case study is presented where the Feature-Driven Development (FDD) methodology is adapted to develop an SPL. Current proposals on error analysis are also studied and a comparison among them and our proposal is provided. Lastly, the support of new kinds of errors and different implementation levels for the proposed framework are proposed as the focus of our future work. Â© 2007 Elsevier Inc. All rights reserved.},
  comment       = {14},
  document_type = {Article},
  doi           = {10.1016/j.jss.2007.10.030},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-42049110531&doi=10.1016%2fj.jss.2007.10.030&partnerID=40&md5=37e7cc86948271c403360a1535f92809},
}

@Article{Apel2008a,
  author        = {Apel, S. and Leich, T. and Saake, G.},
  title         = {Aspectual feature modules},
  journal       = {IEEE Transactions on Software Engineering},
  year          = {2008},
  volume        = {34},
  number        = {2},
  pages         = {162-180},
  note          = {cited By 117},
  __markedentry = {[mac:]},
  abstract      = {Two programming paradigms are gaining attention in the overlapping fields of software product lines (SPLs) and incremental software development (ISD). Feature-oriented programming (FOP) aims at large-scale compositional programming and feature modularity in SPLs using ISD. Aspect-oriented programming (AOP) focuses on the modularization of crosscutting concerns in complex software. While feature modules, the main abstraction mechanisms of FOP, perform well in implementing large-scale software building blocks, they are incapable of modularizing certain kinds of crosscutting concerns. This weakness is exactly the strength of aspects, the main abstraction mechanisms of AOP. In this article we contribute a systematic evaluation and comparison of FOP and AOP. It reveals that aspects and feature modules are complementary techniques. Consequently, we propose the symbiosis of FOP and AOP and aspectual feature modules (AFMs), a programming technique that integrates feature modules and aspects. We provide a set of tools that support implementing AFMs on top of Java and C++. We apply AFMs to a non-trivial case study demonstrating their practical applicability and to justify our design choices. Â© 2008 IEEE.},
  comment       = {19},
  document_type = {Article},
  doi           = {10.1109/TSE.2007.70770},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-42549143913&doi=10.1109%2fTSE.2007.70770&partnerID=40&md5=ca2f4187fd47f2bb362fb20b2d669b4b},
}

@Article{Niemelae2007,
  author        = {NiemelÃ¤, E. and Immonen, A.},
  title         = {Capturing quality requirements of product family architecture},
  journal       = {Information and Software Technology},
  year          = {2007},
  volume        = {49},
  number        = {11-12},
  pages         = {1107-1120},
  note          = {cited By 37},
  __markedentry = {[mac:]},
  abstract      = {Software quality is one of the major issues with software intensive systems. Moreover, quality is a critical success factor in software product families exploiting shared architecture and common components in a set of products. Our contribution is the QRF (Quality Requirements of a software Family) method, which explicitly focuses on how quality requirements have to be defined, represented and transformed to architectural models. The method has been applied to two experiments; one in a laboratory environment and the other in industry. The use of the QRF method is exemplified by the Distribution Service Platform (DiSeP), the laboratory experiment. The lessons learned are also based on our experiences of applying the method in industrial settings. Â© 2006 Elsevier B.V. All rights reserved.},
  comment       = {14},
  document_type = {Article},
  doi           = {10.1016/j.infsof.2006.11.003},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-34848888028&doi=10.1016%2fj.infsof.2006.11.003&partnerID=40&md5=e13e12d15019d1467a408d04910818f6},
}

@Article{Ahmed2007,
  author        = {Ahmed, F. and Capretz, L.F. and Sheikh, S.A.},
  title         = {Institutionalization of software product line: An empirical investigation of key organizational factors},
  journal       = {Journal of Systems and Software},
  year          = {2007},
  volume        = {80},
  number        = {6},
  pages         = {836-849},
  note          = {cited By 17},
  __markedentry = {[mac:]},
  abstract      = {A good fit between the person and the organization is essential in a better organizational performance. This is even more crucial in case of institutionalization of a software product line practice within an organization. Employees' participation, organizational behavior and management contemplation play a vital role in successfully institutionalizing software product lines in a company. Organizational dimension has been weighted as one of the critical dimensions in software product line theory and practice. A comprehensive empirical investigation to study the impact of some organizational factors on the performance of software product line practice is presented in this work. This is the first study to empirically investigate and demonstrate the relationships between some of the key organizational factors and software product line performance of an organization. The results of this investigation provide empirical evidence and further support the theoretical foundations that in order to institutionalize software product lines within an organization, organizational factors play an important role. Â© 2006 Elsevier Inc. All rights reserved.},
  comment       = {14},
  document_type = {Article},
  doi           = {10.1016/j.jss.2006.09.010},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-33947384858&doi=10.1016%2fj.jss.2006.09.010&partnerID=40&md5=bbde19a109b7058e813ca72d3401c217},
}

@Article{Olumofin2007,
  author        = {Olumofin, F.G. and MiÅ¡iÄ‡, V.B.},
  title         = {A holistic architecture assessment method for software product lines},
  journal       = {Information and Software Technology},
  year          = {2007},
  volume        = {49},
  number        = {4},
  pages         = {309-323},
  note          = {cited By 17},
  __markedentry = {[mac:]},
  abstract      = {The success of architecture-centric development of software product lines is critically dependent upon the availability of suitable architecture assessment methods. While a number of architecture assessment methods are available and some of them have been widely used in the process of evaluating single product architectures, none of them is equipped to deal with the main challenges of product line development. In this paper we present an adaptation of the Architecture Tradeoff Analysis Method (ATAM) for the task of assessing product line architectures. The new method, labeled Holistic Product Line Architecture Assessment (HoPLAA), uses a holistic approach that focuses on risks and quality attribute tradeoffs - not only for the common product line architecture, but for the individual product architectures as well. In addition, it prescribes a qualitative analytical treatment of variation points using scenarios. The use of the new method is illustrated through a case study. Â© 2006 Elsevier B.V. All rights reserved.},
  comment       = {15},
  document_type = {Article},
  doi           = {10.1016/j.infsof.2006.05.003},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-33846550580&doi=10.1016%2fj.infsof.2006.05.003&partnerID=40&md5=3881718a2b055a96a89780dbdce15ba3},
}

@Article{Schobbens2007,
  author        = {Schobbens, P.-Y. and Heymans, P. and Trigaux, J.-C. and Bontemps, Y.},
  title         = {Generic semantics of feature diagrams},
  journal       = {Computer Networks},
  year          = {2007},
  volume        = {51},
  number        = {2},
  pages         = {456-479},
  note          = {cited By 178},
  __markedentry = {[mac:]},
  abstract      = {Feature Diagrams (FDs) are a family of popular modelling languages used to address the feature interaction problem, particularly in software product lines, FDs were first introduced by Kang as part of the FODA (Feature-Oriented Domain Analysis) method back in 1990. Afterwards, various extensions of FODA FDs were introduced to compensate for a purported ambiguity and lack of precision and expressiveness. However, they never received a formal semantics, which is the hallmark of precision and unambiguity and a prerequisite for efficient and safe tool automation. The reported work is intended to contribute a more rigorous approach to the definition, understanding, evaluation, selection and implementation of FD languages. First, we provide a survey of FD variants. Then, we give them a formal semantics, thanks to a generic construction that we call Free Feature Diagrams (FFDs). This demonstrates that FDs can be precise and unambiguous. This also defines their expressiveness. Many variants are expressively complete, and thus the endless quest for extensions actually cannot be justified by expressiveness. A finer notion is thus needed to compare these expressively complete languages. Two solutions are well-established: succinctness and embeddability, that express the naturalness of a language. We show that the expressively complete FDs fall into two succinctness classes, of which we of course recommend the most succinct. Among the succinct expressively complete languages, we suggest a new, simple one that is not harmfully redundant: Varied FD (VFD). Finally, we study the execution time that tools will need to solve useful problems in these languages. Â© 2006 Elsevier B.V. All rights reserved.},
  comment       = {24},
  document_type = {Article},
  doi           = {10.1016/j.comnet.2006.08.008},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-33750723486&doi=10.1016%2fj.comnet.2006.08.008&partnerID=40&md5=f069b02d12f436e866959ebaf1a38eb4},
}

@Article{Marew2007,
  author        = {Marew, T. and Kim, J. and Bae, D.H.},
  title         = {Systematic functional decomposition in a product line using aspect-oriented software development: A case study},
  journal       = {International Journal of Software Engineering and Knowledge Engineering},
  year          = {2007},
  volume        = {17},
  number        = {1},
  pages         = {33-55},
  note          = {cited By 2},
  __markedentry = {[mac:]},
  abstract      = {Systematic configuration management is important for successful software product lines. We can use aspect-oriented software development to decompose software product lines based on features that can ease configuration management. In this paper, we present a military maintenance product line that employs such strategy. In particular, we applied a specific approach, feature based modeling (FBM), in the construction of the system. We have extended FBM to address properties specific to product line. We will discuss the advantages of FBM when applied to product lines. Such gains include the functional decomposition of the system along user requirements (features) as aspects. Moreover, those features exhibit unidirectional dependency (i.e. among any two features, at most one depend on another) that enables developers to analyze the effect of any modification they may make on any feature. In addition, any variations can be captured as aspects which can also be incorporated easily into the core asset if such variation is deemed to be important enough to be included in the product line for further evolution. Â© World Scientific Publishing Company.},
  comment       = {23},
  document_type = {Conference Paper},
  doi           = {10.1142/S0218194007003112},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-33947683339&doi=10.1142%2fS0218194007003112&partnerID=40&md5=1643033c2adca039352c12f0b2e89a0e},
}

@Article{Mohan2006,
  author        = {Mohan, K. and Ramesh, B.},
  title         = {Change management patterns in software product lines},
  journal       = {Communications of the ACM},
  year          = {2006},
  volume        = {49},
  number        = {12},
  note          = {cited By 10},
  __markedentry = {[mac:]},
  abstract      = {A case study in a software product line (SPL) development for identifying patterns of change management and to suggest effective practices was investigated. Three commonly faced patterns of changes incorporated in product lines and change management practices that mitigate their adverse effects are independencies among changes in variants, increasing the degree of evolving variance, and reinvented variations. Indepencies among change incorporated in different variants pose a problem in product line evolution, increasing variance results in numerous constraints across variation points, and it is often difficult to leverage existing implementations of variants. Some recommendations for change management include modularizing of change and variation points, tracking the scope and life of the variations, and facilitating reuse based on knowledge sharing.},
  art_number    = {1183269},
  document_type = {Article},
  doi           = {10.1145/1183236.1183269},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-33751555290&doi=10.1145%2f1183236.1183269&partnerID=40&md5=a9e3d542177ea0d01a7f3d8e077727c5},
}

@Article{DelRosso2006,
  author        = {Del Rosso, C.},
  title         = {Continuous evolution through software architecture evaluation: A case study},
  journal       = {Journal of Software Maintenance and Evolution},
  year          = {2006},
  volume        = {18},
  number        = {5},
  pages         = {351-383},
  note          = {cited By 25},
  __markedentry = {[mac:]},
  abstract      = {The need for software architecture evaluation is based on the realization that software development, like all engineering disciplines, is a process of continuous modeling and refinement. Detecting architectural problems before the bulk of development work is done allows re-architecting activities to take place in due time, without having to rework what has already been done. At the same time, tuning activities allow software performance to be enhanced and maintained during the software lifetime. When dealing with product families, architectural evaluations have an even more crucial role: The evaluations are targeted to a set of common products. We have tried different approaches to software assessments with our mobile phone software, an embedded real-time software platform, which must support an increasingly large number of different product variants. In this paper, we present a case study and discuss the experiences gained with three different assessment techniques that we have worked on during the past five years. The assessment techniques presented include scenario-based software architecture assessment, software performance assessment and experience-based assessment. The various evaluation techniques are complementary and, when used together, constitute a tool which a software architect must be aware of in order to maintain and evolve a large software intensive system. Copyright Â© 2006 John Wiley & Sons, Ltd.},
  comment       = {33},
  document_type = {Article},
  doi           = {10.1002/smr.337},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-33750561906&doi=10.1002%2fsmr.337&partnerID=40&md5=c7f0eeb67752e7e05b75dad84bdd4b82},
}

@Article{Kolb2006,
  author        = {Kolb, R. and Muthig, D. and Patzke, T. and Yamauchi, K.},
  title         = {Refactoring a legacy component for reuse in a software product line: A case study},
  journal       = {Journal of Software Maintenance and Evolution},
  year          = {2006},
  volume        = {18},
  number        = {2},
  pages         = {109-132},
  note          = {cited By 36},
  __markedentry = {[mac:]},
  abstract      = {Product lines are a promising approach to improve conceptually the productivity of the software development process and thus to reduce both the cost and time of developing and maintaining increasingly complex systems. An important issue in the adoption of the product-line approach is the migration of legacy software components, which have not been designed for reuse, systematically into reusable product-line components. This article describes activities performed to improve systematically the design and implementation of an existing software component in order to reuse it in a software product line. The activities are embedded in the application of Fraunhofer PuLSEâ„¢-DSSA - an approach for defining domain-specific software architectures (DSSA) and product-line architectures. The component under investigation is the so-called Image Memory Handler (IMH), which is used in Ricoh's current products of office appliances such as copier machines, printers, and multi-functional peripherals. It is responsible for controlling memory usage and compressing and decompressing image data. Improvement of both the component's design and implementation are based on a systematic analysis and focused on increasing maintainability and reusability and hence suitability for use in a product line. As a result of the analysis and refactoring activities, the documentation and implementation of the component has been considerably improved as shown by quantitative data collected at the end of the activities. Despite a number of changes to the code, the external behavior of the component has been preserved without significantly affecting the performance. Copyright Â© 2006 John Wiley & Sons, Ltd.},
  comment       = {24},
  document_type = {Conference Paper},
  doi           = {10.1002/smr.329},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-33646408526&doi=10.1002%2fsmr.329&partnerID=40&md5=501ee942227e40e04ac86fcd865112dd},
}

@Article{Lee2006a,
  author        = {Lee, S.C.},
  title         = {Modeling Variant User Interfaces for Web-Based Software Product Lines},
  journal       = {International Journal of Information Technology and Web Engineering (IJITWE)},
  year          = {2006},
  volume        = {1},
  number        = {1},
  pages         = {1-34},
  note          = {cited By 0},
  __markedentry = {[mac:]},
  abstract      = {Software product line (SPL) is a software engineering paradigm for software development. SPL is important in promoting software reuse, leading to higher productivity and quality. A software product within a product line often has specific functionalities that are not common to all other products within the product line. Those specific functionalities are termed â€œvariant featuresâ€ in a product line. SPL paradigm involves the modeling of variant features. However, little work in SPL investigates and addresses the modeling of variant features specific to UI. UML is the de facto modeling language for object-oriented software systems. It is known that UML needs better support in modeling UIs. Thus, much research developed UML extensions to improve UML support in modeling UIs. Yet little of this work is related to developing such extensions for modeling UIs for SPLs in which variant features specific to user interfaces (UI) modeling must be addressed. This research develops a UML extension, WUIML, to address these problems. WUIML defines elements for modeling variant features specific to UIs for Web-based SPLs. The model elements in WUIML extend from the metaclass and of the UML2.0 metamodel. WUIML integrates the modeling of variant features specific to UIs to UML. For example, in a Web-based patient registration SPL, member products targeting British users may use British date format in the user interface, while member products targeting United States users may use United States date format in the user interface. Thus, this is a variant feature for this product line. WUIML defines a model element, XOR, to represent such exclusive or conditions in a product line user interface model. WUIML would reduce SPL engineersâ€™ efforts needed in UI development. To validate the WUIML research outcome, a case study was conducted. The results of this empirical study indicate that modeling UIs for Web-based SPLs using WUIML is more effective and efficient than using standard UML. Â© 2006, IGI Global. All rights reserved.},
  comment       = {34},
  document_type = {Article},
  doi           = {10.4018/jitwe.2006010101},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85001610191&doi=10.4018%2fjitwe.2006010101&partnerID=40&md5=48768aae9dbeec09b817e0f8045f0510},
}

@Article{Moon2005a,
  author        = {Moon, M. and Yeom, K. and Chae, H.S.},
  title         = {An approach to developing domain requirements as a core asset based on commonality and variability analysis in a product line},
  journal       = {IEEE Transactions on Software Engineering},
  year          = {2005},
  volume        = {31},
  number        = {7},
  pages         = {551-569},
  note          = {cited By 83},
  __markedentry = {[mac:]},
  abstract      = {The methodologies of product line engineering emphasize proactive reuse to construct high-quality products more quickly that are less costly. Requirements engineering for software product families differs significantly from requirements engineering for single software products. The requirements for a product line are written for the group of systems as a whole, with requirements for individual systems specified by a delta or an increment to the generic set. Therefore, it is necessary to identify and explicitly denote the regions of commonality and points of variation at the requirements level. In this paper, we suggest a method of producing requirements that will be a core asset in the product line. We describe a process for developing domain requirements where commonality and variability in a domain are explicitly considered. A CASE environment, named DREAM, for managing commonality and valiability analysis of domain requirements is also described. We also describe a case study for an e-Travel System domain where we found that our approach to developing domain requirements based on commonality and variability analysis helped to produce domain requirements as a core asset for product lines. Â© 2005 IEEE.},
  comment       = {19},
  document_type = {Article},
  doi           = {10.1109/TSE.2005.76},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-25844465686&doi=10.1109%2fTSE.2005.76&partnerID=40&md5=51373b68aac3e74d68e205d6d1f5da7b},
}

@Article{Deelstra2005,
  author        = {Deelstra, S. and Sinnema, M. and Bosch, J.},
  title         = {Product derivation in software product families: A case study},
  journal       = {Journal of Systems and Software},
  year          = {2005},
  volume        = {74},
  number        = {2 SPEC. ISS.},
  pages         = {173-194},
  note          = {cited By 144},
  __markedentry = {[mac:]},
  abstract      = {From our experience with several organizations that employ software product families, we have learned that, contrary to popular belief, deriving individual products from shared software assets is a time-consuming and expensive activity. In this paper we therefore present a study that investigated the source of those problems. We provide the reader with a framework of terminology and concepts regarding product derivation. In addition, we present several problems and issues we identified during a case study at two large industrial organizations that are relevant to other, for example, comparable or less mature organizations. Â© 2003 Elsevier Inc. All rights reserved.},
  comment       = {22},
  document_type = {Article},
  doi           = {10.1016/j.jss.2003.11.012},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-6444234242&doi=10.1016%2fj.jss.2003.11.012&partnerID=40&md5=31f3032b01263f5e37e93d65ed64b827},
}

@Article{Raatikainen2005,
  author        = {Raatikainen, M. and Soininen, T. and MÃ¤nnistÃ¶, T. and Mattila, A.},
  title         = {Characterizing configurable software product families and their derivation},
  journal       = {Software Process Improvement and Practice},
  year          = {2005},
  volume        = {10},
  number        = {1},
  pages         = {41-60},
  note          = {cited By 10},
  __markedentry = {[mac:]},
  abstract      = {Configurable software product families are a subclass of software product families that are customized in a product individual derivation process without design or programming. This article presents results on analyzing two such families and their derivation processes found in a descriptive case study on software product families. Some characteristics particular to such product families and their derivation seem to emerge: There is a configurable product base that is configured to the customer requirements. The variability has two distinct levels, of which the first is visible to the customer at sales. The derivation process consists of requirements specification, configuring, delivery, installing, and tailoring activities. Derivation is very flexible and efficient. Most derivations can be carried out without programming. The activities primarily require application domain understanding, system administrator skills, and understanding of the family. The product developers can be separated organizationally from product derivators. We further proposed some hypotheses on the success factors for configurable software product families: A large enough number of derivations, understanding and stability of the application domain, and clear software product family-oriented culture and processes. However, very advanced or software product family-oriented software engineering methods or similar methods do not seem a prerequisite for success. We also suggest further research on configurable software product families. Copyright Â© 2005 John Wiley & Sons, Ltd.},
  comment       = {20},
  document_type = {Article},
  doi           = {10.1002/spip.211},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-15244350293&doi=10.1002%2fspip.211&partnerID=40&md5=0222d7ccafd9cfee23689d45677a6339},
}

@Article{Schwanke2004,
  author        = {Schwanke, R.W. and Lutz, R.R.},
  title         = {Experience with the architectural design of a modest product family},
  journal       = {Software - Practice and Experience},
  year          = {2004},
  volume        = {34},
  number        = {13},
  pages         = {1273-1296},
  note          = {cited By 7},
  __markedentry = {[mac:]},
  abstract      = {Many product families are modest in the sense that they consist of a sequence of incremental products with, at any point in time, only a few distinct products available and minimal variations among the products. Such product families, nevertheless, are often large, complex systems, widely deployed, and possessing stringent safety and performance requirements. This paper describes a case study that tends to confirm the value of using a product-line approach for the architectural design of a modest product family. The paper describes the process, design alternatives, and lessons learned, both positive and negative, from the architectural design of one such family of medical image analysis products. Realized benefits included identifying previously unrecognized common behavior and sets of features that were likely to change together, aligning the architecture with specific market needs and with the organization, and reducing unplanned dependencies. Most interesting were the unanticipated benefits, including decoupling the product-family architecture from the order of implementation of features, and using the product-family architecture as a 'guiding star' with subsequent releases moving toward, rather than away from, the planned architecture. Copyright Â© 2004 John Wiley & Sons, Ltd.},
  comment       = {24},
  document_type = {Article},
  doi           = {10.1002/spe.613},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-8344271997&doi=10.1002%2fspe.613&partnerID=40&md5=d475d60b8742bd21ec76e32c111f28f0},
}

@Article{Jaring2004b,
  author        = {Jaring, M. and Bosch, J.},
  title         = {Expressing product diversification - Categorizing and classifying variability in software product family engineering},
  journal       = {International Journal of Software Engineering and Knowledge Engineering},
  year          = {2004},
  volume        = {14},
  number        = {5},
  pages         = {449-470},
  note          = {cited By 3},
  __markedentry = {[mac:]},
  abstract      = {In a software product family context, software architects design architectures that support product diversification in both space (multiple contexts) and time (changing contexts). Product diversification is based on the concept of variability: a single architecture and a set of components support a family of products. Software product families have to support increasing amounts of variability, thereby making variability engineering a primary concern in software product family development. The first part of this paper (1) suggests a two-dimensional, orthogonal categorization of variability realization techniques and classifies these variability categories into system maturity levels. The second part (2) discusses a case study of an industrial software product family of mobile communication infrastructure for professional markets such as the military. The study categorizes and classifies the variability in this product family according to criteria common to virtually all software development projects.},
  comment       = {22},
  document_type = {Article},
  doi           = {10.1142/S0218194004001804},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-10044283207&doi=10.1142%2fS0218194004001804&partnerID=40&md5=d4be64864295692ddc6446ec441911ea},
}

@Article{Jaring2004c,
  author        = {Jaring, M. and Krikhaar, R.L. and Bosch, J.},
  title         = {Representing variability in a family of MRI scanners},
  journal       = {Software - Practice and Experience},
  year          = {2004},
  volume        = {34},
  number        = {1},
  pages         = {69-100},
  note          = {cited By 25},
  __markedentry = {[mac:]},
  abstract      = {Promoting software reuse is probably the most promising approach to the cost-effective development and evolution of quality software. An example of reuse is the successful adoption of software product families in industry. In a product family context, software architects anticipate product variation and design architectures that support product derivation in both space (multiple contexts) and time (changing contexts). Product derivation is based on the concept of variability: a single architecture and a set of components support a family of products. Modern software product families need to support increasing amounts of variability, leading to a situation where variability engineering becomes of primary concern. Variability is often introduced as an 'add-on' to the system without taking the consequences for more than one lifecycle phase such as design or architecture into account. This paper (1) suggests a Variability Categorization and Classification Model (VCCM) for representing variability in the software lifecycle and (2) discusses a case study of a large-scale software product family of Magnetic Resonance Imaging (MRI) scanners developed by Philips Medical Systems. The study illustrates how variability can be made an integral part of system development at different levels of abstraction. VCCM has been applied to the scanner family as an analysis tool. Copyright Â©2003 John Wiley & Sons, Ltd.},
  comment       = {32},
  document_type = {Article},
  doi           = {10.1002/spe.558},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-0346500444&doi=10.1002%2fspe.558&partnerID=40&md5=425770c6faae7e017fb677ade8fcbfdc},
}

@Article{Deursen2002,
  author        = {van Deursen, A. and Klint, P.},
  title         = {Domain-specific language design requires feature descriptions},
  journal       = {Journal of Computing and Information Technology},
  year          = {2002},
  volume        = {10},
  number        = {1},
  pages         = {1-17},
  note          = {cited By 142},
  __markedentry = {[mac:]},
  abstract      = {A domain-specific language (DSL) provides a notation tailored towards an application domain and is based on the relevant concepts and features of that domain. As such, a DSL is a means to describe and generate members of a family of programs in the domain. A prerequisite for the design of a DSL is a detailed analysis and structuring of the application domain. Graphical feature diagrams have been proposed to organize the dependencies between such features, and to indicate which ones are common to all family members and which ones vary. In this paper, we study feature diagrams in more details, as well as their relationship to domain-specific languages. We propose the Feature Description Language (FDL), a textual language to describe features. We explore automated manipulation of feature descriptions such as normalization, expansion to disjunctive normal form, variability computation and constraint satisfaction. Feature descriptions can be directly mapped to UML diagrams which in their turn can be used for Java code generation. The value of FDL is assessed via a case study in the use and expressiveness of feature descriptions for the area of documentation generators.},
  comment       = {17},
  document_type = {Article},
  doi           = {10.2498/cit.2002.01.01},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84928037990&doi=10.2498%2fcit.2002.01.01&partnerID=40&md5=71a33563d42f75dfa75532e5a936e15e},
}

@Article{Ardis2000,
  author        = {Ardis, M. and Daley, N. and Hoffman, D. and Siy, H. and Weiss, D.},
  title         = {Software product lines: A case study},
  journal       = {Software - Practice and Experience},
  year          = {2000},
  volume        = {30},
  number        = {7},
  pages         = {825-847},
  note          = {cited By 35},
  __markedentry = {[mac:]},
  abstract      = {A software product line is a family of products that share common features to meet the needs of a market area. Systematic processes have been developed to dramatically reduce the cost of a product line. Such product-line engineering processes have proven practical and effective in industrial use, but are not widely understood. The Family-Oriented Abstraction, Specification and Translation (FAST) process has been used successfully at Lucent Technologies in over 25 domains, providing productivity improvements of as much as four to one. In this paper, we show how to use FAST to document precisely the key abstractions in a domain, exploit design patterns in a generic product-line architecture, generate documentation and Java code, and automate testing to reduce costs. The paper is based on a detailed case study covering all aspects from domain analysis through testing.},
  comment       = {23},
  document_type = {Article},
  doi           = {10.1002/(SICI)1097-024X(200006)30:7<825::AID-SPE322>3.0.CO;2-1},
  source        = {Scopus},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-0033688589&doi=10.1002%2f%28SICI%291097-024X%28200006%2930%3a7%3c825%3a%3aAID-SPE322%3e3.0.CO%3b2-1&partnerID=40&md5=9146e51ed24f7399c09731f8d034fbf4},
}

@Article{Mellado2008b,
  author        = {Daniel Mellado and Eduardo FernÃ¡ndez-Medina and Mario Piattini},
  title         = {Towards security requirements management for software product lines: A security domain requirements engineering process},
  journal       = {Computer Standards \& Interfaces},
  year          = {2008},
  volume        = {30},
  number        = {6},
  pages         = {361 - 371},
  issn          = {0920-5489},
  note          = {Special Issue: State of standards in the information systems security area},
  __markedentry = {[mac:]},
  abstract      = {Security and requirements engineering are one of the most important factors of success in the development of a software product line due to the complexity and extensive nature of them, given that a weakness in security can cause problems throughout the products of a product line. The main contribution of this work is that of providing a security standard-based process for software product line development, which is an add-in of activities in the domain engineering. This process deals with security requirements from the early stages of the product line lifecycle in a systematic and intuitive way especially adapted for product line based development. It is based on the use of the latest security requirements techniques, together with the integration of the Common Criteria (ISO/IEC 15408) and the ISO/IEC 17799 controls into the product line lifecycle. Additionally, it deals with security artefacts variability and traceability, providing us with a Security Core Assets Repository. Moreover, it facilitates the conformance to the most relevant security standards with regard to the management of security requirements, such as ISO/IEC 27001 and ISO/IEC 17799. Finally, we will illustrate our proposed process by describing part of a real case study, as a preliminary validation of it.},
  comment       = {11},
  doi           = {https://doi.org/10.1016/j.csi.2008.03.004},
  keywords      = {Product lines, Common Criteria, ISO/IEC 27001, ISO/IEC 17799, Security requirement, Security requirements engineering, ISMS},
  url           = {http://www.sciencedirect.com/science/article/pii/S092054890800024X},
}

@Article{Eriksson2009a,
  author        = {Magnus Eriksson and JÃ¼rgen BÃ¶rstler and Kjell Borg},
  title         = {Managing requirements specifications for product lines â€“ An approach and industry case study},
  journal       = {Journal of Systems and Software},
  year          = {2009},
  volume        = {82},
  number        = {3},
  pages         = {435 - 447},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {Software product line development has emerged as a leading approach for software reuse. This paper describes an approach to manage natural-language requirements specifications in a software product line context. Variability in such product line specifications is modeled and managed using a feature model. The proposed approach has been introduced in the Swedish defense industry. We present a multiple-case study covering two different product lines with in total eight product instances. These were compared to experiences from previous projects in the organization employing clone-and-own reuse. We conclude that the proposed product line approach performs better than clone-and-own reuse of requirements specifications in this particular industrial context.},
  comment       = {13},
  doi           = {https://doi.org/10.1016/j.jss.2008.07.046},
  keywords      = {Natural-language requirements specification, Software product line, Feature model, Variability management},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121208001830},
}

@Article{Costa2013,
  author        = {Gabriella Castro B. Costa and Regina Braga and JosÃ© Maria N. David and Fernanda Campos and Wagner Arbex},
  title         = {PL-Science: A Scientific Software Product Line},
  journal       = {Procedia Computer Science},
  year          = {2013},
  volume        = {18},
  pages         = {759 - 768},
  issn          = {1877-0509},
  note          = {2013 International Conference on Computational Science},
  __markedentry = {[mac:]},
  abstract      = {A way to improve reusability and maintainability of a family of software products is through the use of Software Product Line (SPL) approach. Software families, also named SPLs, are a set of software intensive systems sharing a common set of features which are managed to satisfy specific needs of a particular market segment or mission and that are developed from a common set of core assets in a prescribed way. This paper presents the PL-Science approach that considers the context of SPL and aims to assist scientists to define a scientific experiment, specifying a workflow that encompasses scientific applications of a given experiment. Using SPL concepts, scientists can reuse models that specify the scientific product line, and carefully can make decisions according to their needs. In the context of this paper, Scientific Software Product Lines (SSPL) differs from the Software Product Lines (SPL) due to the fact that SSPL uses an abstract scientific workflow model. This workflow is defined according to a scientific domain and, using this abstract workflow model, the products (scientific applications/algorithms) will be instantiated. This paper also focuses on the use of ontologies to facilitate the process of applying Software Product Line (SPL) to scientific domains. Through the use of ontology as a domain model, we can provide additional information as well as add more semantics in the context of Scientific Software Product Lines (SSPL).},
  comment       = {10},
  doi           = {https://doi.org/10.1016/j.procs.2013.05.240},
  keywords      = {Product Line, Ontology, Feature Model, Scientific Workflows, Sequence Alignment},
  url           = {http://www.sciencedirect.com/science/article/pii/S1877050913003839},
}

@Article{Peng2011a,
  author        = {Xin Peng and Yijun Yu and Wenyun Zhao},
  title         = {Analyzing evolution of variability in a software product line: From contexts and requirements to features},
  journal       = {Information and Software Technology},
  year          = {2011},
  volume        = {53},
  number        = {7},
  pages         = {707 - 721},
  issn          = {0950-5849},
  __markedentry = {[mac:]},
  abstract      = {Context
In the long run, features of a software product line (SPL) evolve with respect to changes in stakeholder requirements and system contexts. Neither domain engineering nor requirements engineering handles such co-evolution of requirements and contexts explicitly, making it especially hard to reason about the impact of co-changes in complex scenarios.
Objective
In this paper, we propose a problem-oriented and value-based analysis method for variability evolution analysis. The method takes into account both kinds of changes (requirements and contexts) during the life of an evolving software product line.
Method
The proposed method extends the core requirements engineering ontology with the notions to represent variability-intensive problem decomposition and evolution. On the basis of problemorientation, the analysis method identifies candidate changes, detects influenced features, and evaluates their contributions to the value of the SPL.
Results and Conclusion
The process of applying the analysis method is illustrated using a concrete case study of an evolving enterprise software system, which has confirmed that tracing back to requirements and contextual changes is an effective way to understand the evolution of variability in the software product line.},
  comment       = {15},
  doi           = {https://doi.org/10.1016/j.infsof.2011.01.001},
  keywords      = {Software product line, Variability, Evolution, Feature, Requirements, Context},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584911000024},
}

@Article{Guo2011a,
  author        = {Jianmei Guo and Jules White and Guangxin Wang and Jian Li and Yinglin Wang},
  title         = {A genetic algorithm for optimized feature selection with resource constraints in software product lines},
  journal       = {Journal of Systems and Software},
  year          = {2011},
  volume        = {84},
  number        = {12},
  pages         = {2208 - 2221},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {Abstract
Software product line (SPL) engineering is a software engineering approach to building configurable software systems. SPLs commonly use a feature model to capture and document the commonalities and variabilities of the underlying software system. A key challenge when using a feature model to derive a new SPL configuration is determining how to find an optimized feature selection that minimizes or maximizes an objective function, such as total cost, subject to resource constraints. To help address the challenges of optimizing feature selection in the face of resource constraints, this paper presents an approach that uses G enetic A lgorithms for optimized FE ature S election (GAFES) in SPLs. Our empirical results show that GAFES can produce solutions with 86â€“97% of the optimality of other automated feature selection algorithms and in 45â€“99% less time than existing exact and heuristic feature selection techniques.},
  comment       = {14},
  doi           = {https://doi.org/10.1016/j.jss.2011.06.026},
  keywords      = {Software product lines, Feature models, Product derivation, Configuration, Optimization, Genetic algorithm},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121211001518},
}

@Article{Andres2013,
  author        = {CÃ©sar AndrÃ©s and Carlos Camacho and Luis Llana},
  title         = {A formal framework for software product lines},
  journal       = {Information and Software Technology},
  year          = {2013},
  volume        = {55},
  number        = {11},
  pages         = {1925 - 1947},
  issn          = {0950-5849},
  __markedentry = {[mac:]},
  abstract      = {Context
A Software Product Line is a set of software systems that are built from a common set of features. These systems are developed in a prescribed way and they can be adapted to fit the needs of customers. Feature models specify the properties of the systems that are meaningful to customers. A semantics that models the feature level has the potential to support the automatic analysis of entire software product lines.
Objective
The objective of this paper is to define a formal framework for Software Product Lines. This framework needs to be general enough to provide a formal semantics for existing frameworks like FODA (Feature Oriented Domain Analysis), but also to be easily adaptable to new problems.
Method
We define an algebraic language, called SPLA, to describe Software Product Lines. We provide the semantics for the algebra in three different ways. The approach followed to give the semantics is inspired by the semantics of process algebras. First we define an operational semantics, next a denotational semantics, and finally an axiomatic semantics. We also have defined a representation of the algebra into propositional logic.
Results
We prove that the three semantics are equivalent. We also show how FODA diagrams can be automatically translated into SPLA. Furthermore, we have developed our tool, called AT, that implements the formal framework presented in this paper. This tool uses a SAT-solver to check the satisfiability of an SPL.
Conclusion
This paper defines a general formal framework for software product lines. We have defined three different semantics that are equivalent; this means that depending on the context we can choose the most convenient approach: operational, denotational or axiomatic. The framework is flexible enough because it is closely related to process algebras. Process algebras are a well-known paradigm for which many extensions have been defined.},
  comment       = {23},
  doi           = {https://doi.org/10.1016/j.infsof.2013.05.005},
  keywords      = {Formal methods, Software product lines, Feature models},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584913001262},
}

@Article{Guana2013a,
  author        = {Victor Guana and Dario Correal},
  title         = {Improving software product line configuration: A quality attribute-driven approach},
  journal       = {Information and Software Technology},
  year          = {2013},
  volume        = {55},
  number        = {3},
  pages         = {541 - 562},
  issn          = {0950-5849},
  note          = {Special Issue on Software Reuse and Product Lines},
  __markedentry = {[mac:]},
  abstract      = {Context
During the definition of software product lines (SPLs) it is necessary to choose the components that appropriately fulfil a productâ€™s intended functionalities, including its quality requirements (i.e., security, performance, scalability). The selection of the appropriate set of assets from many possible combinations is usually done manually, turning this process into a complex, time-consuming, and error-prone task.
Objective
Our main objective is to determine whether, with the use of modeling tools, we can simplify and automate the definition process of a SPL, improving the selection process of reusable assets.
Method
We developed a model-driven strategy based on the identification of critical points (sensitivity points) inside the SPL architecture. This strategy automatically selects the components that appropriately match the productâ€™s functional and quality requirements. We validated our approach experimenting with different real configuration and derivation scenarios in a mobile healthcare SPL where we have worked during the last three years.
Results
Through our SPL experiment, we established that our approach improved in nearly 98% the selection of reusable assets when compared with the unassisted analysis selection. However, using our approach there is an increment in the time required for the configuration corresponding to the learning curve of the proposed tools.
Conclusion
We can conclude that our domain-specific modeling approach significantly improves the software architectâ€™s decision making when selecting the most suitable combinations of reusable components in the context of a SPL.},
  comment       = {22},
  doi           = {https://doi.org/10.1016/j.infsof.2012.09.007},
  keywords      = {Software architecture, Model driven â€“ software product lines, Variability management, Quality evaluation, Sensitivity points, Domain specific modeling},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584912002017},
}

@Article{Heradio2012,
  author        = {Ruben Heradio and David Fernandez-Amoros and Luis de la Torre and Ismael Abad},
  title         = {Exemplar driven development of software product lines},
  journal       = {Expert Systems with Applications},
  year          = {2012},
  volume        = {39},
  number        = {17},
  pages         = {12885 - 12896},
  issn          = {0957-4174},
  __markedentry = {[mac:]},
  abstract      = {The benefits of following a product line approach to develop similar software systems are well documented. Nevertheless, some case studies have revealed significant barriers to adopt such approach. In order to minimize the paradigm shift between conventional software engineering and software product line engineering, this paper presents a new development process where the products of a domain are made by analogy to an existing product. Furthermore, this paper discusses the capabilities and limitations of different techniques to implement the analogy relation and proposes a new language to overcome such limitations.},
  comment       = {12},
  doi           = {https://doi.org/10.1016/j.eswa.2012.05.004},
  keywords      = {Software product line, Domain engineering, Domain specific language, Code generation},
  url           = {http://www.sciencedirect.com/science/article/pii/S0957417412007026},
}

@Article{Diaz2014a,
  author        = {Jessica DÃ­az and Jennifer PÃ©rez and Juan Garbajosa},
  title         = {Agile product-line architecting in practice: A case study in smart grids},
  journal       = {Information and Software Technology},
  year          = {2014},
  volume        = {56},
  number        = {7},
  pages         = {727 - 748},
  issn          = {0950-5849},
  __markedentry = {[mac:]},
  abstract      = {Context
Software Product Line Engineering implies the upfront design of a Product-Line Architecture (PLA) from which individual product applications can be engineered. The big upfront design associated with PLAs is in conflict with the current need of â€œbeing open to changeâ€. To make the development of product-lines more flexible and adaptable to changes, several companies are adopting Agile Product Line Engineering. However, to put Agile Product Line Engineering into practice it is still necessary to make mechanisms available to assist and guide the agile construction and evolution of PLAs.
Objective
This paper presents the validation of a process for â€œthe agile construction and evolution of product-line architecturesâ€, called Agile Product-Line Architecting (APLA). The contribution of the APLA process is the integration of a set of models for describing, documenting, and tracing PLAs, as well as an algorithm for guiding the change decision-making process of PLAs. The APLA process is assessed to prove that assists Agile Product Line Engineering practitioners in the construction and evolution of PLAs.
Method
Validation is performed through a case study by using both quantitative and qualitative analysis. Quantitative analysis was performed using statistics, whereas qualitative analysis was performed through interviews using constant comparison, triangulation, and supporting tools. This case study was conducted according to the guidelines of Runeson and HÃ¶st in a software factory where three projects in the domain of Smart Grids were involved.
Results
APLA is deployed through the Flexible-PLA modeling framework. This framework supported the successful development and evolution of the PLA of a family of power metering management applications for Smart Grids.
Conclusions
APLA is a well-supported solution for the agile construction and evolution of PLAs. This case study illustrates that the proposed solution for the agile construction of PLAs is viable in an industry project on Smart Grids.},
  comment       = {22},
  doi           = {https://doi.org/10.1016/j.infsof.2014.01.014},
  keywords      = {Agile product-line engineering, Agile product-line architecting, Product-line architectural knowledge, Change impact analysis, Case study},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584914000251},
}

@Article{Asikainen2007,
  author        = {Timo Asikainen and Tomi MÃ¤nnistÃ¶ and Timo Soininen},
  title         = {Kumbang: A domain ontology for modelling variability in software product families},
  journal       = {Advanced Engineering Informatics},
  year          = {2007},
  volume        = {21},
  number        = {1},
  pages         = {23 - 40},
  issn          = {1474-0346},
  __markedentry = {[mac:]},
  abstract      = {Variability is the ability of a system to be efficiently extended, changed, customised or configured for use in a particular context. There is an ever-growing demand for variability of software. Software product families are an important means for implementing software variability. We present a domain ontology called Kumbang for modelling the variability in software product families. Kumbang synthesises previous approaches to modelling variability in software product families. In addition, it incorporates modelling constructs developed in the product configuration domain for modelling the variability in non-software products. The modelling concepts include components and features with compositional structure and attributes, the interfaces of components and connections between them, and constraints. The semantics of Kumbang is rigorously described using natural language and a UML profile. We provide preliminary proof of concept for Kumbang: the domain ontology has been provided with a formal semantics by implementing a translation into a general-purpose knowledge representation language with formal semantics and inference support. A prototype tool for resolving variability has been implemented.},
  comment       = {18},
  doi           = {https://doi.org/10.1016/j.aei.2006.11.007},
  keywords      = {Software product family, Variability, Modelling, Feature modelling, Software architecture, Kumbang},
  url           = {http://www.sciencedirect.com/science/article/pii/S147403460600067X},
}

@Article{Parra2011,
  author        = {Carlos Parra and Xavier Blanc and Anthony Cleve and Laurence Duchien},
  title         = {Unifying design and runtime software adaptation using aspect models},
  journal       = {Science of Computer Programming},
  year          = {2011},
  volume        = {76},
  number        = {12},
  pages         = {1247 - 1260},
  issn          = {0167-6423},
  note          = {Special Issue on Software Evolution, Adaptability and Variability},
  __markedentry = {[mac:]},
  abstract      = {Software systems are seen more and more as evolutive systems. At the design phase, software is constantly in adaptation by the building process itself, and at runtime, it can be adapted in response to changing conditions in the executing environment such as location or resources. Adaptation is generally difficult to specify because of its cross-cutting impact on software. This article introduces an approach to unify adaptation at design and at runtime based on Aspect Oriented Modeling. Our approach proposes a unified aspect metamodel and a platform that realizes two different weaving processes to achieve design and runtime adaptations. This approach is used in a Dynamic Software Product Line which derives products that can be configured at design time and adapted at runtime in order to dynamically fit new requirements or resource changes. Such products are implemented using the Service Component Architecture and Java. Finally, we illustrate the use of our approach based on an adaptive e-shopping scenario. The main advantages of this unification are: a clear separation of concerns, the self-contained aspect model that can be weaved during the design and execution, and the platform independence guaranteed by two different types of weaving.},
  comment       = {14},
  doi           = {https://doi.org/10.1016/j.scico.2010.12.005},
  keywords      = {Aspect oriented modeling, Software product lines},
  url           = {http://www.sciencedirect.com/science/article/pii/S0167642310002303},
}

@Article{Silva2014,
  author        = {Ivonei Freitas da Silva and Paulo Anselmo da Mota Silveira Neto and PÃ¡draig Oâ€™Leary and Eduardo Santana de Almeida and Silvio Romero de Lemos Meira},
  title         = {Software product line scoping and requirements engineering in a small and medium-sized enterprise: An industrial case study},
  journal       = {Journal of Systems and Software},
  year          = {2014},
  volume        = {88},
  pages         = {189 - 206},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {Software product line (SPL) engineering has been applied in several domains, especially in large-scale software development. Given the benefits experienced and reported, SPL engineering has increasingly garnered interest from small to medium-sized companies. It is possible to find a wide range of studies reporting on the challenges of running a SPL project in large companies. However, very little reports exist that consider the situation for small to medium-sized enterprises and these studies try develop universal truths for SPL without lessons learned from empirical evidence need to be contextualized. This study is a step towards bridging this gap in contextual evidence by characterizing the weaknesses discovered in the scoping (SC) and requirements (RE) disciplines of SPL. Moreover, in this study we conducted a case study in a small to medium sized enterprises (SMEs) to justify the use of agile methods when introducing the SPL SC and RE disciplines through the characterization of their bottlenecks. The results of the characterization indicated that ineffective communication and collaboration, long iteration cycles, and the absence of adaptability and flexibility can increase the effort and reduce motivation during project development. These issues can be mitigated by agile methods.},
  comment       = {18},
  doi           = {https://doi.org/10.1016/j.jss.2013.10.040},
  keywords      = {Requirements engineering, Agile methods, Software product line scoping},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121213002598},
}

@Article{Mellado2010,
  author        = {Daniel Mellado and Eduardo FernÃ¡ndez-Medina and Mario Piattini},
  title         = {Security requirements engineering framework for software product lines},
  journal       = {Information and Software Technology},
  year          = {2010},
  volume        = {52},
  number        = {10},
  pages         = {1094 - 1117},
  issn          = {0950-5849},
  __markedentry = {[mac:]},
  abstract      = {Context
The correct analysis and understanding of security requirements are important because they assist in the discovery of any security or requirement defects or mistakes during the early stages of development. Security requirements engineering is therefore both a central task and a critical success factor in product line development owing to the complexity and extensive nature of software product lines (SPL). However, most of the current SPL practices in requirements engineering do not adequately address security requirements engineering.
Objective
The aim of this approach is to describe a holistic security requirements engineering framework with which to facilitate the development of secure SPLs and their derived products. It will conform with the most relevant security standards with regard to the management of security requirements, such as ISO/IEC 27001 and ISO/IEC 15408.
Results
This framework is composed of: a security requirements engineering process for SPL (SREPPLine) driven by security standards; a Security Reference Meta Model to manage the variability of those SPL artefacts related to security requirements; and a tool (SREPPLineTool) which implements the meta-model and supports the process.
Method
A complete explanation of the framework will be provided. The process will be formally specified with SPEM 2.0 and the repository will be formally specified with an XML grammar. The application of SREPPLine and SREPPLineTool will be illustrated through a description of a simple example as a preliminary validation.
Conclusion
Although there have been several attempts to fill the gap between requirements engineering and SPL requirements engineering, no systematic approach with which to define security quality requirements and to manage their variability and their related security artefacts in SPL models is, as yet, available. The contribution of this work is that of providing a systematic approach for the management of the security requirements and their variability from the early stages of product line development in order to facilitate the conformance of SPL products with the most relevant security standards.},
  comment       = {24},
  doi           = {https://doi.org/10.1016/j.infsof.2010.05.007},
  keywords      = {Security requirements engineering, Security software engineering, Product lines, Requirements engineering, Security requirement, ISO 27001},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584910000960},
}

@Article{Cetina2013a,
  author        = {Carlos Cetina and Pau Giner and Joan Fons and Vicente Pelechano},
  title         = {Prototyping Dynamic Software Product Lines to evaluate run-time reconfigurations},
  journal       = {Science of Computer Programming},
  year          = {2013},
  volume        = {78},
  number        = {12},
  pages         = {2399 - 2413},
  issn          = {0167-6423},
  note          = {Special Section on International Software Product Line Conference 2010 and Fundamentals of Software Engineering (selected papers of FSEN 2011)},
  __markedentry = {[mac:]},
  abstract      = {Dynamic Software Product Lines (DSPL) encompass systems that are capable of modifying their own behavior with respect to changes in their operating environment by using run-time reconfigurations. A failure in these reconfigurations can directly impact the user experience since the reconfigurations are performed when the system is already under the users control. In this work, we prototype a Smart Hotel DSPL to evaluate the reliability-based risk of the DSPL reconfigurations, specifically, the probability of malfunctioning (Availability) and the consequences of malfunctioning (Severity). This DSPL prototype was performed with the participation of human subjects by means of a Smart Hotel case study which was deployed with real devices. Moreover, we successfully identified and addressed two challenges associated with the involvement of human subjects in DSPL prototyping: enabling participants to (1) trigger the run-time reconfigurations and to (2) understand the effects of the reconfigurations. The evaluation of the case study reveals positive results regarding both Availability and Severity. However, the participant feedback highlights issues with recovering from a failed reconfiguration or a reconfiguration triggered by mistake. To address these issues, we discuss some guidelines learned in the case study. Finally, although the results achieved by the DSPL may be considered satisfactory for its particular domain, DSPL engineers must provide users with more control over the reconfigurations or the users will not be comfortable with DSPLs.},
  comment       = {15},
  doi           = {https://doi.org/10.1016/j.scico.2012.06.007},
  keywords      = {Dynamic Software Product Line, Variability modeling, Smart Hotel},
  url           = {http://www.sciencedirect.com/science/article/pii/S0167642312001189},
}

@Article{Shatnawi2017,
  author        = {Anas Shatnawi and Abdelhak-Djamel Seriai and Houari Sahraoui},
  title         = {Recovering software product line architecture of a family of object-oriented product variants},
  journal       = {Journal of Systems and Software},
  year          = {2017},
  volume        = {131},
  pages         = {325 - 346},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {Software Product Line Engineering (SPLE) aims at applying a pre-planned systematic reuse of large-grained software artifacts to increase the software productivity and reduce the development cost. The idea of SPLE is to analyze the business domain of a family of products to identify the common and the variable parts between the products. However, it is common for companies to develop, in an ad-hoc manner (e.g. clone and own), a set of products that share common services and differ in terms of others. Thus, many recent research contributions are proposed to re-engineer existing product variants to a software product line. These contributions are mostly focused on managing the variability at the requirement level. Very few contributions address the variability at the architectural level despite its major importance. Starting from this observation, we propose an approach to reverse engineer the architecture of a set of product variants. Our goal is to identify the variability and dependencies among architectural-element variants. Our work relies on formal concept analysis to analyze the variability. To validate the proposed approach, we evaluated on two families of open-source product variants; Mobile Media and Health Watcher. The results of precision and recall metrics of the recovered architectural variability and dependencies are 81%, 91%, 67% and 100%, respectively.},
  comment       = {22},
  doi           = {https://doi.org/10.1016/j.jss.2016.07.039},
  keywords      = {Software reuse, Software architecture recovery, Software product line, Object-oriented product variants, Software component, Formal concept analysis},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121216301327},
}

@Article{Dhungana2010,
  author        = {Deepak Dhungana and Paul GrÃ¼nbacher and Rick Rabiser and Thomas Neumayer},
  title         = {Structuring the modeling space and supporting evolution in software product line engineering},
  journal       = {Journal of Systems and Software},
  year          = {2010},
  volume        = {83},
  number        = {7},
  pages         = {1108 - 1122},
  issn          = {0164-1212},
  note          = {SPLC 2008},
  __markedentry = {[mac:]},
  abstract      = {The scale and complexity of product lines means that it is practically infeasible to develop a single model of the entire system, regardless of the languages or notations used. The dynamic nature of real-world systems means that product line models need to evolve continuously to meet new customer requirements and to reflect changes of product line artifacts. To address these challenges, product line engineers need to apply different strategies for structuring the modeling space to ease the creation and maintenance of models. This paper presents an approach that aims at reducing the maintenance effort by organizing product lines as a set of interrelated model fragments defining the variability of particular parts of the system. We provide support to semi-automatically merge fragments into complete product line models. We also provide support to automatically detect inconsistencies between product line artifacts and the models representing these artifacts after changes. Furthermore, our approach supports the co-evolution of models and their respective meta-models. We discuss strategies for structuring the modeling space and show the usefulness of our approach using real-world examples from our ongoing industry collaboration.},
  comment       = {15},
  doi           = {https://doi.org/10.1016/j.jss.2010.02.018},
  keywords      = {Product line engineering, Model evolution, Variability modeling},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121210000506},
}

@Article{Laguna2013a,
  author        = {Miguel A. Laguna and Yania Crespo},
  title         = {A systematic mapping study on software product line evolution: From legacy system reengineering to product line refactoring},
  journal       = {Science of Computer Programming},
  year          = {2013},
  volume        = {78},
  number        = {8},
  pages         = {1010 - 1034},
  issn          = {0167-6423},
  note          = {Special section on software evolution, adaptability, and maintenance \& Special section on the Brazilian Symposium on Programming Languages},
  __markedentry = {[mac:]},
  abstract      = {Software product lines (SPLs) are used in industry to develop families of similar software systems. Legacy systems, either highly configurable or with a story of versions and local variations, are potential candidates for reconfiguration as SPLs using reengineering techniques. Existing SPLs can also be restructured using specific refactorings to improve their internal quality. Although many contributions (including industrial experiences) can be found in the literature, we lack a global vision covering the whole life cycle of an evolving product line. This study aims to survey existing research on the reengineering of legacy systems into SPLs and the refactoring of existing SPLs in order to identify proven approaches and pending challenges for future research in both subfields. We launched a systematic mapping study to find as much literature as possible, covering the diverse terms involved in the search string (restructuring, refactoring, reengineering, etc. always connected with SPLs) and filtering the papers using relevance criteria. The 74 papers selected were classified with respect to several dimensions: main focus, research and contribution type, academic or industrial validation if included, etc. We classified the research approaches and analyzed their feasibility for use in industry. The results of the study indicate that the initial works focused on the adaptation of generic reengineering processes to SPL extraction. Starting from that foundation, several trends have been detected in recent research: the integrated or guided reengineering of (typically object-oriented) legacy code and requirements; specific aspect-oriented or feature-oriented refactoring into SPLs, and more recently, refactoring for the evolution of existing product lines. A majority of papers include academic or industrial case studies, though only a few are based on quantitative data. The degree of maturity of both subfields is different: Industry examples for the reengineering of the legacy system subfield are abundant, although more evaluation research is needed to provide better evidence for adoption in industry. Product line evolution through refactoring is an emerging topic with some pending challenges. Although it has recently received some attention, the theoretical foundation is rather limited in this subfield and should be addressed in the near future. To sum up, the main contributions of this work are the classification of research approaches as well as the analysis of remaining challenges, open issues, and research opportunities.},
  comment       = {25},
  doi           = {https://doi.org/10.1016/j.scico.2012.05.003},
  keywords      = {Software product line, Evolution, Reengineering, Legacy system, Refactoring},
  url           = {http://www.sciencedirect.com/science/article/pii/S0167642312000895},
}

@Article{Horcas2016a,
  author        = {Jose-Miguel Horcas and MÃ³nica Pinto and Lidia Fuentes},
  title         = {An automatic process for weaving functional quality attributes using a software product line approach},
  journal       = {Journal of Systems and Software},
  year          = {2016},
  volume        = {112},
  pages         = {78 - 95},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {Some quality attributes can be modelled using software components, and are normally known as Functional Quality Attributes (FQAs). Applications may require different FQAs, and each FQA (e.g., security) can be composed of many concerns (e.g., access control or authentication). They normally have dependencies between them and crosscut the system architecture. The goal of the work presented here is to provide the means for software architects to focus only on application functionality, without having to worry about FQAs. The idea is to model FQAs separately from application functionality following a Software Product Line (SPL) approach. By combining SPL and aspect-oriented mechanisms, we will define a generic process to model and automatically inject FQAs into the application without breaking the base architecture. We will provide and compare two implementations of our generic approach using different variability and architecture description languages: (i) feature models and an aspect-oriented architecture description language; and (ii) the Common Variability Language (CVL) and a MOF-compliant language (e.g., UML). We also discuss the benefits and limitations of our approach. Modelling FQAs separately from the base application has many advantages (e.g., reusability, less coupled components, high cohesive architectures).},
  comment       = {18},
  doi           = {https://doi.org/10.1016/j.jss.2015.11.005},
  keywords      = {Quality attributes, Software product lines, Weaving},
  url           = {http://www.sciencedirect.com/science/article/pii/S016412121500240X},
}

@Article{Thurimella2013a,
  author        = {Anil Kumar Thurimella and Bernd BrÃ¼gge},
  title         = {A mixed-method approach for the empirical evaluation of the issue-based variability modeling},
  journal       = {Journal of Systems and Software},
  year          = {2013},
  volume        = {86},
  number        = {7},
  pages         = {1831 - 1849},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {Background
Variability management is the fundamental part of software product line engineering, which deals with customization and reuse of artifacts for developing a family of systems. Rationale approaches structure decision-making by managing the tacit-knowledge behind decisions. This paper reports a quasi-experiment for evaluating a rationale enriched collaborative variability management methodology called issue-based variability modeling.
Objective
We studied the interaction of stakeholders with issue-based modeling to evaluate its applicability in requirements engineering teams. Furthermore, we evaluated the reuse of rationale while instantiating and changing variability.
Approach
We enriched a quasi-experimental design with a variety of methods found in case study research. A sample of 258 students was employed with data collection and analysis based on a mix of qualitative and quantitative methods. Our study was performed in two phases: the first phase focused on variability identification and instantiation, while the second phase included tasks on variability evolution.
Results
We obtained strong empirical evidence on reuse patterns for rationale during instantiation and evolution of variability. The tabular representations used by rationale modeling are learnable and usable in teams of diverse backgrounds.},
  comment       = {19},
  doi           = {https://doi.org/10.1016/j.jss.2013.01.038},
  keywords      = {Rationale management, Software product lines, Variability, Requirements engineering, Empirical software engineering, Mixed-methods},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121213000186},
}

@Article{Benavides2010a,
  author        = {David Benavides and Sergio Segura and Antonio Ruiz-CortÃ©s},
  title         = {Automated analysis of feature models 20 years later: A literature review},
  journal       = {Information Systems},
  year          = {2010},
  volume        = {35},
  number        = {6},
  pages         = {615 - 636},
  issn          = {0306-4379},
  __markedentry = {[mac:]},
  abstract      = {Software product line engineering is about producing a set of related products that share more commonalities than variabilities. Feature models are widely used for variability and commonality management in software product lines. Feature models are information models where a set of products are represented as a set of features in a single model. The automated analysis of feature models deals with the computer-aided extraction of information from feature models. The literature on this topic has contributed with a set of operations, techniques, tools and empirical results which have not been surveyed until now. This paper provides a comprehensive literature review on the automated analysis of feature models 20 years after of their invention. This paper contributes by bringing together previously disparate streams of work to help shed light on this thriving area. We also present a conceptual framework to understand the different proposals as well as categorise future contributions. We finally discuss the different studies and propose some challenges to be faced in the future.},
  comment       = {22},
  doi           = {https://doi.org/10.1016/j.is.2010.01.001},
  keywords      = {Feature models, Automated analyses, Software product lines, Literature review},
  url           = {http://www.sciencedirect.com/science/article/pii/S0306437910000025},
}

@Article{Alsawalqah2014a,
  author        = {Hamad I. Alsawalqah and Sungwon Kang and Jihyun Lee},
  title         = {A method to optimize the scope of a software product platform based on end-user features},
  journal       = {Journal of Systems and Software},
  year          = {2014},
  volume        = {98},
  pages         = {79 - 106},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {Context
Due to increased competition and the advent of mass customization, many software firms are utilizing product families â€“ groups of related products derived from a product platform â€“ to provide product variety in a cost-effective manner. The key to designing a successful software product family is the product platform, so it is important to determine the most appropriate product platform scope related to business objectives, for product line development.
Aim
This paper proposes a novel method to find the optimized scope of a software product platform based on end-user features.
Method
The proposed method, PPSMS (Product Platform Scoping Method for Software Product Lines), mathematically formulates the product platform scope selection as an optimization problem. The problem formulation targets identification of an optimized product platform scope that will maximize life cycle cost savings and the amount of commonality, while meeting the goals and needs of the envisioned customersâ€™ segments. A simulated annealing based algorithm that can solve problems heuristically is then used to help the decision maker in selecting a scope for the product platform, by performing tradeoff analysis of the commonality and cost savings objectives.
Results
In a case study, PPSMS helped in identifying 5 non-dominated solutions considered to be of highest preference for decision making, taking into account both cost savings and commonality objectives. A quantitative and qualitative analysis indicated that human experts perceived value in adopting the method in practice, and that it was effective in identifying appropriate product platform scope.},
  comment       = {27},
  doi           = {https://doi.org/10.1016/j.jss.2014.08.034},
  keywords      = {Product platform scope, Software product line engineering, Commonality decision},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121214001861},
}

@Article{Ahmed2007b,
  author        = {Faheem Ahmed and Luiz Fernando Capretz and Shahbaz Ali Sheikh},
  title         = {Institutionalization of software product line: An empirical investigation of key organizational factors},
  journal       = {Journal of Systems and Software},
  year          = {2007},
  volume        = {80},
  number        = {6},
  pages         = {836 - 849},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {A good fit between the person and the organization is essential in a better organizational performance. This is even more crucial in case of institutionalization of a software product line practice within an organization. Employeesâ€™ participation, organizational behavior and management contemplation play a vital role in successfully institutionalizing software product lines in a company. Organizational dimension has been weighted as one of the critical dimensions in software product line theory and practice. A comprehensive empirical investigation to study the impact of some organizational factors on the performance of software product line practice is presented in this work. This is the first study to empirically investigate and demonstrate the relationships between some of the key organizational factors and software product line performance of an organization. The results of this investigation provide empirical evidence and further support the theoretical foundations that in order to institutionalize software product lines within an organization, organizational factors play an important role.},
  comment       = {14},
  doi           = {https://doi.org/10.1016/j.jss.2006.09.010},
  keywords      = {Software product line, Empirical software engineering, Organizational theory, Organizational management, Organizational behavior},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121206002627},
}

@Article{Beohar2016,
  author        = {Harsh Beohar and Mohammad Reza Mousavi},
  title         = {Inputâ€“output conformance testing for software product lines},
  journal       = {Journal of Logical and Algebraic Methods in Programming},
  year          = {2016},
  volume        = {85},
  number        = {6},
  pages         = {1131 - 1153},
  issn          = {2352-2208},
  note          = {NWPT 2013},
  __markedentry = {[mac:]},
  abstract      = {We extend the theory of inputâ€“output conformance (IOCO) testing to accommodate behavioral models of software product lines (SPLs). We present the notions of residual and spinal testing. These notions allow for structuring the test process for SPLs by taking variability into account and extracting separate test suites for common and specific features of an SPL. The introduced notions of residual and spinal test suites allow for focusing on the newly introduced behavior and avoiding unnecessary re-test of the old one. Residual test suites are very conservative in that they require retesting the old behavior that can reach to new behavior. However, spinal test suites more aggressively prune the old tests and only focus on those test sequences that are necessary in reaching the new behavior. We show that residual testing is complete but does not usually lead to much reduction in the test-suite. In contrast, spinal testing is not necessarily complete but does reduce the test-suite. We give sufficient conditions on the implementation to guarantee completeness of spinal testing. Finally, we specify and analyze an example regarding the Ceiling Speed Monitoring Function from the European Train Control System.},
  comment       = {23},
  doi           = {https://doi.org/10.1016/j.jlamp.2016.09.007},
  keywords      = {Model based testing, Inputâ€“output conformance testing, Software product lines, Inputâ€“output featured transition systems},
  url           = {http://www.sciencedirect.com/science/article/pii/S2352220816301171},
}

@Article{Ferreira2014,
  author        = {Gabriel Coutinho Sousa Ferreira and Felipe Nunes Gaia and Eduardo Figueiredo and Marcelo de Almeida Maia},
  title         = {On the use of feature-oriented programming for evolving software product lines â€” A comparative study},
  journal       = {Science of Computer Programming},
  year          = {2014},
  volume        = {93},
  pages         = {65 - 85},
  issn          = {0167-6423},
  note          = {Special Issue with Selected Papers from the Brazilian Symposium on Programming Languages (SBLP 2011)},
  __markedentry = {[mac:]},
  abstract      = {Feature-oriented programming (FOP) is a programming technique based on composition mechanisms, called refinements. It is often assumed that feature-oriented programming is more suitable than other variability mechanisms for implementing Software Product Lines (SPLs). However, there is no empirical evidence to support this claim. In fact, recent research work found out that some composition mechanisms might degenerate the SPL modularity and stability. However, there is no study investigating these properties focusing on the FOP composition mechanisms. This paper presents quantitative and qualitative analysis of how feature modularity and change propagation behave in the context of two evolving SPLs, namely WebStore and MobileMedia. Quantitative data have been collected from the SPLs developed in three different variability mechanisms: FOP refinements, conditional compilation, and object-oriented design patterns. Our results suggest that FOP requires few changes in source code and a balanced number of added modules, providing better support than other techniques for non-intrusive insertions. Therefore, it adheres closer to the Openâ€“Closed principle. Additionally, FOP seems to be more effective tackling modularity degeneration, by avoiding feature tangling and scattering in source code, than conditional compilation and design patterns. These results are based not only on the variability mechanism itself, but also on careful SPL design. However, the aforementioned results are weaker when the design needs to cope with crosscutting and fine-grained features.},
  comment       = {21},
  doi           = {https://doi.org/10.1016/j.scico.2013.10.010},
  keywords      = {Software product lines, Feature-oriented programming, Variability management, Design patterns, Conditional compilation},
  url           = {http://www.sciencedirect.com/science/article/pii/S0167642313002815},
}

@Article{Buccella2013a,
  author        = {Agustina Buccella and Alejandra Cechich and Maximiliano Arias and Matias Pol'la and Maria del Socorro Doldan and Enrique Morsan},
  title         = {Towards systematic software reuse of GIS: Insights from a case study},
  journal       = {Computers \& Geosciences},
  year          = {2013},
  volume        = {54},
  pages         = {9 - 20},
  issn          = {0098-3004},
  __markedentry = {[mac:]},
  abstract      = {With the development and adoption of geographic information systems, there is an increasingly amount of software resources being stored or recorded as products to be reused. At the same time, complexity of geographic services is addressed through standardization, which allows developers reaching higher quality levels. In this paper, we introduce our domain-oriented approach to developing geographic software product lines focusing on the experiences collected from a case study. It was developed in the Marine Ecology Domain (Patagonia, Argentina) and illustrates insights of the process.},
  comment       = {12},
  doi           = {https://doi.org/10.1016/j.cageo.2012.11.014},
  keywords      = {Geographic information systems, ISO 19119 std., Software product lines, Reused services, Marine ecology, Geographic open source tools},
  url           = {http://www.sciencedirect.com/science/article/pii/S0098300412003913},
}

@Article{Hanssen2012a,
  author        = {Geir K. Hanssen},
  title         = {A longitudinal case study of an emerging software ecosystem: Implications for practice and theory},
  journal       = {Journal of Systems and Software},
  year          = {2012},
  volume        = {85},
  number        = {7},
  pages         = {1455 - 1466},
  issn          = {0164-1212},
  note          = {Software Ecosystems},
  __markedentry = {[mac:]},
  abstract      = {Software ecosystems is an emerging trend within the software industry, implying a shift from closed organizations and processes towards open structures, where actors external to the software development organization are becoming increasingly involved in development. This forms an ecosystem of organizations that are related through the shared interest in a software product, leading to new opportunities and new challenges to the industry and its organizational environment. To understand why and how this change occurs, we have followed the development of a software product line organization for a period of approximately five years. We have studied their change from a waterfall-like approach, via agile software product line engineering, towards an emerging software ecosystem. We discuss implications for practice, and propose a nascent theory on software ecosystems. We conclude that the observed change has led to an increase in collaboration across (previously closed) organizational borders, and to the development of a shared value consisting of two components: the technology (the product line, as an extensible platform), and the business domain it supports. Opening up both the technical interface of the product and the organizational interfaces are key enablers of such a change.},
  comment       = {12},
  doi           = {https://doi.org/10.1016/j.jss.2011.04.020},
  keywords      = {Software ecosystems, Software product line engineering, Agile software development, Longitudinal case study},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121211000963},
}

@Article{Tizzei2011,
  author        = {Leonardo P. Tizzei and Marcelo Dias and CecÃ­lia M.F. Rubira and Alessandro Garcia and Jaejoon Lee},
  title         = {Components meet aspects: Assessing design stability of a software product line},
  journal       = {Information and Software Technology},
  year          = {2011},
  volume        = {53},
  number        = {2},
  pages         = {121 - 136},
  issn          = {0950-5849},
  __markedentry = {[mac:]},
  abstract      = {Context
It is important for Product Line Architectures (PLA) to remain stable accommodating evolutionary changes of stakeholderâ€™s requirements. Otherwise, architectural modifications may have to be propagated to products of a product line, thereby increasing maintenance costs. A key challenge is that several features are likely to exert a crosscutting impact on the PLA decomposition, thereby making it more difficult to preserve its stability in the presence of changes. Some researchers claim that the use of aspects can ameliorate instabilities caused by changes in crosscutting features. Hence, it is important to understand which aspect-oriented (AO) and non-aspect-oriented techniques better cope with PLA stability through evolution.
Objective
This paper evaluates the positive and negative change impact of component and aspect based design on PLAs. The objective of the evaluation is to assess how aspects and components promote PLA stability in the presence of various types of evolutionary change. To support a broader analysis, we also evaluate the PLA stability of a hybrid approach (i.e. combined use of aspects and components) against the isolated use of component-based, OO, and AO approaches.
Method
An quantitative and qualitative analysis of PLA stability which involved four different implementations of a PLA: (i) an OO implementation, (ii) an AO implementation, (iii) a component-based implementation, and (iv) a hybrid implementation where both components and aspects are employed. Each implementation has eight releases and they are functionally equivalent. We used conventional metrics suites for change impact and modularity to measure the architecture stability evaluation of the 4 implementations.
Results
The combination of aspects and components promotes superior PLA resilience than the other PLAs in most of the circumstances.
Conclusion
It is concluded that the combination of aspects and components supports the design of high cohesive and loosely coupled PLAs. It also contributes to improve modularity by untangling feature implementation.},
  comment       = {16},
  doi           = {https://doi.org/10.1016/j.infsof.2010.08.007},
  keywords      = {Product Line Architecture, Component-based Development, Aspect-Oriented Software Development, Design stability},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584910001564},
}

@Article{Ahmed2008b,
  author        = {F. Ahmed and L.F. Capretz and J. Samarabandu},
  title         = {Fuzzy inference system for software product family process evaluation},
  journal       = {Information Sciences},
  year          = {2008},
  volume        = {178},
  number        = {13},
  pages         = {2780 - 2793},
  issn          = {0020-0255},
  __markedentry = {[mac:]},
  abstract      = {When developing multiple products within a common application domain, systematic use of a software product family process can yield increased productivity in cost, quality, effort and schedule. Such a process provides the means for the reuse of software assets which can considerably reduce the development time and the cost of software products. A comprehensive strategy for the evaluating the maturity of a software product family process is needed due to growing popularity of this concept in the software industry. In this paper, we propose a five-level maturity scale for software product family process. We also present a fuzzy inference system for evaluating maturity of software product family process using the proposed maturity scale. This research is aimed at establishing a comprehensive and unified strategy for process evaluation of a software product family. Such a process evaluation strategy will enable an organization to discover and monitor the strengths and weaknesses of the various activities performed during development of multiple products within a common application domain.},
  comment       = {14},
  doi           = {https://doi.org/10.1016/j.ins.2008.03.002},
  keywords      = {Software product family, Adaptive neuro-fuzzy inference system, Fuzzy logic, Process maturity, Software process assessment, Software engineering},
  url           = {http://www.sciencedirect.com/science/article/pii/S0020025508000844},
}

@Article{Costa2015a,
  author        = {Gabriella Castro B. Costa and Regina Braga and JosÃ© Maria N. David and Fernanda Campos},
  title         = {A Scientific Software Product Line for the Bioinformatics domain},
  journal       = {Journal of Biomedical Informatics},
  year          = {2015},
  volume        = {56},
  pages         = {239 - 264},
  issn          = {1532-0464},
  __markedentry = {[mac:]},
  abstract      = {Context
Most specialized users (scientists) that use bioinformatics applications do not have suitable training on software development. Software Product Line (SPL) employs the concept of reuse considering that it is defined as a set of systems that are developed from a common set of base artifacts. In some contexts, such as in bioinformatics applications, it is advantageous to develop a collection of related software products, using SPL approach. If software products are similar enough, there is the possibility of predicting their commonalities, differences and then reuse these common features to support the development of new applications in the bioinformatics area.
Objectives
This paper presents the PL-Science approach which considers the context of SPL and ontology in order to assist scientists to define a scientific experiment, and to specify a workflow that encompasses bioinformatics applications of a given experiment. This paper also focuses on the use of ontologies to enable the use of Software Product Line in biological domains.
Method
In the context of this paper, Scientific Software Product Line (SSPL) differs from the Software Product Line due to the fact that SSPL uses an abstract scientific workflow model. This workflow is defined according to a scientific domain and using this abstract workflow model the products (scientific applications/algorithms) are instantiated.
Results
Through the use of ontology as a knowledge representation model, we can provide domain restrictions as well as add semantic aspects in order to facilitate the selection and organization of bioinformatics workflows in a Scientific Software Product Line. The use of ontologies enables not only the expression of formal restrictions but also the inferences on these restrictions, considering that a scientific domain needs a formal specification.
Conclusions
This paper presents the development of the PL-Science approach, encompassing a methodology and an infrastructure, and also presents an approach evaluation. This evaluation presents case studies in bioinformatics, which were conducted in two renowned research institutions in Brazil.},
  comment       = {26},
  doi           = {https://doi.org/10.1016/j.jbi.2015.05.014},
  keywords      = {Scientific workflow, Sequence alignment, Software Product Line, Ontology, Feature model},
  url           = {http://www.sciencedirect.com/science/article/pii/S1532046415000945},
}

@Article{Ahmed2008c,
  author        = {Faheem Ahmed and Luiz Fernando Capretz},
  title         = {The software product line architecture: An empirical investigation of key process activities},
  journal       = {Information and Software Technology},
  year          = {2008},
  volume        = {50},
  number        = {11},
  pages         = {1098 - 1113},
  issn          = {0950-5849},
  __markedentry = {[mac:]},
  abstract      = {Software architecture has been a key area of concern in software industry due to its profound impact on the productivity and quality of software products. This is even more crucial in case of software product line, because it deals with the development of a line of products sharing common architecture and having controlled variability. The main contributions of this paper is to increase the understanding of the influence of key software product line architecture process activities on the overall performance of software product line by conducting a comprehensive empirical investigation covering a broad range of organizations currently involved in the business of software product lines. This is the first study to empirically investigate and demonstrate the relationships between some of the software product line architecture process activities and the overall software product line performance of an organization at the best of our knowledge. The results of this investigation provide empirical evidence that software product line architecture process activities play a significant role in successfully developing and managing a software product line.},
  comment       = {16},
  doi           = {https://doi.org/10.1016/j.infsof.2007.10.013},
  keywords      = {Software product line, Software architecture, Empirical study, Software engineering, Domain engineering},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584907001243},
}

@Article{Capilla2014,
  author        = {Rafael Capilla and Jan Bosch and Pablo Trinidad and Antonio Ruiz-CortÃ©s and Mike Hinchey},
  title         = {An overview of Dynamic Software Product Line architectures and techniques: Observations from research and industry},
  journal       = {Journal of Systems and Software},
  year          = {2014},
  volume        = {91},
  pages         = {3 - 23},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {Over the last two decades, software product lines have been used successfully in industry for building families of systems of related products, maximizing reuse, and exploiting their variable and configurable options. In a changing world, modern software demands more and more adaptive features, many of them performed dynamically, and the requirements on the software architecture to support adaptation capabilities of systems are increasing in importance. Today, many embedded system families and application domains such as ecosystems, service-based applications, and self-adaptive systems demand runtime capabilities for flexible adaptation, reconfiguration, and post-deployment activities. However, as traditional software product line architectures fail to provide mechanisms for runtime adaptation and behavior of products, there is a shift toward designing more dynamic software architectures and building more adaptable software able to handle autonomous decision-making, according to varying conditions. Recent development approaches such as Dynamic Software Product Lines (DSPLs) attempt to face the challenges of the dynamic conditions of such systems but the state of these solution architectures is still immature. In order to provide a more comprehensive treatment of DSPL models and their solution architectures, in this research work we provide an overview of the state of the art and current techniques that, partially, attempt to face the many challenges of runtime variability mechanisms in the context of Dynamic Software Product Lines. We also provide an integrated view of the challenges and solutions that are necessary to support runtime variability mechanisms in DSPL models and software architectures.},
  comment       = {21},
  doi           = {https://doi.org/10.1016/j.jss.2013.12.038},
  keywords      = {Dynamic Software Product Lines, Dynamic variability, Software architecture, Feature models},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121214000119},
}

@Article{Olumofin2007a,
  author        = {Femi G. Olumofin and Vojislav B. MiÅ¡iÄ‡},
  title         = {A holistic architecture assessment method for software product lines},
  journal       = {Information and Software Technology},
  year          = {2007},
  volume        = {49},
  number        = {4},
  pages         = {309 - 323},
  issn          = {0950-5849},
  __markedentry = {[mac:]},
  abstract      = {The success of architecture-centric development of software product lines is critically dependent upon the availability of suitable architecture assessment methods. While a number of architecture assessment methods are available and some of them have been widely used in the process of evaluating single product architectures, none of them is equipped to deal with the main challenges of product line development. In this paper we present an adaptation of the Architecture Tradeoff Analysis Method (ATAM) for the task of assessing product line architectures. The new method, labeled Holistic Product Line Architecture Assessment (HoPLAA), uses a holistic approach that focuses on risks and quality attribute tradeoffs â€“ not only for the common product line architecture, but for the individual product architectures as well. In addition, it prescribes a qualitative analytical treatment of variation points using scenarios. The use of the new method is illustrated through a case study.},
  comment       = {15},
  doi           = {https://doi.org/10.1016/j.infsof.2006.05.003},
  keywords      = {Software product line architectures, Software architecture assessment, Architecture Tradeoff Analysis Method (ATAM)},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584906000747},
}

@Article{Teixeira2013,
  author        = {Leopoldo Teixeira and Paulo Borba and Rohit Gheyi},
  title         = {Safe composition of configuration knowledge-based software product lines},
  journal       = {Journal of Systems and Software},
  year          = {2013},
  volume        = {86},
  number        = {4},
  pages         = {1038 - 1053},
  issn          = {0164-1212},
  note          = {SI : Software Engineering in Brazil: Retrospective and Prospective Views},
  __markedentry = {[mac:]},
  abstract      = {Mistakes made when implementing or specifying the models of a Software Product Line (SPL) can result in ill-formed products â€” the safe composition problem. Such problem can hinder productivity and it might be hard to detect, since SPLs can have thousands of products. In this article, we propose a language independent approach for verifying safe composition of SPLs with dedicated Configuration Knowledge models. We translate feature model and Configuration Knowledge into propositional logic and use the Alloy Analyzer to perform the verification. To provide evidence for the generality of our approach, we instantiate this approach in different compositional settings. We deal with different kinds of assets such as use case scenarios and Eclipse RCP components. We analyze both the code and the requirements for a larger scale SPL, finding problems that affect thousands of products in minutes. Moreover, our evaluation suggests that the analysis time grows linearly with respect to the number of products in the analyzed SPLs.},
  comment       = {16},
  doi           = {https://doi.org/10.1016/j.jss.2012.11.006},
  keywords      = {Software Product Lines, Configuration Knowledge, Safe composition},
  url           = {http://www.sciencedirect.com/science/article/pii/S016412121200307X},
}

@Article{Oâ€™Leary2012,
  author        = {PÃ¡draig Oâ€™Leary and Eduardo Santana de Almeida and Ita Richardson},
  title         = {The Pro-PD Process Model for Product Derivation within software product lines},
  journal       = {Information and Software Technology},
  year          = {2012},
  volume        = {54},
  number        = {9},
  pages         = {1014 - 1028},
  issn          = {0950-5849},
  __markedentry = {[mac:]},
  abstract      = {Background
The derivation of products from a software product line is a time consuming and expensive activity. Despite recognition that an effective process could alleviate many of the difficulties associated with product derivation, existing approaches have different scope, emphasise different aspects of the derivation process and are frequently too specialised to serve as a general solution.
Objective
To define a systematic process that will provide a structured approach to the derivation of products from a software product line, based on a set of tasks, roles and artefacts.
Method
Through a series of research stages using sources in industry and academia, this research has developed a Process Model for Product Derivation (Pro-PD). We document the evidence for the construction of Pro-PD and the design decisions taken. We evaluate Pro-PD through comparison with prominent existing approaches and standards.
Results
This research presents a Process Model for Product Derivation (Pro-PD). Pro-PD describes the tasks, roles and work artefacts used to derive products from a software product line.
Conclusion
In response to a need for methodological support, we developed Pro-PD (Process Model for Product Derivation). Pro-PD was iteratively developed and evaluated through four research stages. Our research is a first step toward an evidence-based methodology for product derivation and a starting point for the definition of a product derivation approach.},
  comment       = {15},
  doi           = {https://doi.org/10.1016/j.infsof.2012.03.008},
  keywords      = {Software product lines, Product derivation, Process},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584912000572},
}

@Article{Pessoa2017,
  author        = {Leonardo Pessoa and Paula Fernandes and Thiago Castro and Vander Alves and GenaÃ­na N. Rodrigues and Hervaldo Carvalho},
  title         = {Building reliable and maintainable Dynamic Software Product Lines: An investigation in the Body Sensor Network domain},
  journal       = {Information and Software Technology},
  year          = {2017},
  volume        = {86},
  pages         = {54 - 70},
  issn          = {0950-5849},
  __markedentry = {[mac:]},
  abstract      = {Context: Dependability is a key requirement, especially in safety-critical applications. Many of these applications have changing context and configurations at runtime to achieve functional and quality goals and can be realized as Dynamic Software Product Lines (DSPLs). DSPL constitutes an emerging but promising research area. Nevertheless, ensuring dependability in DSPLs remains insufficiently explored, especially in terms of reliability and maintainability. This compromises quality assurance and applicability of DSPLs in safety-critical domains, such as Body Sensor Network (BSN). Objective: To address this issue, we propose an approach to developing reliable and maintainable DSPLs in the context of the BSN domain. Method: Adaptation plans are instances of a Domain Specific Language (DSL) describing reliability goals and adaptability at runtime. These instances are automatically checked for reliability goal satisfiability before being deployed and interpreted at runtime to provide more suitable adaptation goals complying with evolving needs perceived by a domain specialist. Results: The approach is evaluated in the BSN domain. Results show that reliability and maintainability could be provided with execution and reconfiguration times of around 30Â ms, notification and adaptation plan update time over the network around 5Â s, and space consumption around 5Â  MB. Conclusion: The method is feasible at reasonable cost. The incurred benefits are reliable vital signal monitoring for the patientâ€”thus providing early detection of serious health issues and the possibility of proactive treatmentâ€”and a maintainable infrastructure allowing medical DSL instance update to suit the needs of the domain specialist and ultimately of the patient.},
  comment       = {16},
  doi           = {https://doi.org/10.1016/j.infsof.2017.02.002},
  keywords      = {Reliability, Maintainability, Dynamic Software Product Lines, Body Sensor Network, Context-awareness, Adaptiveness},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584917301386},
}

@Article{Thoern2010a,
  author        = {Christer ThÃ¶rn},
  title         = {Current state and potential of variability management practices in software-intensive SMEs: Results from a regional industrial survey},
  journal       = {Information and Software Technology},
  year          = {2010},
  volume        = {52},
  number        = {4},
  pages         = {411 - 421},
  issn          = {0950-5849},
  __markedentry = {[mac:]},
  abstract      = {Context
More and more, small and medium-sized enterprises (SMEs) are using software to augment the functionality of their products and offerings. Variability management of software is becoming an interesting topic for SMEs with expanding portfolios and increasingly complex product structures. While the use of software product lines to resolve high variability is well known in larger organizations, there is less known about the practices in SMEs.
Objective
This paper presents results from a survey of software developing SMEs. The purpose of the paper is to provide a snapshot of the current awareness and practices of variability modeling in organizations that are developing software with the constraints present in SMEs.
Method
A survey with questions regarding the variability practices was distributed to software developing organizations in a region of Sweden that has many SMEs. The response rate was 13% and 25 responses are used in this analysis.
Results
We find that, although there are SMEs that develop implicit software product lines and have relatively sophisticated variability structures for their solution space, the structures of the problem space and the product space have room for improvement.
Conclusions
The answers in the survey indicate that SMEs are in situations where they can benefit from more structured variability management, but the awareness need to be raised. Even though the problem space similarity is high, there is little reuse and traceability activities performed. The existence of SMEs with qualified variability management and product line practices indicates that small organizations are capable to apply such practices.},
  comment       = {11},
  doi           = {https://doi.org/10.1016/j.infsof.2009.10.009},
  keywords      = {Software engineering, Variability, SME, Survey},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584909001906},
}

@Article{Siegmund2013a,
  author        = {Norbert Siegmund and Marko RosenmÃ¼ller and Christian KÃ¤stner and Paolo G. Giarrusso and Sven Apel and Sergiy S. Kolesnikov},
  title         = {Scalable prediction of non-functional properties in software product lines: Footprint and memory consumption},
  journal       = {Information and Software Technology},
  year          = {2013},
  volume        = {55},
  number        = {3},
  pages         = {491 - 507},
  issn          = {0950-5849},
  note          = {Special Issue on Software Reuse and Product Lines},
  __markedentry = {[mac:]},
  abstract      = {Context
A software product line is a family of related software products, typically created from a set of common assets. Users select features to derive a product that fulfills their needs. Users often expect a product to have specific non-functional properties, such as a small footprint or a bounded response time. Because a product line may have an exponential number of products with respect to its features, it is usually not feasible to generate and measure non-functional properties for each possible product.
Objective
Our overall goal is to derive optimal products with respect to non-functional requirements by showing customers which features must be selected.
Method
We propose an approach to predict a productâ€™s non-functional properties based on the productâ€™s feature selection. We aggregate the influence of each selected feature on a non-functional property to predict a productâ€™s properties. We generate and measure a small set of products and, by comparing measurements, we approximate each featureâ€™s influence on the non-functional property in question. As a research method, we conducted controlled experiments and evaluated prediction accuracy for the non-functional properties footprint and main-memory consumption. But, in principle, our approach is applicable for all quantifiable non-functional properties.
Results
With nine software product lines, we demonstrate that our approach predicts the footprint with an average accuracy of 94%, and an accuracy of over 99% on average if feature interactions are known. In a further series of experiments, we predicted main memory consumption of six customizable programs and achieved an accuracy of 89% on average.
Conclusion
Our experiments suggest that, with only few measurements, it is possible to accurately predict non-functional properties of products of a product line. Furthermore, we show how already little domain knowledge can improve predictions and discuss trade-offs between accuracy and required number of measurements. With this technique, we provide a basis for many reasoning and product-derivation approaches.},
  comment       = {17},
  doi           = {https://doi.org/10.1016/j.infsof.2012.07.020},
  keywords      = {Non-functional properties, Prediction, Measurement, Software product lines, SPL Conqueror},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584912001541},
}

@Article{Marinho2013,
  author        = {Fabiana G. Marinho and Rossana M.C. Andrade and ClÃ¡udia Werner and Windson Viana and Marcio E.F. Maia and Lincoln S. Rocha and EldÃ¢nae Teixeira and JoÃ£o B. Ferreira Filho and ValÃ©ria L.L. Dantas and FabrÃ­cio Lima and Saulo Aguiar},
  title         = {MobiLine: A Nested Software Product Line for the domain of mobile and context-aware applications},
  journal       = {Science of Computer Programming},
  year          = {2013},
  volume        = {78},
  number        = {12},
  pages         = {2381 - 2398},
  issn          = {0167-6423},
  note          = {Special Section on International Software Product Line Conference 2010 and Fundamentals of Software Engineering (selected papers of FSEN 2011)},
  __markedentry = {[mac:]},
  abstract      = {Mobile devices are multipurpose and multi-sensor equipments supporting applications able to adapt their behavior according to changes in the userâ€™s context (device, location, time, etc.). Meanwhile, the development of mobile and context-aware software is not a simple task, mostly due to the peculiar characteristics of these devices. Although several solutions have been proposed to facilitate their development, reuse is not systematically used throughout the software development life-cycle. In this paper, we discuss an approach for the development of mobile and context-aware software using the Software Product Line (SPL) paradigm. Furthermore, a Nested SPL for the domain of mobile and context-aware applications is presented, lessons learned in the SPL development are discussed and a product for a context-aware visit guide is shown.},
  comment       = {18},
  doi           = {https://doi.org/10.1016/j.scico.2012.04.009},
  keywords      = {Context-awareness, Mobility, Software product line},
  url           = {http://www.sciencedirect.com/science/article/pii/S0167642312000871},
}

@Article{Martinez-Fernandez2017,
  author        = {Silverio MartÃ­nez-FernÃ¡ndez and Claudia P. Ayala and Xavier Franch and Helena Martins Marques},
  title         = {Benefits and drawbacks of software reference architectures: A case study},
  journal       = {Information and Software Technology},
  year          = {2017},
  volume        = {88},
  pages         = {37 - 52},
  issn          = {0950-5849},
  __markedentry = {[mac:]},
  abstract      = {Context
Software Reference Architectures (SRAs) play a fundamental role for organizations whose business greatly depends on the efficient development and maintenance of complex software applications. However, little is known about the real value and risks associated with SRAs in industrial practice.
Objective
To investigate the current industrial practice of SRAs in a single company from the perspective of different stakeholders.
Method
An exploratory case study that investigates the benefits and drawbacks perceived by relevant stakeholders in nine SRAs designed by a multinational software consulting company.
Results
The study shows the perceptions of different stakeholders regarding the benefits and drawbacks of SRAs (e.g., both SRA designers and users agree that they benefit from reduced development costs; on the contrary, only application builders strongly highlighted the extra learning curve as a drawback associated with mastering SRAs). Furthermore, some of the SRA benefits and drawbacks commonly highlighted in the literature were remarkably not mentioned as a benefit of SRAs (e.g., the use of best practices). Likewise, other aspects arose that are not usually discussed in the literature, such as higher time-to-market for applications when their dependencies on the SRA are managed inappropriately.
Conclusions
This study aims to help practitioners and researchers to better understand real SRAs projects and the contexts where these benefits and drawbacks appeared, as well as some SRA improvement strategies. This would contribute to strengthening the evidence regarding SRAs and support practitioners in making better informed decisions about the expected SRA benefits and drawbacks. Furthermore, we make available the instruments used in this study and the anonymized data gathered to motivate others to provide similar evidence to help mature SRA research and practice.},
  comment       = {16},
  doi           = {https://doi.org/10.1016/j.infsof.2017.03.011},
  keywords      = {Software architecture, Reference architecture, Empirical software engineering, Case study, Benefits, Drawbacks},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584916304438},
}

@Article{Lochau2016,
  author        = {Malte Lochau and Stephan Mennicke and Hauke Baller and Lars Ribbeck},
  title         = {Incremental model checking of delta-oriented software product lines},
  journal       = {Journal of Logical and Algebraic Methods in Programming},
  year          = {2016},
  volume        = {85},
  number        = {1, Part 2},
  pages         = {245 - 267},
  issn          = {2352-2208},
  note          = {Formal Methods for Software Product Line Engineering},
  __markedentry = {[mac:]},
  abstract      = {We propose DeltaCCS, a delta-oriented extension to Milner's process calculus CCS to formalize behavioral variability in software product line specifications in a modular way. In DeltaCCS, predefined change directives are applied to core process semantics by overriding the CCS term rewriting rule in a determined way. On this basis, behavioral properties expressed in the Modal Î¼-Calculus are verifiable for entire product-line specifications both product-by-product as well as in a family-based manner as usual. To overcome potential scalability limitations of those existing strategies, we propose a novel approach for incremental model checking of product lines. Therefore, variability-aware congruence notions and a respective normal form for DeltaCCS specifications allow for a rigorous local reasoning on the preservation of behavioral properties after varying CCS specifications. We present a prototypical DeltaCCS model checker implementation based on Maude and provide evaluation results obtained from various experiments concerning efficiency trade-offs compared to existing approaches.},
  comment       = {23},
  doi           = {https://doi.org/10.1016/j.jlamp.2015.09.004},
  keywords      = {Variability modeling, Operational semantics, Model checking},
  url           = {http://www.sciencedirect.com/science/article/pii/S2352220815000863},
}

@Article{Khurum2009a,
  author        = {Mahvish Khurum and Tony Gorschek},
  title         = {A systematic review of domain analysis solutions for product lines},
  journal       = {Journal of Systems and Software},
  year          = {2009},
  volume        = {82},
  number        = {12},
  pages         = {1982 - 2003},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {Domain analysis is crucial and central to software product line engineering (SPLE) as it is one of the main instruments to decide what to include in a product and how it should fit in to the overall software product line. For this reason many domain analysis solutions have been proposed both by researchers and industry practitioners. Domain analysis comprises various modeling and scoping activities. This paper presents a systematic review of all the domain analysis solutions presented until 2007. The goal of the review is to analyze the level of industrial application and/or empirical validation of the proposed solutions with the purpose of mapping maturity in terms of industrial application, as well as to what extent proposed solutions might have been evaluated in terms of usability and usefulness. The finding of this review indicates that, although many new domain analysis solutions for software product lines have been proposed over the years, the absence of qualitative and quantitative results from empirical application and/or validation makes it hard to evaluate the potential of proposed solutions with respect to their usability and/or usefulness for industry adoption. The detailed results of the systematic review can be used by individual researchers to see large gaps in research that give opportunities for future work, and from a general research perspective lessons can be learned from the absence of validation as well as from good examples presented. From an industry practitioner view, the results can be used to gauge as to what extent solutions have been applied and/or validated and in what manner, both valuable as input prior to industry adoption of a domain analysis solution.},
  comment       = {21},
  doi           = {https://doi.org/10.1016/j.jss.2009.06.048},
  keywords      = {Systematic review, Domain analysis, Domain modeling, Domain scoping, Empirical evidence, Usability, Usefulness},
  url           = {http://www.sciencedirect.com/science/article/pii/S016412120900154X},
}

@Article{Pascual2015,
  author        = {Gustavo G. Pascual and Roberto E. Lopez-Herrejon and MÃ³nica Pinto and Lidia Fuentes and Alexander Egyed},
  title         = {Applying multiobjective evolutionary algorithms to dynamic software product lines for reconfiguring mobile applications},
  journal       = {Journal of Systems and Software},
  year          = {2015},
  volume        = {103},
  pages         = {392 - 411},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {Mobile applications require dynamic reconfiguration services (DRS) to self-adapt their behavior to the context changes (e.g., scarcity of resources). Dynamic Software Product Lines (DSPL) are a well-accepted approach to manage runtime variability, by means of late binding the variation points at runtime. During the systemâ€™s execution, the DRS deploys different configurations to satisfy the changing requirements according to a multiobjective criterion (e.g., insufficient battery level, requested quality of service). Search-based software engineering and, in particular, multiobjective evolutionary algorithms (MOEAs), can generate valid configurations of a DSPL at runtime. Several approaches use MOEAs to generate optimum configurations of a Software Product Line, but none of them consider DSPLs for mobile devices. In this paper, we explore the use of MOEAs to generate at runtime optimum configurations of the DSPL according to different criteria. The optimization problem is formalized in terms of a Feature Model (FM), a variability model. We evaluate six existing MOEAs by applying them to 12 different FMs, optimizing three different objectives (usability, battery consumption and memory footprint). The results are discussed according to the particular requirements of a DRS for mobile applications, showing that PAES and NSGA-II are the most suitable algorithms for mobile environments.},
  comment       = {20},
  doi           = {https://doi.org/10.1016/j.jss.2014.12.041},
  keywords      = {DSPL, Dynamic reconfiguration, Evolutionary algorithms,},
  url           = {http://www.sciencedirect.com/science/article/pii/S016412121400291X},
}

@Article{Souza2013b,
  author        = {Iuri Santos Souza and Gecynalda Soares da Silva Gomes and Paulo Anselmo da Mota Silveira Neto and Ivan do Carmo Machado and Eduardo Santana de Almeida and Silvio Romero de Lemos Meira},
  title         = {Evidence of software inspection on feature specification for software product lines},
  journal       = {Journal of Systems and Software},
  year          = {2013},
  volume        = {86},
  number        = {5},
  pages         = {1172 - 1190},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {In software product lines (SPL), scoping is a phase responsible for capturing, specifying and modeling features, and also their constraints, interactions and variations. The feature specification task, performed in this phase, is usually based on natural language, which may lead to lack of clarity, non-conformities and defects. Consequently, scoping analysts may introduce ambiguity, inconsistency, omissions and non-conformities. In this sense, this paper aims at gathering evidence about the effects of applying an inspection approach to feature specification for SPL. Data from a SPL reengineering project were analyzed in this work and the analysis indicated that the correction activity demanded more effort. Also, Pareto's principle showed that incompleteness and ambiguity reported higher non-conformity occurrences. Finally, the Poisson regression analysis showed that sub-domain risk information can be a good indicator for prioritization of sub-domains in the inspection activity.},
  comment       = {19},
  doi           = {https://doi.org/10.1016/j.jss.2012.11.044},
  keywords      = {Software quality control, Software inspection, Software product lines, Empirical study},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121212003251},
}

@Article{Marinho2017a,
  author        = {Anderson Marinho and Daniel de Oliveira and Eduardo Ogasawara and Vitor Silva and Kary OcaÃ±a and Leonardo Murta and Vanessa Braganholo and Marta Mattoso},
  title         = {Deriving scientific workflows from algebraic experiment lines: A practical approach},
  journal       = {Future Generation Computer Systems},
  year          = {2017},
  volume        = {68},
  pages         = {111 - 127},
  issn          = {0167-739X},
  __markedentry = {[mac:]},
  abstract      = {The exploratory nature of a scientific computational experiment involves executing variations of the same workflow with different approaches, programs, and parameters. However, current approaches do not systematize the derivation process from the experiment definition to the concrete workflows and do not track the experiment provenance down to the workflow executions. Therefore, the composition, execution, and analysis for the entire experiment become a complex task. To address this issue, we propose the Algebraic Experiment Line (AEL). AEL uses a data-centric workflow algebra, which enriches the experiment representation by introducing a uniform data model and its corresponding operators. This representation and the AEL provenance model map concepts from the workflow execution data to the AEL derived workflows with their corresponding experiment abstract definitions. We show how AEL has improved the understanding of a real experiment in the bioinformatics area. By combining provenance data from the experiment and its corresponding executions, AEL provenance queries navigate from experiment concepts defined at high abstraction level to derived workflows and their execution data. It also shows a direct way of querying results from different trials involving activity variations and optionalities, only present at the experiment level of abstraction.},
  comment       = {17},
  doi           = {https://doi.org/10.1016/j.future.2016.08.016},
  keywords      = {Scientific workflows, Software product line, Workflow algebra, Workflow derivation},
  url           = {http://www.sciencedirect.com/science/article/pii/S0167739X1630276X},
}

@Article{Souza2015,
  author        = {Leandro Oliveira de Souza and PÃ¡draig Oâ€™Leary and Eduardo Santana de Almeida and SÃ­lvio Romero de Lemos Meira},
  title         = {Product derivation in practice},
  journal       = {Information and Software Technology},
  year          = {2015},
  volume        = {58},
  pages         = {319 - 337},
  issn          = {0950-5849},
  __markedentry = {[mac:]},
  abstract      = {Context
The process of constructing a product from a product line of software assets is known product derivation. An effective product derivation process is important in order to ensure that the efforts required to develop these shared assets is lower than the benefits achieved through their use. Despite its importance, relatively little work has been dedicated to the product derivation process and the strategies applied in practice. Additionally, there is a lack of empirical reports describing product derivation in industrial settings, and, in general, where these reports are available, they have been conducted as informal studies.
Objective
Our aim is to investigate how product derivation is performed in practice.
Method
We apply a multi-case study design to two different industrial software product line projects with the goal of investigating how they derive their products in practice. The findings from our studies were individually analyzed using the Constant Comparison technique. In order to identify patterns across these studies, the findings were compared using a Cross-case analysis approach.
Results
The research approach allowed us to examine the case study outcomes from different perspectives, capturing similarities and differences. From the cases, we identified context specific strategies for product derivation which are easier for practitioners to contextualise and implement.
Conclusions
The case studies provide method-in-action insights into concepts explored in the literature, such as: iterative and incremental product derivation, instantiation and integration of platform components and derivation of product databases. Practitioners can use this work as a basis for defining, adapting or evaluating their own product derivation approaches. While researchers can use this work as a starting point for new industrial reports, presenting their experiences with product derivation.},
  comment       = {19},
  doi           = {https://doi.org/10.1016/j.infsof.2014.07.004},
  keywords      = {Case study, Multiple case study, Constant comparison analysis, Cross-case analysis, Product derivation, Software product lines},
  url           = {http://www.sciencedirect.com/science/article/pii/S095058491400158X},
}

@Article{Bjarnason2016,
  author        = {Elizabeth Bjarnason and Michael Unterkalmsteiner and Markus Borg and Emelie EngstrÃ¶m},
  title         = {A multi-case study of agile requirements engineering and the use of test cases as requirements},
  journal       = {Information and Software Technology},
  year          = {2016},
  volume        = {77},
  pages         = {61 - 79},
  issn          = {0950-5849},
  __markedentry = {[mac:]},
  abstract      = {Context
It is an enigma that agile projects can succeed â€˜without requirementsâ€™ when weak requirements engineering is a known cause for project failures. While agile development projects often manage well without extensive requirements test cases are commonly viewed as requirements and detailed requirements are documented as test cases.
Objective
We have investigated this agile practice of using test cases as requirements to understand how test cases can support the main requirements activities, and how this practice varies.
Method
We performed an iterative case study at three companies and collected data through 14 interviews and two focus groups.
Results
The use of test cases as requirements poses both benefits and challenges when eliciting, validating, verifying, and managing requirements, and when used as a documented agreement. We have identified five variants of the test-cases-as-requirements practice, namely de facto, behaviour-driven, story-test driven, stand-alone strict and stand-alone manual for which the application of the practice varies concerning the time frame of requirements documentation, the requirements format, the extent to which the test cases are a machine executable specification and the use of tools which provide specific support for the practice of using test cases as requirements.
Conclusions
The findings provide empirical insight into how agile development projects manage and communicate requirements. The identified variants of the practice of using test cases as requirements can be used to perform in-depth investigations into agile requirements engineering. Practitioners can use the provided recommendations as a guide in designing and improving their agile requirements practices based on project characteristics such as number of stakeholders and rate of change.},
  comment       = {19},
  doi           = {https://doi.org/10.1016/j.infsof.2016.03.008},
  keywords      = {Agile development, Requirements, Testing, Test-first development, Test-driven development, Behaviour-driven development, Acceptance test, Case study, Empirical software engineering},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584916300544},
}

@Article{Heradio2016,
  author        = {Ruben Heradio and Hector Perez-Morago and David Fernandez-Amoros and Francisco Javier Cabrerizo and Enrique Herrera-Viedma},
  title         = {A bibliometric analysis of 20 years of research on software product lines},
  journal       = {Information and Software Technology},
  year          = {2016},
  volume        = {72},
  pages         = {1 - 15},
  issn          = {0950-5849},
  __markedentry = {[mac:]},
  abstract      = {Context: Software product line engineering has proven to be an efficient paradigm to developing families of similar software systems at lower costs, in shorter time, and with higher quality. Objective: This paper analyzes the literature on product lines from 1995 to 2014, identifying the most influential publications, the most researched topics, and how the interest in those topics has evolved along the way. Method: Bibliographic data have been gathered from ISI Web of Science and Scopus. The data have been examined using two prominent bibliometric approaches: science mapping and performance analysis. Results: According to the study carried out, (i) software architecture was the initial motor of research in SPL; (ii) work on systematic software reuse has been essential for the development of the area; and (iii) feature modeling has been the most important topic for the last fifteen years, having the best evolution behavior in terms of number of published papers and received citations. Conclusion: Science mapping has been used to identify the main researched topics, the evolution of the interest in those topics and the relationships among topics. Performance analysis has been used to recognize the most influential papers, the journals and conferences that have published most papers, how numerous is the literature on product lines and what is its distribution over time.},
  comment       = {16},
  doi           = {https://doi.org/10.1016/j.infsof.2015.11.004},
  keywords      = {Software product lines, Bibliometrics, Science mapping, Performance analysis},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584915001883},
}

@Article{Font2017a,
  author        = {Jaime Font and Lorena Arcega and Ã˜ystein Haugen and Carlos Cetina},
  title         = {Leveraging variability modeling to address metamodel revisions in Model-based Software Product Lines},
  journal       = {Computer Languages, Systems \& Structures},
  year          = {2017},
  volume        = {48},
  pages         = {20 - 38},
  issn          = {1477-8424},
  note          = {Special Issue on the 14th International Conference on Generative Programming: Concepts \& Experiences (GPCE'15)},
  __markedentry = {[mac:]},
  abstract      = {Metamodels evolve over time, which can break the conformance between the models and the metamodel. Model migration strategies aim to co-evolve models and metamodels together, but their application is currently not fully automatizable and is thus cumbersome and error prone. We introduce the Variable MetaModel (VMM) strategy to address the evolution of the reusable model assets of a model-based Software Product Line. The VMM strategy applies variability modeling ideas to express the evolution of the metamodel in terms of commonalities and variabilities. When the metamodel evolves, changes are automatically formalized into the VMM and models that conform to previous versions of the metamodel continue to conform to the VMM, thus eliminating the need for migration. We have applied both the traditional migration strategy and the VMM strategy to a retrospective case study that includes 13years of evolution of our industrial partner, an induction hobs manufacturer. The comparison between the two strategies shows better results for the VMM strategy in terms of model indirection, automation, and trust leak.},
  comment       = {19},
  doi           = {https://doi.org/10.1016/j.cl.2016.08.003},
  keywords      = {Model-based Software Product Lines, Variability Modeling, Model and metamodel co-evolution},
  url           = {http://www.sciencedirect.com/science/article/pii/S147784241630001X},
}

@Article{Mellado2014,
  author        = {Daniel Mellado and Haralambos Mouratidis and Eduardo FernÃ¡ndez-Medina},
  title         = {Secure Tropos framework for software product lines requirements engineering},
  journal       = {Computer Standards \& Interfaces},
  year          = {2014},
  volume        = {36},
  number        = {4},
  pages         = {711 - 722},
  issn          = {0920-5489},
  note          = {Security in Information Systems: Advances and new Challenges.},
  __markedentry = {[mac:]},
  abstract      = {Security and requirements engineering are two of the most important factors of success in the development of a software product line (SPL). Goal-driven security requirements engineering approaches, such as Secure Tropos, have been proposed as a suitable paradigm for elicitation of security requirements and their analysis on both a social and a technical dimension. Nevertheless, goal-driven security requirements engineering methodologies are not appropriately tailored to the specific demands of SPL, while on the other hand specific proposals of SPL engineering have traditionally ignored security requirements. This paper presents work that fills this gap by proposing â€œSecureTropos-SPLâ€ framework.},
  comment       = {12},
  doi           = {https://doi.org/10.1016/j.csi.2013.12.006},
  keywords      = {Security requirements, Product lines, Requirements engineering, Security requirement engineering, Secure Tropos},
  url           = {http://www.sciencedirect.com/science/article/pii/S0920548913001803},
}

@Article{Bakar2015,
  author        = {Noor Hasrina Bakar and Zarinah M. Kasirun and Norsaremah Salleh},
  title         = {Feature extraction approaches from natural language requirements for reuse in software product lines: A systematic literature review},
  journal       = {Journal of Systems and Software},
  year          = {2015},
  volume        = {106},
  pages         = {132 - 149},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {Requirements for implemented system can be extracted and reused for a production of a new similar system. Extraction of common and variable features from requirements leverages the benefits of the software product lines engineering (SPLE). Although various approaches have been proposed in feature extractions from natural language (NL) requirements, no related literature review has been published to date for this topic. This paper provides a systematic literature review (SLR) of the state-of-the-art approaches in feature extractions from NL requirements for reuse in SPLE. We have included 13 studies in our synthesis of evidence and the results showed that hybrid natural language processing approaches were found to be in common for overall feature extraction process. A mixture of automated and semi-automated feature clustering approaches from data mining and information retrieval were also used to group common features, with only some approaches coming with support tools. However, most of the support tools proposed in the selected studies were not made available publicly and thus making it hard for practitionersâ€™ adoption. As for the evaluation, this SLR reveals that not all studies employed software metrics as ways to validate experiments and case studies. Finally, the quality assessment conducted confirms that practitionersâ€™ guidelines were absent in the selected studies.},
  comment       = {18},
  doi           = {https://doi.org/10.1016/j.jss.2015.05.006},
  keywords      = {Feature extractions, Requirements reuse, Software product lines, Natural language requirements, Systematic literature review},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121215001004},
}

@Article{Mariani2016a,
  author        = {ThainÃ¡ Mariani and Thelma Elita Colanzi and Silvia Regina Vergilio},
  title         = {Preserving architectural styles in the search based design of software product line architectures},
  journal       = {Journal of Systems and Software},
  year          = {2016},
  volume        = {115},
  pages         = {157 - 173},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {Architectural styles help to improve the Product Line Architecture (PLA) design by providing a better organization of its elements, which results in some benefits, like flexibility, extensibility and maintainability. The PLA design can also be improved by using a search based optimization approach, taking into account different metrics, such as cohesion, coupling and feature modularization. However, the application of search operators changes the PLA organization, and consequently may violate the architectural styles rules, impacting negatively in the architecture understanding. To overcome such limitation, this work introduces a set of search operators to be used in the search based design with the goal of preserving the architectural styles during the optimization process. Such operators consider rules of the layered and client/server architectural styles, generally used in the search based design of conventional architectures and PLAs. The operators are implemented and evaluated in the context of MOA4PLA, a Multi-objective Optimization Approach for PLA Design. Results from an empirical evaluation show that the proposed operators contribute to obtain better solutions, preserving the adopted style and also improving some software metric values.},
  comment       = {17},
  doi           = {https://doi.org/10.1016/j.jss.2016.01.039},
  keywords      = {Architectural style, Software product line, Search based design},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121216000364},
}

@Article{Montalvillo2016,
  author        = {Leticia Montalvillo and Oscar DÃ­az},
  title         = {Requirement-driven evolution in software product lines: A systematic mapping study},
  journal       = {Journal of Systems and Software},
  year          = {2016},
  volume        = {122},
  pages         = {110 - 143},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {CONTEXT. Software Product Lines (SPLs) aim to support the development of a whole family of software products through systematic reuse of shared assets. As SPLs exhibit a long life-span, evolution is an even greater concern than for single-systems. For the purpose of this work, evolution refers to the adaptation of the SPL as a result of changing requirements. Hence, evolution is triggered by requirement changes, and not by bug fixing or refactoring. OBJECTIVE. Research on SPL evolution has not been previously mapped. This work provides a mapping study along Petersenâ€™s and Kichenhamâ€™s guidelines, to identify strong areas of knowledge, trends and gaps. RESULTS. We identified 107 relevant contributions. They were classified according to four facets: evolution activity (e.g., identify, analyze and plan, implement), product-derivation approach (e.g., annotation-based, composition-based), research type (e.g., solution, experience, evaluation), and asset type (i.e., variability model, SPL architecture, code assets and products). CONCLUSION. Analyses of the results indicate that â€œSolution proposalsâ€ are the most common type of contribution (31%). Regarding the evolution activity, â€œImplement changeâ€ (43%) and â€œAnalyze and plan changeâ€ (37%) are the most covered ones. A finer-grained analysis uncovered some tasks as being underexposed. A detailed description of the 107 papers is also included.},
  comment       = {34},
  doi           = {https://doi.org/10.1016/j.jss.2016.08.053},
  keywords      = {Systematic mapping study, Software product lines, Evolution},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121216301510},
}

@Article{Deelstra2005a,
  author        = {Sybren Deelstra and Marco Sinnema and Jan Bosch},
  title         = {Product derivation in software product families: a case study},
  journal       = {Journal of Systems and Software},
  year          = {2005},
  volume        = {74},
  number        = {2},
  pages         = {173 - 194},
  issn          = {0164-1212},
  note          = {The new context for software engineering education and training},
  __markedentry = {[mac:]},
  abstract      = {From our experience with several organizations that employ software product families, we have learned that, contrary to popular belief, deriving individual products from shared software assets is a time-consuming and expensive activity. In this paper we therefore present a study that investigated the source of those problems. We provide the reader with a framework of terminology and concepts regarding product derivation. In addition, we present several problems and issues we identified during a case study at two large industrial organizations that are relevant to other, for example, comparable or less mature organizations.},
  comment       = {22},
  doi           = {https://doi.org/10.1016/j.jss.2003.11.012},
  keywords      = {Case study, Software product family, Product derivation, Variability management},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121203003121},
}

@Article{Krishnan2013,
  author        = {Sandeep Krishnan and Chris Strasburg and Robyn R. Lutz and Katerina Goseva-Popstojanova and Karin S. Dorman},
  title         = {Predicting failure-proneness in an evolving software product line},
  journal       = {Information and Software Technology},
  year          = {2013},
  volume        = {55},
  number        = {8},
  pages         = {1479 - 1495},
  issn          = {0950-5849},
  __markedentry = {[mac:]},
  abstract      = {Context
Previous work by researchers on 3years of early data for an Eclipse product has identified some predictors of failure-prone files that work well. Eclipse has also been used previously by researchers to study characteristics of product line software.
Objective
The work reported here investigates whether classification-based prediction of failure-prone files improves as the product line evolves.
Method
This investigation first repeats, to the extent possible, the previous study and then extends it by including four more recent years of data, comparing the prominent predictors with the previous results. The research then looks at the data for three additional Eclipse products as they evolve over time. The analysis compares results from three different types of datasets with alternative data collection and prediction periods.
Results
Our experiments with a variety of learners show that the difference between the performance of J48, used in this work, and the other top learners is not statistically significant. Furthermore, new results show that the effectiveness of classification significantly depends on the data collection period and prediction period. The study identifies change metrics that are prominent predictors across all four releases of all four products in the product line for the three different types of datasets. From the product line perspective, prediction of failure-prone files for the four products studied in the Eclipse product line shows statistically significant improvement in accuracy but not in recall across releases.
Conclusion
As the product line matures, the learner performance improves significantly for two of the three datasets, but not for prediction of post-release failure-prone files using only pre-release change data. This suggests that it may be difficult to detect failure-prone files in the evolving product line. At least in part, this may be due to the continuous change, even for commonalities and high-reuse variation components, which we previously have shown to exist.},
  comment       = {17},
  doi           = {https://doi.org/10.1016/j.infsof.2012.11.008},
  keywords      = {Software product lines, Change metrics, Reuse, Prediction, Post-release defects, Failure-prone files},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584912002340},
}

@Article{Seidl2017a,
  author        = {Christoph Seidl and Sven Schuster and Ina Schaefer},
  title         = {Generative software product line development using variability-aware design patterns},
  journal       = {Computer Languages, Systems \& Structures},
  year          = {2017},
  volume        = {48},
  pages         = {89 - 111},
  issn          = {1477-8424},
  note          = {Special Issue on the 14th International Conference on Generative Programming: Concepts \& Experiences (GPCE'15)},
  __markedentry = {[mac:]},
  abstract      = {Software Product Lines (SPLs) are an approach to reuse in-the-large that models a set of closely related software systems in terms of commonalities and variabilities. Design patterns are best practices for addressing recurring design problems in object-oriented source code. In the practice of implementing SPL, instances of certain design patterns are employed to handle variability, which makes these â€œvariability-aware design patternsâ€ a best practice for SPL design. However, currently there is no dedicated method for proactively developing SPLs using design patterns suitable for realizing variable functionality. In this paper, we present a method to perform generative SPL development with design patterns. We use role models to capture design patterns and their relation to a variability model. We further allow mapping of individual design pattern roles to (parts of) implementation elements to be generated (e.g., classes, methods) and check the conformance of the realization with the specification of the pattern. We provide definitions for the variability-aware versions of the design patterns Observer, Strategy, Template Method and Composite. Furthermore, we support generation of realizations in Java, C++ and UML class diagrams utilizing annotative, compositional and transformational variability realization mechanisms. Hence, we support proactive development of SPLs using design patterns to apply best practices for the realization of variability. We realize our concepts within the Eclipse IDE and demonstrate them within a case study.},
  comment       = {23},
  doi           = {https://doi.org/10.1016/j.cl.2016.08.006},
  url           = {http://www.sciencedirect.com/science/article/pii/S1477842415300609},
}

@Article{Rosso2008,
  author        = {Christian Del Rosso},
  title         = {Software performance tuning of software product family architectures: Two case studies in the real-time embedded systems domain},
  journal       = {Journal of Systems and Software},
  year          = {2008},
  volume        = {81},
  number        = {1},
  pages         = {1 - 19},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {Software performance is an important non-functional quality attribute and software performance evaluation is an essential activity in the software development process. Especially in embedded real-time systems, software design and evaluation are driven by the needs to optimize the limited resources, to respect time deadlines and, at the same time, to produce the best experience for end-users. Software product family architectures add additional requirements to the evaluation process. In this case, the evaluation includes the analysis of the optimizations and tradeoffs for the whole products in the family. Performance evaluation of software product family architectures requires knowledge and a clear understanding of different domains: software architecture assessments, software performance and software product family architecture. We have used a scenario-driven approach to evaluate performance and dynamic memory management efficiency in one Nokia software product family architecture. In this paper we present two case studies. Furthermore, we discuss the implications and tradeoffs of software performance against evolvability and maintenability in software product family architectures.},
  comment       = {19},
  doi           = {https://doi.org/10.1016/j.jss.2007.07.006},
  keywords      = {Software performance, Software product family, Software architecture assessments, Embedded real-time systems, Dynamic memory management},
  url           = {http://www.sciencedirect.com/science/article/pii/S016412120700180X},
}

@Article{Deelstra2009,
  author        = {Sybren Deelstra and Marco Sinnema and Jan Bosch},
  title         = {Variability assessment in software product families},
  journal       = {Information and Software Technology},
  year          = {2009},
  volume        = {51},
  number        = {1},
  pages         = {195 - 218},
  issn          = {0950-5849},
  note          = {Special Section - Most Cited Articles in 2002 and Regular Research Papers},
  __markedentry = {[mac:]},
  abstract      = {Software variability management is a key factor in the success of software systems and software product families. An important aspect of software variability management is the evolution of variability in response to changing markets, business needs, and advances in technology. To be able to determine whether, when, and how variability should evolve, we have developed the COVAMOF software variability assessment method (COSVAM). The contribution of COSVAM is that it is a novel, and industry-strength assessment process that addresses the issues that are associated to the current variability assessment practice. In this paper, we present the successful validation of COSVAM in an industrial software product family.},
  comment       = {24},
  doi           = {https://doi.org/10.1016/j.infsof.2008.04.002},
  keywords      = {Software product families, Variability, Assessment, Evolution},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584908000542},
}

@Article{Lutz2003,
  author        = {Robyn R. Lutz and Gerald C. Gannod},
  title         = {Analysis of a software product line architecture: an experience report},
  journal       = {Journal of Systems and Software},
  year          = {2003},
  volume        = {66},
  number        = {3},
  pages         = {253 - 267},
  issn          = {0164-1212},
  note          = {Software architecture -- Engineering quality attributes},
  __markedentry = {[mac:]},
  abstract      = {This paper describes experiences with the architectural specification and tool-assisted architectural analysis of a mission-critical, high-performance software product line. The approach used defines a â€œgoodâ€ product line architecture in terms of those quality attributes required by the particular product line under development. Architectures are analyzed against several criteria by both manual and tool-supported methods. The approach described in this paper provides a structured analysis of an existing product line architecture using (1) architecture recovery and specification, (2) architecture evaluation, and (3) model checking of behavior to determine the level of robustness and fault tolerance at the architectural level that are required for all systems in the product line. Results of an application to a software product line of spaceborne telescopes are used to explain the approach and describe lessons learned.},
  comment       = {15},
  doi           = {https://doi.org/10.1016/S0164-1212(02)00081-X},
  url           = {http://www.sciencedirect.com/science/article/pii/S016412120200081X},
}

@Article{Tuezuen2015a,
  author        = {Eray TÃ¼zÃ¼n and Bedir Tekinerdogan},
  title         = {Analyzing impact of experience curve on ROI in the software product line adoption process},
  journal       = {Information and Software Technology},
  year          = {2015},
  volume        = {59},
  pages         = {136 - 148},
  issn          = {0950-5849},
  __markedentry = {[mac:]},
  abstract      = {Context
Experience curve is a well-known concept in management and education science, which explains the phenomenon of increased worker efficiency with repetitive production of a good or service.
Objective
We aim to analyze the impact of the experience curve effect on the Return on Investment (ROI) in the software product line engineering (SPLE) process.
Method
We first present the results of a systematic literature review (SLR) to explicitly depict the studies that have considered the impact of experience curve effect on software development in general. Subsequently, based on the results of the SLR, the experience curve effect models in the literature, and the SPLE cost models, we define an approach for extending the cost models with the experience curve effect. Finally, we discuss the application of the refined cost models in a real industrial context.
Results
The SLR resulted in 15 primary studies which confirm the impact of experience curve effect on software development in general but the experience curve effect in the adoption of SPLE got less attention. The analytical discussion of the cost models and the application of the refined SPLE cost models in the industrial context showed a clear impact of the experience curve effect on the time-to-market, cost of development and ROI in the SPLE adoption process.
Conclusions
The proposed analysis with the newly defined cost models for SPLE adoption provides a more precise analysis tool for the management, and as such helps to support a better decision making.},
  comment       = {13},
  doi           = {https://doi.org/10.1016/j.infsof.2014.09.008},
  keywords      = {Experience curve, Learning curve, Software product line engineering, Cost models, Productivity, Software reuse},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584914002079},
}

@Article{White2014a,
  author        = {Jules White and JosÃ© A. Galindo and Tripti Saxena and Brian Dougherty and David Benavides and Douglas C. Schmidt},
  title         = {Evolving feature model configurations in software product lines},
  journal       = {Journal of Systems and Software},
  year          = {2014},
  volume        = {87},
  pages         = {119 - 136},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {The increasing complexity and cost of software-intensive systems has led developers to seek ways of reusing software components across development projects. One approach to increasing software reusability is to develop a software product-line (SPL), which is a software architecture that can be reconfigured and reused across projects. Rather than developing software from scratch for a new project, a new configuration of the SPL is produced. It is hard, however, to find a configuration of an SPL that meets an arbitrary requirement set and does not violate any configuration constraints in the SPL. Existing research has focused on techniques that produce a configuration of an SPL in a single step. Budgetary constraints or other restrictions, however, may require multi-step configuration processes. For example, an aircraft manufacturer may want to produce a series of configurations of a plane over a span of years without exceeding a yearly budget to add features. This paper provides three contributions to the study of multi-step configuration for SPLs. First, we present a formal model of multi-step SPL configuration and map this model to constraint satisfaction problems (CSPs). Second, we show how solutions to these SPL configuration problems can be automatically derived with a constraint solver by mapping them to CSPs. Moreover, we show how feature model changes can be mapped to our approach in a multi-step scenario by using feature model drift. Third, we present empirical results demonstrating that our CSP-based reasoning technique can scale to SPL models with hundreds of features and multiple configuration steps.},
  comment       = {18},
  doi           = {https://doi.org/10.1016/j.jss.2013.10.010},
  keywords      = {Software product line, Feature model, Multi-step configuration},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121213002434},
}

@Article{Bosch2010a,
  author        = {Jan Bosch and Petra Bosch-Sijtsema},
  title         = {From integration to composition: On the impact of software product lines, global development and ecosystems},
  journal       = {Journal of Systems and Software},
  year          = {2010},
  volume        = {83},
  number        = {1},
  pages         = {67 - 76},
  issn          = {0164-1212},
  note          = {SI: Top Scholars},
  __markedentry = {[mac:]},
  abstract      = {Three trends accelerate the increase in complexity of large-scale software development, i.e. software product lines, global development and software ecosystems. For the case study companies we studied, these trends caused several problems, which are organized around architecture, process and organization, and the problems are related to the efficiency and effectiveness of software development as these companies used too integration-centric approaches. We present five approaches to software development, organized from integration-centric to composition-oriented and describe the areas of applicability.},
  comment       = {10},
  doi           = {https://doi.org/10.1016/j.jss.2009.06.051},
  keywords      = {Software product lines, Software ecosystems, Global development, Software integration, Software composition},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121209001617},
}

@Article{Sinnema2007,
  author        = {Marco Sinnema and Sybren Deelstra},
  title         = {Classifying variability modeling techniques},
  journal       = {Information and Software Technology},
  year          = {2007},
  volume        = {49},
  number        = {7},
  pages         = {717 - 739},
  issn          = {0950-5849},
  __markedentry = {[mac:]},
  abstract      = {Variability modeling is important for managing variability in software product families, especially during product derivation. In the past few years, several variability modeling techniques have been developed, each using its own concepts to model the variability provided by a product family. The publications regarding these techniques were written from different viewpoints, use different examples, and rely on a different technical background. This paper sheds light on the similarities and differences between six variability modeling techniques, by exemplifying the techniques with one running example, and classifying them using a framework of key characteristics for variability modeling. It furthermore discusses the relation between differences among those techniques, and the scope, size, and application domain of product families.},
  comment       = {23},
  doi           = {https://doi.org/10.1016/j.infsof.2006.08.001},
  keywords      = {Classification, Software product family, Variability management, Variability modeling},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584906001042},
}

@Article{Magalhaes2015,
  author        = {Cleyton V.C. de MagalhÃ£es and Fabio Q.B. da Silva and Ronnie E.S. Santos and Marcos Suassuna},
  title         = {Investigations about replication of empirical studies in software engineering: A systematic mapping study},
  journal       = {Information and Software Technology},
  year          = {2015},
  volume        = {64},
  pages         = {76 - 101},
  issn          = {0950-5849},
  __markedentry = {[mac:]},
  abstract      = {Context
Two recent mapping studies which were intended to verify the current state of replication of empirical studies in Software Engineering (SE) identified two sets of studies: empirical studies actually reporting replications (published between 1994 and 2012) and a second group of studies that are concerned with definitions, classifications, processes, guidelines, and other research topics or themes about replication work in empirical software engineering research (published between 1996 and 2012).
Objective
In this current article, our goal is to analyze and discuss the contents of the second set of studies about replications to increase our understanding of the current state of the work on replication in empirical software engineering research.
Method
We applied the systematic literature review method to build a systematic mapping study, in which the primary studies were collected by two previous mapping studies covering the period 1996â€“2012 complemented by manual and automatic search procedures that collected articles published in 2013.
Results
We analyzed 37 papers reporting studies about replication published in the last 17years. These papers explore different topics related to concepts and classifications, presented guidelines, and discuss theoretical issues that are relevant for our understanding of replication in our field. We also investigated how these 37 papers have been cited in the 135 replication papers published between 1994 and 2012.
Conclusions
Replication in SE still lacks a set of standardized concepts and terminology, which has a negative impact on the replication work in our field. To improve this situation, it is important that the SE research community engage on an effort to create and evaluate taxonomy, frameworks, guidelines, and methodologies to fully support the development of replications.},
  comment       = {26},
  doi           = {https://doi.org/10.1016/j.infsof.2015.02.001},
  keywords      = {Replications, Experiments, Empirical studies, Mapping study, Systematic literature review, Software engineering},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584915000300},
}

@Article{Tuezuen2015b,
  author        = {Eray TÃ¼zÃ¼n and Bedir Tekinerdogan and Mert Emin Kalender and Semih Bilgen},
  title         = {Empirical evaluation of a decision support model for adopting software product line engineering},
  journal       = {Information and Software Technology},
  year          = {2015},
  volume        = {60},
  pages         = {77 - 101},
  issn          = {0950-5849},
  __markedentry = {[mac:]},
  abstract      = {Context
The software product line engineering (SPLE) community has provided several different approaches for assessing the feasibility of SPLE adoption and selecting transition strategies. These approaches usually include many rules and guidelines which are very often implicit or scattered over different publications. Hence, for the practitioners it is not always easy to select and use these rules to support the decision making process. Even in case the rules are known, the lack of automated support for storing and executing the rules seriously impedes the decision making process.
Objective
We aim to evaluate the impact of a decision support system (DSS) on decision-making in SPLE adoption. In alignment with this goal, we provide a decision support model (DSM) and the corresponding DSS.
Method
First, we apply a systematic literature review (SLR) on the existing primary studies that discuss and present approaches for analyzing the feasibility of SPLE adoption and transition strategies. Second, based on the data extraction and synthesis activities of the SLR, the required questions and rules are derived and implemented in the DSS. Third, for validation of the approach we conduct multiple case studies.
Results
In the course of the SLR, 31 primary studies were identified from which we could construct 25 aspects, 39 questions and 312 rules. We have developed the DSS tool Transit-PL that embodies these elements.
Conclusions
The multiple case study validation showed that the adoption of the developed DSS tool is justified to support the decision making process in SPLE adoption.},
  comment       = {25},
  doi           = {https://doi.org/10.1016/j.infsof.2014.12.007},
  keywords      = {Software product line engineering, Software product line transition strategies, Software product line engineering feasibility analysis, Decision support system, Systematic literature review, Case study design},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584915000026},
}

@Article{Ghezzi2013,
  author        = {Carlo Ghezzi and Amir Molzam Sharifloo},
  title         = {Model-based verification of quantitative non-functional properties for software product lines},
  journal       = {Information and Software Technology},
  year          = {2013},
  volume        = {55},
  number        = {3},
  pages         = {508 - 524},
  issn          = {0950-5849},
  note          = {Special Issue on Software Reuse and Product Lines},
  __markedentry = {[mac:]},
  abstract      = {Evaluating quality attributes of a design model in the early stages of development can significantly reduce the cost and risks of developing a low quality product. To make this possible, software designers should be able to predict quality attributes by reasoning on a model of the system under development. Although there exists a variety of quality-driven analysis techniques for software systems, only a few work address software product lines. This paper describes how probabilistic model checking techniques and tools can be used to verify non-functional properties of different configurations of a software product line. We propose a model-based approach that enables software engineers to assess their design solutions for software product lines in the early stages of development. Furthermore, we discuss how the analysis time can be surprisingly reduced by applying parametric model checking instead of classic model checking. The results show that the parametric approach is able to substantially alleviate the verification time and effort required to analyze non-functional properties of software product lines.},
  comment       = {17},
  doi           = {https://doi.org/10.1016/j.infsof.2012.07.017},
  keywords      = {Quality analysis, Software product lines, Non-functional requirements, Probabilistic model checking, Parametric verification},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584912001516},
}

@Article{Cafeo2016,
  author        = {Bruno B.P. Cafeo and Elder Cirilo and Alessandro Garcia and Francisco Dantas and Jaejoon Lee},
  title         = {Feature dependencies as change propagators: An exploratory study of software product lines},
  journal       = {Information and Software Technology},
  year          = {2016},
  volume        = {69},
  pages         = {37 - 49},
  issn          = {0950-5849},
  __markedentry = {[mac:]},
  abstract      = {Context
A Software Product Line (SPL) is a set of software systems that share common functionalities, so-called features. When features are related, we consider this relation a feature dependency. Whenever a new feature is added, the presence of feature dependencies in the source code may increase the maintenance effort. In particular, along the maintenance of SPL implementation, added features may induce changes in other features, the so-called change propagation. Change propagation is the set of ripple changes required to other features whenever a particular feature is added or changed.
Objective
The relationship between feature dependency and change propagation is not well understood. Therefore, the objective of our study is to examine the relation between feature dependency and change propagation.
Method
We investigate change propagation through feature dependencies in additive changes on five evolving SPLs. We analysed a wide range of additive changes in 21 representations of those SPLs. This analysis enabled us to understand whether and how features dependencies and change propagations are related.
Results
The results have empirically confirmed for the first time the strong relation between feature dependency and change propagation. We also identified what are the circumstances involving dependent features that are more likely to cause change propagation. Surprisingly, the results also suggested that the extent of change propagation across SPL features might be higher than the one found in previous studies of dependent modules in non-SPLs. We also found a concentration of change propagation in a few feature dependencies.
Conclusion
Even though the results show that there is a strong relation between feature dependencies and change propagation, such relation is not alike for all dependencies. This indicates that (i) a general feature dependency minimisation might not ameliorate the change propagation, and (ii) feature dependency properties must be analysed beforehand to drive maintenance effort to important dependencies.},
  comment       = {13},
  doi           = {https://doi.org/10.1016/j.infsof.2015.08.009},
  keywords      = {Software Product Line, Maintenance, Feature dependency, Change propagation},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584915001512},
}

@Article{Garousi2015,
  author        = {Golara Garousi and Vahid Garousi-YusifoÄŸlu and Guenther Ruhe and Junji Zhi and Mahmoud Moussavi and Brian Smith},
  title         = {Usage and usefulness of technical software documentation: An industrial case study},
  journal       = {Information and Software Technology},
  year          = {2015},
  volume        = {57},
  pages         = {664 - 682},
  issn          = {0950-5849},
  __markedentry = {[mac:]},
  abstract      = {Context
Software documentation is an integral part of any software development process. However, software practitioners are often concerned about the value, degree of usage and usefulness of documentation during development and maintenance.
Objective
Motivated by the needs of NovAtel Inc. (NovAtel), a world-leading company developing software systems in support of global navigation satellite systems, and based on the results of a former systematic mapping study, we aimed at better understanding of the usage and the usefulness of various technical documents during software development and maintenance.
Method
We utilized the results of a former systematic mapping study and performed an industrial case study at NovAtel. From the joint definition of the analysis goals, the research method incorporates qualitative and quantitative analysis of 55 documents (design, test and process related) and 1630 of their revisions. In addition, we conducted a survey on the usage and usefulness of documents. A total of 25 staff members from the industrial partner, all having a medium to high level of experience, participated in the survey.
Results
In the context of the case study, a number of findings were derived. They include that (1) technical documentation was consulted least frequently for maintenance purpose and most frequently as an information source for development, (2) source code was considered most frequently as the preferred information source during software maintenance, (3) there is no significant difference between the usage of various documentation types during both development and maintenance, and (4) initial hypotheses stating that up-to-date information, accuracy and preciseness have the highest impact on usefulness of technical documentation.
Conclusions
It is concluded that the usage of documentation differs for various purposes and it depends on the type of the information needs as well as the tasks to be completed (e.g., development and maintenance). The results have been confirmed to be helpful for the company under study, and the firm is currently implementing some of the recommendations given.},
  comment       = {19},
  doi           = {https://doi.org/10.1016/j.infsof.2014.08.003},
  keywords      = {Technical software documentation, Usage, Usefulness, Industrial context, Case study},
  url           = {http://www.sciencedirect.com/science/article/pii/S095058491400192X},
}

@Article{Hutchesson2013,
  author        = {Stuart Hutchesson and John McDermid},
  title         = {Trusted Product Lines},
  journal       = {Information and Software Technology},
  year          = {2013},
  volume        = {55},
  number        = {3},
  pages         = {525 - 540},
  issn          = {0950-5849},
  note          = {Special Issue on Software Reuse and Product Lines},
  __markedentry = {[mac:]},
  abstract      = {Context
The paper addresses the use of a Software Product Line approach in the context of developing software for a high-integrity, regulated domain such as civil aerospace. The success of a Software Product Line approach must be judged on whether useful products can be developed more effectively (lower cost, reduced schedule) than with traditional single-system approaches. When developing products for regulated domains, the usefulness of the product is critically dependent on the ability of the development process to provide approval evidence for scrutiny by the regulating authority.
Objective
The objective of the work described is to propose a framework for arguing that a product instantiated using a Software Product Line approach can be approved and used within a regulated domain, such that the development cost of that product would be less than if it had been developed in isolation.
Method
The paper identifies and surveys the issues relating the adoption of Software Product Lines as currently understood (including related technologies such as feature modelling, component-based development and model transformation) when applied to high-integrity software development. We develop an argument framework using Goal Structuring Notation to structure the claims made and the evidence required to support the approval of an instantiated product in such domains. Any unsubstantiated claims or missing/sub-standard evidence is identified, and we propose potential approaches or pose research questions to help address this.
Results
The paper provides an argument framework supporting the use of a Software Product Line approach within a high-integrity regulated domain. It shows how lifecycle evidence can be collected, managed and used to credibly support a regulatory approval process, and provides a detailed example showing how claims regarding model transformation may be supported. Any attempt to use a Software Product Line approach in a regulated domain will need to provide evidence to support their approach in accordance with the argument outlined in the paper.
Conclusion
Product Line practices may complicate the generation of convincing evidence for approval of instantiated products, but it is possible to define a credible Trusted Product Line approach.},
  comment       = {16},
  doi           = {https://doi.org/10.1016/j.infsof.2012.06.005},
  keywords      = {Software Product Lines, High-integrity software, DO-178B/ED-12B, SPARK, Model transformation, GSN},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584912001085},
}

@Article{Ahmed2007c,
  author        = {Faheem Ahmed and Luiz Fernando Capretz},
  title         = {Managing the business of software product line: An empirical investigation of key business factors},
  journal       = {Information and Software Technology},
  year          = {2007},
  volume        = {49},
  number        = {2},
  pages         = {194 - 208},
  issn          = {0950-5849},
  __markedentry = {[mac:]},
  abstract      = {Business has been highlighted as a one of the critical dimensions of software product line engineering. This paperâ€™s main contribution is to increase the understanding of the influence of key business factors by showing empirically that they play an imperative role in managing a successful software product line. A quantitative survey of software organizations currently involved in the business of developing software product lines over a wide range of operations, including consumer electronics, telecommunications, avionics, and information technology, was designed to test the conceptual model and hypotheses of the study. This is the first study to demonstrate the relationships between the key business factors and software product lines. The results provide evidence that organizations in the business of software product line development have to cope with multiple key business factors to improve the overall performance of the business, in addition to their efforts in software development. The conclusions of this investigation reinforce current perceptions of the significance of key business factors in successful software product line business.},
  comment       = {15},
  doi           = {https://doi.org/10.1016/j.infsof.2006.05.004},
  keywords      = {Key business factor, Software product line, Software engineering economics, Management, Strategic planning, Marketing strategy},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584906000759},
}

@Article{Vale2017,
  author        = {Tassio Vale and Eduardo Santana de Almeida and Vander Alves and UirÃ¡ Kulesza and Nan Niu and Ricardo de Lima},
  title         = {Software product lines traceability: A systematic mapping study},
  journal       = {Information and Software Technology},
  year          = {2017},
  volume        = {84},
  pages         = {1 - 18},
  issn          = {0950-5849},
  __markedentry = {[mac:]},
  abstract      = {Context: Traceability in Software Product Lines (SPL) is the ability to interrelate software engineering artifacts through required links to answer specific questions related to the families of products and underlying development processes. Despite the existence of studies to map out available evidence on traceability for single systems development, there is a lack of understanding on common strategies, activities, artifacts, and research gaps for SPL traceability. Objective: This paper analyzes 62 studies dating from 2001 to 2015 and discusses seven aspects of SPL traceability: main goals, strategies, application domains, research intensity, research challenges, rigor, and industrial relevance. In addition to the analysis, this paper also synthesizes the available evidence, identifies open issues and points out areas calling for further research. Method: To gather evidence, we defined a mapping study process adapted from existing guidelines. Driven by a set of research questions, this process comprises three major phases: planning, conducting, and documenting the review. Results: This work provides a structured understanding of SPL traceability, indicating areas for further research. The lack of evidence regarding the application of research methods indicates the need for more rigorous SPL traceability studies with better description of context, study design, and limitations. For practitioners, although most identified studies have low industrial relevance, a few of them have high relevance and thus could provide some decision making support for application of SPL traceability in practice. Conclusions: This work concludes that SPL traceability is maturing and pinpoints areas where further investigation should be performed. As future work, we intend to improve the comparison between traceability proposals for SPL and single-system development.},
  comment       = {18},
  doi           = {https://doi.org/10.1016/j.infsof.2016.12.004},
  keywords      = {Systematic mapping study, Software product lines, Software and systems traceability, Software reuse},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584916304463},
}

@Article{Gharsellaoui2017,
  author        = {Hamza Gharsellaoui and Jihen Maazoun and Nadia Bouassida and Samir Ben Ahmed and Hanene Ben-Abdallah},
  title         = {A Software Product Line Design Based Approach for Real-time Scheduling of Reconfigurable Embedded Systems},
  journal       = {Computers in Human Behavior},
  year          = {2017},
  issn          = {0747-5632},
  __markedentry = {[mac:]},
  abstract      = {In this paper, we deal with the development of dynamically reconfigurable embedded systems in terms of the production of execution schedules of system tasks (feasible configuration) under hard real-time constraints. Indeed, several real-time embedded systems must be dynamically reconfigured to account for hardware/software faults and/or maintain acceptable performances. Depending on the run-time environment, some reconfigurations might be unfeasible, i.e., they violate some real-time constraints of the system. More specifically, we propose an approach that starts from a set of reconfigurations to construct a Software Product Line (SPL) that can be reused in a predictive and organized way to derive real-time embedded systems. To make sure that the SPL offers various feasible reconfigurations, we define an intelligent agent (IA) that automatically checks the system's feasibility after a reconfiguration scenario is applied on a multiprocessor embedded system. This agent dynamically determines precious technical solutions to define a new product whenever a reconfiguration is unfeasible. The set of products thus defined by the agent can then be unified into an SPL. The originality of our approach is its capacity to extract, from the unfeasible configurations of an embedded system, an SPL design enriched with real-time constraints and modeled with a UML Marte profile. The SPL design can assist in the comprehension, reconfiguration as well as evolution of the SPL in order to satisfy real-time requirements and to obtain a feasible system under normal and overload conditions.},
  doi           = {https://doi.org/10.1016/j.chb.2017.04.026},
  keywords      = {Real-time scheduling, Reconfigurable embedded systems, SPL design, UML marte},
  url           = {http://www.sciencedirect.com/science/article/pii/S0747563217302686},
}

@Article{Saeed2016a,
  author        = {Mazin Saeed and Faisal Saleh and Sadiq Al-Insaif and Mohamed El-Attar},
  title         = {Empirical validating the cognitive effectiveness of a new feature diagrams visual syntax},
  journal       = {Information and Software Technology},
  year          = {2016},
  volume        = {71},
  pages         = {1 - 26},
  issn          = {0950-5849},
  __markedentry = {[mac:]},
  abstract      = {Context
Feature models are commonly used to capture and communicate the commonality and variability of features in a Software Product Line. The core component of Feature models is feature diagrams, which graphically depict features in a hierarchical form. In previous work we have proposed a new notation that aims to improve the cognitive effectiveness of feature diagrams.
Objective
The objective of this paper is to empirically validate the cognitive effectiveness of the new feature diagrams notation in comparison to its original form.
Methods
We use two distinct empirical user-studies to validate the new notation. The first empirical study uses the survey approach while the second study is a subject-based experiment. The survey study investigates the semantic transparency of the new notation while the second study investigates the speed and accuracy of reading the notation.
Results
The results of the studies indicate that the proposed changes have significantly improved its cognitive effectiveness.
Conclusions
The cognitive effectiveness of feature diagrams has been improved, however there remains further research for full acceptance of the new notation by its potential user community.},
  comment       = {26},
  doi           = {https://doi.org/10.1016/j.infsof.2015.10.012},
  keywords      = {Feature models, Visual syntax evaluation, Software product lines},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584915001780},
}

@Article{Zanardini2016a,
  author        = {Damiano Zanardini and Elvira Albert and Karina Villela},
  title         = {Resourceâ€“usageâ€“aware configuration in software product lines},
  journal       = {Journal of Logical and Algebraic Methods in Programming},
  year          = {2016},
  volume        = {85},
  number        = {1, Part 2},
  pages         = {173 - 199},
  issn          = {2352-2208},
  note          = {Formal Methods for Software Product Line Engineering},
  __markedentry = {[mac:]},
  abstract      = {Deriving concrete products from a product-line infrastructure requires resolving the variability captured in the product line, based on the company market strategy or requirements from specific customers. Selecting the most appropriate set of features for a product is a complex task, especially if quality requirements have to be considered. Resourceâ€“usageâ€“aware configuration aims at providing awareness of resourceâ€“usage properties of artifacts throughout the configuration process. This article envisages several strategies for resourceâ€“usageâ€“aware configuration which feature different performance and efficiency trade-offs. The common idea in all strategies is the use of resourceâ€“usage estimates obtained by an off-the-shelf static resourceâ€“usage analyzer as a heuristic for choosing among different candidate configurations. We report on a prototype implementation of the most practical strategies for resourceâ€“usageâ€“aware configuration and apply it on an industrial case study.},
  comment       = {27},
  doi           = {https://doi.org/10.1016/j.jlamp.2015.08.003},
  url           = {http://www.sciencedirect.com/science/article/pii/S2352220815000814},
}

@Article{Afzal2016,
  author        = {Uzma Afzal and Tariq Mahmood and Zubair Shaikh},
  title         = {Intelligent software product line configurations: A literature review},
  journal       = {Computer Standards \& Interfaces},
  year          = {2016},
  volume        = {48},
  pages         = {30 - 48},
  issn          = {0920-5489},
  note          = {Special Issue on Information System in Distributed Environment},
  __markedentry = {[mac:]},
  abstract      = {A software product line (SPL) is a set of industrial software-intensive systems for configuring similar software products in which personalized feature sets are configured by different business teams. The integration of these feature sets can generate inconsistencies that are typically resolved through manual deliberation. This is a time-consuming process and leads to a potential loss of business resources. Artificial intelligence (AI) techniques can provide the best solution to address this issue autonomously through more efficient configurations, lesser inconsistencies and optimized resources. This paper presents the first literature review of both research and industrial AI applications to SPL configuration issues. Our results reveal only 19 relevant research works which employ traditional AI techniques on small feature sets with no real-life testing or application in industry. We categorize these works in a typology by identifying 8 perspectives of SPL. We also show that only 2 standard industrial SPL tools employ AI in a limited way to resolve inconsistencies. To inject more interest and application in this domain, we motivate and present future research directions. Particularly, using real-world SPL data, we demonstrate how predictive analytics (a state of the art AI technique) can separately model inconsistent and consistent patterns, and then predict inconsistencies in advance to help SPL designers during the configuration of a product.},
  comment       = {19},
  doi           = {https://doi.org/10.1016/j.csi.2016.03.003},
  keywords      = {Software product line, Literature review, Automated feature selection, Inconsistencies, Artificial intelligence, Industrial SPL tools, Predictive analytics},
  url           = {http://www.sciencedirect.com/science/article/pii/S0920548916300198},
}

@Article{Mendez-Acuna2016,
  author        = {David MÃ©ndez-AcuÃ±a and JosÃ© A. Galindo and Thomas Degueule and BenoÃ®t Combemale and BenoÃ®t Baudry},
  title         = {Leveraging Software Product Lines Engineering in the development of external DSLs: A systematic literature review},
  journal       = {Computer Languages, Systems \& Structures},
  year          = {2016},
  volume        = {46},
  pages         = {206 - 235},
  issn          = {1477-8424},
  __markedentry = {[mac:]},
  abstract      = {The use of domain-specific languages (DSLs) has become a successful technique in the development of complex systems. Consequently, nowadays we can find a large variety of DSLs for diverse purposes. However, not all these DSLs are completely different; many of them share certain commonalities coming from similar modeling patterns â€“ such as state machines or petri nets â€“ used for several purposes. In this scenario, the challenge for language designers is to take advantage of the commonalities existing among similar DSLs by reusing, as much as possible, formerly defined language constructs. The objective is to leverage previous engineering efforts to minimize implementation from scratch. To this end, recent research in software language engineering proposes the use of product line engineering, thus introducing the notion of language product lines. Nowadays, there are several approaches that result useful in the construction of language product lines. In this article, we report on an effort for organizing the literature on language product line engineering. More precisely, we propose a definition for the life-cycle of language product lines, and we use it to analyze the capabilities of current approaches. In addition, we provide a mapping between each approach and the technological space it supports.},
  comment       = {30},
  doi           = {https://doi.org/10.1016/j.cl.2016.09.004},
  keywords      = {Software language engineering, Domain-specific languages, Variability management, Software Product Lines Engineering},
  url           = {http://www.sciencedirect.com/science/article/pii/S1477842416300768},
}

@Article{Oliveira2017a,
  author        = {Raphael Pereira de Oliveira and Alcemir Rodrigues Santos and Eduardo Santana de Almeida and Gecynalda Soares da Silva Gomes},
  title         = {Evaluating Lehmanâ€™s Laws of software evolution within software product lines industrial projects},
  journal       = {Journal of Systems and Software},
  year          = {2017},
  volume        = {131},
  pages         = {347 - 365},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {The evolution of a single system is a task where we deal with the modification of a single product. Lehmanâ€™s Laws of software evolution were broadly evaluated within this type of system and the results shown that these single systems evolve according to his stated laws over time. However, considering Software Product Lines (SPL), we need to deal with the modification of several products which include common, variable, and product specific assets. Because of the several assets within SPL, each stated law may have a different behavior for each asset kind. Nonetheless, we do not know if all of the stated laws are still valid for SPL since they were partially evaluated in this context. Thus, this paper details an empirical investigation where Lehmanâ€™s Laws (LL) of Software Evolution were used in two SPL industrial projects to understand how the SPL assets evolve over time. These projects are related to an application in the medical domain and another in the financial domain, developed by medium-size companies in Brazil. They contain a total of 71 modules and a total of 71.442 bug requests in their tracking system, gathered along the total of more than 10 years. We employed two techniques - the KPSS Test and linear regression analysis, to assess the relationship between LL and SPL assets. Results showed that one law was completely supported (conservation of organizational stability) for all assets within both empirical studies. Two laws were partially supported for both studies depending on the asset type (continuous growth and conservation of familiarity). Finally, the remaining laws had differences among their results for all assets (continuous change, increasing complexity, and declining quality).},
  comment       = {19},
  doi           = {https://doi.org/10.1016/j.jss.2016.07.038},
  keywords      = {Software product lines, Software evolution, Lehmanâ€™s Laws of software evolution, Empirical study},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121216301339},
}

@Article{Sabouri2014,
  author        = {Hamideh Sabouri and Ramtin Khosravi},
  title         = {Reducing the verification cost of evolving product families using static analysis techniques},
  journal       = {Science of Computer Programming},
  year          = {2014},
  volume        = {83},
  pages         = {35 - 55},
  issn          = {0167-6423},
  note          = {Formal Aspects of Component Software (FACS 2011 selected \& extended papers)},
  __markedentry = {[mac:]},
  abstract      = {Software product line engineering enables proactive reuse among a set of related products through explicit modeling of commonalities and differences among them. Software product lines are intended to be used in a long period of time. As a result, they evolve over time, due to the changes in the requirements. Having several individual products in a software family, verification of the entire family may take a considerable effort. In this paper we aim to decrease this cost by reducing the number of verified products using static analysis techniques. Furthermore, to reduce model checking costs after product line evolution, we restrict the number of products that should be re-verified by reusing the previous verification result. All proposed techniques are based on static analysis of the product family model with respect to the property and can be automated. To show the effectiveness of these techniques we apply them on a set of case studies and present the results.},
  comment       = {21},
  doi           = {https://doi.org/10.1016/j.scico.2013.06.009},
  keywords      = {Software product lines, Model checking, Program slicing, Static analysis, Reduction techniques},
  url           = {http://www.sciencedirect.com/science/article/pii/S0167642313001585},
}

@Article{Taulavuori2004,
  author        = {Anne Taulavuori and Eila NiemelÃ¤ and PÃ¤ivi Kallio},
  title         = {Component documentationâ€”a key issue in software product lines},
  journal       = {Information and Software Technology},
  year          = {2004},
  volume        = {46},
  number        = {8},
  pages         = {535 - 546},
  issn          = {0950-5849},
  __markedentry = {[mac:]},
  abstract      = {Product lines embody a strategic reuse of both intellectual effort and existing artefacts, such as software architectures and components. Third-party components are increasingly being used in product line based software engineering, in which case the integration is controlled by the product line architecture. However, the software integrators have difficulties in finding out the capabilities of components, because components are not documented in a standard way. Documentation is often the only way of assessing the applicability, credibility and quality of a third-party component. Our contribution is a standard documentation pattern for software components. The pattern provides guidelines and structure for component documentation and ensures the quality of documentation. The pattern has been validated by applying and analysing it in practice.},
  comment       = {12},
  doi           = {https://doi.org/10.1016/j.infsof.2003.10.004},
  keywords      = {Third-party component, Component documentation, Software product line},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584903002131},
}

@Article{Neves2015,
  author        = {L. Neves and P. Borba and V. Alves and L. Turnes and L. Teixeira and D. Sena and U. Kulesza},
  title         = {Safe evolution templates for software product lines},
  journal       = {Journal of Systems and Software},
  year          = {2015},
  volume        = {106},
  pages         = {42 - 58},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {Software product lines enable generating related software products from reusable assets. Adopting a product line strategy can bring significant quality and productivity improvements. However, evolving a product line can be risky, since it might impact many products. When introducing new features or improving its design, it is important to make sure that the behavior of existing products is not affected. To ensure that, one usually has to analyze different types of artifacts, an activity that can lead to errors. To address this issue, in this work we discover and analyze concrete evolution scenarios from five different product lines. We discover a total of 13 safe evolution templates, which are generic transformations that developers can apply when evolving compositional and annotative product lines, with the goal of preserving the behavior of existing products. We also evaluate the templates by analyzing the evolution history of these product lines. In this evaluation, we observe that the templates can address the modifications that developers performed in the analyzed scenarios, which corroborates the expressiveness of our template set. We also observe that the templates could also have helped to avoid the errors that we identified during our analysis.},
  comment       = {17},
  doi           = {https://doi.org/10.1016/j.jss.2015.04.024},
  keywords      = {Software product lines, Refinement, Evolution},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121215000801},
}

@Article{Niemelae2007a,
  author        = {Eila NiemelÃ¤ and Anne Immonen},
  title         = {Capturing quality requirements of product family architecture},
  journal       = {Information and Software Technology},
  year          = {2007},
  volume        = {49},
  number        = {11},
  pages         = {1107 - 1120},
  issn          = {0950-5849},
  __markedentry = {[mac:]},
  abstract      = {Software quality is one of the major issues with software intensive systems. Moreover, quality is a critical success factor in software product families exploiting shared architecture and common components in a set of products. Our contribution is the QRF (Quality Requirements of a software Family) method, which explicitly focuses on how quality requirements have to be defined, represented and transformed to architectural models. The method has been applied to two experiments; one in a laboratory environment and the other in industry. The use of the QRF method is exemplified by the Distribution Service Platform (DiSeP), the laboratory experiment. The lessons learned are also based on our experiences of applying the method in industrial settings.},
  comment       = {14},
  doi           = {https://doi.org/10.1016/j.infsof.2006.11.003},
  keywords      = {Quality requirement, Software architecture, Traceability, Software product family},
  url           = {http://www.sciencedirect.com/science/article/pii/S095058490600190X},
}

@Article{AlcantaradosSantosNeto2016,
  author        = {Pedro de AlcÃ¢ntara dos Santos Neto and Ricardo Britto and Ricardo de Andrade Lira RabÃªlo and Jonathas Jivago de Almeida Cruz and Werney Ayala Luz Lira},
  title         = {A hybrid approach to suggest software product line portfolios},
  journal       = {Applied Soft Computing},
  year          = {2016},
  volume        = {49},
  pages         = {1243 - 1255},
  issn          = {1568-4946},
  __markedentry = {[mac:]},
  abstract      = {Software product line (SPL) development is a new approach to software engineering which aims at the development of a whole range of products. However, as long as SPL can be useful, there are many challenges regarding the use of that approach. One of the main problems which hinders the adoption of software product line (SPL) is the complexity regarding product management. In that context, we can remark the scoping problem. One of the existent ways to deal with scoping is the product portfolio scoping (PPS). PPS aims to define the products that should be developed as well as their key features. In general, that approach is driven by marketing aspects, like cost of the product and customer satisfaction. Defining a product portfolio by using the many different available aspects is a NP-hard problem. This work presents an improved hybrid approach to solve the feature model selection problem, aiming at supporting product portfolio scoping. The proposal is based in a hybrid approach not dependent on any particular algorithm/technology. We have evaluated the usefulness and scalability of our approach using one real SPL (ArgoUML-SPL) and synthetic SPLs. As per the evaluation results, our approach is both useful from a practitioner's perspective and scalable.},
  comment       = {13},
  doi           = {https://doi.org/10.1016/j.asoc.2016.08.024},
  keywords      = {Software product lines, Search based software engineering, Search based feature model selection, Feature model selection problem, Product portfolio scoping, NSGA-II, Fuzzy inference systems},
  url           = {http://www.sciencedirect.com/science/article/pii/S1568494616304185},
}

@Article{Liu2007,
  author        = {Jing Liu and Josh Dehlinger and Robyn Lutz},
  title         = {Safety analysis of software product lines using state-based modeling},
  journal       = {Journal of Systems and Software},
  year          = {2007},
  volume        = {80},
  number        = {11},
  pages         = {1879 - 1892},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {The difficulty of managing variations and their potential interactions across an entire product line currently hinders safety analysis in safety-critical, software product lines. The work described here contributes to a solution by integrating product-line safety analysis with model-based development. This approach provides a structured way to construct state-based models of a product line having significant, safety-related variations and to systematically explore the relationships between behavioral variations and potential hazardous states through scenario-guided executions of the state model over the variations. The paper uses a product line of safety-critical medical devices to demonstrate and evaluate the technique and results.},
  comment       = {14},
  doi           = {https://doi.org/10.1016/j.jss.2007.01.047},
  keywords      = {Product lines, Safety-critical systems, Model-based development, State-based modeling},
  url           = {http://www.sciencedirect.com/science/article/pii/S016412120700057X},
}

@Article{Sepulveda2016a,
  author        = {Samuel SepÃºlveda and Ania Cravero and Cristina Cachero},
  title         = {Requirements modeling languages for software product lines: A systematic literature review},
  journal       = {Information and Software Technology},
  year          = {2016},
  volume        = {69},
  pages         = {16 - 36},
  issn          = {0950-5849},
  __markedentry = {[mac:]},
  abstract      = {Context: Software product lines (SPLs) have reached a considerable level of adoption in the software industry, having demonstrated their cost-effectiveness for developing higher quality products with lower costs. For this reason, in the last years the requirements engineering community has devoted much effort to the development of a myriad of requirements modelling languages for SPLs. Objective: In this paper, we review and synthesize the current state of research of requirements modelling languages used in SPLs with respect to their degree of empirical validation, origin and context of use, level of expressiveness, maturity, and industry adoption. Method: We have conducted a systematic literature review with six research questions that cover the main objective. It includes 54 studies, published from 2000 to 2013. Results: The mean level of maturity of the modelling languages is 2.59 over 5, with 46% of them falling within level 2 or below -no implemented abstract syntax reported-. They show a level of expressiveness of 0.7 over 1.0. Some constructs (feature, mandatory, optional, alternative, exclude and require) are present in all the languages, while others (cardinality, attribute, constraint and label) are less common. Only 6% of the languages have been empirically validated, 41% report some kind of industry adoption and 71% of the languages are independent from any development process. Last but not least, 57% of the languages have been proposed by the academia, while 43% have been the result of a joint effort between academia and industry. Conclusions: Research on requirements modeling languages for SPLs has generated a myriad of languages that differ in the set of constructs provided to express SPL requirements. Their general lack of empirical validation and adoption in industry, together with their differences in maturity, draws the picture of a discipline that still needs to evolve.},
  comment       = {21},
  doi           = {https://doi.org/10.1016/j.infsof.2015.08.007},
  keywords      = {Requirements engineering, Modeling languages, Software product lines, Systematic literature review},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584915001494},
}

@Article{Heradio2012a,
  author        = {Ruben Heradio and David Fernandez-Amoros and Luis Torre-Cubillo and Alberto Perez Garcia-Plaza},
  title         = {Improving the accuracy of COPLIMO to estimate the payoff of a software product line},
  journal       = {Expert Systems with Applications},
  year          = {2012},
  volume        = {39},
  number        = {9},
  pages         = {7919 - 7928},
  issn          = {0957-4174},
  __markedentry = {[mac:]},
  abstract      = {Software product line engineering pursues the efficient development of families of similar products. COPLIMO is an economic model that relies on COCOMO II to estimate the benefits of adopting a product line approach compared to developing the products one by one. Although COPLIMO is an ideal economic model to support decision making on the incremental development of a product line, it makes some simplifying assumptions that may produce high distortions in the estimates (e.g., COPLIMO takes for granted that all the products have the same size). This paper proposes a COPLIMO reformulation that avoids such assumptions and, consequently, improves the accuracy of the estimates. To support our proposal, we present an algorithm that infers the additional information that our COPLIMO reformulation requires from feature diagrams, which is a widespread notation to model the domain of a product line.},
  comment       = {10},
  doi           = {https://doi.org/10.1016/j.eswa.2012.01.109},
  keywords      = {Software product line, Economic model, Feature diagram, Product counting, Decision support},
  url           = {http://www.sciencedirect.com/science/article/pii/S0957417412001273},
}

@Article{Bezerra2017a,
  author        = {Carla I.M. Bezerra and Rossana M.C. Andrade and Jose Maria Monteiro},
  title         = {Exploring quality measures for the evaluation of feature models: a case study},
  journal       = {Journal of Systems and Software},
  year          = {2017},
  volume        = {131},
  pages         = {366 - 385},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {Evaluating the quality of a feature model is essential to ensure that errors in the early stages do not spread throughout the Software Product Line (SPL). One way to evaluate the feature model is to use measures that could be associated with the feature model quality characteristics and their quality attributes. In this paper, we aim at investigating how measures can be applied to the quality assessment of SPL feature models. We performed an exploratory case study using the COfFEE maintainability measures catalog and the S.P.L.O.T. feature models repository. In order to support this case study, we built a dataset (denoted by MAcchiATO) containing the values of 32 measures from COfFEE for 218 software feature models, extracted from S.P.L.O.T. This research approach allowed us to explore three different data analysis techniques. First, we applied the Spearmanâ€™s rank correlation coefficient in order to identify relationships between the measures. This analysis showed that not all 32 measures in COfFEE are necessary to reveal the quality of a feature model and just 15 measures could be used. Next, the 32 measures in COfFEE were grouped by applying the Principal Component Analysis and a set of 9 new grouped measures were defined. Finally, we used the Tolerance Interval technique to define statistical thresholds for these 9 new grouped measures. So, our findings suggest that measures can be effectively used to support the quality evaluation of SPL feature models.},
  comment       = {20},
  doi           = {https://doi.org/10.1016/j.jss.2016.07.040},
  keywords      = {Software product line, Quality evaluation, Measures, Feature models},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121216301340},
}

@Article{Xue2016a,
  author        = {Yinxing Xue and Jinghui Zhong and Tian Huat Tan and Yang Liu and Wentong Cai and Manman Chen and Jun Sun},
  title         = {IBED: Combining IBEA and DE for optimal feature selection in software product line engineering},
  journal       = {Applied Soft Computing},
  year          = {2016},
  volume        = {49},
  pages         = {1215 - 1231},
  issn          = {1568-4946},
  __markedentry = {[mac:]},
  abstract      = {Software configuration, which aims to customize the software for different users (e.g., Linux kernel configuration), is an important and complicated task. In software product line engineering (SPLE), feature oriented domain analysis is adopted and feature model is used to guide the configuration of new product variants. In SPLE, product configuration is an optimal feature selection problem, which needs to find a set of features that have no conflicts and meanwhile achieve multiple design objectives (e.g., minimizing cost and maximizing the number of features). In previous studies, several multi-objective evolutionary algorithms (MOEAs) were used for the optimal feature selection problem and indicator-based evolutionary algorithm (IBEA) was proven to be the best MOEA for this problem. However, IBEA still suffers from the issues of correctness and diversity of found solutions. In this paper, we propose a dual-population evolutionary algorithm, named IBED, to achieve both correctness and diversity of solutions. In IBED, two populations are individually evolved with two different types of evolutionary operators, i.e., IBEA operators and differential evolution (DE) operators. Furthermore, we propose two enhancement techniques for existing MOEAs, namely the feedback-directed mechanism to fast find the correct solutions (e.g., solutions that satisfy the feature model constraints) and the preprocessing method to reduce the search space. Our empirical results have shown that IBED with the enhancement techniques can outperform several state-of-the-art MOEAs on most case studies in terms of correctness and diversity of found solutions.},
  comment       = {17},
  doi           = {https://doi.org/10.1016/j.asoc.2016.07.040},
  keywords      = {Optimal feature selection, Indicator-based evolutionary algorithm (IBEA), Differential evolutionary algorithm (DE), Software product line engineering},
  url           = {http://www.sciencedirect.com/science/article/pii/S1568494616303751},
}

@Article{Ganesan2013,
  author        = {Dharmalingam Ganesan and Mikael Lindvall and David McComas and Maureen Bartholomew and Steve Slegel and Barbara Medina and Rene Krikhaar and Chris Verhoef and Lisa P. Montgomery},
  title         = {An analysis of unit tests of a flight software product line},
  journal       = {Science of Computer Programming},
  year          = {2013},
  volume        = {78},
  number        = {12},
  pages         = {2360 - 2380},
  issn          = {0167-6423},
  note          = {Special Section on International Software Product Line Conference 2010 and Fundamentals of Software Engineering (selected papers of FSEN 2011)},
  __markedentry = {[mac:]},
  abstract      = {This paper presents an analysis of the unit testing approach developed and used by the Core Flight Software System (CFS) product line team at the NASA Goddard Space Flight Center (GSFC). The goal of the analysis is to understand, review, and recommend strategies for improving the CFSâ€™ existing unit testing infrastructure as well as to capture lessons learned and best practices that can be used by other software product line (SPL) teams for their unit testing. The results of the analysis show that the core and application modules of the CFS are unit tested in isolation using a stub framework developed by the CFS team. The application developers can unit test their code without waiting for the core modules to be completed, and vice versa. The analysis found that this unit testing approach incorporates many practical and useful solutions such as allowing for unit testing without requiring hardware and special OS features in-the-loop by defining stub implementations of dependent modules. These solutions are worth considering when deciding how to design the testing architecture for a SPL.},
  comment       = {21},
  doi           = {https://doi.org/10.1016/j.scico.2012.02.006},
  keywords      = {Unit testing, Stub, Metrics, Software architecture, Self-testable components, Flight software},
  url           = {http://www.sciencedirect.com/science/article/pii/S0167642312000317},
}

@Article{Rabiser2011,
  author        = {Rick Rabiser and PÃ¡draig Oâ€™Leary and Ita Richardson},
  title         = {Key activities for product derivation in software product lines},
  journal       = {Journal of Systems and Software},
  year          = {2011},
  volume        = {84},
  number        = {2},
  pages         = {285 - 300},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {More and more organizations adopt software product lines to leverage extensive reuse and deliver a multitude of benefits such as increased quality and productivity and a decrease in cost and time-to-market of their software development. When compared to the vast amount of research on developing product lines, relatively little work has been dedicated to the actual use of product lines to derive individual products, i.e., the process of product derivation. Existing approaches to product derivation have been developed independently for different aims and purposes. While the definition of a general approach applicable to every domain may not be possible, it would be interesting for researchers and practitioners to know which activities are common in existing approaches, i.e., what are the key activities in product derivation. In this paper we report on how we compared two product derivation approaches developed by the authors in two different, independent research projects. Both approaches independently sought to identify product derivation activities, one through a process reference model and the other through a tool-supported derivation approach. Both approaches have been developed and validated in research industry collaborations with different companies. Through the comparison of the approaches we identify key product derivation activities. We illustrate the activitiesâ€™ importance with examples from industry collaborations. To further validate the activities, we analyze three existing product derivation approaches for their support for these activities. The validation provides evidence that the identified activities are relevant to product derivation and we thus conclude that they should be considered (e.g., as a checklist) when developing or evaluating a product derivation approach.},
  comment       = {16},
  doi           = {https://doi.org/10.1016/j.jss.2010.09.042},
  keywords      = {Software product lines, Product derivation, Process},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121210002700},
}

@Article{Karimpour2017,
  author        = {Reza Karimpour and Guenther Ruhe},
  title         = {Evolutionary robust optimization for software product line scoping: An explorative study},
  journal       = {Computer Languages, Systems \& Structures},
  year          = {2017},
  volume        = {47},
  pages         = {189 - 210},
  issn          = {1477-8424},
  __markedentry = {[mac:]},
  abstract      = {Background: Software product line (SPL) scoping is an important phase when planning for product line adoption. An SPL scope specifies: (1) the extent of the domain supported by the product line, (2) portfolio of products in the product line and (3) list of assets to be developed for reuse across the family of products. Issue: SPL scope planning is usually based on estimates about the state of the market and the engineering capabilities of the development team. One challenge with these estimates is that there are inaccuracies due to uncertainty in the environment or accuracy of measurement. This may result in issues ranging from suboptimal plans to infeasible plans. Objective: To address the above, we propose to include uncertainty as part of the SPL scoping model. Plans developed in consideration of uncertainty would be more robust against possible fluctuations in estimates. Approach: In this paper, a method to incorporate uncertainty in scoping optimization and its application to generate robust solutions is proposed. We capture uncertainty as part of the formulation and model scoping optimization as a multi-objective problem with profit and stability as fitness functions. Profit stability and feasibility stability are considered to represent stability concerns. Results: Results show that, compared to other scope optimization approaches, both performance stability and feasibility stability are improved while maintaining near optimal performance for profit objective. Also, generated results consist of solutions with trade-offs between profit and stability, providing the decision maker with enhanced decision support. Conclusion: Multi-objective optimization with stability consideration for SPL scoping provides project managers with a robust and flexible way to address uncertainty in the process of SPL scoping.},
  comment       = {22},
  doi           = {https://doi.org/10.1016/j.cl.2016.07.007},
  keywords      = {Search-based software engineering, Software product line scoping, Robust optimization, Evolutionary optimization},
  url           = {http://www.sciencedirect.com/science/article/pii/S1477842416301063},
}

@Article{Borba2012,
  author        = {Paulo Borba and Leopoldo Teixeira and Rohit Gheyi},
  title         = {A theory of software product line refinement},
  journal       = {Theoretical Computer Science},
  year          = {2012},
  volume        = {455},
  pages         = {2 - 30},
  issn          = {0304-3975},
  note          = {International Colloquium on Theoretical Aspects of Computing 2010},
  __markedentry = {[mac:]},
  abstract      = {To safely evolve a software product line, it is important to have a notion of product line refinement that assures behavior preservation of the original product line products. So in this article we present a language independent theory of product line refinement, establishing refinement properties that justify stepwise and compositional product line evolution. Moreover, we instantiate our theory with the formalization of specific languages for typical product lines artifacts, and then introduce and prove soundness of a number of associated product line refinement transformation templates. These templates can be used to reason about specific product lines and as a basis to derive comprehensive product line refinement catalogues.},
  comment       = {29},
  doi           = {https://doi.org/10.1016/j.tcs.2012.01.031},
  keywords      = {Software product lines, Software evolution, Refinement, Refactoring},
  url           = {http://www.sciencedirect.com/science/article/pii/S0304397512000679},
}

@Article{Silva2015,
  author        = {Ivonei Freitas da Silva and Paulo Anselmo da Mota Silveira Neto and PÃ¡draig Oâ€™Leary and Eduardo Santana de Almeida and Silvio Romero de Lemos Meira},
  title         = {Using a multi-method approach to understand Agile software product lines},
  journal       = {Information and Software Technology},
  year          = {2015},
  volume        = {57},
  pages         = {527 - 542},
  issn          = {0950-5849},
  __markedentry = {[mac:]},
  abstract      = {Context
Software product lines (SPLs) and Agile are approaches that share similar objectives. The main difference is the way in which these objectives are met. Typically evidence on what activities of Agile and SPL can be combined and how they can be integrated stems from different research methods performed separately. The generalizability of this evidence is low, as the research topic is still relatively new and previous studies have been conducted using only one research method.
Objective
This study aims to increase understanding of Agile SPL and improve the generalizability of the identified evidence through the use of a multi-method approach.
Method
Our multi-method research combines three complementary methods (Mapping Study, Case Study and Expert Opinion) to consolidate the evidence.
Results
This combination results in 23 findings that provide evidence on how Agile and SPL could be combined.
Conclusion
Although multi-method research is time consuming and requires a high degree of effort to plan, design, and perform, it helps to increase the understanding on Agile SPL and leads to more generalizable evidence. The findings confirm a synergy between Agile and SPL and serve to improve the body of evidence in Agile SPL. When researchers and practitioners develop new Agile SPL approaches, it will be important to consider these synergies.},
  comment       = {16},
  doi           = {https://doi.org/10.1016/j.infsof.2014.06.004},
  keywords      = {Agile, Software product lines, Multi-method approach, Case study, Mapping study, Expert opinion},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584914001438},
}

@Article{Beohar2016a,
  author        = {Harsh Beohar and Mahsa Varshosaz and Mohammad Reza Mousavi},
  title         = {Basic behavioral models for software product lines: Expressiveness and testing pre-orders},
  journal       = {Science of Computer Programming},
  year          = {2016},
  volume        = {123},
  pages         = {42 - 60},
  issn          = {0167-6423},
  note          = {SELECTED AND EXTENDED PAPERS FROM ACM SVT 2014},
  __markedentry = {[mac:]},
  abstract      = {In order to provide a rigorous foundation for Software Product Lines (SPLs), several fundamental approaches have been proposed to their formal behavioral modeling. In this paper, we provide a structured overview of those formalisms based on labeled transition systems and compare their expressiveness in terms of the set of products they can specify. Moreover, we define the notion of tests for each of these formalisms and show that our notions of testing precisely capture product derivation, i.e., all valid products will pass the set of test cases of the product line and each invalid product fails at least one test case of the product line.},
  comment       = {19},
  doi           = {https://doi.org/10.1016/j.scico.2015.06.005},
  keywords      = {Software product lines, Formal specification, Behavioral specification, Labeled transition systems, Featured transition systems, Modal transition systems, Calculus of communicating systems (CCS), Product line CCS (PL-CCS)},
  url           = {http://www.sciencedirect.com/science/article/pii/S0167642315001288},
}

@Article{Engstroem2013a,
  author        = {Emelie EngstrÃ¶m and Per Runeson},
  title         = {Test overlay in an emerging software product line â€“ An industrial case study},
  journal       = {Information and Software Technology},
  year          = {2013},
  volume        = {55},
  number        = {3},
  pages         = {581 - 594},
  issn          = {0950-5849},
  note          = {Special Issue on Software Reuse and Product Lines},
  __markedentry = {[mac:]},
  abstract      = {Context
In large software organizations with a product line development approach, system test planning and scope selection is a complex task. Due to repeated testing: across different testing levels, over time (test for regression) as well as of different variants, the risk of redundant testing is large as well as the risk of overlooking important tests, hidden by the huge amount of possible tests.
Aims
This study assesses the amount and type of overlaid manual testing across feature, integration and system test in such context, it explores the causes of potential redundancy and elaborates on how to provide decision support in terms of visualization for the purpose of avoiding redundancy.
Method
An in-depth case study was launched including both qualitative and quantitative observations.
Results
A high degree of test overlay is identified originating from distributed test responsibilities, poor documentation and structure of test cases, parallel work and insufficient delta analysis. The amount of test overlay depends on which level of abstraction is studied.
Conclusions
Avoiding redundancy requires tool support, e.g. visualization of test design coverage, test execution progress, priorities of coverage items as well as visualized priorities of variants to support test case selection.},
  comment       = {14},
  doi           = {https://doi.org/10.1016/j.infsof.2012.04.009},
  keywords      = {Product-line, Software testing, Case study, Overlay, Redundancy, Efficiency},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584912001061},
}

@Article{Hanssen2008a,
  author        = {Geir K. Hanssen and Tor E. FÃ¦gri},
  title         = {Process fusion: An industrial case study on agile software product line engineering},
  journal       = {Journal of Systems and Software},
  year          = {2008},
  volume        = {81},
  number        = {6},
  pages         = {843 - 854},
  issn          = {0164-1212},
  note          = {Agile Product Line Engineering},
  __markedentry = {[mac:]},
  abstract      = {This paper presents a case study of a software product company that has successfully integrated practices from software product line engineering and agile software development. We show how practices from the two fields support the companyâ€™s strategic and tactical ambitions, respectively. We also discuss how the company integrates strategic, tactical and operational processes to optimize collaboration and consequently improve its ability to meet market needs, opportunities and challenges. The findings from this study are relevant to software product companies seeking ways to balance agility and product management. The findings also contribute to research on industrializing software engineering.},
  comment       = {12},
  doi           = {https://doi.org/10.1016/j.jss.2007.10.025},
  keywords      = {Software product development, Software product management, Software product line engineering, Agile software development},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121207002518},
}

@Article{Dermeval2015a,
  author        = {Diego Dermeval and Thyago TenÃ³rio and Ig Ibert Bittencourt and Alan Silva and Seiji Isotani and MÃ¡rcio Ribeiro},
  title         = {Ontology-based feature modeling: An empirical study in changing scenarios},
  journal       = {Expert Systems with Applications},
  year          = {2015},
  volume        = {42},
  number        = {11},
  pages         = {4950 - 4964},
  issn          = {0957-4174},
  __markedentry = {[mac:]},
  abstract      = {A software product line (SPL) is a set of software systems that have a particular set of common features and that satisfy the needs of a particular market segment or mission. Feature modeling is one of the key activities involved in the design of SPLs. The feature diagram produced in this activity captures the commonalities and variabilities of SPLs. In some complex domains (e.g., ubiquitous computing, autonomic systems and context-aware computing), it is difficult to foresee all functionalities and variabilities a specific SPL may require. Thus, Dynamic Software Product Lines (DSPLs) bind variation points at runtime to adapt to fluctuations in user needs as well as to adapt to changes in the environment. In this context, relying on formal representations of feature models is important to allow them to be automatically analyzed during system execution. Among the mechanisms used for representing and analyzing feature models, description logic (DL) based approaches demand to be better investigated in DSPLs since it provides capabilities, such as automated inconsistency detection, reasoning efficiency, scalability and expressivity. Ontology is the most common way to represent feature models knowledge based on DL reasoners. Previous works conceived ontologies for feature modeling either based on OWL classes and properties or based on OWL individuals. However, considering change or evolution scenarios of feature models, we need to compare whether a class-based or an individual-based feature modeling style is recommended to describe feature models to support SPLs, and especially its capabilities to deal with changes in feature models, as required by DSPLs. In this paper, we conduct a controlled experiment to empirically compare two approaches based on each one of these modeling styles in several changing scenarios (e.g., add/remove mandatory feature, add/remove optional feature and so on). We measure time to perform changes, structural impact of changes (flexibility) and correctness for performing changes in our experiment. Our results indicate that using OWL individuals requires less time to change and is more flexible than using OWL classes and properties. These results provide insightful assumptions towards the definition of an approach relying on reasoning capabilities of ontologies that can effectively support products reconfiguration in the context of DSPL.},
  comment       = {15},
  doi           = {https://doi.org/10.1016/j.eswa.2015.02.020},
  keywords      = {Feature modeling, Ontology, Software product line, Empirical software engineering},
  url           = {http://www.sciencedirect.com/science/article/pii/S0957417415001190},
}

@Article{Gaia2014,
  author        = {Felipe Nunes Gaia and Gabriel Coutinho Sousa Ferreira and Eduardo Figueiredo and Marcelo de Almeida Maia},
  title         = {A quantitative and qualitative assessment of aspectual feature modules for evolving software product lines},
  journal       = {Science of Computer Programming},
  year          = {2014},
  volume        = {96},
  pages         = {230 - 253},
  issn          = {0167-6423},
  note          = {Selected and extended papers of the Brazilian Symposium on Programming Languages 2012 (SBLP 2012)},
  __markedentry = {[mac:]},
  abstract      = {Feature-Oriented Programming (FOP) and Aspect-Oriented Programming (AOP) are programming techniques based on composition mechanisms, called refinements and aspects, respectively. These techniques are assumed to be good variability mechanisms for implementing Software Product Lines (SPLs). Aspectual Feature Modules (AFM) is an approach that combines advantages of feature modules and aspects to increase concern modularity. Some guidelines on how to integrate these techniques have been established in some studies, but these studies do not focus the analysis on how effectively AFM can preserve the modularity and stability facilitating SPL evolution. The main purpose of this paper is to investigate whether the simultaneous use of aspects and features through the AFM approach facilitates the evolution of SPLs. The quantitative data were collected from two SPLs developed using four different variability mechanisms: (1) feature modules, aspects and aspects refinements of AFM, (2) aspects of aspect-oriented programming (AOP), (3) feature modules of feature-oriented programming (FOP), and (4) conditional compilation (CC) with object-oriented programming. Metrics for change propagation and modularity were calculated and the results support the benefits of the AFM option in a context where the product line has been evolved with addition or modification of crosscutting concerns. However a drawback of this approach is that refactoring components' design requires a higher degree of modifications to the SPL structure.},
  comment       = {24},
  doi           = {https://doi.org/10.1016/j.scico.2014.03.006},
  keywords      = {Software product lines, Feature-oriented programming, Aspect-oriented programming, Aspectual feature modules, Variability mechanisms},
  url           = {http://www.sciencedirect.com/science/article/pii/S0167642314001336},
}

@Article{Martini2015,
  author        = {Antonio Martini and Jan Bosch and Michel Chaudron},
  title         = {Investigating Architectural Technical Debt accumulation and refactoring over time: A multiple-case study},
  journal       = {Information and Software Technology},
  year          = {2015},
  volume        = {67},
  pages         = {237 - 253},
  issn          = {0950-5849},
  __markedentry = {[mac:]},
  abstract      = {Context
A known problem in large software companies is to balance the prioritization of short-term with long-term feature delivery speed. Specifically, Architecture Technical Debt is regarded as sub-optimal architectural solutions taken to deliver fast that might hinder future feature development, which, in turn, would hinder agility.
Objective
This paper aims at improving software management by shedding light on the current factors responsible for the accumulation of Architectural Technical Debt and to understand how it evolves over time.
Method
We conducted an exploratory multiple-case embedded case study in 7 sites at 5 large companies. We evaluated the results with additional cross-company interviews and an in-depth, company-specific case study in which we initially evaluate factors and models.
Results
We compiled a taxonomy of the factors and their influence in the accumulation of Architectural Technical Debt, and we provide two qualitative models of how the debt is accumulated and refactored over time in the studied companies. We also list a set of exploratory propositions on possible refactoring strategies that can be useful as insights for practitioners and as hypotheses for further research.
Conclusion
Several factors cause constant and unavoidable accumulation of Architecture Technical Debt, which leads to development crises. Refactorings are often overlooked in prioritization and they are often triggered by development crises, in a reactive fashion. Some of the factors are manageable, while others are external to the companies. ATD needs to be made visible, in order to postpone the crises according to the strategic goals of the companies. There is a need for practices and automated tools to proactively manage ATD.},
  comment       = {17},
  doi           = {https://doi.org/10.1016/j.infsof.2015.07.005},
  keywords      = {Architectural Technical Debt, Software management, Software architecture, Agile software development, Software life-cycle, Qualitative model},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584915001287},
}

@Article{Nasr2017a,
  author        = {Sana Ben Nasr and Guillaume BÃ©can and Mathieu Acher and JoÃ£o Bosco Ferreira Filho and Nicolas Sannier and Benoit Baudry and Jean-Marc Davril},
  title         = {Automated extraction of product comparison matrices from informal product descriptions},
  journal       = {Journal of Systems and Software},
  year          = {2017},
  volume        = {124},
  pages         = {82 - 103},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {Domain analysts, product managers, or customers aim to capture the important features and differences among a set of related products. A case-by-case reviewing of each product description is a laborious and time-consuming task that fails to deliver a condense view of a family of product. In this article, we investigate the use of automated techniques for synthesizing a product comparison matrix (PCM) from a set of product descriptions written in natural language. We describe a tool-supported process, based on term recognition, information extraction, clustering, and similarities, capable of identifying and organizing features and values in a PCM â€“ despite the informality and absence of structure in the textual descriptions of products. We evaluate our proposal against numerous categories of products mined from BestBuy. Our empirical results show that the synthesized PCMs exhibit numerous quantitative, comparable information that can potentially complement or even refine technical descriptions of products. The user study shows that our automatic approach is capable of extracting a significant portion of correct features and correct values. This approach has been implemented in MatrixMiner a web environment with an interactive support for automatically synthesizing PCMs from informal product descriptions. MatrixMiner also maintains traceability with the original descriptions and the technical specifications for further refinement or maintenance by users.},
  comment       = {22},
  doi           = {https://doi.org/10.1016/j.jss.2016.11.018},
  keywords      = {Software product lines, Variability mining, Feature mining, Product comparison matrices, Reverse engineering},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121216302242},
}

@Article{Mendonca2010,
  author        = {Marcilio Mendonca and Donald Cowan},
  title         = {Decision-making coordination and efficient reasoning techniques for feature-based configuration},
  journal       = {Science of Computer Programming},
  year          = {2010},
  volume        = {75},
  number        = {5},
  pages         = {311 - 332},
  issn          = {0167-6423},
  note          = {Coordination Models, Languages and Applications (SACâ€™08)},
  __markedentry = {[mac:]},
  abstract      = {Software Product Lines is a contemporary approach to software development that exploits the similarities and differences within a family of systems in a particular domain of interest in order to provide a common infrastructure for deriving members of this family in a timely fashion, with high-quality standards, and at lower costs. In Software Product Lines, feature-based product configuration is the process of selecting the desired features for a given software product from a repository of features called a feature model. This process is usually carried out collaboratively by people with distinct skills and interests called stakeholders. Collaboration benefits stakeholders by allowing them to directly intervene in the configuration process. However, collaboration also raises an important side effect, i.e., the need of stakeholders to cope with decision conflicts. Conflicts arise when decisions that are locally consistent cannot be applied globally because they violate one or more constraints in the feature model. Unfortunately, current product configuration systems are typically single-user-based in the sense that they do not provide means to coordinate concurrent decision-making on the feature model. As a consequence, configuration is carried out by a single person that is in charge of representing the interests of all stakeholders and managing decision conflicts on their own. This results in an error-prone and time-consuming process that requires past decisions to be revisited continuously either to correct misinterpreted stakeholder requirements or to handle decision conflicts. Yet another challenging issue related to configuration problems is the typically high computational cost of configuration algorithms. In fact, these algorithms frequently fall into the category of NP-hard and thus can become intractable in practice. In this paper, our goal is two-fold. First, we revisit our work on Collaborative Product Configuration (CPC) in which we proposed an approach to describe and validate collaborative configuration scenarios. We discuss how collaborative configuration can be described in terms of a workflow-like plan that safely guides stakeholders during the configuration process. Second, we propose a preliminary set of reasoning algorithms tailored to the feature modelling domain that can be used to provide automated support for product configuration. In addition, we compare empirically the performance of the proposed algorithms to that of a general-purpose solution. We hope that the insights provided in this paper will encourage other researchers to develop new algorithms in the near future.},
  comment       = {22},
  doi           = {https://doi.org/10.1016/j.scico.2009.12.004},
  keywords      = {Software Product Lines, Product configuration, Feature models, Feature modelling, Decision-making coordination, Automated reasoning, Constraint-based reasoning},
  url           = {http://www.sciencedirect.com/science/article/pii/S0167642309001713},
}

@Article{Ampatzoglou2011,
  author        = {Apostolos Ampatzoglou and Apostolos Kritikos and George Kakarontzas and Ioannis Stamelos},
  title         = {An empirical investigation on the reusability of design patterns and software packages},
  journal       = {Journal of Systems and Software},
  year          = {2011},
  volume        = {84},
  number        = {12},
  pages         = {2265 - 2283},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {Nowadays open-source software communities are thriving. Successful open-source projects are competitive and the amount of source code that is freely available offers great reuse opportunities to software developers. Thus, it is expected that several requirements can be implemented based on open source software reuse. Additionally, design patterns, i.e. well-known solution to common design problems, are introduced as elements of reuse. This study attempts to empirically investigate the reusability of design patterns, classes and software packages. Thus, the results can help developers to identify the most beneficial starting points for white box reuse, which is quite popular among open source communities. In order to achieve this goal we conducted a case study on one hundred (100) open source projects. More specifically, we identified 27,461 classes that participate in design patterns and compared the reusability of each of these classes with the reusability of the pattern and the package that this class belongs to. In more than 40% of the cases investigated, design pattern based class selection, offers the most reusable starting point for white-box reuse. However there are several cases when package based selection might be preferable. The results suggest that each pattern has different level of reusability.},
  comment       = {18},
  doi           = {https://doi.org/10.1016/j.jss.2011.06.047},
  keywords      = {Design patterns, Design, Quality, Reusability, Empirical approach},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121211001592},
}

@Article{Cirilo2012,
  author        = {Elder Cirilo and Ingrid Nunes and UirÃ¡ Kulesza and Carlos Lucena},
  title         = {Automating the product derivation process of multi-agent systems product lines},
  journal       = {Journal of Systems and Software},
  year          = {2012},
  volume        = {85},
  number        = {2},
  pages         = {258 - 276},
  issn          = {0164-1212},
  note          = {Special issue with selected papers from the 23rd Brazilian Symposium on Software Engineering},
  __markedentry = {[mac:]},
  abstract      = {Agent-oriented software engineering and software product lines are two promising software engineering techniques. Recent research work has been exploring their integration, namely multi-agent systems product lines (MAS-PLs), to promote reuse and variability management in the context of complex software systems. However, current product derivation approaches do not provide specific mechanisms to deal with MAS-PLs. This is essential because they typically encompass several concerns (e.g., trust, coordination, transaction, state persistence) that are constructed on the basis of heterogeneous technologies (e.g., object-oriented frameworks and platforms). In this paper, we propose the use of multi-level models to support the configuration knowledge specification and automatic product derivation of MAS-PLs. Our approach provides an agent-specific architecture model that uses abstractions and instantiation rules that are relevant to this application domain. In order to evaluate the feasibility and effectiveness of the proposed approach, we have implemented it as an extension of an existing product derivation tool, called GenArch. The approach has also been evaluated through the automatic instantiation of two MAS-PLs, demonstrating its potential and benefits to product derivation and configuration knowledge specification.},
  comment       = {19},
  doi           = {https://doi.org/10.1016/j.jss.2011.04.066},
  keywords      = {Multi-agent systems, Software product lines, Application engineering, Model-driven development, Product derivation tool},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121211001075},
}

@Article{Segura2011,
  author        = {Sergio Segura and Robert M. Hierons and David Benavides and Antonio Ruiz-CortÃ©s},
  title         = {Mutation testing on an object-oriented framework: An experience report},
  journal       = {Information and Software Technology},
  year          = {2011},
  volume        = {53},
  number        = {10},
  pages         = {1124 - 1136},
  issn          = {0950-5849},
  note          = {Special Section on Mutation Testing},
  __markedentry = {[mac:]},
  abstract      = {Context
The increasing presence of Object-Oriented (OO) programs in industrial systems is progressively drawing the attention of mutation researchers toward this paradigm. However, while the number of research contributions in this topic is plentiful, the number of empirical results is still marginal and mostly provided by researchers rather than practitioners.
Objective
This article reports our experience using mutation testing to measure the effectiveness of an automated test data generator from a user perspective.
Method
In our study, we applied both traditional and class-level mutation operators to FaMa, an open source Java framework currently being used for research and commercial purposes. We also compared and contrasted our results with the data obtained from some motivating faults found in the literature and two real tools for the analysis of feature models, FaMa and SPLOT.
Results
Our results are summarized in a number of lessons learned supporting previous isolated results as well as new findings that hopefully will motivate further research in the field.
Conclusion
We conclude that mutation testing is an effective and affordable technique to measure the effectiveness of test mechanisms in OO systems. We found, however, several practical limitations in current tool support that should be addressed to facilitate the work of testers. We also missed specific techniques and tools to apply mutation testing at the system level.},
  comment       = {13},
  doi           = {https://doi.org/10.1016/j.infsof.2011.03.006},
  keywords      = {Mutation testing, Test adequacy, Test data generation, Automated analysis, Feature models},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584911000826},
}

@Article{Nikolov2015,
  author        = {Nikolay Nikolov and Alessandro Rossini and Kyriakos Kritikos},
  title         = {Integration of DSLs and Migration of Models: A Case Study in the Cloud Computing Domain},
  journal       = {Procedia Computer Science},
  year          = {2015},
  volume        = {68},
  pages         = {53 - 66},
  issn          = {1877-0509},
  note          = {1st International Conference on Cloud Forward: From Distributed to Complete Computing},
  __markedentry = {[mac:]},
  abstract      = {Domain-specific languages (DSLs) are high-level software languages representing concepts in a particular domain. In real-world scenarios, it is common to adopt multiple DSLs to solve different aspects of a specific problem. As any other software artefact, DSLs evolve independently in response to changing requirements, which leads to two challenges. First, the concepts from the DSLs have to be integrated into a single language. Second, models that conform to an old version of the language have to be migrated to conform to its current version. In this paper, we discuss how we tackled the challenge of integrating the DSLs that comprise the Cloud Application Modelling and Execution Language (CAMEL) by leveraging upon Eclipse Modeling Framework (EMF) and Object Constraint Language (OCL). Moreover, we propose a solution to the challenge of persisting and automatically migrating CAMEL models based on Connected Data Objects (CDO) and Edapt.},
  comment       = {14},
  doi           = {https://doi.org/10.1016/j.procs.2015.09.223},
  keywords      = {model-driven engineering, domain-specific language, metamodel migration, model co-evolution, cloud computing, CAMEL, EMF, OCL, CDO, Edapt},
  url           = {http://www.sciencedirect.com/science/article/pii/S1877050915030689},
}

@Article{Pena2007,
  author        = {Joaquin PeÃ±a and Michael G. Hinchey and Manuel Resinas and Roy Sterritt and James L. Rash},
  title         = {Designing and managing evolving systems using a MAS product line approach},
  journal       = {Science of Computer Programming},
  year          = {2007},
  volume        = {66},
  number        = {1},
  pages         = {71 - 86},
  issn          = {0167-6423},
  note          = {Special Issue on the 5th International Workshop on System/Software Architectures (IWSSAâ€™06)},
  __markedentry = {[mac:]},
  abstract      = {We view an evolutionary system as being a software product line. The core architecture is the unchanging part of the system, and each version of the system may be viewed as a product from the product line. Each â€œproductâ€ may be described as the core architecture with some agent-based additions. The result is a multiagent system software product line. We describe an approach to such a software product line-based approach using the MaCMAS agent-oriented methodology. The approach scales to enterprise architectures as a multiagent system is an appropriate means of representing a changing enterprise architecture and the interaction between components in it. In addition, we reduce the gap between the enterprise architecture and the software architecture.},
  comment       = {16},
  doi           = {https://doi.org/10.1016/j.scico.2006.10.007},
  keywords      = {Multiagent systems product lines, Enterprise architecture evolution, Swarm-based systems},
  url           = {http://www.sciencedirect.com/science/article/pii/S0167642306002498},
}

@Article{Berger2014,
  author        = {Thorsten Berger and Rolf-Helge Pfeiffer and Reinhard Tartler and Steffen Dienst and Krzysztof Czarnecki and Andrzej WÄ…sowski and Steven She},
  title         = {Variability mechanisms in software ecosystems},
  journal       = {Information and Software Technology},
  year          = {2014},
  volume        = {56},
  number        = {11},
  pages         = {1520 - 1535},
  issn          = {0950-5849},
  note          = {Special issue on Software Ecosystems},
  __markedentry = {[mac:]},
  abstract      = {Context
Software ecosystems are increasingly popular for their economic, strategic, and technical advantages. Application platforms such as Android or iOS allow users to highly customize a system by selecting desired functionality from a large variety of assets. This customization is achieved using variability mechanisms.
Objective
Variability mechanisms are well-researched in the context of software product lines. Although software ecosystems are often seen as conceptual successors, the technology that sustains their success and growth is much less understood. Our objective is to improve empirical understanding of variability mechanisms used in successful software ecosystems.
Method
We analyze five ecosystems, ranging from the Linux kernel through Eclipse to Android. A qualitative analysis identifies and characterizes variability mechanisms together with their organizational context. This analysis leads to a conceptual framework that unifies ecosystem-specific aspects using a common terminology. A quantitative analysis investigates scales, growth rates, andâ€”most importantlyâ€”dependency structures of the ecosystems.
Results
In all the studied ecosystems, we identify rich dependency languages and variability descriptions that declare many direct and indirect dependencies. Indirect dependencies to abstract capabilities, as opposed to concrete variability units, are used predominantly in fast-growing ecosystems. We also find that variability modelsâ€”while providing system-wide abstractions over codeâ€”work best in centralized variability management and are, thus, absent in ecosystems with large free markets. These latter ecosystems tend to emphasize maintaining capabilities and common vocabularies, dynamic discovery, and binding with strong encapsulation of contributions, together with uniform distribution channels.
Conclusion
The use of specialized mechanisms in software ecosystems with large free markets, as opposed to software product lines, calls for recognition of a new disciplineâ€”variability encouragement.},
  comment       = {16},
  doi           = {https://doi.org/10.1016/j.infsof.2014.05.005},
  keywords      = {Software ecosystems, Empirical software engineering, Software product lines, Variability management, Mining software repositories},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584914001220},
}

@Article{Eklund2013,
  author        = {Ulrik Eklund and HÃ¥kan Gustavsson},
  title         = {Architecting automotive product lines: Industrial practice},
  journal       = {Science of Computer Programming},
  year          = {2013},
  volume        = {78},
  number        = {12},
  pages         = {2347 - 2359},
  issn          = {0167-6423},
  note          = {Special Section on International Software Product Line Conference 2010 and Fundamentals of Software Engineering (selected papers of FSEN 2011)},
  __markedentry = {[mac:]},
  abstract      = {This paper presents an in-depth view of how architects work with maintaining product line architectures at two internationally well-known automotive companies. The case study shows several interesting results. The process of managing architectural changes as well as the information the architects maintain and update is surprisingly similar between the two companies, despite that one has a strong line organisation and the other a strong project organisation. The architecting process found does not differ from what can be seen in other business domains. What does differ is that the architects studied see themselves interacting much more with other stakeholders than architects in general. The actual architectures are based on similar technology, e.g. CAN, but the network topology, S/W deployment and interfaces are totally different. The results indicate how the companyâ€™s different core values influence the architects when defining and maintaining the architectures over time. One company maintains four similar architectures in parallel, each at a different stage in their respective life-cycle, while the other has a single architecture for all products since 2002. The organisational belonging of the architects in the former company has been turbulent in contrast to the latter and there is some speculation if this is correlated.},
  comment       = {13},
  doi           = {https://doi.org/10.1016/j.scico.2012.06.008},
  keywords      = {Architecting, Process, Case study, Automotive industry},
  url           = {http://www.sciencedirect.com/science/article/pii/S0167642312001190},
}

@Article{Heradio2009,
  author        = {R. Heradio and J.A. Cerrada and J.C. Lopez and J.R. Coz},
  title         = {Code Generation with the Exemplar Flexibilization Language},
  journal       = {Electronic Notes in Theoretical Computer Science},
  year          = {2009},
  volume        = {238},
  number        = {2},
  pages         = {25 - 34},
  issn          = {1571-0661},
  note          = {Proceedings of the First Workshop on Generative Technologies (WGT) 2008.},
  __markedentry = {[mac:]},
  abstract      = {Code Generation is an increasing popular technique for implementing Software Product Lines that produces code from abstract specifications written in Domain Specific Languages (DSLs). This paper proposes to take advantage of the similitude among the products in a domain to generate them by analogy. That is, instead of synthesizing the final code from scratch or transforming the DSL specifications, the final products are obtained by adapting a previously developed domain product. The paper also discusses the capabilities and limitations of several currently available tools and languages to implement this kind of generators and introduce a new language to overcome the limitations.},
  comment       = {10},
  doi           = {https://doi.org/10.1016/j.entcs.2009.05.004},
  keywords      = {Code Generation, Domain Specific Language, Software Product Line},
  url           = {http://www.sciencedirect.com/science/article/pii/S1571066109001261},
}

@Article{Anjorin2013a,
  author        = {Anthony Anjorin and Karsten Saller and Ingo Reimund and Sebastian Oster and Ivan Zorcic and Andy SchÃ¼rr},
  title         = {Model-driven rapid prototyping with programmed graph transformations},
  journal       = {Journal of Visual Languages \& Computing},
  year          = {2013},
  volume        = {24},
  number        = {6},
  pages         = {441 - 462},
  issn          = {1045-926X},
  __markedentry = {[mac:]},
  abstract      = {Modern software systems are constantly increasing in complexity and supporting the rapid prototyping of such systems has become crucial to check the feasibility of extensions and optimizations, thereby reducing risks and, consequently, the cost of development. As modern software systems are also expected to be reused, extended, and adapted over a much longer lifetime than ever before, ensuring the maintainability of such systems is equally gaining relevance. In this paper, we present the development, optimization and maintenance of MoSo-PoLiTe, a framework for Software Product Line (SPL) testing, as a novel case study for rapid prototyping via metamodelling and programmed graph transformations. The first part of the case study evaluates the use of programmed graph transformations for optimizing an existing, hand-written system (MoSo-PoLiTe) via rapid prototyping of various strategies. In the second part, we present a complete re-engineering of the hand-written system with programmed graph transformations and provide a critical comparison of both implementations. Our results and conclusions indicate that metamodelling and programmed graph transformation are not only suitable techniques for rapid prototyping, but also lead to more maintainable systems.},
  comment       = {22},
  doi           = {https://doi.org/10.1016/j.jvlc.2013.08.001},
  keywords      = {Rapid prototyping, Programmed graph transformations, Metamodelling, Software product lines, Model-driven testing},
  url           = {http://www.sciencedirect.com/science/article/pii/S1045926X1300044X},
}

@Article{Acher2013,
  author        = {Mathieu Acher and Philippe Collet and Philippe Lahire and Robert B. France},
  title         = {FAMILIAR: A domain-specific language for large scale management of feature models},
  journal       = {Science of Computer Programming},
  year          = {2013},
  volume        = {78},
  number        = {6},
  pages         = {657 - 681},
  issn          = {0167-6423},
  note          = {Special section: The Programming Languages track at the 26th ACM Symposium on Applied Computing (SAC 2011) \& Special section on Agent-oriented Design Methods and Programming Techniques for Distributed Computing in Dynamic and Complex Environments},
  __markedentry = {[mac:]},
  abstract      = {The feature model formalism has become the de facto standard for managing variability in software product lines (SPLs). In practice, developing an SPL can involve modeling a large number of features representing different viewpoints, sub-systems or concerns of the software system. This activity is generally tedious and error-prone. In this article, we present FAMILIAR a Domain-Specific Language (DSL) that is dedicated to the large scale management of feature models and that complements existing tool support. The language provides a powerful support for separating concerns in feature modeling, through the provision of composition and decomposition operators, reasoning facilities and scripting capabilities with modularization mechanisms. We illustrate how an SPL consisting of medical imaging services can be practically managed using reusable FAMILIAR scripts that implement reasoning mechanisms. We also report on various usages and applications of FAMILIAR and its operators, to demonstrate their applicability to different domains and use for different purposes.},
  comment       = {25},
  doi           = {https://doi.org/10.1016/j.scico.2012.12.004},
  keywords      = {Domain-specific language, Feature model, Software product lines, Variability, Model management},
  url           = {http://www.sciencedirect.com/science/article/pii/S0167642312002158},
}

@Article{Oberweis2007,
  author        = {Andreas Oberweis and Victor Pankratius and Wolffried Stucky},
  title         = {Product lines for digital information products},
  journal       = {Information Systems},
  year          = {2007},
  volume        = {32},
  number        = {6},
  pages         = {909 - 939},
  issn          = {0306-4379},
  __markedentry = {[mac:]},
  abstract      = {The growth of the Web has fueled the creation, storage, and exchange of digital information products (DIPs), whose main purpose is the delivery of information, entertainment, education, or training. Very often, after their initial creation, the growing amount of content also leads to a more complicated maintenance, since updates typically occur rather often and the potential variability of modifications is not limited in advance. Moreover, commonalities between different parts of similar information products are not exploited, which often leads to redundancy. At the moment, there is hardly any attempt to compose information products from different sources and to produce more complex information products in a coordinated way. To help remedy this situation, this paper introduces the Product Lines for digitAl iNformation producTs (PLANT) approach, which applies the concept of software product lines to DIPs. The PLANT approach explicitly manages the commonalities of similar DIPs by defining common requirements, limiting variability in advance, as well as planning and coordinating reuse. This article focuses on the modeling of such product lines. In particular, the developed general concepts will be exemplified throughout the paper in the area of e-learning, in which DIPs play an important role. The application of PLANT in other areas and an implemented tool that supports the creation of information products in a product line are outlined as well.},
  comment       = {31},
  doi           = {https://doi.org/10.1016/j.is.2006.09.003},
  keywords      = {Digital products, Digital information products, Software product lines, Feature models, Workflow management, e-learning},
  url           = {http://www.sciencedirect.com/science/article/pii/S0306437906000809},
}

@Article{Holl2012,
  author        = {Gerald Holl and Paul GrÃ¼nbacher and Rick Rabiser},
  title         = {A systematic review and an expert survey on capabilities supporting multi product lines},
  journal       = {Information and Software Technology},
  year          = {2012},
  volume        = {54},
  number        = {8},
  pages         = {828 - 852},
  issn          = {0950-5849},
  note          = {Special Issue: Voice of the Editorial Board},
  __markedentry = {[mac:]},
  abstract      = {Context
Complex software-intensive systems comprise many subsystems that are often based on heterogeneous technological platforms and managed by different organizational units. Multi product lines (MPLs) are an emerging area of research addressing variability management for such large-scale or ultra-large-scale systems. Despite the increasing number of publications addressing MPLs the research area is still quite fragmented.
Objective
The aims of this paper are thus to identify, describe, and classify existing approaches supporting MPLs and to increase the understanding of the underlying research issues. Furthermore, the paper aims at defining success-critical capabilities of infrastructures supporting MPLs.
Method
Using a systematic literature review we identify and analyze existing approaches and research issues regarding MPLs. Approaches described in the literature support capabilities needed to define and operate MPLs. We derive capabilities supporting MPLs from the results of the systematic literature review. We validate and refine these capabilities based on a survey among experts from academia and industry.
Results
The paper discusses key research issues in MPLs and presents basic and advanced capabilities supporting MPLs. We also show examples from research approaches that demonstrate how these capabilities can be realized.
Conclusions
We conclude that approaches supporting MPLs need to consider both technical aspects like structuring large models and defining dependencies between product lines as well as organizational aspects such as distributed modeling and product derivation by multiple stakeholders. The identified capabilities can help to build, enhance, and evaluate MPL approaches.},
  comment       = {25},
  doi           = {https://doi.org/10.1016/j.infsof.2012.02.002},
  keywords      = {Product line engineering, Large-scale systems, Multi product lines, Systematic literature review},
  url           = {http://www.sciencedirect.com/science/article/pii/S095058491200033X},
}

@Article{Lemos2013,
  author        = {OtÃ¡vio Augusto Lazzarini Lemos and Fabiano Cutigi Ferrari and Marcelo Medeiros Eler and JosÃ© Carlos Maldonado and Paulo Cesar Masiero},
  title         = {Evaluation studies of software testing research in Brazil and in the world: A survey of two premier software engineering conferences},
  journal       = {Journal of Systems and Software},
  year          = {2013},
  volume        = {86},
  number        = {4},
  pages         = {951 - 969},
  issn          = {0164-1212},
  note          = {SI : Software Engineering in Brazil: Retrospective and Prospective Views},
  __markedentry = {[mac:]},
  abstract      = {This paper reports on a historical perspective of the evaluation studies present in software testing research published in the Brazilian Symposium on Software Engineering (SBES) in comparison to the International Conference on Software Engineering (ICSE). The survey characterizes the software testing-related papers published in the 25-year history of SBES, investigates the types of evaluation presented in these publications, and how the rate of evaluations has evolved over the years. A similar analysis within the same period is made for ICSE, allowing for a comparison between the national and international scenario. Results show that the rate of papers that present evaluation studies in SBES has significantly increased over the years. However, among the papers that described some kind of evaluation, only around 20% performed more rigorous evaluations (i.e. case studies, quasi experiments, or controlled experiments). Such percentage is low when compared to ICSE, which presented 40% of papers with more rigorous evaluations within the same period. Nevertheless, we noticed that both venues still lack the publication of research reporting controlled experiments: only a single paper in each conference presented this type of evaluation.},
  comment       = {19},
  doi           = {https://doi.org/10.1016/j.jss.2012.11.040},
  keywords      = {Software testing, Evaluation studies, Software testing research in Brazil},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121212003238},
}

@Article{Nunez-Varela2017,
  author        = {Alberto S. NuÃ±ez-Varela and HÃ©ctor G. PÃ©rez-Gonzalez and Francisco E. MartÃ­nez-Perez and Carlos Soubervielle-Montalvo},
  title         = {Source code metrics: A systematic mapping study},
  journal       = {Journal of Systems and Software},
  year          = {2017},
  volume        = {128},
  pages         = {164 - 197},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {Context
Source code metrics are essential components in the software measurement process. They are extracted from the source code of the software, and their values allow us to reach conclusions about the quality attributes measured by the metrics.
Objectives
This paper aims to collect source code metrics related studies, review them, and perform an analysis, while providing an overview on the current state of source code metrics and their current trends.
Method
A systematic mapping study was conducted. A total of 226 studies, published between the years 2010 and 2015, were selected and analyzed.
Results
Almost 300 source code metrics were found. Object oriented programming is the most commonly studied paradigm with the Chidamber and Kemerer metrics, lines of code, McCabe's cyclomatic complexity, and number of methods and attributes being the most used metrics. Research on aspect and feature oriented programming is growing, especially for the current interest in programming concerns and software product lines.
Conclusions
Object oriented metrics have gained much attention, but there is a current need for more studies on aspect and feature oriented metrics. Software fault prediction, complexity and quality assessment are recurrent topics, while concerns, big scale software and software product lines represent current trends.},
  comment       = {34},
  doi           = {https://doi.org/10.1016/j.jss.2017.03.044},
  keywords      = {Source code metrics, Software metrics, Object-oriented metrics, Aspect-oriented metrics, Feature-oriented metrics, Systematic mapping study},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121217300663},
}

@Article{Lavoie2017,
  author        = {Thierry Lavoie and Mathieu MÃ©rineau and Ettore Merlo and Pascal Potvin},
  title         = {A case study of TTCN-3 test scripts clone analysis in an industrial telecommunication setting},
  journal       = {Information and Software Technology},
  year          = {2017},
  volume        = {87},
  pages         = {32 - 45},
  issn          = {0950-5849},
  __markedentry = {[mac:]},
  abstract      = {Context: This paper presents a novel experiment focused on detecting and analyzing clones in test suites written in TTCN-3, a standard telecommunication test script language, for different industrial projects. Objective: This paper investigates frequencies, types, and similarity distributions of TTCN-3 clones in test scripts from three industrial projects in telecommunication. We also compare the distribution of clones in TTCN-3 test scripts with the distribution of clones in C/C++ and Java projects from the telecommunication domain. We then perform a statistical analysis to validate the significance of differences between these distributions. Method: Similarity is computed using CLAN, which compares metrics syntactically derived from script fragments. Metrics are computed from the Abstract Syntax Trees produced by a TTCN-3 parser called Titan developed by Ericsson as an Eclipse plugin. Finally, clone classification of similar script pairs is computed using the Longest Common Subsequence algorithm on token types and token images. Results: This paper presents figures and diagrams reporting TTCN-3 clone frequencies, types, and similarity distributions. We show that the differences between the distribution of clones in test scripts and the distribution of clones in applications are statistically significant. We also present and discuss some lessons that can be learned about the transferability of technology from this study. Conclusion: About 24% of fragments in the test suites are cloned, which is a very high proportion of clones compared to what is generally found in source code. The difference in proportion of Type-1 and Type-2 clones is statistically significant and remarkably higher in TTCN-3 than in source code. Type-1 and Type-2 clones represent 82.9% and 15.3% of clone fragments for a total of 98.2%. Within the projects this study investigated, this represents more and easier potential re-factoring opportunities for test scripts than for code.},
  comment       = {14},
  doi           = {https://doi.org/10.1016/j.infsof.2017.01.008},
  keywords      = {Clone detection, Telecommunications software, Test},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584917300605},
}

@Article{Sturm2014,
  author        = {Arnon Sturm and Oded Kramer},
  title         = {Evaluating the productivity of a reference-based programming approach: A controlled experiment},
  journal       = {Information and Software Technology},
  year          = {2014},
  volume        = {56},
  number        = {10},
  pages         = {1390 - 1402},
  issn          = {0950-5849},
  __markedentry = {[mac:]},
  abstract      = {Context
Domain engineering aims at facilitating software development in an efficient and economical way. One way to measure that is through productivity indicators, which refer to the ability of creating a quality software product in a limited period and with limited resources. Many approaches have been devised to increase productivity; however, these approaches seem to suffer from a tension between expressiveness on the one hand, and applicability (or the lack of it) in providing guidance for developers.
Objective
This paper evaluates the applicability and efficiency of adopting a domain engineering approach, called Application-based DOmain Modeling (ADOM), in the context of the programming task with Java, and thus termed ADOM-Java, for improving productivity in terms of code quality and development time.
Method
To achieve that objective we have qualitatively evaluate the approach using questionnaires and following a text analysis procedure. We also set a controlled experiment in which 50 undergraduate students performed a Java-based programming task using either ADOM-Java or Java alone.
Results
The qualitative evaluation reveal that the approach is easy to uses and provides valuable guidance. Nevertheless, it requires training. The outcomes of the experiment indicate that the approach is applicable and that the students that used ADOM-Java achieved better code quality, as well as better functionality and within less time than the students who used only Java.
Conclusion
The results of the experiments imply that by providing a code base equipped with reuse guidelines for programmers can increase programming productivity in terms of quality and development time. These guidelines may also enforce coding standards and architectural design.},
  comment       = {13},
  doi           = {https://doi.org/10.1016/j.infsof.2014.05.003},
  keywords      = {Productivity, Programming, Software reusability, Software quality, Domain engineering},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584914001062},
}

@Article{Kim2007a,
  author        = {Minseong Kim and Sooyong Park and Vijayan Sugumaran and Hwasil Yang},
  title         = {Managing requirements conflicts in software product lines: A goal and scenario based approach},
  journal       = {Data \& Knowledge Engineering},
  year          = {2007},
  volume        = {61},
  number        = {3},
  pages         = {417 - 432},
  issn          = {0169-023X},
  note          = {Advances on Natural Language Processing},
  __markedentry = {[mac:]},
  abstract      = {The product line approach is recognized as a successful approach to reuse in software development. However, in many cases, it has resulted in interactions between requirements and/or features. Interaction detection, especially conflict detection between requirements has become more challenging. Thus, detecting conflicts between requirements is essential for successful product line development. Formal methods have been proposed to address this problem, however, they are hard to understand by non-experts and are limited to restricted domains. In addition, there is no overall process that covers all the steps for managing conflicts. We propose an approach for systematically identifying and managing requirements conflicts, which is based on requirements partition in natural language and supported by a tool. To demonstrate its feasibility, the proposed approach has been applied to the home integration system (HIS) domain and the results are discussed.},
  comment       = {16},
  doi           = {https://doi.org/10.1016/j.datak.2006.06.009},
  keywords      = {Requirements conflicts, Software product line, Goal and scenario authoring, Requirements partitioning, Syntactic and semantic requirements conflict detection},
  url           = {http://www.sciencedirect.com/science/article/pii/S0169023X06001121},
}

@Article{Jezek2015,
  author        = {Kamil Jezek and Jens Dietrich and Premek Brada},
  title         = {How Java APIs break â€“ An empirical study},
  journal       = {Information and Software Technology},
  year          = {2015},
  volume        = {65},
  pages         = {129 - 146},
  issn          = {0950-5849},
  __markedentry = {[mac:]},
  abstract      = {Context
It has become common practice to build programs by using libraries. While the benefits of reuse are well known, an often overlooked risk are system runtime failures due to API changes in libraries that evolve independently. Traditionally, the consistency between a program and the libraries it uses is checked at build time when the entire system is compiled and tested. However, the trend towards partially upgrading systems by redeploying only evolved library versions results in situations where these crucial verification steps are skipped. For Java programs, partial upgrades create additional interesting problems as the compiler and the virtual machine use different rule sets to enforce contracts between the providers and the consumers of APIs.
Objective
We have studied the extent of the problem in real world programs. We were interested in two aspects: the compatibility of API changes as libraries evolve, and the impact this has on programs using these libraries.
Method
This study is based on the qualitas corpus version 20120401. A data set consisting of 109 Java open-source programs and 564 program versions was used from this corpus. We have investigated two types of library dependencies: explicit dependencies to embedded libraries, and dependencies defined by symbolic references in Maven build files that are resolved at build time. We have used JaCC for API analysis, this tool is based on the popular ASM byte code analysis library.
Results
We found that for most of the programs we investigated, APIs are unstable as incompatible changes are common. Surprisingly, there are more compatibility problems in projects that use automated dependency resolution. However, we found only a few cases where this has an actual impact on other programs using such an API.
Conclusion
It is concluded that API instability is common and causes problems for programs using these APIs. Therefore, better tools and methods are needed to safeguard library evolution.},
  comment       = {18},
  doi           = {https://doi.org/10.1016/j.infsof.2015.02.014},
  keywords      = {Binary compatibility, API evolution, Backward compatibility, Byte-code, Java},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584915000506},
}

@Article{Rosenmueller2009,
  author        = {Marko RosenmÃ¼ller and Sven Apel and Thomas Leich and Gunter Saake},
  title         = {Tailor-made data management for embedded systems: A case study on Berkeley DB},
  journal       = {Data \& Knowledge Engineering},
  year          = {2009},
  volume        = {68},
  number        = {12},
  pages         = {1493 - 1512},
  issn          = {0169-023X},
  note          = {Including Special Section: 21st IEEE International Symposium on Computer-Based Medical Systems (IEEE CBMS 2008) â€“ Seven selected and extended papers on Biomedical Data Mining},
  __markedentry = {[mac:]},
  abstract      = {Applications in the domain of embedded systems are diverse and store an increasing amount of data. In order to satisfy the varying requirements of these applications, data management functionality is needed that can be tailored to the applicationsâ€™ needs. Furthermore, the resource restrictions of embedded systems imply a need for data management that is customized to the hardware platform. In this paper, we present an approach for decomposing data management software for embedded systems using feature-oriented programming. The result of such a decomposition is a software product line that allows us to generate tailor-made data management systems. While existing approaches for tailoring software have significant drawbacks regarding customizability and performance, a feature-oriented approach overcomes these limitations, as we will demonstrate. In a non-trivial case study on Berkeley DB, we evaluate our approach and compare it to other approaches for tailoring DBMS.},
  comment       = {20},
  doi           = {https://doi.org/10.1016/j.datak.2009.07.013},
  keywords      = {Tailor-made data management, Embedded systems, Software product lines, Feature-oriented programming, FeatureC++},
  url           = {http://www.sciencedirect.com/science/article/pii/S0169023X09001128},
}

@Article{Ajila2007,
  author        = {Samuel A. Ajila and Razvan T. Dumitrescu},
  title         = {Experimental use of code delta, code churn, and rate of change to understand software product line evolution},
  journal       = {Journal of Systems and Software},
  year          = {2007},
  volume        = {80},
  number        = {1},
  pages         = {74 - 91},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {This research is a longitudinal study of change processes. It links changes in the product line architecture of a large telecommunications equipment supplier with the companyâ€™s customers, inner context, and eight line card products over six-year period. There are three important time related constructs in this study: the time it takes to develop a new product line release; the frequency in which a metric is collected; and the frequency at which financial results and metrics related to the customer layer are collected and made available. Data collection has been organized by product release. The original goal of this research is to study the economic impact of market reposition on the product line and identify metrics that can be used to records changes in product line. We later look at the product line evolution vis-Ã -vis the changes in the products that form the product line. Our results show that there is no relationship between the size of the code added to the product line and the number of designers required to develop and test it; and there is a positive relationship between designer turnover and impact of change.},
  comment       = {18},
  doi           = {https://doi.org/10.1016/j.jss.2006.05.034},
  keywords      = {Software product line, Software evolution, Code delta, Code churn, Rate of change, Impact analysis, Software metric},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121206001658},
}

@Article{Rabiser2010a,
  author        = {Rick Rabiser and Paul GrÃ¼nbacher and Deepak Dhungana},
  title         = {Requirements for product derivation support: Results from a systematic literature review and an expert survey},
  journal       = {Information and Software Technology},
  year          = {2010},
  volume        = {52},
  number        = {3},
  pages         = {324 - 346},
  issn          = {0950-5849},
  __markedentry = {[mac:]},
  abstract      = {Context
An increasing number of publications in product line engineering address product derivation, i.e., the process of building products from reusable assets. Despite its importance, there is still no consensus regarding the requirements for product derivation support.
Objective
Our aim is to identify and validate requirements for tool-supported product derivation.
Method
We identify the requirements through a systematic literature review and validate them with an expert survey.
Results
We discuss the resulting requirements and provide implementation examples from existing product derivation approaches.
Conclusions
We conclude that key requirements are emerging in the research literature and are also considered relevant by experts in the field.},
  comment       = {23},
  doi           = {https://doi.org/10.1016/j.infsof.2009.11.001},
  keywords      = {Product derivation, Software product line, Product line engineering, Systematic literature review},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584909001931},
}

@Article{Albuquerque2015,
  author        = {Diego Albuquerque and Bruno Cafeo and Alessandro Garcia and Simone Barbosa and Silvia AbrahÃ£o and AntÃ³nio Ribeiro},
  title         = {Quantifying usability of domain-specific languages: An empirical study on software maintenance},
  journal       = {Journal of Systems and Software},
  year          = {2015},
  volume        = {101},
  pages         = {245 - 259},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {A domain-specific language (DSL) aims to support software development by offering abstractions to a particular domain. It is expected that DSLs improve the maintainability of artifacts otherwise produced with general-purpose languages. However, the maintainability of the DSL artifacts and, hence, their adoption in mainstream development, is largely dependent on the usability of the language itself. Unfortunately, it is often hard to identify their usability strengths and weaknesses early, as there is no guidance on how to objectively reveal them. Usability is a multi-faceted quality characteristic, which is challenging to quantify beforehand by DSL stakeholders. There is even less support on how to quantitatively evaluate the usability of DSLs used in maintenance tasks. In this context, this paper reports a study to compare the usability of textual DSLs under the perspective of software maintenance. A usability measurement framework was developed based on the cognitive dimensions of notations. The framework was evaluated both qualitatively and quantitatively using two DSLs in the context of two evolving object-oriented systems. The results suggested that the proposed metrics were useful: (1) to early identify DSL usability limitations, (2) to reveal specific DSL features favoring maintenance tasks, and (3) to successfully analyze eight critical DSL usability dimensions.},
  comment       = {15},
  doi           = {https://doi.org/10.1016/j.jss.2014.11.051},
  keywords      = {DSL, Usability, Metrics},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121214002799},
}

@Article{Andersson2013,
  author        = {Henric Andersson and Erik Herzog and Johan Ã–lvander},
  title         = {Experience from model and software reuse in aircraft simulator product line engineering},
  journal       = {Information and Software Technology},
  year          = {2013},
  volume        = {55},
  number        = {3},
  pages         = {595 - 606},
  issn          = {0950-5849},
  note          = {Special Issue on Software Reuse and Product Lines},
  __markedentry = {[mac:]},
  abstract      = {Context
â€œReuseâ€ and â€œModel Based Developmentâ€ are two prominent trends for improving industrial development efficiency. Product lines are used to reduce the time to create product variants by reusing components. The model based approach provides the opportunity to enhance knowledge capture for a system in the early stages in order to be reused throughout its lifecycle. This paper describes how these two trends are combined to support development and support of a simulator product line for the SAAB 39 Gripen fighter aircraft.
Objective
The work aims at improving the support (in terms of efficiency and quality) when creating simulation model configurations. Software based simulators are flexible so variants and versions of included models may easily be exchanged. The objective is to increase the reuse when combining models for usage in a range of development and training simulators.
Method
The research has been conducted with an interactive approach using prototyping and demonstrations, and the evaluation is based on an iterative and a retrospective method.
Results
A product line of simulator models for the SAAB 39 Gripen aircraft has been analyzed and defined in a Product Variant Master. A configurator system has been implemented for creation, integration, and customization of stringent simulator model configurations. The system is currently under incorporation in the standard development process at SAAB Aeronautics.
Conclusion
The explicit and visual description of products and their variability through a configurator system enables better insights and a common understanding so that collaboration on possible product configurations improves and the potential of software reuse increases. The combination of application fields imposes constraints on how traditional tools and methods may be utilized. Solutions for Design Automation and Knowledge Based Engineering are available, but their application has limitations for Software Product Line engineering and the reuse of simulation models.},
  comment       = {12},
  doi           = {https://doi.org/10.1016/j.infsof.2012.06.014},
  keywords      = {Software Product Line, SPL, Knowledge Based Engineering, Configurator, Model Based Development, PDM},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584912001280},
}

@Article{Asadi2014,
  author        = {Mohsen Asadi and Samaneh Soltani and Dragan Gasevic and Marek Hatala and Ebrahim Bagheri},
  title         = {Toward automated feature model configuration with optimizing non-functional requirements},
  journal       = {Information and Software Technology},
  year          = {2014},
  volume        = {56},
  number        = {9},
  pages         = {1144 - 1165},
  issn          = {0950-5849},
  note          = {Special Sections from â€œAsia-Pacific Software Engineering Conference (APSEC), 2012â€ and â€œ Software Product Line conference (SPLC), 2012â€},
  __markedentry = {[mac:]},
  abstract      = {Context
A software product line is a family of software systems that share some common features but also have significant variabilities. A feature model is a variability modeling artifact, which represents differences among software products with respect to the variability relationships among their features. Having a feature model along with a reference model developed in the domain engineering lifecycle, a concrete product of the family is derived by binding the variation points in the feature model (called configuration process) and by instantiating the reference model.
Objective
In this work we address the feature model configuration problem and propose a framework to automatically select suitable features that satisfy both the functional and non-functional preferences and constraints of stakeholders. Additionally, interdependencies between various non-functional properties are taken into account in the framework.
Method
The proposed framework combines Analytical Hierarchy Process (AHP) and Fuzzy Cognitive Maps (FCM) to compute the non-functional properties weights based on stakeholdersâ€™ preferences and interdependencies between non-functional properties. Afterwards, Hierarchical Task Network (HTN) planning is applied to find the optimal feature model configuration.
Result
Our approach improves state-of-art of feature model configuration by considering positive or negative impacts of the features on non-functional properties, the stakeholdersâ€™ preferences, and non-functional interdependencies. The approach presented in this paper extends earlier work presented in [1] from several distinct perspectives including mechanisms handling interdependencies between non-functional properties, proposing a novel tooling architecture, and offering visualization and interaction techniques for representing functional and non-functional aspects of feature models.
Conclusion
our experiments show the scalability of our configuration approach when considering both functional and non-functional requirements of stakeholders.},
  comment       = {22},
  doi           = {https://doi.org/10.1016/j.infsof.2014.03.005},
  keywords      = {Software product lines, Feature model configuration, Stakeholdersâ€™ preferences, Non-functional interdependencies},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584914000640},
}

@Article{Classen2014,
  author        = {Andreas Classen and Maxime Cordy and Patrick Heymans and Axel Legay and Pierre-Yves Schobbens},
  title         = {Formal semantics, modular specification, and symbolic verification of product-line behaviour},
  journal       = {Science of Computer Programming},
  year          = {2014},
  volume        = {80},
  pages         = {416 - 439},
  issn          = {0167-6423},
  __markedentry = {[mac:]},
  abstract      = {Formal techniques for specifying and verifying Software Product Lines (SPL) are actively studied. While the foundations of this domain recently made significant progress with the introduction of Featured Transition Systems (FTSs) and associated algorithms, SPL model checking still faces the well-known state explosion problem. Moreover, there is a need for high-level specification languages usable in industry. We address the state explosion problem by applying the principles of symbolic model checking to FTS-based verification of SPLs. In order to specify properties on specific products only, we extend the temporal logic CTL with feature quantifiers. Next, we show how SPL behaviour can be specified with fSMV, a variant of SMV, the specification language of the industry-strength model checker NuSMV. fSMV is a feature-oriented extension of SMV originally introduced by Plath and Ryan. We prove that fSMV and FTSs are expressively equivalent. Finally, we connect these results to a NuSMV extension we developed for verifying SPLs against CTL properties.},
  comment       = {24},
  doi           = {https://doi.org/10.1016/j.scico.2013.09.019},
  keywords      = {Software product line, Verification, Feature, Language, Specification},
  url           = {http://www.sciencedirect.com/science/article/pii/S0167642313002517},
}

@Article{Santos2010,
  author        = {AndrÃ© L. Santos and Kai Koskimies and AntÃ³nia Lopes},
  title         = {Automating the construction of domain-specific modeling languages for object-oriented frameworks},
  journal       = {Journal of Systems and Software},
  year          = {2010},
  volume        = {83},
  number        = {7},
  pages         = {1078 - 1093},
  issn          = {0164-1212},
  note          = {SPLC 2008},
  __markedentry = {[mac:]},
  abstract      = {The extension of frameworks with domain-specific modeling languages (DSML) has proved to be an effective way of improving the productivity in software product-line engineering. However, developing and evolving a DSML is typically a difficult and time-consuming task because it requires to develop and maintain a code generator, which transforms application models into framework-based code. In this paper, we propose a new approach for extending object-oriented frameworks that aims to alleviate this problem. The approach is based on developing an additional aspect-oriented layer that encodes a DSML for building framework-based applications, eliminating the need of implementing a code generator. We further show how a language workbench is capable of automating the construction of DSMLs using the proposed layer.},
  comment       = {16},
  doi           = {https://doi.org/10.1016/j.jss.2010.01.047},
  keywords      = {Domain-specific modeling, Object-oriented frameworks, Software product-lines, Aspect-oriented programming},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121210000312},
}

@Article{Alegre2016,
  author        = {Unai Alegre and Juan Carlos Augusto and Tony Clark},
  title         = {Engineering context-aware systems and applications: A survey},
  journal       = {Journal of Systems and Software},
  year          = {2016},
  volume        = {117},
  pages         = {55 - 83},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {Context-awareness is an essential component of systems developed in areas like Intelligent Environments, Pervasive & Ubiquitous Computing and Ambient Intelligence. In these emerging fields, there is a need for computerized systems to have a higher understanding of the situations in which to provide services or functionalities, to adapt accordingly. The literature shows that researchers modify existing engineering methods in order to better fit the needs of context-aware computing. These efforts are typically disconnected from each other and generally focus on solving specific development issues. We encourage the creation of a more holistic and unified engineering process that is tailored for the demands of these systems. For this purpose, we study the state-of-the-art in the development of context-aware systems, focusing on: (A) Methodologies for developing context-aware systems, analyzing the reasons behind their lack of adoption and features that the community wish they can use; (B) Context-aware system engineering challenges and techniques applied during the most common development stages; (C) Context-aware systems conceptualization.},
  comment       = {29},
  doi           = {https://doi.org/10.1016/j.jss.2016.02.010},
  keywords      = {Context-Aware Systems Engineering, Context-aware computing, Context-awareness, Context-sensitive, Sentient computing, Pervasive & Ubiquitous Computing, Intelligent Environments, Ambient Intelligence, Software engineering},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121216000467},
}

@Article{Gholami2016,
  author        = {Mahdi Fahmideh Gholami and Farhad Daneshgar and Graham Low and Ghassan Beydoun},
  title         = {Cloud migration processâ€”A survey, evaluation framework, and open challenges},
  journal       = {Journal of Systems and Software},
  year          = {2016},
  volume        = {120},
  pages         = {31 - 69},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {Moving mission-oriented enterprise software applications to cloud environments is a crucial IT task and requires a systematic approach. The foci of this paper is to provide a detailed review of extant cloud migration approaches from the perspective of the process model. To this aim, an evaluation framework is proposed and used to appraise and compare existing approaches for highlighting their features, similarities, and key differences. The survey distills the status quo and makes a rich inventory of important activities, recommendations, techniques, and concerns that are common in a typical cloud migration process in one place. This enables both academia and practitioners in the cloud computing community to get an overarching view of the process of the legacy application migration to the cloud. Furthermore, the survey identifies a number challenges that have not been yet addressed by existing approaches, developing opportunities for further research endeavours.},
  comment       = {39},
  doi           = {https://doi.org/10.1016/j.jss.2016.06.068},
  keywords      = {Cloud migration, Legacy application, Evaluation framework, Migration methodology, Process model, Cloud computing},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121216300966},
}

@Article{Larrucea2016,
  author        = {Xabier Larrucea and Felix Nanclares and Izaskun Santamaria},
  title         = {A method for defining a regional software ecosystem strategy: Colombia as a case study},
  journal       = {Technological Forecasting and Social Change},
  year          = {2016},
  volume        = {104},
  pages         = {247 - 258},
  issn          = {0040-1625},
  __markedentry = {[mac:]},
  abstract      = {Software ecosystems (SECO) have been related to products or to a community of developers around a product. The SECO concept can also be applied to describe regional software ecosystems in which different software companies collaborate in a specific market based on a set of concrete technologies and using a set of capabilities. This paper details a regional SECO concept and a method based on regional endogenous capabilities and country needs to define a SECO strategy. Traditional strategy definition approaches are top-down, whereas this approach is a blended approach that merges bottom-up based on current regional capabilities and top-down based on market and technology trends. This paper presents a large case study performed in 6 regions of Colombia. We conducted 49 interviews and 16 workshops in which 654 attendees participated, and we developed the Colombian ICT national strategic plan based on this approach.},
  comment       = {12},
  doi           = {https://doi.org/10.1016/j.techfore.2016.01.008},
  keywords      = {Regional software ecosystems, Strategy, TRM},
  url           = {http://www.sciencedirect.com/science/article/pii/S0040162516000093},
}

@Article{Midtgaard2015,
  author        = {Jan Midtgaard and Aleksandar S. Dimovski and Claus Brabrand and Andrzej WÄ…sowski},
  title         = {Systematic derivation of correct variability-aware program analyses},
  journal       = {Science of Computer Programming},
  year          = {2015},
  volume        = {105},
  pages         = {145 - 170},
  issn          = {0167-6423},
  __markedentry = {[mac:]},
  abstract      = {A recent line of work lifts particular verification and analysis methods to Software Product Linesâ€‰(SPL). In an effort to generalize such case-by-case approaches, we develop a systematic methodology for lifting single-program analyses to SPLs using abstract interpretation. Abstract interpretation is a classical framework for deriving static analyses in a compositional, step-by-step manner. We show how to take an analysis expressed as an abstract interpretation and lift each of the abstract interpretation steps to a family of programs (SPL). This includes schemes for lifting domain types, and combinators for lifting analyses and Galois connections. We prove that for analyses developed using our method, the soundness of lifting follows by construction. The resulting variational abstract interpretation is a conceptual framework for understanding, deriving, and validating static analyses for SPLs. Then we show how to derive the corresponding variational dataflow equations for an example static analysis, a constant propagation analysis. We also describe how to approximate variability by applying variability-aware abstractions to SPL analysis. Finally, we discuss how to efficiently implement our method and present some evaluation results.},
  comment       = {26},
  doi           = {https://doi.org/10.1016/j.scico.2015.04.005},
  keywords      = {Software Product Lines, Software variability, Verification, Static analysis, Abstract interpretation},
  url           = {http://www.sciencedirect.com/science/article/pii/S0167642315000714},
}

@Article{Petersen2009,
  author        = {Kai Petersen and Claes Wohlin},
  title         = {A comparison of issues and advantages in agile and incremental development between state of the art and an industrial case},
  journal       = {Journal of Systems and Software},
  year          = {2009},
  volume        = {82},
  number        = {9},
  pages         = {1479 - 1490},
  issn          = {0164-1212},
  note          = {SI: QSIC 2007},
  __markedentry = {[mac:]},
  abstract      = {Recent empirical studies have been conducted identifying a number of issues and advantages of incremental and agile methods. However, the majority of studies focused on one model (Extreme Programming) and small projects. To draw more general conclusions we conduct a case study in large-scale development identifying issues and advantages, and compare the results with previous empirical studies on the topic. The principle results are that (1) the case study and literature agree on the benefits while new issues arise when using agile in large-scale and (2) an empirical research framework is needed to make agile studies comparable.},
  comment       = {12},
  doi           = {https://doi.org/10.1016/j.jss.2009.03.036},
  keywords      = {Agile, Incremental, State of the art, Case study},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121209000855},
}

@Article{Belli2016,
  author        = {Fevzi Belli and Christof J. Budnik and Axel Hollmann and Tugkan Tuglular and W. Eric Wong},
  title         = {Model-based mutation testingâ€”Approach and case studies},
  journal       = {Science of Computer Programming},
  year          = {2016},
  volume        = {120},
  pages         = {25 - 48},
  issn          = {0167-6423},
  __markedentry = {[mac:]},
  abstract      = {This paper rigorously introduces the concept of model-based mutation testing (MBMT) and positions it in the landscape of mutation testing. Two elementary mutation operators, insertion and omission, are exemplarily applied to a hierarchy of graph-based models of increasing expressive power including directed graphs, event sequence graphs, finite-state machines and statecharts. Test cases generated based on the mutated models (mutants) are used to determine not only whether each mutant can be killed but also whether there are any faults in the corresponding system under consideration (SUC) developed based on the original model. Novelties of our approach are: (1) evaluation of the fault detection capability (in terms of revealing faults in the SUC) of test sets generated based on the mutated models, and (2) superseding of the great variety of existing mutation operators by iterations and combinations of the two proposed elementary operators. Three case studies were conducted on industrial and commercial real-life systems to demonstrate the feasibility of using the proposed MBMT approach in detecting faults in SUC, and to analyze its characteristic features. Our experimental data suggest that test sets generated based on the mutated models created by insertion operators are more effective in revealing faults in SUC than those generated by omission operators. Worth noting is that test sets following the MBMT approach were able to detect faults in the systems that were tested by manufacturers and independent testing organizations before they were released.},
  comment       = {24},
  doi           = {https://doi.org/10.1016/j.scico.2016.01.003},
  keywords      = {Mutation testing, Model-based testing, Model-based mutation testing, Mutation operator, Fault detection capability},
  url           = {http://www.sciencedirect.com/science/article/pii/S0167642316000137},
}

@Article{Gamez2013,
  author        = {Nadia Gamez and Lidia Fuentes},
  title         = {Architectural evolution of FamiWare using cardinality-based feature models},
  journal       = {Information and Software Technology},
  year          = {2013},
  volume        = {55},
  number        = {3},
  pages         = {563 - 580},
  issn          = {0950-5849},
  note          = {Special Issue on Software Reuse and Product Lines},
  __markedentry = {[mac:]},
  abstract      = {Context
Ambient Intelligence systems domain is an outstanding example of modern systems that are in permanent evolution, as new devices, technologies or facilities are continuously appearing. This means it would be desirable to have a mechanism that helps with the propagation of evolution changes in deployed systems.
Objective
We present a software product line engineering process to manage the evolution of FamiWare, a family of middleware for ambient intelligence environments. This process drives the evolution of FamiWare middleware configurations using cardinality-based feature models, which are especially well suited to express the structural variability of ambient intelligence systems.
Method
FamiWare uses cardinality-based feature models and clonable features to model the structural variability present in ambient intelligence systems, composed of a large variety of heterogeneous devices. Since the management evolution of configurations with clonable features is manually untreatable due to the high number of features, our process automates it and propagates changes made at feature level to the architectural components of the FamiWare middleware. This is a model driven development process as the evolution management, the propagation of evolution changes and the code generation are performed using some kind of model mappings and transformations. Concretely we present a variability modelling language to map the selection of features to the corresponding FamiWare middleware architectural components.
Results
Our process is able to manage the evolution of cardinality-based feature models with thousands of features, something which is not possible to tackle manually. Thanks to the use of the variability language and the automatic code generation it is possible to propagate and maintain a correspondence between the FamiWare architectural model and the code. The process is then able to calculate the architectural differences between the evolved configuration and the previous one. Checking these differences, our process helps to calculate the effort needed to perform the evolution changes in the customized products. To perform those tasks we have defined two operators, one to calculate the differences between two feature model configurations and another to create a new configuration from a previous one.
Conclusion
Our process automatically propagates the evolution changes of the middleware family into the existing configurations where the middleware is already deployed and also helps us to calculate the effort in performing the changes in every configuration. Finally, we validated our approach, demonstrating the functioning of the defined operators and showing that by using our tool we can generate evolved configurations for FamiWare with thousands of cloned features, for several case studies.},
  comment       = {18},
  doi           = {https://doi.org/10.1016/j.infsof.2012.06.012},
  keywords      = {Middleware family, Software Product Lines, Feature Models, Evolution},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584912001152},
}

@Article{Babur2015,
  author        = {Ã–nder Babur and Vit Smilauer and Tom Verhoeff and Mark van den Brand},
  title         = {A Survey of Open Source Multiphysics Frameworks in Engineering},
  journal       = {Procedia Computer Science},
  year          = {2015},
  volume        = {51},
  pages         = {1088 - 1097},
  issn          = {1877-0509},
  note          = {International Conference On Computational Science, ICCS 2015},
  __markedentry = {[mac:]},
  abstract      = {This paper presents a systematic survey of open source multiphysics frameworks in the en- gineering domains. These domains share many commonalities despite the diverse application areas. A thorough search for the available frameworks with both academic and industrial ori- gins has revealed numerous candidates. Considering key characteristics such as project size, maturity and visibility, we selected Elmer, OpenFOAM and Salome for a detailed analysis. All the public documentation for these tools has been manually collected and inspected. Based on the analysis, we built a feature model for multiphysics in engineering, which captures the commonalities and variability in the domain. We in turn validated the resulting model via two other tools; Kratos by manual inspection, and OOFEM by means of expert validation by domain experts.},
  comment       = {10},
  doi           = {https://doi.org/10.1016/j.procs.2015.05.273},
  keywords      = {Multiphysics, Multiscale, Modelling and Simulation, Domain Analysis, Feature Model},
  url           = {http://www.sciencedirect.com/science/article/pii/S1877050915010819},
}

@Article{Lamancha2015,
  author        = {Beatriz PÃ©rez Lamancha and Macario Polo and Mario Piattini},
  title         = {PROW: A Pairwise algorithm with constRaints, Order and Weight},
  journal       = {Journal of Systems and Software},
  year          = {2015},
  volume        = {99},
  pages         = {1 - 19},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {Testing systems with many variables and/or values is often quite expensive due to the huge number of possible combinations to be tested. There are several criteria available to combine test data and produce scalable test suites. One of them is pairwise. With the pairwise criterion, each pair of values of any two parameters is included in at least one test case. Although this is a widely-used coverage criterion, two main characteristics improve considerably pairwise: constraints handling and prioritisation. This paper presents an algorithm and a tool. The algorithm (called PROW: Pairwise with constRaints, Order and Weight) handles constraints and prioritisation for pairwise coverage. The tool called CTWeb adds functionalities to execute PROW in different contexts, one of them is product sampling in Software Product Lines via importing feature models. Software Product Line (SPL) development is a recent paradigm, where a family of software systems is constructed by means of the reuse of a set of common functionalities and some variable functionalities. An essential artefact of a SPL is the feature model, which shows the features offered by the product line, jointly with the relationships (includes and excludes) among them. Pairwise testing could be used to obtain the product sampling to test in a SPL, using features as pairwise parameters. In this context, the constraint handling becomes essential. As a difference with respect to other tools, CTWeb does not require SAT solvers. This paper describes the PROW algorithm, also analysing its complexity and efficiency. The CTWeb tool is presented, including two examples of the PROW application to two real environments: the first corresponds to the migration of the subsystem of transactions processing of a credit card management system from AS400 to Oracle with.NET; the second applies both the algorithm and the tool to a SPL that monitors and controls some parameters of the load in trucks.},
  comment       = {19},
  doi           = {https://doi.org/10.1016/j.jss.2014.08.005},
  keywords      = {Software testing, Combinatorial testing},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121214001733},
}

@Article{Lago2009,
  author        = {Patricia Lago and Henry Muccini and Hans van Vliet},
  title         = {A scoped approach to traceability management},
  journal       = {Journal of Systems and Software},
  year          = {2009},
  volume        = {82},
  number        = {1},
  pages         = {168 - 182},
  issn          = {0164-1212},
  note          = {Special Issue: Software Performance - Modeling and Analysis},
  __markedentry = {[mac:]},
  abstract      = {Traceability is the ability to describe and follow the life of a software artifact and a means for modeling the relations between software artifacts in an explicit way. Traceability has been successfully applied in many software engineering communities and has recently been adopted to document the transition among requirements, architecture and implementation. We present an approach to customize traceability to the situation at hand. Instead of automating tracing, or representing all possible traces, we scope the traces to be maintained to the activities stakeholders must carry out. We define core traceability paths, consisting of essential traceability links required to support the activities. We illustrate the approach through two examples: product derivation in software product lines, and release planning in software process management. By using a running software product line example, we explain why the core traceability paths identified are needed when navigating from feature to structural models and from family to product level and backward between models used in software product derivation. A feasibility study in release planning carried out in an industrial setting further illustrates the use of core traceability paths during production and measures the increase in performance of the development processes supported by our approach. These examples show that our approach can be successfully used to support both product and process traceability in a pragmatic yet efficient way.},
  comment       = {15},
  doi           = {https://doi.org/10.1016/j.jss.2008.08.026},
  keywords      = {Traceability paths, Software product line, Traceability issues, Software process management},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121208002033},
}

@Article{Galster2015,
  author        = {Matthias Galster and Paris Avgeriou},
  title         = {An industrial case study on variability handling in large enterprise software systems},
  journal       = {Information and Software Technology},
  year          = {2015},
  volume        = {60},
  pages         = {16 - 31},
  issn          = {0950-5849},
  __markedentry = {[mac:]},
  abstract      = {Context
Enterprise software systems (e.g., enterprise resource planning software) are often deployed in different contexts (e.g., different organizations or different business units or branches of one organization). However, even though organizations, business units or branches have the same or similar business goals, they may differ in how they achieve these goals. Thus, many enterprise software systems are subject to variability and adapted depending on the context in which they are used.
Objective
Our goal is to provide a snapshot of variability in large scale enterprise software systems. We aim at understanding the types of variability that occur in large industrial enterprise software systems. Furthermore, we aim at identifying how variability is handled in such systems.
Method
We performed an exploratory case study in two large software organizations, involving two large enterprise software systems. Data were collected through interviews and document analysis. Data were analyzed following a grounded theory approach.
Results
We identified seven types of variability (e.g., functionality, infrastructure) and eight mechanisms to handle variability (e.g., add-ons, code switches).
Conclusions
We provide generic types for classifying variability in enterprise software systems, and reusable mechanisms for handling such variability. Some variability types and handling mechanisms for enterprise software systems found in the real world extend existing concepts and theories. Others confirm findings from previous research literature on variability in software in general and are therefore not specific to enterprise software systems. Our findings also offer a theoretical foundation for describing variability handling in practice. Future work needs to provide more evaluations of the theoretical foundations, and refine variability handling mechanisms into more detailed practices.},
  comment       = {16},
  doi           = {https://doi.org/10.1016/j.infsof.2014.12.003},
  keywords      = {Variability, Enterprise software systems, Case study, Grounded theory},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584914002572},
}

@Article{Abramov2012,
  author        = {Jenny Abramov and Arnon Sturm and Peretz Shoval},
  title         = {Evaluation of the Pattern-based method for Secure Development (PbSD): A controlled experiment},
  journal       = {Information and Software Technology},
  year          = {2012},
  volume        = {54},
  number        = {9},
  pages         = {1029 - 1043},
  issn          = {0950-5849},
  __markedentry = {[mac:]},
  abstract      = {Context
Security in general, and database protection from unauthorized access in particular, are crucial for organizations. Although it has been long accepted that the important system requirements should be considered from the early stages of the development process, non-functional requirements such as security tend to get neglected or dealt with only at later stages of the development process.
Objective
We present an empirical study conducted to evaluate a Pattern-based method for Secure Development â€“ PbSD â€“ that aims to help developers, in particular database designers, to design database schemata that comply with the organizational security policies regarding authorization, from the early stages of development. The method provides a complete framework to guide, enforce and verify the correct implementation of security policies within a system design, and eventually generate a database schema from that design.
Method
The PbSD method was evaluated in comparison with a popular existing method that directly specifies the security requirements in SQL and Oracleâ€™s VPD. The two methods were compared with respect to the quality of the created access control specifications, the time it takes to complete the specification, and the perceived quality of the methods.
Results
We found that the quality of the access control specifications using the PbSD method for secure development were better with respect to privileges granted in the table, column and row granularity levels. Moreover, subjects who used the PbSD method completed the specification task in less time compared to subjects who used SQL. Finally, the subjects perceived the PbSD method clearer and more easy to use.
Conclusion
The pattern-based method for secure development can enhance the quality of security specification of databases, and decrease the software development time and cost. The results of the experiment may also indicate that the use of patterns in general has similar benefits; yet this requires further examinations.},
  comment       = {15},
  doi           = {https://doi.org/10.1016/j.infsof.2012.04.001},
  keywords      = {Database design, Model driven development, Secure software development, Authorization, Security patterns, Controlled experiment},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584912000729},
}

@Article{Bakar2016a,
  author        = {Noor Hasrina Bakar and Zarinah M. Kasirun and Norsaremah Salleh and Hamid A. Jalab},
  title         = {Extracting features from online software reviews to aid requirements reuse},
  journal       = {Applied Soft Computing},
  year          = {2016},
  volume        = {49},
  pages         = {1297 - 1315},
  issn          = {1568-4946},
  __markedentry = {[mac:]},
  abstract      = {Sets of common features are essential assets to be reused in fulfilling specific needs in software product line methodology. In Requirements Reuse (RR), the extraction of software features from Software Requirement Specifications (SRS) is viable only to practitioners who have access to these software artefacts. Due to organisational privacy, SRS are always kept confidential and not easily available to the public. As alternatives, researchers opted to use the publicly available software descriptions such as product brochures and online software descriptions to identify potential software features to initiate the RR process. The aim of this paper is to propose a semi-automated approach, known as Feature Extraction for Reuse of Natural Language requirements (FENL), to extract phrases that can represent software features from software reviews in the absence of SRS as a way to initiate the RR process. FENL is composed of four stages, which depend on keyword occurrences from several combinations of nouns, verbs, and/or adjectives. In the experiment conducted, phrases that could reflect software features, which reside within online software reviews were extracted by utilising the techniques from information retrieval (IR) area. As a way to demonstrate the feature groupings phase, a semi-automated approach to group the extracted features were then conducted with the assistance of a modified word overlap algorithm. As for the evaluation, the proposed extraction approach is evaluated through experiments against the truth data set created manually. The performance results obtained from the feature extraction phase indicates that the proposed approach performed comparably with related works in terms of recall, precision, and F-Measure.},
  comment       = {19},
  doi           = {https://doi.org/10.1016/j.asoc.2016.07.048},
  keywords      = {Requirements reuse, Software engineering, Natural language processing, Unsupervised learning, Latent semantic analysis},
  url           = {http://www.sciencedirect.com/science/article/pii/S1568494616303830},
}

@Article{Bauer2016,
  author        = {Veronika Bauer and Antonio Vetro'},
  title         = {Comparing reuse practices in two large software-producing companies},
  journal       = {Journal of Systems and Software},
  year          = {2016},
  volume        = {117},
  pages         = {545 - 582},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {Context
Reuse can improve productivity and maintainability in software development. Research has proposed a wide range of methods and techniques. Are these successfully adopted in practice?
Objective
We propose a preliminary answer by integrating two in-depth empirical studies on software reuse at two large software-producing companies.
Method
We compare and interpret the study results with a focus on reuse practices, effects, and context.
Results
Both companies perform pragmatic reuse of code produced within the company, not leveraging other available artefacts. Reusable entities are retrieved from a central repository, if present. Otherwise, direct communication with trusted colleagues is crucial for access. Reuse processes remain implicit and reflect the development style. In a homogeneous infrastructure-supported context, participants strongly agreed on higher development pace and less maintenance effort as reuse benefits. In a heterogeneous context with fragmented infrastructure, these benefits did not materialize. Neither case reports statistically significant evidence of negative side effects of reuse nor inhibitors. In both cases, a lack of reuse led to duplicate implementations.
Conclusion
Technological advances have improved the way reuse concepts can be applied in practice. Homogeneity in development process and tool support seem necessary preconditions. Developing and adopting adequate reuse strategies in heterogeneous contexts remains challenging.},
  comment       = {38},
  doi           = {https://doi.org/10.1016/j.jss.2016.03.067},
  keywords      = {Software reuse, Survey research, Technology transfer, Empirical, Software engineering},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121216300176},
}

@Article{Rebelo2013,
  author        = {Henrique RebÃªlo and Ricardo Lima and Gary T. Leavens and MÃ¡rcio CornÃ©lio and Alexandre Mota and CÃ©sar Oliveira},
  title         = {Optimizing generated aspect-oriented assertion checking code for JML using program transformations: An empirical study},
  journal       = {Science of Computer Programming},
  year          = {2013},
  volume        = {78},
  number        = {8},
  pages         = {1137 - 1156},
  issn          = {0167-6423},
  note          = {Special section on software evolution, adaptability, and maintenance \& Special section on the Brazilian Symposium on Programming Languages},
  __markedentry = {[mac:]},
  abstract      = {The AspectJ JML compiler (ajmlc) explores aspect-oriented programming (AOP) mechanisms to implement JML specifications, such as pre- and postconditions, and enforce them during runtime. This compiler was created to improve source-code modularity. Some experiments were conducted to evaluate the performance of the code generated through ajmlc. Results demonstrated that the strategy of adopting AOP to implement JML specifications is very promising. However, there is still a need for optimization of the generated codeâ€™s bytecode size and running time. This paper presents a catalog of transformations which represent the optimizations implemented in the new optimized version of the ajmlc compiler. We employ such transformations to reduce the bytecode size and running time of the code generated through the ajmlc compiler. Aiming at demonstrating the impact of such transformation on the code quality, we conduct an empirical study using four applications in optimized and non-optimized versions generated by ajmlc. We show that our AOP transformations provide a significant improvement, regarding bytecode size and running time.},
  comment       = {20},
  doi           = {https://doi.org/10.1016/j.scico.2012.09.003},
  keywords      = {Aspect-oriented programming, Program transformation, JML},
  url           = {http://www.sciencedirect.com/science/article/pii/S0167642312001682},
}

@Article{Buchmann2013,
  author        = {Thomas Buchmann and Alexander Dotor and Bernhard Westfechtel},
  title         = {MOD2-SCM: A model-driven product line for software configuration management systems},
  journal       = {Information and Software Technology},
  year          = {2013},
  volume        = {55},
  number        = {3},
  pages         = {630 - 650},
  issn          = {0950-5849},
  note          = {Special Issue on Software Reuse and Product Lines},
  __markedentry = {[mac:]},
  abstract      = {Context
Software Configuration Management (SCM) is the discipline of controlling the evolution of large and complex software systems. Over the years many different SCM systems sharing similar concepts have been implemented from scratch. Since these concepts usually are hard-wired into the respective program code, reuse is hardly possible.
Objective
Our objective is to create a model-driven product line for SCM systems. By explicitly describing the different concepts using models, reuse can be performed on the modeling level. Since models are executable, the need for manual programming is eliminated. Furthermore, by providing a library of loosely coupled modules, we intend to support flexible composition of SCM systems.
Method
We developed a method and a tool set for model-driven software product line engineering which we applied to the SCM domain. For domain analysis, we applied the FORM method, resulting in a layered feature model for SCM systems. Furthermore, we developed an executable object-oriented domain model which was annotated with features from the feature model. A specific SCM system is configured by selecting features from the feature model and elements of the domain model realizing these features.
Results
Due to the orthogonality of both feature model and domain model, a very large number of SCM systems may be configured. We tested our approach by creating instances of the product line which mimic wide-spread systems such as CVS, GIT, Mercurial, and Subversion.
Conclusion
The experiences gained from this project demonstrate the feasibility of our approach to model-driven software product line engineering. Furthermore, our work advances the state of the art in the domain of SCM systems since it support the modular composition of SCM systems at the model rather than the code level.},
  comment       = {21},
  doi           = {https://doi.org/10.1016/j.infsof.2012.07.010},
  keywords      = {Model-driven software engineering, Software product line engineering, Software configuration management, Feature models, Executable models, Model transformation, Code generation},
  url           = {http://www.sciencedirect.com/science/article/pii/S095058491200136X},
}

@Article{Rodrigues2012,
  author        = {GenaÃ­na Nunes Rodrigues and Vander Alves and Renato Silveira and Luiz A. Laranjeira},
  title         = {Dependability analysis in the Ambient Assisted Living Domain: An exploratory case study},
  journal       = {Journal of Systems and Software},
  year          = {2012},
  volume        = {85},
  number        = {1},
  pages         = {112 - 131},
  issn          = {0164-1212},
  note          = {Dynamic Analysis and Testing of Embedded Software},
  __markedentry = {[mac:]},
  abstract      = {Ambient Assisted Living (AAL) investigates the development of systems involving the use of different types of sensors, which monitor activities and vital signs of lonely elderly people in order to detect emergency situations or deviations from desirable medical patterns. Instead of requiring the elderly person to manually push a button to request assistance, state-of-the-art AAL solutions automate the process by â€˜perceivingâ€™ lonely elderly people in their home environment through various sensors and performing appropriate actions under the control of the underlying software. Dependability in the AAL domain is a critical requirement, since poor system availability, reliability, safety, or integrity may cause inappropriate emergency assistance to potentially have fatal consequences. Nevertheless, contemporary research has not focused on assessing dependability in this domain. This work attempts to fill this gap presenting an approach which relies on modern quantitative and qualitative dependability analysis techniques based on software architecture. The analysis method presented in this paper consists of conversion patterns from Unified Modeling Language (UML) behavior models of the AAL software architecture into a formal executable specification, based on a probabilistic process algebra description language, which enables a sound quantitative and qualitative analysis. The UML models specify system component interactions and are annotated with component failure probabilities and system usage profile information. The resulting formal specification is executed on PRISM, a model checking tool adequate for the purpose of our analysis in order to identify a set of domain-specific dependability properties expressed declaratively in Probabilistic Computational Tree Logic (PCTL). The benefits of using these techniques are twofold. Firstly, they allow us to seamlessly integrate the analysis during subsequent software lifecycle stages in critical scenarios. Secondly, we identify the components which have the highest impact on software system dependability, and therefore, be able to address software architecture and individual software component problems prior to implementation and the occurrence of critical errors.},
  comment       = {20},
  doi           = {https://doi.org/10.1016/j.jss.2011.07.037},
  keywords      = {Ambient Assisted Living, Dependability analysis, Component-based system},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121211002056},
}

@Article{Haghighatkhah2017,
  author        = {Alireza Haghighatkhah and Ahmad Banijamali and Olli-Pekka Pakanen and Markku Oivo and Pasi Kuvaja},
  title         = {Automotive software engineering: A systematic mapping study},
  journal       = {Journal of Systems and Software},
  year          = {2017},
  volume        = {128},
  pages         = {25 - 55},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {The automotive industry is going through a fundamental change by moving from a mechanical to a software-intensive industry in which most innovation and competition rely on software engineering competence. Over the last few decades, the importance of software engineering in the automotive industry has increased significantly and has attracted much attention from both scholars and practitioners. A large body-of-knowledge on automotive software engineering has accumulated in several scientific publications, yet there is no systematic analysis of that knowledge. This systematic mapping study aims to classify and analyze the literature related to automotive software engineering in order to provide a structured body-of-knowledge, identify well-established topics and potential research gaps. The review includes 679 articles from multiple research sub-area, published between 1990 and 2015. The primary studies were analyzed and classified with respect to five different dimensions. Furthermore, potential research gaps and recommendations for future research are presented. Three areas, namely system/software architecture and design, qualification testing, and reuse were the most frequently addressed topics in the literature. There were fewer comparative and validation studies, and the literature lacks practitioner-oriented guidelines. Overall, research activity on automotive software engineering seems to have high industrial relevance but is relatively lower in its scientific rigor.},
  comment       = {31},
  doi           = {https://doi.org/10.1016/j.jss.2017.03.005},
  keywords      = {Literature survey, Systematic mapping study, Automotive software engineering, Automotive systems, Embedded systems, Software-intensive systems},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121217300560},
}

@Article{Segura2014,
  author        = {Sergio Segura and JosÃ© A. Parejo and Robert M. Hierons and David Benavides and Antonio Ruiz-CortÃ©s},
  title         = {Automated generation of computationally hard feature models using evolutionary algorithms},
  journal       = {Expert Systems with Applications},
  year          = {2014},
  volume        = {41},
  number        = {8},
  pages         = {3975 - 3992},
  issn          = {0957-4174},
  __markedentry = {[mac:]},
  abstract      = {A feature model is a compact representation of the products of a software product line. The automated extraction of information from feature models is a thriving topic involving numerous analysis operations, techniques and tools. Performance evaluations in this domain mainly rely on the use of random feature models. However, these only provide a rough idea of the behaviour of the tools with average problems and are not sufficient to reveal their real strengths and weaknesses. In this article, we propose to model the problem of finding computationally hard feature models as an optimization problem and we solve it using a novel evolutionary algorithm for optimized feature models (ETHOM). Given a tool and an analysis operation, ETHOM generates input models of a predefined size maximizing aspects such as the execution time or the memory consumption of the tool when performing the operation over the model. This allows users and developers to know the performance of tools in pessimistic cases providing a better idea of their real power and revealing performance bugs. Experiments using ETHOM on a number of analyses and tools have successfully identified models producing much longer executions times and higher memory consumption than those obtained with random models of identical or even larger size.},
  comment       = {17},
  doi           = {https://doi.org/10.1016/j.eswa.2013.12.028},
  keywords      = {Search-based testing, Software product lines, Evolutionary algorithms, Feature models, Performance testing, Automated analysis},
  url           = {http://www.sciencedirect.com/science/article/pii/S0957417413010038},
}

@Article{Srikanth2016,
  author        = {Hema Srikanth and Mikaela Cashman and Myra B. Cohen},
  title         = {Test case prioritization of build acceptance tests for an enterprise cloud application: An industrial case study},
  journal       = {Journal of Systems and Software},
  year          = {2016},
  volume        = {119},
  pages         = {122 - 135},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {The use of cloud computing brings many new opportunities for companies to deliver software in a highly-customizable and dynamic way. One such paradigm, Software as a Service (SaaS), allows users to subscribe and unsubscribe to services as needed. While beneficial to both subscribers and SaaS service providers, failures escaping to the field in these systems can potentially impact an entire customer base. Build Acceptance Testing (BAT) is a black box technique performed to validate the quality of a SaaS system every time a build is generated. In BAT, the same set of test cases is executed simultaneously across many different servers, making this a time consuming test process. Since BAT contains the most critical use cases, it may not be obvious which tests to perform first, given that the time to complete all test cases across different servers in any given day may be insufficient. While all tests must be eventually run, it is critical to run those tests first which are likely to find failures. In this work, we ask if it is possible to prioritize BAT tests for improved time to fault detection and present several different approaches, each based on the services executed when running each BAT. In an empirical study on a production enterprise system, we first analyze the historical data from several months in the field, and then use that data to derive the prioritization order for the current development BATs. We then examine if the orders change significantly when we consider fault severity using a cost-based prioritization metric. We find that the prioritization order in which we run the tests does matter, and that the use of historical information is a good heuristic for this order. Prioritized tests have an increase in the rate of fault detection, with the average percent of faults detected (APFD) increasing from less than 0.30 to as high as 0.77 on a scale of zero to one. Although severity slightly changes which order performs best, we see that there are clusters of orderings, ones which improve time to early fault detection ones which donâ€™t.},
  comment       = {14},
  doi           = {https://doi.org/10.1016/j.jss.2016.06.017},
  keywords      = {Regression testing, Prioritization, Software as a service, Cloud computing},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121216300851},
}

@Article{Khan2016,
  author        = {Saif Ur Rehman Khan and Sai Peck Lee and Raja Wasim Ahmad and Adnan Akhunzada and Victor Chang},
  title         = {A survey on Test Suite Reduction frameworks and tools},
  journal       = {International Journal of Information Management},
  year          = {2016},
  volume        = {36},
  number        = {6, Part A},
  pages         = {963 - 975},
  issn          = {0268-4012},
  __markedentry = {[mac:]},
  abstract      = {Software testing is a widely accepted practice that ensures the quality of a System under Test (SUT). However, the gradual increase of the test suite size demands high portion of testing budget and time. Test Suite Reduction (TSR) is considered a potential approach to deal with the test suite size problem. Moreover, a complete automation support is highly recommended for software testing to adequately meet the challenges of a resource constrained testing environment. Several TSR frameworks and tools have been proposed to efficiently address the test-suite size problem. The main objective of the paper is to comprehensively review the state-of-the-art TSR frameworks to highlights their strengths and weaknesses. Furthermore, the paper focuses on devising a detailed thematic taxonomy to classify existing literature that helps in understanding the underlying issues and proof of concept. Moreover, the paper investigates critical aspects and related features of TSR frameworks and tools based on a set of defined parameters. We also rigorously elaborated various testing domains and approaches followed by the extant TSR frameworks. The results reveal that majority of TSR frameworks focused on randomized unit testing, and a considerable number of frameworks lacks in supporting multi-objective optimization problems. Moreover, there is no generalized framework, effective for testing applications developed in any programming domain. Conversely, Integer Linear Programming (ILP) based TSR frameworks provide an optimal solution for multi-objective optimization problems and improve execution time by running multiple ILP in parallel. The study concludes with new insights and provides an unbiased view of the state-of-the-art TSR frameworks. Finally, we present potential research issues for further investigation to anticipate efficient TSR frameworks.},
  comment       = {13},
  doi           = {https://doi.org/10.1016/j.ijinfomgt.2016.05.025},
  keywords      = {Regression testing, Test suite optimization, Test Suite Reduction, Frameworks, Fault localization},
  url           = {http://www.sciencedirect.com/science/article/pii/S0268401216303437},
}

@Article{Gonzalez-Huerta2015,
  author        = {Javier Gonzalez-Huerta and Emilio Insfran and Silvia AbrahÃ£o and Giuseppe Scanniello},
  title         = {Validating a model-driven software architecture evaluation and improvement method: A family of experiments},
  journal       = {Information and Software Technology},
  year          = {2015},
  volume        = {57},
  pages         = {405 - 429},
  issn          = {0950-5849},
  __markedentry = {[mac:]},
  abstract      = {Context
Software architectures should be evaluated during the early stages of software development in order to verify whether the non-functional requirements (NFRs) of the product can be fulfilled. This activity is even more crucial in software product line (SPL) development, since it is also necessary to identify whether the NFRs of a particular product can be achieved by exercising the variation mechanisms provided by the product line architecture or whether additional transformations are required. These issues have motivated us to propose QuaDAI, a method for the derivation, evaluation and improvement of software architectures in model-driven SPL development.
Objective
We present in this paper the results of a family of four experiments carried out to empirically validate the evaluation and improvement strategy of QuaDAI.
Method
The family of experiments was carried out by 92 participants: Computer Science Masterâ€™s and undergraduate students from Spain and Italy. The goal was to compare the effectiveness, efficiency, perceived ease of use, perceived usefulness and intention to use with regard to participants using the evaluation and improvement strategy of QuaDAI as opposed to the Architecture Tradeoff Analysis Method (ATAM).
Results
The main result was that the participants produced their best results when applying QuaDAI, signifying that the participants obtained architectures with better values for the NFRs faster, and that they found the method easier to use, more useful and more likely to be used. The results of the meta-analysis carried out to aggregate the results obtained in the individual experiments also confirmed these results.
Conclusions
The results support the hypothesis that QuaDAI would achieve better results than ATAM in the experiments and that QuaDAI can be considered as a promising approach with which to perform architectural evaluations that occur after the product architecture derivation in model-driven SPL development processes when carried out by novice software evaluators.},
  comment       = {25},
  doi           = {https://doi.org/10.1016/j.infsof.2014.05.018},
  keywords      = {Software architectures, Software architecture evaluation methods, Quality attributes, ATAM, Family of experiments, Meta-analysis},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584914001359},
}

@Article{Parejo2016,
  author        = {JosÃ© A. Parejo and Ana B. SÃ¡nchez and Sergio Segura and Antonio Ruiz-CortÃ©s and Roberto E. Lopez-Herrejon and Alexander Egyed},
  title         = {Multi-objective test case prioritization in highly configurable systems: A case study},
  journal       = {Journal of Systems and Software},
  year          = {2016},
  volume        = {122},
  pages         = {287 - 310},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {Test case prioritization schedules test cases for execution in an order that attempts to accelerate the detection of faults. The order of test cases is determined by prioritization objectives such as covering code or critical components as rapidly as possible. The importance of this technique has been recognized in the context of Highly-Configurable Systems (HCSs), where the potentially huge number of configurations makes testing extremely challenging. However, current approaches for test case prioritization in HCSs suffer from two main limitations. First, the prioritization is usually driven by a single objective which neglects the potential benefits of combining multiple criteria to guide the detection of faults. Second, instead of using industry-strength case studies, evaluations are conducted using synthetic data, which provides no information about the effectiveness of different prioritization objectives. In this paper, we address both limitations by studying 63 combinations of up to three prioritization objectives in accelerating the detection of faults in the Drupal framework. Results show that nonâ€“functional properties such as the number of changes in the features are more effective than functional metrics extracted from the configuration model. Results also suggest that multi-objective prioritization typically results in faster fault detection than mono-objective prioritization.},
  comment       = {24},
  doi           = {https://doi.org/10.1016/j.jss.2016.09.045},
  keywords      = {Variability, Test case prioritization, Automated software testing, Highly-configurable systems},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121216301935},
}

@Article{Ruiz2016,
  author        = {Jael Zela Ruiz and CecÃ­lia M. Rubira},
  title         = {Quality of Service Conflict During Web Service Monitoring: A Case Study},
  journal       = {Electronic Notes in Theoretical Computer Science},
  year          = {2016},
  volume        = {321},
  pages         = {113 - 127},
  issn          = {1571-0661},
  note          = {CLEI 2015, the XLI Latin American Computing Conference},
  __markedentry = {[mac:]},
  abstract      = {Web services have become one of the most used technologies in service-oriented systems. Its popularity is due to its property to adapt to any context. As a consequence of the increasing number of Web services on the Internet and its important role in many applications today, Web service quality has become a crucial requirement and demanded by service consumers. Terms of quality levels are written between service providers and service consumers to ensure a degree of quality. The use of monitoring tools to control service quality levels is very important. Quality attributes suffer variations in their values during runtime, this is produced by many factors such as a memory leak, deadlock, race data, inconsistent data, etc. However, sometimes monitoring tools can impact negatively affecting the quality of service when they are not properly used and configured, producing possible conflicts between quality attributes. This paper aims to show the impact of monitoring tools over service quality, two of the most important quality attributes â€“ performance and accuracy â€“ were chosen to be monitored. A case study is conducted to present and evaluate the relationship between performance and accuracy over a Web service. As a result, conflict is found between performance and accuracy, where performance was the most affected, because it presented a degradation in its quality level during monitoring.},
  comment       = {15},
  doi           = {https://doi.org/10.1016/j.entcs.2016.02.007},
  keywords      = {Web Services, SOA, Quality of Service, Quality Attributes, Conflict, Performance, Accuracy, Monitoring Tools},
  url           = {http://www.sciencedirect.com/science/article/pii/S1571066116300081},
}

@Article{Karhu2009,
  author        = {Katja Karhu and Ossi Taipale and Kari Smolander},
  title         = {Investigating the relationship between schedules and knowledge transfer in software testing},
  journal       = {Information and Software Technology},
  year          = {2009},
  volume        = {51},
  number        = {3},
  pages         = {663 - 677},
  issn          = {0950-5849},
  __markedentry = {[mac:]},
  abstract      = {This empirical study investigates the relationship between schedules and knowledge transfer in software testing. In our exploratory survey, statistical analysis indicated that increased knowledge transfer between testing and earlier phases of software development was associated with testing schedule over-runs. A qualitative case study was conducted to interpret this result. We found that this relationship can be explained with the size and complexity of software, knowledge management issues, and customer involvement. We also found that the primary strategies for avoiding testing schedule over-runs were reducing the scope of testing, leaving out features from the software, and allocating more resources to testing.},
  comment       = {15},
  doi           = {https://doi.org/10.1016/j.infsof.2008.09.001},
  keywords      = {Software testing, Schedule over-runs, Knowledge transfer, Survey, Case study},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584908001249},
}

@Article{Amalio2011a,
  author        = {Nuno AmÃ¡lio and Christian Glodt and Frederico Pinto and Pierre Kelsen},
  title         = {Platform-Variant Applications from Platform-Independent Models via Templates},
  journal       = {Electronic Notes in Theoretical Computer Science},
  year          = {2011},
  volume        = {279},
  number        = {3},
  pages         = {3 - 25},
  issn          = {1571-0661},
  note          = {Proceedings of the Third Workshop on Generative Technologies (WGT) 2011},
  __markedentry = {[mac:]},
  abstract      = {By raising the level of abstraction from code to models, model-driven development (MDD) emphasises design rather than implementation and platform-specificity. This paper presents an experiment with a MDD approach, which takes platform-independent models and generates code for various platforms from them. The platform code is generated from templates. Our approach is based on EP, a formal executable modelling language, supplemented with OCL, and FTL, a formal language of templates. The paperÊ¼s experiment generates code for the mobile platforms Android and iPhone from the same abstract functional model of a case study. The experiment shows the feasibility of MDD to tackle present day problems, highlighting many benefits of the MDD approach and opportunities for improvement.},
  comment       = {23},
  doi           = {https://doi.org/10.1016/j.entcs.2011.11.035},
  keywords      = {Software Product families, model-driven development, executable models, templates},
  url           = {http://www.sciencedirect.com/science/article/pii/S1571066111001848},
}

@Article{Rincon2015,
  author        = {L. RincÃ³n and G. Giraldo and R. Mazo and C. Salinesi and D. Diaz},
  title         = {Method to Identify Corrections of Defects on Product Line Models},
  journal       = {Electronic Notes in Theoretical Computer Science},
  year          = {2015},
  volume        = {314},
  pages         = {61 - 81},
  issn          = {1571-0661},
  note          = {CLEI 2014, the XL Latin American Conference in Informatic},
  __markedentry = {[mac:]},
  abstract      = {Software product line engineering is a promising paradigm for developing software intensive systems. Among their proven benefits are reduced time to market, better asset reuse and improved software quality. To achieve this, the collection of products of the product line are specified by means of product line models. Feature Models (FMs) are a common notation to represent product lines that express the set of feature combinations that software products can have. Experience shows that these models can have defects. Defects in FMs be inherited to the products configured from these models. Consequently, defects must be early identified and corrected. Several works reported in scientific literature, deal with identification of defects in FMs. However, only few of these proposals are able to explain how to fix defects, and only some corrections are suggested. This paper proposes a new method to detect all possible corrections from a defective product line model. The originality of the contribution is that corrections can be found when the method systematically eliminates dependencies from the FMs. The proposed method was applied on 78 distinct FMs with sizes up to 120 dependencies. Evaluation indicates that the method proposed in this paper scale up, is accurate, and sometimes useful in real scenarios.},
  comment       = {21},
  doi           = {https://doi.org/10.1016/j.entcs.2015.05.005},
  keywords      = {Software product lines, Features Models, Corrections, Defects, Software Engineering},
  url           = {http://www.sciencedirect.com/science/article/pii/S1571066115000286},
}

@Article{Galindo2015,
  author        = {JosÃ© A. Galindo and Deepak Dhungana and Rick Rabiser and David Benavides and Goetz Botterweck and Paul GrÃ¼nbacher},
  title         = {Supporting distributed product configuration by integrating heterogeneous variability modeling approaches},
  journal       = {Information and Software Technology},
  year          = {2015},
  volume        = {62},
  pages         = {78 - 100},
  issn          = {0950-5849},
  __markedentry = {[mac:]},
  abstract      = {Context
In industrial settings products are developed by more than one organization. Software vendors and suppliers commonly typically maintain their own product lines, which contribute to a larger (multi) product line or software ecosystem. It is unrealistic to assume that the participating organizations will agree on using a specific variability modeling techniqueâ€”they will rather use different approaches and tools to manage the variability of their systems.
Objective
We aim to support product configuration in software ecosystems based on several variability models with different semantics that have been created using different notations.
Method
We present an integrative approach that provides a unified perspective to users configuring products in multi product line environments, regardless of the different modeling methods and tools used internally. We also present a technical infrastructure and a prototype implementation based on web services.
Results
We show the feasibility of the approach and its implementation by using it with the three most widespread types of variability modeling approaches in the product line community, i.e., feature-based, OVM-style, and decision-oriented modeling. To demonstrate the feasibility and flexibility of our approach, we present an example derived from industrial experience in enterprise resource planning. We further applied the approach to support the configuration of privacy settings in the Android ecosystem based on multiple variability models. We also evaluated the performance of different model enactment strategies used in our approach.
Conclusions
Tools and techniques allowing stakeholders to handle variability in a uniform manner can considerably foster the initiation and growth of software ecosystems from the perspective of software reuse and configuration.},
  comment       = {23},
  doi           = {https://doi.org/10.1016/j.infsof.2015.02.002},
  keywords      = {Software product lines, Product configuration, Automated analysis},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584915000312},
}

@Article{Classen2011a,
  author        = {Andreas Classen and Quentin Boucher and Patrick Heymans},
  title         = {A text-based approach to feature modelling: Syntax and semantics of TVL},
  journal       = {Science of Computer Programming},
  year          = {2011},
  volume        = {76},
  number        = {12},
  pages         = {1130 - 1143},
  issn          = {0167-6423},
  note          = {Special Issue on Software Evolution, Adaptability and Variability},
  __markedentry = {[mac:]},
  abstract      = {In the scientific community, feature models are the de-facto standard for representing variability in software product line engineering. This is different from industrial settings where they appear to be used much less frequently. We and other authors found that in a number of cases, they lack concision, naturalness and expressiveness. This is confirmed by industrial experience. When modelling variability, an efficient tool for making models intuitive and concise are feature attributes. Yet, the semantics of feature models with attributes is not well understood and most existing notations do not support them at all. Furthermore, the graphical nature of feature modelsâ€™ syntax also appears to be a barrier to industrial adoption, both psychological and rational. Existing tool support for graphical feature models is lacking or inadequate, and inferior in many regards to tool support for text-based formats. To overcome these shortcomings, we designed TVL, a text-based feature modelling language. In terms of expressiveness, TVL subsumes most existing dialects. The main goal of designing TVL was to provide engineers with a human-readable language with a rich syntax to make modelling easy and models natural, but also with a formal semantics to avoid ambiguity and allow powerful automation.},
  comment       = {14},
  doi           = {https://doi.org/10.1016/j.scico.2010.10.005},
  keywords      = {Feature models, Code, Modelling, Language, Syntax, Semantics, Software product lines},
  url           = {http://www.sciencedirect.com/science/article/pii/S0167642310001899},
}

@Article{Krupitzer2015,
  author        = {Christian Krupitzer and Felix Maximilian Roth and Sebastian VanSyckel and Gregor Schiele and Christian Becker},
  title         = {A survey on engineering approaches for self-adaptive systems},
  journal       = {Pervasive and Mobile Computing},
  year          = {2015},
  volume        = {17},
  pages         = {184 - 206},
  issn          = {1574-1192},
  note          = {10 years of Pervasive Computing' In Honor of Chatschik Bisdikian},
  __markedentry = {[mac:]},
  abstract      = {The complexity of information systems is increasing in recent years, leading to increased effort for maintenance and configuration. Self-adaptive systems (SASs) address this issue. Due to new computing trends, such as pervasive computing, miniaturization of IT leads to mobile devices with the emerging need for context adaptation. Therefore, it is beneficial that devices are able to adapt context. Hence, we propose to extend the definition of SASs and include context adaptation. This paper presents a taxonomy of self-adaptation and a survey on engineering SASs. Based on the taxonomy and the survey, we motivate a new perspective on SAS including context adaptation.},
  comment       = {23},
  doi           = {https://doi.org/10.1016/j.pmcj.2014.09.009},
  keywords      = {Taxonomy, Self-adaptation, Survey, Self-adaptive systems, Context adaptation},
  url           = {http://www.sciencedirect.com/science/article/pii/S157411921400162X},
}

@Article{Axelsson2014,
  author        = {Jakob Axelsson and Efi Papatheocharous and Jesper Andersson},
  title         = {Characteristics of software ecosystems for Federated Embedded Systems: A case study},
  journal       = {Information and Software Technology},
  year          = {2014},
  volume        = {56},
  number        = {11},
  pages         = {1457 - 1475},
  issn          = {0950-5849},
  note          = {Special issue on Software Ecosystems},
  __markedentry = {[mac:]},
  abstract      = {Context
Traditionally, Embedded Systems (ES) are tightly linked to physical products, and closed both for communication to the surrounding world and to additions or modifications by third parties. New technical solutions are however emerging that allow addition of plug-in software, as well as external communication for both software installation and data exchange. These mechanisms in combination will allow for the construction of Federated Embedded Systems (FES). Expected benefits include the possibility of third-party actors developing add-on functionality; a shorter time to market for new functions; and the ability to upgrade existing products in the field. This will however require not only new technical solutions, but also a transformation of the software ecosystems for ES.
Objective
This paper aims at providing an initial characterization of the mechanisms that need to be present to make a FES ecosystem successful. This includes identification of the actors, the possible business models, the effects on product development processes, methods and tools, as well as on the product architecture.
Method
The research was carried out as an explorative case study based on interviews with 15 senior staff members at 9 companies related to ES that represent different roles in a future ecosystem for FES. The interview data was analyzed and the findings were mapped according to the Business Model Canvas (BMC).
Results
The findings from the study describe the main characteristics of a FES ecosystem, and identify the challenges for future research and practice.
Conclusions
The case study indicates that new actors exist in the FES ecosystem compared to a traditional supply chain, and that their roles and relations are redefined. The business models include new revenue streams and services, but also create the need for trade-offs between, e.g., openness and dependability in the architecture, as well as new ways of working.},
  comment       = {19},
  doi           = {https://doi.org/10.1016/j.infsof.2014.03.011},
  keywords      = {Software ecosystems, Embedded Systems, Systems-of-Systems, Architecture, Case study},
  url           = {http://www.sciencedirect.com/science/article/pii/S095058491400072X},
}

@Article{Guo2012,
  author        = {Jianmei Guo and Yinglin Wang and Pablo Trinidad and David Benavides},
  title         = {Consistency maintenance for evolving feature models},
  journal       = {Expert Systems with Applications},
  year          = {2012},
  volume        = {39},
  number        = {5},
  pages         = {4987 - 4998},
  issn          = {0957-4174},
  __markedentry = {[mac:]},
  abstract      = {Software product line (SPL) techniques handle the construction of customized systems. One of the most common representations of the decisions a customer can make in SPLs is feature models (FMs). An FM represents the relationships among common and variable features in an SPL. Features are a representation of the characteristics in a system that are relevant to customers. FMs are subject to change since the set of features and their relationships can change along an SPL lifecycle. Due to this evolution, the consistency of FMs may be compromised. There exist some approaches to detect and explain inconsistencies in FMs, however this process can take a long time for large FMs. In this paper we present a complementary approach to dealing with inconsistencies in FM evolution scenarios that improves the performance for existing approaches reducing the impact of change to the smallest part of an FM that changes. To achieve our goal, we formalize FMs from an ontological perspective and define constraints that must be satisfied in FMs to be consistent. We define a set of primitive operations that modify FMs and which are responsible for the FM evolution, analyzing their impact on the FM consistency. We propose a set of predefined strategies to keep the consistency for error-prone operations. As a proof-of-concept we present the results of our experiments, where we check for the effectiveness and efficiency of our approach in FMs with thousands of features. Although our approach is limited by the kinds of consistency constraints and the primitive operations we define, the experiments present a significant improvement in performance results in those cases where they are applicable.},
  comment       = {12},
  doi           = {https://doi.org/10.1016/j.eswa.2011.10.014},
  keywords      = {Software product lines, Feature models, Evolution, Consistency maintenance, Ontology, Semantics},
  url           = {http://www.sciencedirect.com/science/article/pii/S0957417411014990},
}

@Article{Cetina2017,
  author        = {Carlos Cetina and Jaime Font and Lorena Arcega and Francisca PÃ©rez},
  title         = {Improving feature location in long-living model-based product families designed with sustainability goals},
  journal       = {Journal of Systems and Software},
  year          = {2017},
  volume        = {134},
  pages         = {261 - 278},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {The benefits of Software Product Lines (SPL) are very appealing: software development becomes better, faster, and cheaper. Unfortunately, these benefits come at the expense of a migration from a family of products to a SPL. Feature Location could be useful in achieving the transition to SPLs. This work presents our FeLLaCaM approach for Feature Location. Our approach calculates similarity to a description of the feature to locate, occurrences where the candidate features remain unchanged, and changes performed to the candidate features throughout the retrospective of the product family. We evaluated our approach in two long-living industrial domains: a model-based family of firmwares for induction hobs that was developed over more than 15 years, and a model-based family of PLC software to control trains that was developed over more than 25 years. In our evaluation, we compare our FeLLaCaM approach with two other approaches for Feature Location: (1) FLL (Feature Location through Latent Semantic Analysis) and (2) FLC (Feature Location through Comparisons). We measure the performance of FeLLaCaM, FLL, and FLC in terms of recall, precision, Matthews Correlation Coefficient, and Area Under the Receiver Operating Characteristics curve. The results show that FeLLaCaM outperforms FLL and FLC.},
  comment       = {17},
  doi           = {https://doi.org/10.1016/j.jss.2017.09.022},
  keywords      = {Feature location, Long-Living software systems, Architecture sustainability, Software product lines, Modelâ€“Driven engineering},
  url           = {http://www.sciencedirect.com/science/article/pii/S016412121730211X},
}

@Article{Lucas2017a,
  author        = {Edson M. Lucas and Toacy C. Oliveira and Kleinner Farias and Paulo S.C. Alencar},
  title         = {CollabRDL: A language to coordinate collaborative reuse},
  journal       = {Journal of Systems and Software},
  year          = {2017},
  volume        = {131},
  pages         = {505 - 527},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {Coordinating software reuse activities is a complex problem when considering collaborative software development. This is mainly motivated due to the difficulty in specifying how the artifacts and the knowledge produced in previous projects can be applied in future ones. In addition, modern software systems are developed in group working in separate geographical locations. Therefore, techniques to enrich collaboration on software development are important to improve quality and reduce costs. Unfortunately, the current literature fails to address this problem by overlooking existing reuse techniques. There are many reuse approaches proposed in academia and industry, including Framework Instantiation, Software Product Line, Transformation Chains, and Staged Configuration. But, the current approaches do not support the representation and implementation of collaborative instantiations that involve individual and group roles, the simultaneous performance of multiple activities, restrictions related to concurrency and synchronization of activities, and allocation of activities to reuse actors as a coordination mechanism. These limitations are the main reasons why the Reuse Description Language (RDL) is unable to promote collaborative reuse, i.e., those related to reuse activities in collaborative software development. To overcome these shortcomings, this work, therefore, proposes CollabRDL, a language to coordinate collaborative reuse by providing essential concepts and constructs for allowing group-based reuse activities. For this purpose, we extend RDL by introducing three new commands, including role, parallel, and doparallel. To evaluate CollabRDL we have conducted a case study in which developer groups performed reuse activities collaboratively to instantiate a mainstream Java framework. The results indicated that CollabRDL was able to represent critical workflow patterns, including parallel split pattern, synchronization pattern, multiple-choice pattern, role-based distribution pattern, and multiple instances with decision at runtime. Overall, we believe that the provision of a new language that supports group-based activities in framework instantiation can help enable software organizations to document their coordinated efforts and achieve the benefits of software mass customization with significantly less development time and effort.},
  comment       = {23},
  doi           = {https://doi.org/10.1016/j.jss.2017.01.031},
  keywords      = {Software reuse, Collaboration, Framework, Language, Reuse process},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121217300225},
}

@Article{Stettina2015,
  author        = {Christoph Johann Stettina and Jeannette HÃ¶rz},
  title         = {Agile portfolio management: An empirical perspective on the practice in use},
  journal       = {International Journal of Project Management},
  year          = {2015},
  volume        = {33},
  number        = {1},
  pages         = {140 - 152},
  issn          = {0263-7863},
  __markedentry = {[mac:]},
  abstract      = {Agile project management methods revolutionized the way how software projects are executed and organized. The question, however, on how to enable agility outside of individual projects and help larger organizations to compete with small entrepreneurial companies requires further attention. As a possible perspective, project portfolio management provides a global view on resources and their distribution across individual projects according to strategic choices. Based on 30 interviews conducted in 14 large European organizations this study contributes to the understanding of agile project management methods applied in IT project portfolios. First, we empirically identify the domains of practice. Then, guided by literature and our data we discuss the characteristics and implications of the agile portfolio management practice in our case organizations.},
  comment       = {13},
  doi           = {https://doi.org/10.1016/j.ijproman.2014.03.008},
  keywords      = {Agile project management, Project portfolio management, Software project management, Organizational routines, Empirical study},
  url           = {http://www.sciencedirect.com/science/article/pii/S0263786314000489},
}

@Article{Heuer2013,
  author        = {AndrÃ© Heuer and Vanessa Stricker and Christof J. Budnik and Sascha Konrad and Kim Lauenroth and Klaus Pohl},
  title         = {Defining variability in activity diagrams and Petri nets},
  journal       = {Science of Computer Programming},
  year          = {2013},
  volume        = {78},
  number        = {12},
  pages         = {2414 - 2432},
  issn          = {0167-6423},
  note          = {Special Section on International Software Product Line Conference 2010 and Fundamentals of Software Engineering (selected papers of FSEN 2011)},
  __markedentry = {[mac:]},
  abstract      = {Control flow models, such as UML activity diagrams or Petri nets, are widely accepted modeling languages used to support quality assurance activities in single system engineering as well as software product line (SPL) engineering. Quality assurance in product line engineering is a challenging task since a defect in a domain artifact may affect several products of the product line. Thus, proper quality assurance approaches need to pay special attention to the product line variability. Automation is essential to support quality assurance approaches. A prerequisite for automation is a profound formalization of the underlying control flow models and, in the context of SPLs, of the variability therein. In this paper, we propose a formal syntax and semantics for defining variability in Petri nets. We use these extended Petri nets as a foundation to formally define variability in UML activity diagrams; UML activity diagrams serve as a basis for several testing techniques in product line engineering. We illustrate the contribution of such a formalization to assurance activities in product line engineering by describing its usage in three application examples.},
  comment       = {19},
  doi           = {https://doi.org/10.1016/j.scico.2012.06.003},
  keywords      = {Variability, Variability in activity diagrams, Documenting variability, Quality assurance for product line engineering},
  url           = {http://www.sciencedirect.com/science/article/pii/S0167642312001141},
}

@Article{White2010,
  author        = {J. White and D. Benavides and D.C. Schmidt and P. Trinidad and B. Dougherty and A. Ruiz-Cortes},
  title         = {Automated diagnosis of feature model configurations},
  journal       = {Journal of Systems and Software},
  year          = {2010},
  volume        = {83},
  number        = {7},
  pages         = {1094 - 1107},
  issn          = {0164-1212},
  note          = {SPLC 2008},
  __markedentry = {[mac:]},
  abstract      = {Software product-lines (SPLs) are software platforms that can be readily reconfigured for different project requirements. A key part of an SPL is a model that captures the rules for reconfiguring the software. SPLs commonly use feature models to capture SPL configuration rules. Each SPL configuration is represented as a selection of features from the feature model. Invalid SPL configurations can be created due to feature conflicts introduced via staged or parallel configuration or changes to the constraints in a feature model. When invalid configurations are created, a method is needed to automate the diagnosis of the errors and repair the feature selections. This paper provides two contributions to research on automated configuration of SPLs. First, it shows how configurations and feature models can be transformed into constraint satisfaction problems to automatically diagnose errors and repair invalid feature selections. Second, it presents empirical results from diagnosing configuration errors in feature models ranging in size from 100 to 5,000 features. The results of our experiments show that our CSP-based diagnostic technique can scale up to models with thousands of features.},
  comment       = {14},
  doi           = {https://doi.org/10.1016/j.jss.2010.02.017},
  keywords      = {Software product-lines, Configuration, Diagnosis, Constraint satisfaction, Optimization},
  url           = {http://www.sciencedirect.com/science/article/pii/S016412121000049X},
}

@Article{Guenther2012,
  author        = {Sebastian GÃ¼nther and Sagar Sunkle},
  title         = {rbFeatures: Feature-oriented programming with Ruby},
  journal       = {Science of Computer Programming},
  year          = {2012},
  volume        = {77},
  number        = {3},
  pages         = {152 - 173},
  issn          = {0167-6423},
  note          = {Feature-Oriented Software Development (FOSD 2009)},
  __markedentry = {[mac:]},
  abstract      = {Features are pieces of core functionality of a program that is relevant to particular stakeholders. Features pose dependencies and constraints among each other. These dependencies and constraints describe the possible number of variants of the program: A valid feature configuration generates a specific variant with unique behavior. Feature-Oriented Programming is used to implement features as program units. This paper introduces rbFeatures, a feature-oriented programming language implemented on top of the dynamic programming language Ruby. With rbFeatures, programmers use software product lines, variants, and features as first-class entities. This allows several runtime reflection and modification capabilities, including the extension of the product line with new features and the provision of multiple variants. The paper gives a broad overview to the implementation and application of rbFeatures. We explain how features as first-class entities are designed and implemented, and discuss how the semantics of features are carefully added to Ruby programs. We show two case studies: The expression product line, a common example in feature-oriented programming, and a web application.},
  comment       = {22},
  doi           = {https://doi.org/10.1016/j.scico.2010.12.007},
  keywords      = {Feature-oriented programming, Domain-specific languages, Dynamic programming languages},
  url           = {http://www.sciencedirect.com/science/article/pii/S0167642311000025},
}

@Article{Lucredio2008,
  author        = {Daniel LucrÃ©dio and Kellyton dos Santos Brito and Alexandre Alvaro and Vinicius Cardoso Garcia and Eduardo Santana de Almeida and Renata Pontin de Mattos Fortes and Silvio Lemos Meira},
  title         = {Software reuse: The Brazilian industry scenario},
  journal       = {Journal of Systems and Software},
  year          = {2008},
  volume        = {81},
  number        = {6},
  pages         = {996 - 1013},
  issn          = {0164-1212},
  note          = {Agile Product Line Engineering},
  __markedentry = {[mac:]},
  abstract      = {This paper aims at identifying some of the key factors in adopting an organization-wide software reuse program. The factors are derived from practical experience reported by industry professionals, through a survey involving 57 Brazilian small, medium and large software organizations. Some of them produce software with commonality between applications, and have mature processes, while others successfully achieved reuse through isolated, ad hoc efforts. The paper compiles the answers from the survey participants, showing which factors were more associated with reuse success. Based on this relationship, a guide is presented, pointing out which factors should be more strongly considered by small, medium and large organizations attempting to establish a reuse program.},
  comment       = {18},
  doi           = {https://doi.org/10.1016/j.jss.2007.08.036},
  keywords      = {Survey, Software reuse, Reuse success factors, Survey, Best practices},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121207002221},
}

@Article{O.Melo2013,
  author        = {Claudia de O. Melo and Daniela S. Cruzes and Fabio Kon and Reidar Conradi},
  title         = {Interpretative case studies on agile team productivity and management},
  journal       = {Information and Software Technology},
  year          = {2013},
  volume        = {55},
  number        = {2},
  pages         = {412 - 427},
  issn          = {0950-5849},
  note          = {Special Section: Component-Based Software Engineering (CBSE), 2011},
  __markedentry = {[mac:]},
  abstract      = {Context
The management of software development productivity is a key issue in software organizations, where the major drivers are lower cost and shorter time-to-market. Agile methods, including Extreme Programming and Scrum, have evolved as â€œlightâ€ approaches that simplify the software development process, potentially leading to increased team productivity. However, little empirical research has examined which factors do have an impact on productivity and in what way, when using agile methods.
Objective
Our objective is to provide a better understanding of the factors and mediators that impact agile team productivity.
Method
We have conducted a multiple-case study for 6months in three large Brazilian companies that have been using agile methods for over 2years. We have focused on the main productivity factors perceived by team members through interviews, documentation from retrospectives, and non-participant observation.
Results
We developed a novel conceptual framework, using thematic analysis to understand the possible mechanisms behind such productivity factors. Agile team management was found to be the most influential factor in achieving agile team productivity. At the intra-team level, the main productivity factors were team design (structure and work allocation) and member turnover. At the inter-team level, the main productivity factors were how well teams could be effectively coordinated by proper interfaces and other dependencies and avoiding delays in providing promised software to dependent teams.
Conclusion
Teams should be aware of the influence and magnitude of turnover, which has been shown negative for agile team productivity. Team design choices remain an important factor impacting team productivity, even more pronounced on agile teams that rely on teamwork and people factors. The intra-team coordination processes must be adjusted to enable productive work by considering priorities and pace between teams. Finally, the revised conceptual framework for agile team productivity supports further tests through confirmatory studies.},
  comment       = {16},
  doi           = {https://doi.org/10.1016/j.infsof.2012.09.004},
  keywords      = {Agile software development, Team productivity factors, Team management, Thematic analysis, Industrial case studies},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584912001875},
}

@Article{Koziolek2010,
  author        = {Heiko Koziolek},
  title         = {Performance evaluation of component-based software systems: A survey},
  journal       = {Performance Evaluation},
  year          = {2010},
  volume        = {67},
  number        = {8},
  pages         = {634 - 658},
  issn          = {0166-5316},
  note          = {Special Issue on Software and Performance},
  __markedentry = {[mac:]},
  abstract      = {Performance prediction and measurement approaches for component-based software systems help software architects to evaluate their systems based on component performance specifications created by component developers. Integrating classical performance models such as queueing networks, stochastic Petri nets, or stochastic process algebras, these approaches additionally exploit the benefits of component-based software engineering, such as reuse and division of work. Although researchers have proposed many approaches in this direction during the last decade, none of them has attained widespread industrial use. On this basis, we have conducted a comprehensive state-of-the-art survey of more than 20 of these approaches assessing their applicability. We classified the approaches according to the expressiveness of their component performance modelling languages. Our survey helps practitioners to select an appropriate approach and scientists to identify interesting topics for future research.},
  comment       = {25},
  doi           = {https://doi.org/10.1016/j.peva.2009.07.007},
  keywords      = {Performance, Software component, CBSE, Prediction, Modelling, Measurement, Survey, Classification},
  url           = {http://www.sciencedirect.com/science/article/pii/S016653160900100X},
}

@Article{Boucke2010,
  author        = {Nelis BouckÃ© and Danny Weyns and Tom Holvoet},
  title         = {Composition of architectural models: Empirical analysis and language support},
  journal       = {Journal of Systems and Software},
  year          = {2010},
  volume        = {83},
  number        = {11},
  pages         = {2108 - 2127},
  issn          = {0164-1212},
  note          = {Interplay between Usability Evaluation and Software Development},
  __markedentry = {[mac:]},
  abstract      = {Managing the architectural description (AD) of a complex software system and maintaining consistency among the different models is a demanding task. To understand the underlying problems, we analyse several non-trivial software architectures. The empirical study shows that a substantial amount of information of ADs is repeated, mainly by integrating information of different models in new models. Closer examination reveals that the absence of rigorously specified dependencies among models and the lack of support for automated composition of models are primary causes of management and consistency problems in software architecture. To tackle these problems, we introduce an approach in which compositions of models, together with relations among models, are explicitly supported in the ADL. We introduce these concepts formally and discuss a proof-of-concept instantiation of composition in xADL and its supporting tools. The approach is evaluated by comparing the original and revised ADs in an empirical study. The study indicates that our approach reduces the number of manually specified elements by 29%, and reduces the number of manual changes to elements for several realistic change scenarios by 52%.},
  comment       = {20},
  doi           = {https://doi.org/10.1016/j.jss.2010.06.011},
  keywords      = {Composition, Relations, Software architecture, Architectural models, Empirical analysis, Architectural description language (ADL)},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121210001639},
}

@Article{Angeren2016,
  author        = {Joey van Angeren and Carina Alves and Slinger Jansen},
  title         = {Can we ask you to collaborate? Analyzing app developer relationships in commercial platform ecosystems},
  journal       = {Journal of Systems and Software},
  year          = {2016},
  volume        = {113},
  pages         = {430 - 445},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {Previous studies have emphasized the necessity for software platform owners to govern their platform ecosystem in order to create durable opportunities for themselves and the app developers that surround the platform. To date, platform ecosystems have been widely analyzed from the perspective of platform owners. However, how and to what extent app developers collaborate with their peers needs to be investigated further. In this article, we study the interfirm relationships among app developers in commercial platform ecosystems and explore the causes of variation in the network structure of these ecosystems. By means of a comparative study of four commercial platform ecosystems of Google (Google Apps and Google Chrome) and Microsoft (Microsoft Office365 and Internet Explorer), we illustrate substantial variation in the extent to which app developers initiated interfirm relationships. Further, we analyze how the degree of enforced entry barriers to the app store, the use of a partnership model, and the domain of the software platform that underpins the ecosystem affect the properties of these commercial platform ecosystems. We present subsequent explanations as a set of propositions that can be tested in future empirical research.},
  comment       = {16},
  doi           = {https://doi.org/10.1016/j.jss.2015.11.025},
  keywords      = {Case study, Interfirm network analysis, Software ecosystem},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121215002502},
}

@Article{Galster2013,
  author        = {Matthias Galster and Paris Avgeriou and Dan Tofan},
  title         = {Constraints for the design of variability-intensive service-oriented reference architectures â€“ An industrial case study},
  journal       = {Information and Software Technology},
  year          = {2013},
  volume        = {55},
  number        = {2},
  pages         = {428 - 441},
  issn          = {0950-5849},
  note          = {Special Section: Component-Based Software Engineering (CBSE), 2011},
  __markedentry = {[mac:]},
  abstract      = {Context
Service-oriented architecture has become a widely used concept in software industry. However, we currently lack support for designing variability-intensive service-oriented systems. Such systems could be used in different environments, without the need to design them from scratch. To support the design of variability-intensive service-oriented systems, reference architectures that facilitate variability in instantiated service-oriented architectures can help.
Objective
The design of variability-intensive service-oriented reference architectures is subject to specific constraints. Architects need to know these constraints when designing such reference architectures. Our objective is to identify these constraints.
Method
An exploratory case study was performed in the context of local e-government in the Netherlands to study constraints from the perspective of (a) the users of a variability-intensive service-oriented system (municipalities that implement national laws), and (b) the implementing organizations (software vendors). We collected data through interviews with representatives from five organizations, document analyses and expert meetings.
Results
We identified ten constraints (e.g., organizational constraints, integration-related constraints) which affect the process of designing reference architectures for variability-intensive service-oriented systems. Also, we identified how stakeholders are affected by these constraints, and how constraints are specific to the case study domain.
Conclusions
Our results help design variability-intensive service-oriented reference architectures. Furthermore, our results can be used to define processes to design such reference architectures.},
  comment       = {14},
  doi           = {https://doi.org/10.1016/j.infsof.2012.09.011},
  keywords      = {Variability, Service-oriented architecture, SOA, Reference architectures, e-Government, Case study},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584912002054},
}

@Article{Kofron2009a,
  author        = {Jan KofroÅˆ and FrantiÅ¡ek PlÃ¡Å¡il and OndÅ™ej Å erÃ½},
  title         = {Modes in component behavior specification via EBP and their application in product lines},
  journal       = {Information and Software Technology},
  year          = {2009},
  volume        = {51},
  number        = {1},
  pages         = {31 - 41},
  issn          = {0950-5849},
  note          = {Special Section - Most Cited Articles in 2002 and Regular Research Papers},
  __markedentry = {[mac:]},
  abstract      = {The concept of software product lines (SPL) is a modern approach to software development simplifying construction of related variants of a product thus lowering development costs and shortening time-to-market. In SPL, software components play an important role. In this paper, we show how the original idea of component mode can be captured and further developed in behavior specification via the formalism of extended behavior protocols (EBP). Moreover, we demonstrate how the modes in behavior specification can be used for modeling behavior of an entire product line. The main benefits include (i) the existence of a single behavior specification capturing the behavior of all product variants, and (ii) automatic verification of absence of communication errors among the cooperating components taking the variability into account. These benefits are demonstrated on a part of a non-trivial case study.},
  comment       = {11},
  doi           = {https://doi.org/10.1016/j.infsof.2008.09.011},
  keywords      = {Behavior specification, Component modes, Software product lines},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584908001341},
}

@Article{Barros2011,
  author        = {Heitor Barros and Alan Silva and Evandro Costa and Ig Ibert Bittencourt and Olavo Holanda and Leandro Sales},
  title         = {Steps, techniques, and technologies for the development of intelligent applications based on Semantic Web Services: A case study in e-learning systems},
  journal       = {Engineering Applications of Artificial Intelligence},
  year          = {2011},
  volume        = {24},
  number        = {8},
  pages         = {1355 - 1367},
  issn          = {0952-1976},
  note          = {Semantic-based Information and Engineering Systems},
  __markedentry = {[mac:]},
  abstract      = {Semantic Web Services domain has gained special attention in academia and industry. It has been adopted as a promise to enable automation of all aspects of Web Services provision and uses, such as service creation, selection, discovery, composition, and invocation. However, the development of intelligent systems based on Semantic Web Services (SWS) is still a complex and time-consuming task, mainly with respect to the choice and integration of technologies. In this paper, we discuss some empirical issues associated with the development process for such systems and propose a systematic way for building intelligent applications based on SWS by providing the development process with steps, techniques and technologies. In addition, one experiment concerning the implementation of a real e-learning system using the proposed approach is described. The evaluation results from this experiment showed that our approach has been effective and relevant in terms of improvements in the development process of intelligent applications based on SWS.},
  comment       = {13},
  doi           = {https://doi.org/10.1016/j.engappai.2011.05.007},
  keywords      = {Semantic Web Services, Intelligent Tutoring System, Grinv Middleware, Ontology},
  url           = {http://www.sciencedirect.com/science/article/pii/S0952197611000893},
}

@Article{Rincon2014a,
  author        = {L.F. RincÃ³n and G.L. Giraldo and R. Mazo and C. Salinesi},
  title         = {An Ontological Rule-Based Approach for Analyzing Dead and False Optional Features in Feature Models},
  journal       = {Electronic Notes in Theoretical Computer Science},
  year          = {2014},
  volume        = {302},
  pages         = {111 - 132},
  issn          = {1571-0661},
  note          = {Proceedings of the XXXIX Latin American Computing Conference (CLEI 2013)},
  __markedentry = {[mac:]},
  abstract      = {Feature models are a common way to represent variability requirements of software product lines by expressing the set of feature combinations that software products can have. Assuring quality of feature models is thus of paramount importance for assuring quality in software product line engineering. However, feature models can have several types of defects that disminish benefits of software product line engineering.Two of such defects are dead features and false optional features. Several state-of-the-art techniques identify these defects, but only few of them tackle the problem of identifying their causes. Besides, the explanations they provide are cumbersome and hard to understand by humans. In this paper, we propose an ontological rule-based approach to: (a) identify dead and false optional features; (b)identify certain causes of these defects; and (c) explain these causes in natural language helping modelers to correct found defects. We represent our approach with a feature model taken from literature. A preliminary empirical evaluation of our approach over 31 FMs shows that our proposal is effective, accurate and scalable to 150 features.},
  comment       = {22},
  doi           = {https://doi.org/10.1016/j.entcs.2014.01.023},
  keywords      = {Feature Models, Defects, Ontologies, Software Engineering},
  url           = {http://www.sciencedirect.com/science/article/pii/S1571066114000243},
}

@Article{Wohlin2013a,
  author        = {Claes Wohlin and Per Runeson and Paulo Anselmo da Mota Silveira Neto and Emelie EngstrÃ¶m and Ivan do Carmo Machado and Eduardo Santana de Almeida},
  title         = {On the reliability of mapping studies in software engineering},
  journal       = {Journal of Systems and Software},
  year          = {2013},
  volume        = {86},
  number        = {10},
  pages         = {2594 - 2610},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {Background
Systematic literature reviews and systematic mapping studies are becoming increasingly common in software engineering, and hence it becomes even more important to better understand the reliability of such studies.
Objective
This paper presents a study of two systematic mapping studies to evaluate the reliability of mapping studies and point out some challenges related to this type of study in software engineering.
Method
The research is based on an in-depth case study of two published mapping studies on software product line testing.
Results
We found that despite the fact that the two studies are addressing the same topic, there are quite a number of differences when it comes to papers included and in terms of classification of the papers included in the two mapping studies.
Conclusions
From this we conclude that although mapping studies are important, their reliability cannot simply be taken for granted. Based on the findings we also provide four conjectures that further research has to address to make secondary studies (systematic mapping studies and systematic literature reviews) even more valuable to both researchers and practitioners.},
  comment       = {17},
  doi           = {https://doi.org/10.1016/j.jss.2013.04.076},
  keywords      = {Systematic mapping study, Software product lines, Systematic literature review, Review of reviews, Software testing},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121213001234},
}

@Article{Mendez-Acuna2017,
  author        = {David MÃ©ndez-AcuÃ±a and JosÃ© A. Galindo and BenoÃ®t Combemale and Arnaud Blouin and BenoÃ®t Baudry},
  title         = {Reverse engineering language product lines from existing DSL variants},
  journal       = {Journal of Systems and Software},
  year          = {2017},
  volume        = {133},
  pages         = {145 - 158},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {The use of domain-specific languages (DSLs) has become a successful technique to develop complex systems. In this context, an emerging phenomenon is the existence of DSL variants, which are different versions of a DSL adapted to specific purposes but that still share commonalities. In such a case, the challenge for language designers is to reuse, as much as possible, previously defined language constructs to narrow implementation from scratch. To overcome this challenge, recent research in software languages engineering introduced the notion of language product lines. Similarly to software product lines, language product lines are often built from a set of existing DSL variants. In this article, we propose a reverse-engineering technique to ease-off such a development scenario. Our approach receives a set of DSL variants which are used to automatically recover a language modular design and to synthesize the corresponding variability models. The validation is performed in a project involving industrial partners that required three different variants of a DSL for finite state machines. This validation shows that our approach is able to correctly identify commonalities and variability.},
  comment       = {13},
  doi           = {https://doi.org/10.1016/j.jss.2017.05.042},
  keywords      = {Language product lines, Software languages engineering, Domain-specific languages, Reverse-engineering},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121217300857},
}

@Article{Noor2008a,
  author        = {Muhammad A. Noor and Rick Rabiser and Paul GrÃ¼nbacher},
  title         = {Agile product line planning: A collaborative approach and a case study},
  journal       = {Journal of Systems and Software},
  year          = {2008},
  volume        = {81},
  number        = {6},
  pages         = {868 - 882},
  issn          = {0164-1212},
  note          = {Agile Product Line Engineering},
  __markedentry = {[mac:]},
  abstract      = {Agile methods and product line engineering (PLE) have both proven successful in increasing customer satisfaction and decreasing time to market under certain conditions. Key characteristics of agile methods are lean and highly iterative development with a strong emphasis on stakeholder involvement. PLE leverages reuse through systematic approaches such as variability modeling or product derivation. Integrating agile approaches with product line engineering is an interesting proposition which â€“ not surprisingly â€“ entails several challenges: Product lines (PL) rely on complex plans and models to ensure their long-term evolution while agile methods emphasize simplicity and short-term value-creation for customers. When incorporating agility in product line engineering, it is thus essential to define carefully how agile principles can support particular PLE processes. For instance, the processes of defining and setting up a product line (domain engineering) and deriving products (application engineering) differ significantly in practices and focus with implications on the suitability of agile principles. This paper presents practical experiences of adopting agile principles in product line planning (a domain engineering activity). ThinkLets, i.e., collaborative practices from the area of collaboration engineering, are the building blocks of the presented approach as they codify agile principles such as stakeholder involvement, rapid feedback, or value-based prioritization. We discuss how our approach balances agility and the intrinsic needs of product line planning. A case study carried out with an industrial partner indicates that the approach is practicable, usable, and useful.},
  comment       = {15},
  doi           = {https://doi.org/10.1016/j.jss.2007.10.028},
  keywords      = {Product line engineering, Agile methods, Collaboration engineering, Product line planning},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121207002531},
}

@Article{Schobbens2007a,
  author        = {Pierre-Yves Schobbens and Patrick Heymans and Jean-Christophe Trigaux and Yves Bontemps},
  title         = {Generic semantics of feature diagrams},
  journal       = {Computer Networks},
  year          = {2007},
  volume        = {51},
  number        = {2},
  pages         = {456 - 479},
  issn          = {1389-1286},
  note          = {Feature Interaction},
  __markedentry = {[mac:]},
  abstract      = {Feature Diagrams (FDs) are a family of popular modelling languages used to address the feature interaction problem, particularly in software product lines, FDs were first introduced by Kang as part of the FODA (Feature-Oriented Domain Analysis) method back in 1990. Afterwards, various extensions of FODA FDs were introduced to compensate for a purported ambiguity and lack of precision and expressiveness. However, they never received a formal semantics, which is the hallmark of precision and unambiguity and a prerequisite for efficient and safe tool automation. The reported work is intended to contribute a more rigorous approach to the definition, understanding, evaluation, selection and implementation of FD languages. First, we provide a survey of FD variants. Then, we give them a formal semantics, thanks to a generic construction that we call Free Feature Diagrams (FFDs). This demonstrates that FDs can be precise and unambiguous. This also defines their expressiveness. Many variants are expressively complete, and thus the endless quest for extensions actually cannot be justified by expressiveness. A finer notion is thus needed to compare these expressively complete languages. Two solutions are well-established: succinctness and embeddability, that express the naturalness of a language. We show that the expressively complete FDs fall into two succinctness classes, of which we of course recommend the most succinct. Among the succinct expressively complete languages, we suggest a new, simple one that is not harmfully redundant: Varied FD (VFD). Finally, we study the execution time that tools will need to solve useful problems in these languages.},
  comment       = {24},
  doi           = {https://doi.org/10.1016/j.comnet.2006.08.008},
  keywords      = {Feature diagram, Survey, Formal semantics, Feature interaction, Software product lines},
  url           = {http://www.sciencedirect.com/science/article/pii/S1389128606002179},
}

@Article{Bjarnason2012,
  author        = {Elizabeth Bjarnason and Krzysztof Wnuk and BjÃ¶rn Regnell},
  title         = {Are you biting off more than you can chew? A case study on causes and effects of overscoping in large-scale software engineering},
  journal       = {Information and Software Technology},
  year          = {2012},
  volume        = {54},
  number        = {10},
  pages         = {1107 - 1124},
  issn          = {0950-5849},
  __markedentry = {[mac:]},
  abstract      = {Context
Scope management is a core part of software release management and often a key factor in releasing successful software products to the market. In a market-driven case, when only a few requirements are known a priori, the risk of overscoping may increase.
Objective
This paper reports on findings from a case study aimed at understanding overscoping in large-scale, market-driven software development projects, and how agile requirements engineering practices may affect this situation.
Method
Based on a hypothesis of which factors that may be involved in an overscoping situation, semi-structured interviews were performed with nine practitioners at a large, market-driven software company. The results from the interviews were validated by six (other) practitioners at the case company via a questionnaire.
Results
The results provide a detailed picture of overscoping as a phenomenon including a number of causes, root causes and effects, and indicate that overscoping is mainly caused by operating in a fast-moving market-driven domain and how this ever-changing inflow of requirements is managed. Weak awareness of overall goals, in combination with low development involvement in early phases, may contribute to â€˜biting offâ€™ more than a project can â€˜chewâ€™. Furthermore, overscoping may lead to a number of potentially serious and expensive consequences, including quality issues, delays and failure to meet customer expectations. Finally, the study indicates that overscoping occurs also when applying agile requirements engineering practices, though the overload is more manageable and perceived to result in less wasted effort when applying a continuous scope prioritization, in combination with gradual requirements detailing and a close cooperation within cross-functional teams.
Conclusion
The results provide an increased understanding of scoping as a complex and continuous activity, including an analysis of the causes, effects, and a discussion on possible impact of agile requirements engineering practices to the issue of overscoping. The results presented in this paper can be used to identify potential factors to address in order to achieve a more realistic project scope.},
  comment       = {18},
  doi           = {https://doi.org/10.1016/j.infsof.2012.04.006},
  keywords      = {Requirements scoping, Empirical study, Software release planning, Case study, Agile requirements engineering},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584912000778},
}

@Article{Fernandez2013,
  author        = {Adrian Fernandez and Silvia AbrahÃ£o and Emilio Insfran},
  title         = {Empirical validation of a usability inspection method for model-driven Web development},
  journal       = {Journal of Systems and Software},
  year          = {2013},
  volume        = {86},
  number        = {1},
  pages         = {161 - 186},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {Web applications should be usable in order to be accepted by users and to improve their success probability. Despite the fact that this requirement has promoted the emergence of several usability evaluation methods, there is a need for empirically validated methods that provide evidence about their effectiveness and that can be properly integrated into early stages of Web development processes. Model-driven Web development processes have grown in popularity over the last few years, and offer a suitable context in which to perform early usability evaluations due to their intrinsic traceability mechanisms. These issues have motivated us to propose a Web Usability Evaluation Process (WUEP) which can be integrated into model-driven Web development processes. This paper presents a family of experiments that we have carried out to empirically validate WUEP. The family of experiments was carried out by 64 participants, including PhD and Master's computer science students. The objective of the experiments was to evaluate the participantsâ€™ effectiveness, efficiency, perceived ease of use and perceived satisfaction when using WUEP in comparison to an industrial widely used inspection method: Heuristic Evaluation (HE). The statistical analysis and meta-analysis of the data obtained separately from each experiment indicated that WUEP is more effective and efficient than HE in the detection of usability problems. The evaluators were also more satisfied when applying WUEP, and found it easier to use than HE. Although further experiments must be carried out to strengthen these results, WUEP has proved to be a promising usability inspection method for Web applications which have been developed by using model-driven development processes.},
  comment       = {26},
  doi           = {https://doi.org/10.1016/j.jss.2012.07.043},
  keywords      = {Usability inspection, Web applications, Model-driven development, Family of experiments},
  url           = {http://www.sciencedirect.com/science/article/pii/S016412121200218X},
}

@Article{Hartmann2013a,
  author        = {Herman Hartmann and Mila Keren and Aart Matsinger and Julia Rubin and Tim Trew and Tali Yatzkar-Haham},
  title         = {Using MDA for integration of heterogeneous components in software supply chains},
  journal       = {Science of Computer Programming},
  year          = {2013},
  volume        = {78},
  number        = {12},
  pages         = {2313 - 2330},
  issn          = {0167-6423},
  note          = {Special Section on International Software Product Line Conference 2010 and Fundamentals of Software Engineering (selected papers of FSEN 2011)},
  __markedentry = {[mac:]},
  abstract      = {Software product lines are increasingly built using components from specialized suppliers. A company that is in the middle of a supply chain has to integrate components from its suppliers and offer (partially configured) products to its customers. To satisfy both the variability required by each customer and the variability required to satisfy different customersâ€™ needs, it may be necessary for such a company to use components from different suppliers, partly offering the same feature set. This leads to a product line with alternative components, possibly using different mechanisms for interfacing, binding and variability, which commonly occurs in embedded software development. In this paper, we describe the limitations of the current practice of combining heterogeneous components in a product line and describe the challenges that arise from software supply chains. We introduce a model-driven approach for automating the integration between components that can generate a partially or fully configured variant, including glue between mismatched components. We analyze the consequences of using this approach in an industrial context, using a case study derived from an existing supply chain and describe the process and roles associated with this approach.},
  comment       = {18},
  doi           = {https://doi.org/10.1016/j.scico.2012.04.004},
  keywords      = {Software product line engineering, Software supply chains, Model driven engineering, Component technology, Resource constrained products, Software integration},
  url           = {http://www.sciencedirect.com/science/article/pii/S0167642312000688},
}

@Article{Usman2017a,
  author        = {Muhammad Usman and Muhammad Zohaib Iqbal and Muhammad Uzair Khan},
  title         = {A product-line model-driven engineering approach for generating feature-based mobile applications},
  journal       = {Journal of Systems and Software},
  year          = {2017},
  volume        = {123},
  pages         = {1 - 32},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {A significant challenge faced by the mobile application industry is developing and maintaining multiple native variants of mobile applications to support different mobile operating systems, devices and varying application functional requirements. The current industrial practice is to develop and maintain these variants separately. Any potential change has to be applied across variants manually, which is neither efficient nor scalable. We consider the problem of supporting multiple platforms as a â€˜software product-line engineeringâ€™ problem. The paper proposes a novel application of product-line model-driven engineering to mobile application development and addresses the key challenges of feature-based native mobile application variants for multiple platforms. Specifically, we deal with three types of variations in mobile applications: variation due to operation systems and their versions, software and hardware capabilities of mobile devices, and functionalities offered by the mobile application. We develop a tool MOPPET that automates the proposed approach. Finally, the results of applying the approach on two industrial case studies show that the proposed approach is applicable to industrial mobile applications and have potential to significantly reduce the development effort and time.},
  comment       = {32},
  doi           = {https://doi.org/10.1016/j.jss.2016.09.049},
  keywords      = {Mobile applications, Software product-line engineering, Feature model},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121216301972},
}

@Article{She2014,
  author        = {Steven She and Uwe Ryssel and Nele Andersen and Andrzej WÄ…sowski and Krzysztof Czarnecki},
  title         = {Efficient synthesis of feature models},
  journal       = {Information and Software Technology},
  year          = {2014},
  volume        = {56},
  number        = {9},
  pages         = {1122 - 1143},
  issn          = {0950-5849},
  note          = {Special Sections from â€œAsia-Pacific Software Engineering Conference (APSEC), 2012â€ and â€œ Software Product Line conference (SPLC), 2012â€},
  __markedentry = {[mac:]},
  abstract      = {Context
Variability modeling, and in particular feature modeling, is a central element of model-driven software product line architectures. Such architectures often emerge from legacy code, but, creating feature models from large, legacy systems is a long and arduous task. We describe three synthesis scenarios that can benefit from the algorithms in this paper.
Objective
This paper addresses the problem of automatic synthesis of feature models from propositional constraints. We show that the decision version of the problem is NP-hard. We designed two efficient algorithms for synthesis of feature models from CNF and DNF formulas respectively.
Method
We performed an experimental evaluation of the algorithms against a binary decision diagram (BDD)-based approach and a formal concept analysis (FCA)-based approach using models derived from realistic models.
Results
Our evaluation shows a 10 to 1,000-fold performance improvement for our algorithms over the BDD-based approach. The performance of the DNF-based algorithm was similar to the FCA-based approach, with advantages for both techniques. We identified input properties that affect the runtimes of the CNF- and DNF-based algorithms.
Conclusions
Our algorithms are the first known techniques that are efficient enough to be used on dependencies extracted from real systems, opening new possibilities of creating reverse engineering and model management tools for variability models.},
  comment       = {22},
  doi           = {https://doi.org/10.1016/j.infsof.2014.01.012},
  keywords      = {Feature models, Variability models, Software product lines},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584914000238},
}

@Article{Silva2015a,
  author        = {Alberto Rodrigues da Silva},
  title         = {Model-driven engineering: A survey supported by the unified conceptual model},
  journal       = {Computer Languages, Systems \& Structures},
  year          = {2015},
  volume        = {43},
  pages         = {139 - 155},
  issn          = {1477-8424},
  __markedentry = {[mac:]},
  abstract      = {During the last decade a new trend of approaches has emerged, which considers models not just documentation artefacts, but also central artefacts in the software engineering field, allowing the creation or automatic execution of software systems starting from those models. These proposals have been classified generically as Model-Driven Engineering (MDE) and share common concepts and terms that need to be abstracted, discussed and understood. This paper presents a survey on MDE based on a unified conceptual model that clearly identifies and relates these essential concepts, namely the concepts of system, model, metamodel, modeling language, transformations, software platform, and software product. In addition, this paper discusses the terminologies relating MDE, MDD, MDA and others. This survey is based on earlier work, however, contrary to those, it intends to give a simple, broader and integrated view of the essential concepts and respective terminology commonly involved in the MDE, answering to key questions such as: What is a model? What is the relation between a model and a metamodel? What are the key facets of a modeling language? How can I use models in the context of a software development process? What are the relations between models and source code artefacts and software platforms? and What are the relations between MDE, MDD, MDA and other MD approaches?},
  comment       = {17},
  doi           = {https://doi.org/10.1016/j.cl.2015.06.001},
  keywords      = {Model, Metamodel, Modeling language, Software system, Model-driven engineering, Model-driven approaches},
  url           = {http://www.sciencedirect.com/science/article/pii/S1477842415000408},
}

@Article{Strickler2016,
  author        = {Andrei Strickler and Jackson A. Prado Lima and Silvia R. Vergilio and Aurora T.R. Pozo},
  title         = {Deriving products for variability test of Feature Models with a hyper-heuristic approach},
  journal       = {Applied Soft Computing},
  year          = {2016},
  volume        = {49},
  pages         = {1232 - 1242},
  issn          = {1568-4946},
  __markedentry = {[mac:]},
  abstract      = {Deriving products from a Feature Model (FM) for testing Software Product Lines (SPLs) is a hard task. It is important to select a minimum number of products but, at the same time, to consider the coverage of testing criteria such as pairwise, among other factors. To solve such problems Multi-Objective Evolutionary Algorithms (MOEAs) have been successfully applied. However, to design a solution for this and other software engineering problems can be very difficult, because it is necessary to choose among different search operators and parameters. Hyper-heuristics can help in this task, and have raised interest in the Search-Based Software Engineering (SBSE) field. Considering the growing adoption of SPL in the industry and crescent demand for SPL testing approaches, this paper introduces a hyper-heuristic approach to automatically derive products to variability testing of SPLs. The approach works with MOEAs and two selection methods, random and based on FRR-MAB (Fitness Rate Rank based Multi-Armed Bandit). It was evaluated with real FMs and the results show that the proposed approach outperforms the traditional algorithms used in the literature, and that both selection methods present similar performance.},
  comment       = {11},
  doi           = {https://doi.org/10.1016/j.asoc.2016.07.059},
  keywords      = {Software Product Line, Software testing, Hyper-heuristic},
  url           = {http://www.sciencedirect.com/science/article/pii/S1568494616303994},
}

@Article{Needham2007,
  author        = {D.M. Needham and S.A. Jones},
  title         = {A software fault tree key node metric},
  journal       = {Journal of Systems and Software},
  year          = {2007},
  volume        = {80},
  number        = {9},
  pages         = {1530 - 1540},
  issn          = {0164-1212},
  note          = {Evaluation and Assessment in Software Engineering},
  __markedentry = {[mac:]},
  abstract      = {Analysis of software fault trees exposes failure events that can impact safety within safety-critical software product lines. This paper presents a software fault tree key node safety metric for measuring software safety within product lines. Fault tree structures impacting the metricâ€™s composition are provided, and the mathematical basis for the metric is defined. The metric is applied to an embedded control system as well as to a series of experiments expected to either improve or degrade system safety. The effectiveness of the metric is analyzed, and lessons learned during the application of the metric are discussed.},
  comment       = {11},
  doi           = {https://doi.org/10.1016/j.jss.2007.01.042},
  keywords      = {Safety-critical software, Software fault trees, Software metrics, Software product lines},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121207000064},
}

@Article{Hutchinson2014,
  author        = {John Hutchinson and Jon Whittle and Mark Rouncefield},
  title         = {Model-driven engineering practices in industry: Social, organizational and managerial factors that lead to success or failure},
  journal       = {Science of Computer Programming},
  year          = {2014},
  volume        = {89},
  pages         = {144 - 161},
  issn          = {0167-6423},
  note          = {Special issue on Success Stories in Model Driven Engineering},
  __markedentry = {[mac:]},
  abstract      = {In this article, we attempt to address the relative absence of empirical studies of model driven engineering (MDE) in two different but complementary ways. First, we present an analysis of a large online survey of MDE deployment and experience that provides some rough quantitative measures of MDE practices in industry. Second, we supplement these figures with qualitative data obtained from some semi-structured, in-depth interviews with MDE practitioners, and, in particular, through describing the practices of four commercial organizations as they adopted a model driven engineering approach to their software development practices. Using in-depth semi-structured interviewing, we invited practitioners to reflect on their experiences and selected four to use as exemplars or case studies. In documenting some details of their attempts to deploy model driven practices, we identify a number of factors, in particular the importance of complex organizational, managerial and social factorsâ€“as opposed to simple technical factorsâ€“that appear to influence the relative success, or failure, of the endeavor. Three of the case study companies describe genuine success in their use of model driven development, but explain that as examples of organizational change management, the successful deployment of model driven engineering appears to require: a progressive and iterative approach; transparent organizational commitment and motivation; integration with existing organizational processes and a clear business focus.},
  comment       = {18},
  doi           = {https://doi.org/10.1016/j.scico.2013.03.017},
  keywords      = {Model driven engineering, Empirical software engineering, Industry practice},
  url           = {http://www.sciencedirect.com/science/article/pii/S0167642313000786},
}

@Article{Snook2008,
  author        = {Colin Snook and Michael Poppleton and Ian Johnson},
  title         = {Rigorous engineering of product-line requirements: A case study in failure management},
  journal       = {Information and Software Technology},
  year          = {2008},
  volume        = {50},
  number        = {1},
  pages         = {112 - 129},
  issn          = {0950-5849},
  note          = {Special issue with two special sections. Section 1: Most-cited software engineering articles in 2001. Section 2: Requirement engineering: Foundation for software quality},
  __markedentry = {[mac:]},
  abstract      = {We consider the failure detection and management function for engine control systems as an application domain where product line engineering is indicated. The need to develop a generic requirement set â€“ for subsequent system instantiation â€“ is complicated by the addition of the high levels of verification demanded by this safety-critical domain, subject to avionics industry standards. We present our case study experience in this area as a candidate method for the engineering, validation and verification of generic requirements using domain engineering and Formal Methods techniques and tools. For a defined class of systems, the case study produces a generic requirement set in UML and an example system instance. Domain analysis and engineering produce a validated model which is integrated with the formal specification/verification method B by the use of our UML-B profile. The formal verification both of the generic requirement set, and of a simple system instance, is demonstrated using our U2B, ProB and prototype Requirements Manager tools. This work is a demonstrator for a tool-supported method which will be an output of EU project RODIN (This work is conducted in the setting of the EU funded Research Project: IST 511599 RODIN (Rigorous Open Development Environment for Complex Systems) http://rodin.cs.ncl.ac.uk/). The use of existing and prototype formal verification and support tools is discussed. The method, developed in application to this novel combination of product line, failure management and safety-critical engineering, is evaluated and considered to be applicable to a wide range of domains.},
  comment       = {18},
  doi           = {https://doi.org/10.1016/j.infsof.2007.10.010},
  keywords      = {Formal specification, Generic requirements, Product line, Refinement, Tools, UML-B, Verification},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584907001176},
}

@Article{Zhang2013,
  author        = {He Zhang and Muhammad Ali Babar},
  title         = {Systematic reviews in software engineering: An empirical investigation},
  journal       = {Information and Software Technology},
  year          = {2013},
  volume        = {55},
  number        = {7},
  pages         = {1341 - 1354},
  issn          = {0950-5849},
  __markedentry = {[mac:]},
  abstract      = {Background
Systematic Literature Reviews (SLRs) have gained significant popularity among Software Engineering (SE) researchers since 2004. Several researchers have also been working on improving the scientific and methodological infrastructure to support SLRs in SE. We argue that there is also an apparent and essential need for evidence-based body of knowledge about different aspects of the adoption of SLRs in SE.
Objective
The main objective of this research is to empirically investigate the adoption, value, and use of SLRs in SE research from various perspectives.
Method
We used mixed-methods approach (systematically integrating tertiary literature review, semi-structured interviews and questionnaire-based survey) as it is based on a combination of complementary research methods which are expected to compensate each othersâ€™ limitations.
Results
A large majority of the participants are convinced of the value of using a rigourous and systematic methodology for literature reviews in SE research. However, there are concerns about the required time and resources for SLRs. One of the most important motivators for performing SLRs is new findings and inception of innovative ideas for further research. The reported SLRs are more influential compared to the traditional literature reviews in terms of number of citations. One of the main challenges of conducting SLRs is drawing a balance between methodological rigour and required effort.
Conclusions
SLR has become a popular research methodology for conducting literature review and evidence aggregation in SE. There is an overall positive perception about this relatively new methodology to SE research. The findings provide interesting insights into different aspects of SLRs. We expect that the findings can provide valuable information to readers about what can be expected from conducting SLRs and the potential impact of such reviews.},
  comment       = {14},
  doi           = {https://doi.org/10.1016/j.infsof.2012.09.008},
  keywords      = {Systematic (literature) reviews, Evidence-based software engineering, Research methodology, Methodology adoption, Mixed-methods research, Tertiary study},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584912002029},
}

@Article{Thuem2014,
  author        = {Thomas ThÃ¼m and Christian KÃ¤stner and Fabian Benduhn and Jens Meinicke and Gunter Saake and Thomas Leich},
  title         = {FeatureIDE: An extensible framework for feature-oriented software development},
  journal       = {Science of Computer Programming},
  year          = {2014},
  volume        = {79},
  pages         = {70 - 85},
  issn          = {0167-6423},
  note          = {Experimental Software and Toolkits (EST 4): A special issue of the Workshop on Academic Software Development Tools and Techniques (WASDeTT-3 2010)},
  __markedentry = {[mac:]},
  abstract      = {FeatureIDE is an open-source framework for feature-oriented software development (FOSD) based on Eclipse. FOSD is a paradigm for the construction, customization, and synthesis of software systems. Code artifacts are mapped to features, and a customized software system can be generated given a selection of features. The set of software systems that can be generated is called a software product line (SPL). FeatureIDE supports several FOSD implementation techniques such as feature-oriented programming, aspect-oriented programming, delta-oriented programming, and preprocessors. All phases of FOSD are supported in FeatureIDE, namely domain analysis, requirements analysis, domain implementation, and software generation.},
  comment       = {16},
  doi           = {https://doi.org/10.1016/j.scico.2012.06.002},
  keywords      = {Feature-oriented software development, Software product lines, Feature modeling, Feature-oriented programming, Aspect-oriented programming, Delta-oriented programming, Preprocessors, Tool support},
  url           = {http://www.sciencedirect.com/science/article/pii/S0167642312001128},
}

@Article{Ajoudanian2015,
  author        = {Shohreh Ajoudanian and Seyed-Hassan Mirian Hosseinabadi},
  title         = {Automatic promotional specialization, generalization and analysis of extended feature models with cardinalities in Alloy},
  journal       = {Journal of Logical and Algebraic Methods in Programming},
  year          = {2015},
  volume        = {84},
  number        = {5},
  pages         = {640 - 667},
  issn          = {2352-2208},
  __markedentry = {[mac:]},
  abstract      = {Software product line engineering is a method of producing a set of related products that share more commonalities than variability in a cost-effective approach. Software product lines provide systematic reuse within a product family. Extended feature models with cardinalities are widely used for managing variability and commonality in the software product line domains. In this paper, we use promotion technique in Alloy to formalize constraint based extended feature models with cardinalities and their specialization and generalization. This technique has a significant influence on applying analysis operations on feature models. To show the benefits of the promotion technique, we calculate the reuse ratio of a feature in a large scale software product line. In the presented method, in addition to feature and group cardinalities, we consider different combinations of cardinalities with each other as well as feature cloning.},
  comment       = {28},
  doi           = {https://doi.org/10.1016/j.jlamp.2014.11.005},
  keywords      = {Extended feature model with cardinality, Specialization and generalization of SPLS, Multiple multi-level promotions in Alloy},
  url           = {http://www.sciencedirect.com/science/article/pii/S2352220814000959},
}

@Article{Pleuss2012,
  author        = {Andreas Pleuss and Goetz Botterweck and Deepak Dhungana and Andreas Polzer and Stefan Kowalewski},
  title         = {Model-driven support for product line evolution on feature level},
  journal       = {Journal of Systems and Software},
  year          = {2012},
  volume        = {85},
  number        = {10},
  pages         = {2261 - 2274},
  issn          = {0164-1212},
  note          = {Automated Software Evolution},
  __markedentry = {[mac:]},
  abstract      = {Software Product Lines (SPL) are an engineering technique to efficiently derive a set of similar products from a set of shared assets. In particular in conjunction with model-driven engineering, SPL engineering promises high productivity benefits. There is however, a lack of support for systematic management of SPL evolution, which is an important success factor as a product line often represents a long term investment. In this article, we present a model-driven approach for managing SPL evolution on feature level. To reduce complexity we use model fragments to cluster related elements. The relationships between these fragments are specified using feature model concepts itself leading to a specific kind of feature model called EvoFM. A configuration of EvoFM represents an evolution step and can be transformed to a concrete instance of the product line (i.e., a feature model for the corresponding point in time). Similarly, automatic transformations allow the derivation of an EvoFM from a given set of feature models. This enables retrospective analysis of historic evolution and serves as a starting point for introduction of EvoFM, e.g., to plan future evolution steps.},
  comment       = {14},
  doi           = {https://doi.org/10.1016/j.jss.2011.08.008},
  keywords      = {Feature modeling, Software Product Lines, Model-driven engineering, Evolving systems},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121211002093},
}

@Article{Tang2013,
  author        = {Jiafu Tang and Wu Zhiqiao and C.K. Kwong and Xinggang Luo},
  title         = {Integrated production strategy and reuse scenario: A CoFAQ model and case study of mail server system development},
  journal       = {Omega},
  year          = {2013},
  volume        = {41},
  number        = {3},
  pages         = {536 - 552},
  issn          = {0305-0483},
  __markedentry = {[mac:]},
  abstract      = {One of the core problems in software product family (SPF) is the coordination of product building and core asset development, specifically the integration of production strategy decision and core asset scenario selection. In the current paper, a model of Cost Optimization under Functional And Quality (CoFAQ) goal satisfaction constraints is developed. It provides a systematic mechanism for management to analyze all possible products and evaluate various reuse alternatives at the organizational level. The CoFAQ model facilitates decision-makers to optimize the SPF development process by determining which products are involved in the SPF (i.e. production strategy) and which reuse scenario for each module should be selected to implement the SPF toward minimum total developing cost under the constraints of satisfying functional and quality goals. A two-phase algorithm with heuristic (TPA) is developed to solve the model efficiently. Based on the TPA, the CoFAQ is reduced to a weighted set-covering problem for production strategy decision and a knapsack problem for the reuse scenario selection. An application of the model in mail server domain development is presented to illustrate how it has been used in practice.},
  comment       = {17},
  doi           = {https://doi.org/10.1016/j.omega.2012.07.003},
  keywords      = {Software reuse, Integrated decision, Optimization, Heuristic},
  url           = {http://www.sciencedirect.com/science/article/pii/S0305048312001053},
}

@Article{Anand2013,
  author        = {Saswat Anand and Edmund K. Burke and Tsong Yueh Chen and John Clark and Myra B. Cohen and Wolfgang Grieskamp and Mark Harman and Mary Jean Harrold and Phil McMinn and Antonia Bertolino and J. Jenny Li and Hong Zhu},
  title         = {An orchestrated survey of methodologies for automated software test case generation},
  journal       = {Journal of Systems and Software},
  year          = {2013},
  volume        = {86},
  number        = {8},
  pages         = {1978 - 2001},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {Test case generation is among the most labour-intensive tasks in software testing. It also has a strong impact on the effectiveness and efficiency of software testing. For these reasons, it has been one of the most active research topics in software testing for several decades, resulting in many different approaches and tools. This paper presents an orchestrated survey of the most prominent techniques for automatic generation of software test cases, reviewed in self-standing sections. The techniques presented include: (a) structural testing using symbolic execution, (b) model-based testing, (c) combinatorial testing, (d) random testing and its variant of adaptive random testing, and (e) search-based testing. Each section is contributed by world-renowned active researchers on the technique, and briefly covers the basic ideas underlying the method, the current state of the art, a discussion of the open research problems, and a perspective of the future development of the approach. As a whole, the paper aims at giving an introductory, up-to-date and (relatively) short overview of research in automatic test case generation, while ensuring a comprehensive and authoritative treatment.},
  comment       = {24},
  doi           = {https://doi.org/10.1016/j.jss.2013.02.061},
  keywords      = {Adaptive random testing, Combinatorial testing, Model-based testing, Orchestrated survey, Search-based software testing, Software testing, Symbolic execution, Test automation, Test case generation},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121213000563},
}

@Article{Ng2015,
  author        = {Pan-Wei Ng},
  title         = {Integrating software engineering theory and practice using essence: A case study},
  journal       = {Science of Computer Programming},
  year          = {2015},
  volume        = {101},
  pages         = {66 - 78},
  issn          = {0167-6423},
  note          = {Towards general theories of software engineering},
  __markedentry = {[mac:]},
  abstract      = {Software engineering is complex and success depends on many inter-related factors. Theory Based Software Engineering (TBSE) is about providing a practical way for software teams to understand the relationships and the influence of these factors to thereby adapt the way they work. This paper proposes an approach to TBSE based on Essence, a software engineering kernel distilled by the SEMAT (Software Engineering Method and Theory) initiative. Essence supports TBSE by providing a domain model that is useful for organizing and relating software engineering factors. Essence also helps make recommended practices precise and actionable to software teams. We provide a step-by-step application of our approach on an industrial software process improvement case study. The case study achieved 21% productivity gains and 58% decrease in defects. But more importantly than these results, it demonstrates the value of Essence in supporting TBSE.},
  comment       = {13},
  doi           = {https://doi.org/10.1016/j.scico.2014.11.009},
  keywords      = {Software engineering, Theory, Kernel, SEMAT, Essence},
  url           = {http://www.sciencedirect.com/science/article/pii/S0167642314005413},
}

@Article{Vale2016,
  author        = {Tassio Vale and Ivica Crnkovic and Eduardo Santana de Almeida and Paulo Anselmo da Mota Silveira Neto and YguaratÃ£ Cerqueira Cavalcanti and Silvio Romero de Lemos Meira},
  title         = {Twenty-eight years of component-based software engineering},
  journal       = {Journal of Systems and Software},
  year          = {2016},
  volume        = {111},
  pages         = {128 - 148},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {The idea of developing software components was envisioned more than forty years ago. In the past two decades, Component-Based Software Engineering (CBSE) has emerged as a distinguishable approach in software engineering, and it has attracted the attention of many researchers, which has led to many results being published in the research literature. There is a huge amount of knowledge encapsulated in conferences and journals targeting this area, but a systematic analysis of that knowledge is missing. For this reason, we aim to investigate the state-of-the-art of the CBSE area through a detailed literature review. To do this, 1231 studies dating from 1984 to 2012 were analyzed. Using the available evidence, this paper addresses five dimensions of CBSE: main objectives, research topics, application domains, research intensity and applied research methods. The main objectives found were to increase productivity, save costs and improve quality. The most addressed application domains are homogeneously divided between commercial-off-the-shelf (COTS), distributed and embedded systems. Intensity of research showed a considerable increase in the last fourteen years. In addition to the analysis, this paper also synthesizes the available evidence, identifies open issues and points out areas that call for further research.},
  comment       = {21},
  doi           = {https://doi.org/10.1016/j.jss.2015.09.019},
  keywords      = {Systematic mapping study, Component-based software engineering, Component-based software development, Software component},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121215002095},
}

@Article{Goedicke2004,
  author        = {Michael Goedicke and Carsten KÃ¶llmann and Uwe Zdun},
  title         = {Designing runtime variation points in product line architectures: three cases},
  journal       = {Science of Computer Programming},
  year          = {2004},
  volume        = {53},
  number        = {3},
  pages         = {353 - 380},
  issn          = {0167-6423},
  note          = {Software Variability Management},
  __markedentry = {[mac:]},
  abstract      = {Software product lines provide a common architecture, reusable code, and other common assets for a set of related software products. Variation is a central requirement in this context, as the product line components have to be instantiated, composed, and configured in the context of the products. In many approaches either static composition techniques or dynamic composition techniques based on loose relationships, such as association, aggregation, and replacement of entities, are proposed to design the variation points. If the domain of the product requires runtime variation, however, these approaches do not provide any central management facility for the runtime variation points. As a solution to this problem, we propose a pattern language that provides a domain-specific variation language and runtime variation point management facilities as part of the product line. We present three case studies from the areas of interactive digital television and document archiving in which we have applied this pattern language.},
  comment       = {28},
  doi           = {https://doi.org/10.1016/j.scico.2003.04.006},
  url           = {http://www.sciencedirect.com/science/article/pii/S0167642304000966},
}

@Article{Preuveneers2016,
  author        = {Davy Preuveneers and Thomas Heyman and Yolande Berbers and Wouter Joosen},
  title         = {Systematic scalability assessment for feature oriented multi-tenant services},
  journal       = {Journal of Systems and Software},
  year          = {2016},
  volume        = {116},
  pages         = {162 - 176},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {Recent software engineering paradigms such as software product lines, supporting development techniques like feature modeling, and cloud provisioning models such as platform and infrastructure as a service, allow for great flexibility during both software design and deployment, resulting in potentially large cost savings. However, all this flexibility comes with a catch: as the combinatorial complexity of optional design features and deployment variability increases, the difficulty of assessing system qualities such as scalability and quality of service increases too. And if the software itself is not scalable (for instance, because of a specific set of selected features), deploying additional service instances is a futile endeavor. Clearly there is a need to systematically measure the impact of feature selection on scalability, as the potential cost savings can be completely mitigated by the risk of having a system that is unable to meet service demand. In this work, we document our results on systematic load testing for automated quality of service and scalability analysis. The major contribution of our work is tool support and a methodology to analyze the scalability of these distributed, feature oriented multi-tenant software systems in a continuous integration process. We discuss our approach to select features for load testing such that a representative set of feature combinations is used to elicit valuable information on the performance impact and feature interactions. Additionally, we highlight how our methodology and framework for performance and scalability prediction differs from state-of-practice solutions. We take the viewpoint of both the tenant of the service and the service provider, and report on our experiences applying the approach to an industrial use case in the domain of electronic payments. We conclude that the integration of systematic scalability tests in a continuous integration process offers strong advantages to software developers and service providers, such as the ability to quantify the impact of new features in existing service compositions, and the early detection of hidden feature interactions that may negatively affect the overall performance of multi-tenant services.},
  comment       = {15},
  doi           = {https://doi.org/10.1016/j.jss.2015.12.024},
  keywords      = {Distributed systems, Scalability, Tool support},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121215002897},
}

@Article{Nurdiani2016,
  author        = {Indira Nurdiani and JÃ¼rgen BÃ¶rstler and Samuel A. Fricker},
  title         = {The impacts of agile and lean practices on project constraints: A tertiary study},
  journal       = {Journal of Systems and Software},
  year          = {2016},
  volume        = {119},
  pages         = {162 - 183},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {The growing interest in Agile and Lean software development is reflected in the increasing number of secondary studies on the benefits and limitations of Agile and Lean processes and practices. The aim of this tertiary study is to consolidate empirical evidence regarding Agile and Lean practices and their respective impacts on project constraints as defined in the Project Management Body of Knowledge (PMBOK): scope, quality, schedule, budget, resources, communication, and risk. In this tertiary study, 13 secondary studies were included for detailed analysis. Given the heterogeneity of the data, we were unable to perform a rigorous synthesis. Instead, we mapped the identified Agile and Lean practices, and their impacts on the project constraints described in PMBOK. From 13 secondary studies, we identified 13 Agile and Lean practices. Test-Driven Development (TDD) is studied in ten secondary studies, meanwhile other practices are studied in only one or two secondary studies. This tertiary study provides a consolidated view of the impacts of Agile and Lean practices. The result of this tertiary study indicates that TDD has a positive impact on external quality. However, due to insufficient data or contradictory results, we were unable to make inferences on other Agile and Lean practices. Implications for research and practice are further discussed in the paper.},
  comment       = {22},
  doi           = {https://doi.org/10.1016/j.jss.2016.06.043},
  keywords      = {Tertiary study, Agile software development, Lean software development, Project constraints},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121216300863},
}

@Article{Jonge2009,
  author        = {Merijn de Jonge},
  title         = {Developing Product Lines with Third-Party Components},
  journal       = {Electronic Notes in Theoretical Computer Science},
  year          = {2009},
  volume        = {238},
  number        = {5},
  pages         = {63 - 80},
  issn          = {1571-0661},
  note          = {Proceedings of the 8th Workshop on Language Descriptions, Tools and Applications (LDTA 2008)},
  __markedentry = {[mac:]},
  abstract      = {The trends toward product line development and toward adopting more third-party software are hard to combine. The reason is that product lines demand fine control over the software (e.g., for diversity management), while third-party software (almost by definition) provides only little or no control. A growing use of third-party software may therefore lead to less control over the product development process or, vice-versa, requiring large control over the software may limit the ability to use third-party components. Since both are means to reduce costs and to shorten time to market, the question is whether they can be combined effectively. In this paper, we describe our solution to this problem which combines the Koala component model developed within Philips with the concept of build-level components. We show that by lifting component granularity of Koala components from individual C files to build-level components, both trends can be united. The Koala architectural description language is used to orchestrate product composition and to manage diversity, while build-level components form the unit of third-party component composition.},
  comment       = {18},
  doi           = {https://doi.org/10.1016/j.entcs.2009.09.041},
  keywords      = {Koala, software product lines, build-level components, third-party sofware, software composition},
  url           = {http://www.sciencedirect.com/science/article/pii/S1571066109003958},
}

@Article{Thurimella2012a,
  author        = {Anil Kumar Thurimella and Bernd Bruegge},
  title         = {Issue-based variability management},
  journal       = {Information and Software Technology},
  year          = {2012},
  volume        = {54},
  number        = {9},
  pages         = {933 - 950},
  issn          = {0950-5849},
  __markedentry = {[mac:]},
  abstract      = {Context
Variability management is a key activity in software product line engineering. This paper focuses on managing rationale information during the decision-making activities that arise during variability management. By decision-making we refer to systematic problem solving by considering and evaluating various alternatives. Rationale management is a branch of science that enables decision-making based on the argumentation of stakeholders while capturing the reasons and justifications behind these decisions.
Objective
Decision-making should be supported to identify variability in domain engineering and to resolve variation points in application engineering. We capture the rationale behind variability management decisions. The captured rationale information is useful to evaluate future changes of variability models as well as to handle future instantiations of variation points. We claim that maintaining rationale will enhance the longevity of variability models. Furthermore, decisions should be performed using a formal communication between domain engineering and application engineering.
Method
We initiate the novel area of issue-based variability management (IVM) by extending variability management with rationale management. The key contributions of this paper are: (i) an issue-based variability management methodology (IVMM), which combines questions, options and criteria (QOC) and a specific variability approach; (ii) a meta-model for IVMM and a process for variability management and (iii) a tool for the methodology, which was developed by extending an open source rationale management tool.
Results
Rationale approaches (e.g. questions, options and criteria) guide distributed stakeholders when selecting choices for instantiating variation points. Similarly, rationale approaches also aid the elicitation of variability and the evaluation of changes. The rationale captured within the decision-making process can be reused to perform future decisions on variability.
Conclusion
IVMM was evaluated comparatively based on an experimental survey, which provided evidence that IVMM is more effective than a variability modeling approach that does not use issues.},
  comment       = {18},
  doi           = {https://doi.org/10.1016/j.infsof.2012.02.005},
  keywords      = {Rationale management, Requirements engineering, Product line engineering, Empirical software engineering},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584912000481},
}

@Article{Wang2015a,
  author        = {Shuai Wang and Shaukat Ali and Arnaud Gotlieb},
  title         = {Cost-effective test suite minimization in product lines using search techniques},
  journal       = {Journal of Systems and Software},
  year          = {2015},
  volume        = {103},
  pages         = {370 - 391},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {Cost-effective testing of a product in a product line requires obtaining a set of relevant test cases from the entire test suite via test selection and minimization techniques. In this paper, we particularly focus on test minimization for product lines, which identifies and eliminates redundant test cases from test suites in order to reduce the total number of test cases to execute, thereby improving the efficiency of testing. However, such minimization may result in the minimized test suite with low test coverage, low fault revealing capability, low priority test cases, and require more time than the allowed testing budget (e.g., time) as compared to the original test suite. To deal with the above issues, we formulated the minimization problem as a search problem and defined a fitness function considering various optimization objectives based on the above issues. To assess the performance of our fitness function, we conducted an extensive empirical evaluation by investigating the fitness function with three weight-based Genetic Algorithms (GAs) and seven multi-objective search algorithms using an industrial case study and 500 artificial problems inspired from the industrial case study. The results show that Random-Weighted Genetic Algorithm (RWGA) significantly outperforms the other algorithms since RWGA can balance all the objectives together by dynamically updating weights during each generation. Based on the results of our empirical evaluation, we also implemented a tool called TEst Minimization using Search Algorithms (TEMSA) to support test minimization using various search algorithms in the context of product lines.},
  comment       = {22},
  doi           = {https://doi.org/10.1016/j.jss.2014.08.024},
  keywords      = {Product line, Search algorithm, Test suite minimization},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121214001757},
}

@Article{Buccella2014,
  author        = {Agustina Buccella and Alejandra Cechich and Matias Pol×³la and Maximiliano Arias and Maria del Socorro Doldan and Enrique Morsan},
  title         = {Marine ecology service reuse through taxonomy-oriented SPL development},
  journal       = {Computers \& Geosciences},
  year          = {2014},
  volume        = {73},
  pages         = {108 - 121},
  issn          = {0098-3004},
  __markedentry = {[mac:]},
  abstract      = {Nowadays, reusing software applications encourages researchers and industrials to collaborate in order to increase software quality and to reduce software development costs. However, effective reuse is not easy and only a limited portion of reusable models actually offers effective evidence regarding their appropriateness, usability and/or effectiveness. Focusing reuse on a particular domain, such as marine ecology, allows us to narrow the scope; and along with a systematic approach such as software product line development, helps us to potentially improving reuse. From our experiences developing a subdomain-oriented software product line (SPL for the marine ecology subdomain), in this paper we describe semantic resources created for assisting this development and thus promoting systematic software reuse. The main contributions of our work are focused on the definition of a standard conceptual model for marine ecology applications together with a set of services and guides which assist the process of product derivation. The services are structured in a service taxonomy (as a specialization of the ISO 19119 std) in which we create a new set of categories and services built over a conceptual model for marine ecology applications. We also define and exemplify a set of guides for composing the services of the taxonomy in order to fulfill different functionalities of particular systems in the subdomain.},
  comment       = {14},
  doi           = {https://doi.org/10.1016/j.cageo.2014.09.004},
  keywords      = {Domain-specific taxonomies, Software reuse, Domain engineering, Geographic information systems, ISO 19100 standards},
  url           = {http://www.sciencedirect.com/science/article/pii/S0098300414002155},
}

@Article{Bass2016,
  author        = {Julian M. Bass},
  title         = {Artefacts and agile method tailoring in large-scale offshore software development programmes},
  journal       = {Information and Software Technology},
  year          = {2016},
  volume        = {75},
  pages         = {1 - 16},
  issn          = {0950-5849},
  __markedentry = {[mac:]},
  abstract      = {Context: Large-scale offshore software development programmes are complex, with challenging deadlines and a high risk of failure. Agile methods are being adopted, despite the challenges of coordinating multiple development teams. Agile processes are tailored to support team coordination. Artefacts are tangible products of the software development process, intended to ensure consistency in the approach of teams on the same development programme. Objective: This study aims to increase understanding of how development processes are tailored to meet the needs of large-scale offshore software development programmes, by focusing on artefact inventories used in the development process. Method: A grounded theory approach using 46 practitioner interviews, supplemented with documentary sources and observations, in nine international companies was adopted. The grounded theory concepts of open coding, memoing, constant comparison and saturation were used in data analysis. Results: The study has identified 25 artefacts, organised into five categories: feature, sprint, release, product and corporate governance. It was discovered that conventional agile artefacts are enriched with artefacts associated with plan-based methods in order to provide governance. The empirical evidence collected in the study has been used to identify a primary owner of each artefact and map each artefact to specific activities within each of the agile roles. Conclusion: The development programmes in this study create agile and plan-based artefacts to improve compliance with enterprise quality standards and technology strategies, whilst also mitigating risk of failure. Management of these additional artefacts is currently improvised because agile development processes lack corresponding ceremonies.},
  comment       = {16},
  doi           = {https://doi.org/10.1016/j.infsof.2016.03.001},
  keywords      = {Agile software development, Software development artefacts, Scrum, Large-scale, Enterprise, Offshore, Outsourced, Grounded theory, Process tailoring},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584916300350},
}

@Article{Saeed2016b,
  author        = {Aneesa Saeed and Siti Hafizah Ab Hamid and Mumtaz Begum Mustafa},
  title         = {The experimental applications of search-based techniques for model-based testing: Taxonomy and systematic literature review},
  journal       = {Applied Soft Computing},
  year          = {2016},
  volume        = {49},
  pages         = {1094 - 1117},
  issn          = {1568-4946},
  __markedentry = {[mac:]},
  abstract      = {Context
Model-based testing (MBT) aims to generate executable test cases from behavioral models of software systems. MBT gains interest in industry and academia due to its provision of systematic, automated, and comprehensive testing. Researchers have successfully applied search-based techniques (SBTs) by automating the search for an optimal set of test cases at reasonable cost compared to other more expensive techniques. Thus, there is a recent surge toward the applications of SBTs for MBT because the generated test cases are optimal and have low computational cost. However, successful, future SBTs for MBT applications demand deep insight into its existing experimental applications that underlines stringent issues and challenges, which is lacking in the literature.
Objective
The objective of this study is to comprehensively analyze the current state-of-the-art of the experimental applications of SBTs for MBT and present the limitations of the current literature to direct future research.
Method
We conducted a systematic literature review (SLR) using 72 experimental papers from six data sources. We proposed a taxonomy based on the literature to categorize the characteristics of the current applications.
Results
The results indicate that the majority of the existing applications of SBTs for MBT focus on functional and structural coverage purposes, as opposed to stress testing, regression testing and graphical user interface (GUI) testing. We found research gaps in the existing applications in five areas: applying multi-objective SBTs, proposing hybrid techniques, handling complex constraints, addressing data and requirement-based adequacy criteria, and adapting landscape visualization. Only twelve studies proposed and empirically evaluated the SBTs for complex systems in MBT.
Conclusion
This extensive systematic analysis of the existing literature based on the proposed taxonomy enables to assist researchers in exploring the existing research efforts and reveal the limitations that need additional investigation.},
  comment       = {24},
  doi           = {https://doi.org/10.1016/j.asoc.2016.08.030},
  keywords      = {Software testing, Systematic literature review, Model-based testing, Search-based techniques, Taxonomy, Test case generation},
  url           = {http://www.sciencedirect.com/science/article/pii/S1568494616304240},
}

@Article{Conejero2012a,
  author        = {JosÃ© M. Conejero and Eduardo Figueiredo and Alessandro Garcia and Juan HernÃ¡ndez and Elena Jurado},
  title         = {On the relationship of concern metrics and requirements maintainability},
  journal       = {Information and Software Technology},
  year          = {2012},
  volume        = {54},
  number        = {2},
  pages         = {212 - 238},
  issn          = {0950-5849},
  __markedentry = {[mac:]},
  abstract      = {Context
Maintainability has become one of the most essential attributes of software quality, as software maintenance has shown to be one of the most costly and time-consuming tasks of software development. Many studies reveal that maintainability is not often a major consideration in requirements and design stages, and software maintenance costs may be reduced by a more controlled design early in the software life cycle. Several problem factors have been identified as harmful for software maintainability, such as lack of upfront consideration of proper modularity choices. In that sense, the presence of crosscutting concerns is one of such modularity anomalies that possibly exert negative effects on software maintainability. However, to the date there is little or no knowledge about how characteristics of crosscutting concerns, observable in early artefacts, are correlated with maintainability.
Objective
In this setting, this paper introduces an empirical analysis where the correlation between crosscutting properties and two ISO/IEC 9126 maintainability attributes, namely changeability and stability, is presented.
Method
This correlation is based on the utilization of a set of concern metrics that allows the quantification of crosscutting, scattering and tangling.
Results
Our study confirms that a change in a crosscutting concern is more difficult to be accomplished and that artefacts addressing crosscutting concerns are found to be less stable later as the system evolves. Moreover, our empirical analysis reveals that crosscutting properties introduce non-syntactic dependencies between software artefacts, thereby decreasing the quality of software in terms of changeability and stability as well. These subtle dependencies cannot be easily detected without the use of concern metrics.
Conclusion
The correlation provides evidence that the presence of certain crosscutting properties negatively affects to changeability and stability. The whole analysis is performed using as target cases three software product lines, where maintainability properties are of upmost importance not only for individual products but also for the core architecture of the product line.},
  comment       = {27},
  doi           = {https://doi.org/10.1016/j.infsof.2011.09.003},
  keywords      = {Requirements engineering, Product lines, Crosscutting, Concern metrics, Maintainability, Stability},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584911001923},
}

@Article{Trinidad2008a,
  author        = {P. Trinidad and D. Benavides and A. DurÃ¡n and A. Ruiz-CortÃ©s and M. Toro},
  title         = {Automated error analysis for the agilization of feature modeling},
  journal       = {Journal of Systems and Software},
  year          = {2008},
  volume        = {81},
  number        = {6},
  pages         = {883 - 896},
  issn          = {0164-1212},
  note          = {Agile Product Line Engineering},
  __markedentry = {[mac:]},
  abstract      = {Software Product Lines (SPL) and agile methods share the common goal of rapidly developing high-quality software. Although they follow different approaches to achieve it, some synergies can be found between them by (i) applying agile techniques to SPL activities so SPL development becomes more agile; and (ii) tailoring agile methodologies to support the development of SPL. Both options require an intensive use of feature models, which are usually strongly affected by changes on requirements. Changing large-scale feature models as a consequence of changes on requirements is a well-known error-prone activity. Since one of the objectives of agile methods is a rapid response to changes in requirements, it is essential an automated error analysis support in order to make SPL development more agile and to produce error-free feature models. As a contribution to find the intended synergies, this article sets the basis to provide an automated support to feature model error analysis by means of a framework which is organized in three levels: a feature model level, where the problem of error treatment is described; a diagnosis level, where an abstract solution that relies on Reiterâ€™s theory of diagnosis is proposed; and an implementation level, where the abstract solution is implemented by using Constraint Satisfaction Problems (CSP). To show an application of our proposal, a real case study is presented where the Feature-Driven Development (FDD) methodology is adapted to develop an SPL. Current proposals on error analysis are also studied and a comparison among them and our proposal is provided. Lastly, the support of new kinds of errors and different implementation levels for the proposed framework are proposed as the focus of our future work.},
  comment       = {14},
  doi           = {https://doi.org/10.1016/j.jss.2007.10.030},
  keywords      = {Feature models, Agile methods, Error analysis, Theory of diagnosis, Constraint programming},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121207002543},
}

@Article{Tanhaei2016a,
  author        = {Mohammad Tanhaei and Jafar Habibi and Seyed-Hassan Mirian-Hosseinabadi},
  title         = {Automating feature model refactoring: A Model transformation approach},
  journal       = {Information and Software Technology},
  year          = {2016},
  volume        = {80},
  pages         = {138 - 157},
  issn          = {0950-5849},
  __markedentry = {[mac:]},
  abstract      = {Context: Feature model is an appropriate and indispensable tool for modeling similarities and differences among products of the Software Product Line (SPL). It not only exposes the validity of the productsâ€™ configurations in an SPL but also changes in the course of time to support new requirements of the SPL. Modifications made on the feature model in the course of time raise a number of issues. Useless enlargements of the feature model, the existence of dead features, and violated constraints in the feature model are some of the key problems that make its maintenance difficult. Objective: The initial approach to dealing with the above-mentioned problems and improving maintainability of the feature model is refactoring. Refactoring modifies software artifacts in a way that their externally visible behavior does not change. Method: We introduce a method for defining refactoring rules and executing them on the feature model. We use the ATL model transformation language to define the refactoring rules. Moreover, we provide an Alloy model to check the feature model and the safety of the refactorings that are performed on it. Results: In this research, we propose a safe framework for refactoring a feature model. This framework enables users to perform automatic and semi-automatic refactoring on the feature model. Conclusions: Automated tool support for refactoring is a key issue for adopting approaches such as utilizing feature models and integrating them into the software development process of companies. In this work, we define some of the important refactoring rules on the feature model and provide tools that enable users to add new rules using the ATL M2M language. Our framework assesses the correctness of the refactorings using the Alloy language.},
  comment       = {20},
  doi           = {https://doi.org/10.1016/j.infsof.2016.08.011},
  keywords      = {Feature model refactoring, Model transformation & refactoring},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584916301422},
}

@Article{Kulesza2013,
  author        = {UirÃ¡ Kulesza and SÃ©rgio Soares and Christina Chavez and Fernando Castor and Paulo Borba and Carlos Lucena and Paulo Masiero and Claudio Santâ€™Anna and Fabiano Ferrari and Vander Alves and Roberta Coelho and Eduardo Figueiredo and Paulo F. Pires and FlÃ¡via Delicato and Eduardo Piveta and Carla Silva and Valter Camargo and Rosana Braga and Julio Leite and OtÃ¡vio Lemos and Nabor MendonÃ§a and Thais Batista and Rodrigo BonifÃ¡cio and NÃ©lio Cacho and Lyrene Silva and Arndt von Staa and FÃ¡bio Silveira and Marco TÃºlio Valente and Fernanda Alencar and Jaelson Castro and Ricardo Ramos and Rosangela Penteado and CecÃ­lia Rubira},
  title         = {The crosscutting impact of the AOSD Brazilian research community},
  journal       = {Journal of Systems and Software},
  year          = {2013},
  volume        = {86},
  number        = {4},
  pages         = {905 - 933},
  issn          = {0164-1212},
  note          = {SI : Software Engineering in Brazil: Retrospective and Prospective Views},
  __markedentry = {[mac:]},
  abstract      = {Background
Aspect-Oriented Software Development (AOSD) is a paradigm that promotes advanced separation of concerns and modularity throughout the software development lifecycle, with a distinctive emphasis on modular structures that cut across traditional abstraction boundaries. In the last 15 years, research on AOSD has boosted around the world. The AOSD-BR research community (AOSD-BR stands for AOSD in Brazil) emerged in the last decade, and has provided different contributions in a variety of topics. However, despite some evidence in terms of the number and quality of its outcomes, there is no organized characterization of the AOSD-BR community that positions it against the international AOSD Research community and the Software Engineering Research community in Brazil.
Aims
In this paper, our main goal is to characterize the AOSD-BR community with respect to the research developed in the last decade, confronting it with the AOSD international community and the Brazilian Software Engineering community.
Method
Data collection, validation and analysis were performed in collaboration with several researchers of the AOSD-BR community. The characterization was presented from three different perspectives: (i) a historical timeline of events and main milestones achieved by the community; (ii) an overview of the research developed by the community, in terms of key challenges, open issues and related work; and (iii) an analysis on the impact of the AOSD-BR community outcomes in terms of well-known indicators, such as number of papers and number of citations.
Results
Our analysis showed that the AOSD-BR community has impacted both the international AOSD Research community and the Software Engineering Research community in Brazil.},
  comment       = {29},
  doi           = {https://doi.org/10.1016/j.jss.2012.08.031},
  keywords      = {Aspect-Oriented Software Development, Modularity, Research impact},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121212002427},
}

@Article{Angelov2012,
  author        = {Samuil Angelov and Paul Grefen and Danny Greefhorst},
  title         = {A framework for analysis and design of software reference architectures},
  journal       = {Information and Software Technology},
  year          = {2012},
  volume        = {54},
  number        = {4},
  pages         = {417 - 431},
  issn          = {0950-5849},
  __markedentry = {[mac:]},
  abstract      = {Context
A software reference architecture is a generic architecture for a class of systems that is used as a foundation for the design of concrete architectures from this class. The generic nature of reference architectures leads to a less defined architecture design and application contexts, which makes the architecture goal definition and architecture design non-trivial steps, rooted in uncertainty.
Objective
The paper presents a structured and comprehensive study on the congruence between context, goals, and design of software reference architectures. It proposes a tool for the design of congruent reference architectures and for the analysis of the level of congruence of existing reference architectures.
Method
We define a framework for congruent reference architectures. The framework is based on state of the art results from literature and practice. We validate our framework and its quality as analytical tool by applying it for the analysis of 24 reference architectures. The conclusions from our analysis are compared to the opinions of experts on these reference architectures documented in literature and dedicated communication.
Results
Our framework consists of a multi-dimensional classification space and of five types of reference architectures that are formed by combining specific values from the multi-dimensional classification space. Reference architectures that can be classified in one of these types have better chances to become a success. The validation of our framework confirms its quality as a tool for the analysis of the congruence of software reference architectures.
Conclusion
This paper facilitates software architects and scientists in the inception, design, and application of congruent software reference architectures. The application of the tool improves the chance for success of a reference architecture.},
  comment       = {15},
  doi           = {https://doi.org/10.1016/j.infsof.2011.11.009},
  keywords      = {Software reference architecture, Software domain architecture, Software architecture design, Software product line architecture},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584911002333},
}

@Article{Chen2009,
  author        = {Chung-Yang Chen and Pei-Chi Chen},
  title         = {A holistic approach to managing software change impact},
  journal       = {Journal of Systems and Software},
  year          = {2009},
  volume        = {82},
  number        = {12},
  pages         = {2051 - 2067},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {Change is inevitable in the software product lifecycle. When a software change occurs, all of the stakeholders and related artifacts should be considered in determining the success of the change action in a collaborative development environment such as JAD (joint application development). In this regard, current implementation-based or homogeneous impact analyses are insufficient; therefore, this paper presents a holistic approach to change impact analysis in handling not only software contents but also other items such as requirements, documents and data. This approach characterizes product contents and relates heterogeneous items by using attributes and linkages. It also uses an object-oriented propagation mechanism to handle dynamic looping in determining the impact of changes. A prototype, EPIC, was built to realize this approach and these concepts. A walkthrough example is provided in order to verify the work of the proposed approach. An empirical study is presented to discuss the benefits of the proposed approach and the application of EPIC in a software company. Lessons learned from the case study and improvement issues of the proposed approach and the tool are also discussed.},
  comment       = {17},
  doi           = {https://doi.org/10.1016/j.jss.2009.06.052},
  keywords      = {Software change management, Object technology, Holistic approach, Collaborative development},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121209001654},
}

@Article{Basso2016,
  author        = {FÃ¡bio Paulo Basso and Raquel Mainardi Pillat and Toacy Cavalcante Oliveira and Fabricia Roos-Frantz and Rafael Z. Frantz},
  title         = {Automated design of multi-layered web information systems},
  journal       = {Journal of Systems and Software},
  year          = {2016},
  volume        = {117},
  pages         = {612 - 637},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {In the development of web information systems, design tasks are commonly used in approaches for Model-Driven Web Engineering (MDWE) to represent models. To generate fully implemented prototypes, these models require a rich representation of the semantics for actions (e.g., database persistence operations). In the development of some use case scenarios for the multi-layered development of web information systems, these design tasks may consume weeks of work even for experienced designers. The literature pointed out that the impossibility for executing a software project with short iterations hampers the adoption of some approaches for design in some contexts, such as start-up companies. A possible solution to introduce design tasks in short iterations is the use of automated design techniques, which assist the production of models by means of transformation tasks and refinements. This paper details our methodology for MDWE, which is supported by automated design techniques strictly associated with use case patterns of type CRUD. The novelty relies on iterations that are possible for execution with short time-scales. This is a benefit from automated design techniques not observed in MDWE approaches based on manual design tasks. We also report on previous experiences and address open questions relevant for the theory and practice of MDWE.},
  comment       = {26},
  doi           = {https://doi.org/10.1016/j.jss.2016.04.060},
  keywords      = {Model-driven web engineering, Rapid application prototype, Domain-specific language, Prototyping, Automated design, Mockup, Experience report},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121216300358},
}

@Article{A.G.Saraiva2015,
  author   = {Juliana de A.G. Saraiva and Micael S. de FranÃ§a and SÃ©rgio C.B. Soares and Fernando J.C.L. Filho and RenataÂ M.C.R. de Souza},
  title    = {Classifying metrics for assessing Object-Oriented Software Maintainability: A family of metricsâ€™ catalogs},
  journal  = {Journal of Systems and Software},
  year     = {2015},
  volume   = {103},
  pages    = {85 - 101},
  issn     = {0164-1212},
  abstract = {Object-Oriented Programming is one of the most used paradigms. Complementarily, the software maintainability is considered a software attribute playing an important role in quality level. In this context, Object-Oriented Software Maintainability (OOSM) has been studied through years, and many researchers have proposed a large number of metrics to measure it. Consequently, the decision-making process about which metrics can be adopted in experiments on OOSM is a hard task. Therefore, a metricsâ€™ categorization has been proposed to facilitate this process. As result, 7 categories and 17 subcategories were identified. These categories represent the scenarios of OOSM metrics adoption, and a family of OOSM metrics catalog was generated based on the selection of a metricsâ€™ categorization. Additionally, a quasi-experiment was conducted to check the coverage index of the catalogs generated using our approach over the catalogs suggested by experts. 90% of coverage was obtained with 99% of confidential level using the Wilcoxon Test. Complementarily, a survey was conducted to check the expertsâ€™ opinion about the catalog generated by the portal when they were compared by the catalogs suggested by them. Therefore, this evaluation can be the first evidences of the usefulness of the family of the catalogs based on the metricsâ€™ categorization.},
  comment  = {17},
  doi      = {https://doi.org/10.1016/j.jss.2015.01.014},
  keywords = {Software maintainability, Metrics, Object-Oriented Software Development},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121215000126},
}

@Article{Huysegoms2013,
  author        = {Tom Huysegoms and Monique Snoeck and Guido Dedene and Antoon Goderis and Frank Stumpe},
  title         = {Visualizing Variability Management in Requirements Engineering through Formal Concept Analysis},
  journal       = {Procedia Technology},
  year          = {2013},
  volume        = {9},
  pages         = {189 - 199},
  issn          = {2212-0173},
  note          = {CENTERIS 2013 - Conference on ENTERprise Information Systems / ProjMAN 2013 - International Conference on Project MANagement/ HCIST 2013 - International Conference on Health and Social Care Information Systems and Technologies},
  __markedentry = {[mac:]},
  abstract      = {While research on the visualization and documentation of variability in software artefacts by means of e.g. feature diagrams is well established, most of these documentation methods in the field of variability management assume the presence of variability as a given fact. The decision whether variability within the requirements should actually give rise to variability in the envisaged software artefact is often taken unconsciously and as a result techniques to visualize and document the amount, the structure and the impact of requirements evolution on variability are scarce. This paper provides a real life proof of concept that formal concept analysis (FCA) can be used for the visualization and documentation of variability re- lated decisions during (early) requirements engineering. FCA is used in a real-life case study to check the usability of FCA as a visualization method to support variability management during requirements engineering. The real-life case study also provides initial proof that useful documentation can be obtained by representing the requirements in a FCA concept lattice.},
  comment       = {11},
  doi           = {https://doi.org/10.1016/j.protcy.2013.12.021},
  keywords      = {Requirements management, variability management, formal concept analysis, harmonization, variabilization},
  url           = {http://www.sciencedirect.com/science/article/pii/S2212017313001758},
}

@Article{Dey2017,
  author        = {Sangeeta Dey and Seok-Won Lee},
  title         = {REASSURE: Requirements elicitation for adaptive socio-technical systems using repertory grid},
  journal       = {Information and Software Technology},
  year          = {2017},
  volume        = {87},
  pages         = {160 - 179},
  issn          = {0950-5849},
  __markedentry = {[mac:]},
  abstract      = {Context
Socio-technical systems are expected to understand the dynamics of the execution environment and behave accordingly. Significant work has been done on formalizing and modeling requirements of such adaptive systems. However, not enough attention is paid on eliciting requirements from users and introducing flexibility in the system behavior at an early phase of requirements engineering. Most of the work is based on an assumption that general usersâ€™ cognitive level would be able to support the inherent complexity of variability acquisition.
Objective
Our main focus is on providing help to the users with ordinary cognitive level to express their expectations from the complex system considering various contexts. This work also helps the designers to explore the design variability based on the general usersâ€™ preferences.
Method
We explore the idea of using a cognitive technique Repertory Grid (RG) to acquire knowledge from users and experts along multiple dimensions of problem and design space. We propose REASSURE methodology which guides requirements engineers to explore the intentional and design variability in an organized way. We also provide a tool support to analyze the knowledge captured in multiple repertory grid files and detect potential conflicts in the intentional variability. Finally, we evaluate the proposed idea by performing an empirical study using smart home system domain.
Results
The result of our study shows that a greater number of requirements can be elicited after applying our approach. With the help of the provided tool support, it is even possible to detect a greater number of conflicts in userâ€™s requirements than the traditional practices.
Conclusion
We envision RG as a technique to filter design options based on the intentional variability in various contexts. The promising results of empirical study open up new research questions: â€œhow to elicit requirements from multiple stakeholders and reach consensus for multi-dimensional problem domainâ€.},
  comment       = {20},
  doi           = {https://doi.org/10.1016/j.infsof.2017.03.004},
  keywords      = {Requirements elicitation, Adaptive systems, Socio-technical systems, Repertory grid},
  url           = {http://www.sciencedirect.com/science/article/pii/S095058491730229X},
}

@Article{2012,
  author   = {Fernanda dâ€™Amorim and Paulo Borba},
  title    = {Modularity analysis of use case implementations},
  journal  = {Journal of Systems and Software},
  year     = {2012},
  volume   = {85},
  number   = {4},
  pages    = {1012 - 1027},
  issn     = {0164-1212},
  abstract = {A component-based decomposition can result in implementations having use cases code tangled with other concerns and scattered across components. Modularity mechanisms such as aspects, mixins, and virtual classes have been proposed to address this kind of problem. One can use such mechanisms to group together code related to a single use case. This paper quantitatively analyzes the impact of this kind of use case modularization. We apply one specific technique, aspect oriented programming, to modularize the use case implementations of two information systems that conform to the layered architecture pattern. We extract traditional and contemporary metrics â€“ including cohesion, coupling, and separation of concerns â€“ to analyze modularity in terms of quality attributes such as changeability, support for independent development, and pluggability. Our findings indicate that the results of a given modularity analysis depend on other factors beyond the chosen system, metrics, and the applied modularity technique.},
  comment  = {16},
  doi      = {https://doi.org/10.1016/j.jss.2011.11.1025},
  keywords = {Modularity, Use cases, Aspect-oriented programming, Empirical software engineering},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121211002950},
}

@Article{Gomez2014,
  author        = {Abel GÃ³mez and M. Carmen PenadÃ©s and JosÃ© H. CanÃ³s and Marcos R.S. Borges and Manuel Llavador},
  title         = {A framework for variable content document generation with multiple actors},
  journal       = {Information and Software Technology},
  year          = {2014},
  volume        = {56},
  number        = {9},
  pages         = {1101 - 1121},
  issn          = {0950-5849},
  note          = {Special Sections from â€œAsia-Pacific Software Engineering Conference (APSEC), 2012â€ and â€œ Software Product Line conference (SPLC), 2012â€},
  __markedentry = {[mac:]},
  abstract      = {Context
Advances in customization have highlighted the need for tools supporting variable content document management and generation in many domains. Current tools allow the generation of highly customized documents that are variable in both content and layout. However, most frameworks are technology-oriented, and their use requires advanced skills in implementation-related tools, which means their use by end users (i.e. document designers) is severely limited.
Objective
Starting from past and current trends for customized document authoring, our goal is to provide a document generation alternative in which variants are specified at a high level of abstraction and content reuse can be maximized in high variability scenarios.
Method
Based on our experience in Document Engineering, we identified areas in the variable content document management and generation field open to further improvement. We first classified the primary sources of variability in document composition processes and then developed a methodology, which we called DPL â€“ based on Software Product Lines principles â€“ to support document generation in high variability scenarios.
Results
In order to validate the applicability of our methodology we implemented a tool â€“ DPLfw â€“ to carry out DPL processes. After using this in different scenarios, we compared our proposal with other state-of-the-art tools for variable content document management and generation.
Conclusion
The DPLfw showed a good capacity for the automatic generation of variable content documents equal to or in some cases surpassing other currently available approaches. To the best of our knowledge, DPLfw is the only framework that combines variable content and document workflow facilities, easing the generation of variable content documents in which multiple actors play different roles.},
  comment       = {21},
  doi           = {https://doi.org/10.1016/j.infsof.2013.12.006},
  keywords      = {Variable data printing, Document product line, Feature modeling, Model driven engineering, Document generation, Document workflow},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584913002358},
}

@Article{Aleem2016,
  author        = {Saiqa Aleem and Luiz Fernando Capretz and Faheem Ahmed},
  title         = {A Digital Game Maturity Model (DGMM)},
  journal       = {Entertainment Computing},
  year          = {2016},
  volume        = {17},
  pages         = {55 - 73},
  issn          = {1875-9521},
  __markedentry = {[mac:]},
  abstract      = {Game development is an interdisciplinary concept that embraces artistic, software engineering, management, and business disciplines. This research facilitates a better understanding of important dimensions of digital game development methodology. Game development is considered as one of the most complex tasks in software engineering. The increased popularity of digital games, the challenges faced by game development organizations in developing quality games, and high competition in the digital game industry demand a game development maturity assessment. Consequently, this study presents a Digital Game Maturity Model to evaluate the current development methodology in an organization. The framework of this model consists of assessment questionnaires, a performance scale, and a rating method. The main goal of the questionnaires is to collect information about current processes and practices. In general, this research contributes towards formulating a comprehensive and unified strategy for game development maturity evaluation. Two case studies were conducted and their assessment results reported. These demonstrate the level of maturity of current development practices in two organizations.},
  comment       = {19},
  doi           = {https://doi.org/10.1016/j.entcom.2016.08.004},
  keywords      = {Software game, Game performance, Video game, Online game, Process assessment, Software process improvement, Game development methodology},
  url           = {http://www.sciencedirect.com/science/article/pii/S1875952116300246},
}

@Article{Yu2015,
  author        = {Jian Yu and Quan Z. Sheng and Joshua K.Y. Swee and Jun Han and Chengfei Liu and Talal H. Noor},
  title         = {Model-driven development of adaptive web service processes with aspects and rules},
  journal       = {Journal of Computer and System Sciences},
  year          = {2015},
  volume        = {81},
  number        = {3},
  pages         = {533 - 552},
  issn          = {0022-0000},
  note          = {Special Issue on selected papers from the 4th International Conference on Ambient Systems, Networks and Technologies (ANT 2013)},
  __markedentry = {[mac:]},
  abstract      = {Modern software systems are frequently required to be adaptive in order to cope with constant changes. Unfortunately, service-oriented systems built with WS-BPEL are still too rigid. In this paper, we propose a novel model-driven approach to supporting the development of dynamically adaptive WS-BPEL based systems. We model the system functionality with two distinct but highly correlated parts: a stable part called the base model describing the flow logic aspect and a volatile part called the variable model describing the decision logic aspect. We develop an aspect-oriented method to weave the base model and the variable model together so that runtime changes can be applied to the variable model without affecting the base model. A model-driven platform has been implemented to support the development of adaptive WS-BPEL processes. In-lab experiments show that our approach has low performance overhead. A real-life case study also validates the applicability of our approach.},
  comment       = {20},
  doi           = {https://doi.org/10.1016/j.jcss.2014.11.008},
  keywords      = {Web services, Adaptive systems, Model-driven development, Aspect-oriented methodology, Design tools and techniques},
  url           = {http://www.sciencedirect.com/science/article/pii/S0022000014001494},
}

@Article{Asadi2014a,
  author        = {Mohsen Asadi and Bardia Mohabbati and Gerd GrÃ¶ner and Dragan Gasevic},
  title         = {Development and validation of customized process models},
  journal       = {Journal of Systems and Software},
  year          = {2014},
  volume        = {96},
  pages         = {73 - 92},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {Configurable reference process models encompass common and variable processes of organizations from different business domains. These reference process models are designed and reused to guide and derive customized business processes according to the requirements of stakeholders. The customization process is generally initiated by a configuration step, selecting a subset of the reference process model. Configuration is followed by a customization step, which assumes adapting or extending the configured business process based on the specific or unforeseen requirements. Hence, it is crucial to validate the correctness and compliance of the final customized business process with respect to the patterns and business constraints that are specified in the reference model. In this paper, we firstly introduce a technique to develop a customized process model and then present a set of identified inconsistency patterns that may happen during the configuration of a reference model and the customization of configured process models. Furthermore, we describe our proposed approach including formal representations and algorithms that provide logical reasoning and enable automatic inconsistency detection by leveraging description logic. In order to explore the scalability of the approach, we designed the experiments with various process models sizes and inconsistency distributions. The results of the experiments revealed the scalability of our approach with large size process models (500 activities).},
  comment       = {20},
  doi           = {https://doi.org/10.1016/j.jss.2014.05.063},
  keywords      = {Reference process models, Feature models, Description logics},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121214001344},
}

@Article{Apel2013,
  author        = {Sven Apel and Alexander von Rhein and Thomas ThÃ¼m and Christian KÃ¤stner},
  title         = {Feature-interaction detection based on feature-based specifications},
  journal       = {Computer Networks},
  year          = {2013},
  volume        = {57},
  number        = {12},
  pages         = {2399 - 2409},
  issn          = {1389-1286},
  note          = {Feature Interaction in Communications and Software Systems},
  __markedentry = {[mac:]},
  abstract      = {Formal specification and verification techniques have been used successfully to detect feature interactions. We investigate whether feature-based specifications can be used for this task. Feature-based specifications are a special class of specifications that aim at modularity in open-world, feature-oriented systems. The question we address is whether modularity of specifications impairs the ability to detect feature interactions, which cut across feature boundaries. In an exploratory study on 10 feature-oriented systems, we found that the majority of feature interactions could be detected based on feature-based specifications, but some specifications have not been modularized properly and require undesirable workarounds to modularization. Based on the study, we discuss the merits and limitations of feature-based specifications, as well as open issues and perspectives. A goal that underlies our work is to raise awareness of the importance and challenges of feature-based specification.},
  comment       = {11},
  doi           = {https://doi.org/10.1016/j.comnet.2013.02.025},
  keywords      = {Feature orientation, Feature interaction, Feature-based specification, Modularity, Software product lines},
  url           = {http://www.sciencedirect.com/science/article/pii/S1389128613001102},
}

@Article{Eichelberger2014,
  author        = {Holger Eichelberger and Klaus Schmid},
  title         = {Flexible resource monitoring of Java programs},
  journal       = {Journal of Systems and Software},
  year          = {2014},
  volume        = {93},
  pages         = {163 - 186},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {Monitoring resource consumptions is fundamental in software engineering, e.g., in validation of quality requirements, performance engineering, or adaptive software systems. However, resource monitoring does not come for free as it typically leads to overhead in the observed program. Minimizing this overhead and increasing the reliability of the monitored data is a major goal in realizing resource monitoring tools. Typically, this is achieved by limiting capabilities, e.g., supported resources, granularity of the monitoring focus, or runtime access to results. Thus, in practice often several approaches must be combined to obtain relevant information. We describe SPASS-meter, a novel resource monitoring approach for Java and Android Apps, which combines these conflicting capabilities with low overhead. SPASS-meter supports a large set of resources, flexible configuration of the monitoring scope even for user-defined semantic units (components), runtime analysis and online access to monitoring results in a platform-independent way. We discuss the concepts of SPASS-meter, its architecture, realization and validation, the latter in terms of case studies and an overhead analysis based on performance experiments with SPASS-meter, OpenCore and Kieker. SPASS-meter provides a detailed view of the runtime resource consumption at reasonable overhead of less than 3% processing power and 0.5% memory consumption in our experiments.},
  comment       = {24},
  doi           = {https://doi.org/10.1016/j.jss.2014.02.022},
  keywords      = {Resource monitoring, Software components, Performance engineering, Empirical analysis, Monitoring overhead, Java},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121214000533},
}

@Article{Alferez2017,
  author        = {GermÃ¡n H. AlfÃ©rez and Vicente Pelechano},
  title         = {Achieving autonomic Web service compositions with models at runtime},
  journal       = {Computers \& Electrical Engineering},
  year          = {2017},
  volume        = {63},
  pages         = {332 - 352},
  issn          = {0045-7906},
  __markedentry = {[mac:]},
  abstract      = {Several exceptional situations may arise in the complex, heterogeneous, and changing contexts where Web service operations run. For instance, a Web service operation may have greatly increased its execution time or may have become unavailable. The contribution of this article is to provide a tool-supported framework to guide autonomic adjustments of context-aware service compositions using models at runtime. During execution, when problematic events arise in the context, models are used by an autonomic architecture to guide changes of the service composition. Under the closed-world assumption, the possible context events are fully known at design time. Nevertheless, it is difficult to foresee all the possible situations arising in uncertain contexts where service compositions run. Therefore, the proposed framework also covers the dynamic evolution of service compositions to deal with unexpected events in the open world. An evaluation demonstrates that our framework is efficient during dynamic adjustments.},
  comment       = {21},
  doi           = {https://doi.org/10.1016/j.compeleceng.2017.08.004},
  keywords      = {Web service compositions, Models at runtime, Autonomic computing, Dynamic software product lines, Dynamic adaptation, Dynamic evolution},
  url           = {http://www.sciencedirect.com/science/article/pii/S0045790617324965},
}

@Article{Mizouni2014,
  author        = {Rabeb Mizouni and Mohammad Abu Matar and Zaid Al Mahmoud and Salwa Alzahmi and Aziz Salah},
  title         = {A framework for context-aware self-adaptive mobile applications SPL},
  journal       = {Expert Systems with Applications},
  year          = {2014},
  volume        = {41},
  number        = {16},
  pages         = {7549 - 7564},
  issn          = {0957-4174},
  __markedentry = {[mac:]},
  abstract      = {Mobile Applications are rapidly emerging as a convenient medium for using a variety of services. Over time and with the high penetration of smartphones in society, self-adaptation has become an essential capability required by mobile application users. In an ideal scenario, an application is required to adjust its behavior according to the current context of its use. This raises the challenge in mobile computing towards the design and development of applications that sense and react to contextual changes to provide a value-added user experience. In its general sense, context information can relate to the environment, the user, or the device status. In this paper, we propose a novel framework for building context aware and adaptive mobile applications. Based on feature modeling and Software Product Lines (SPL) concepts, this framework guides the modeling of adaptability at design time and supports context awareness and adaptability at runtime. In the core of the approach, is a feature meta-model that incorporates, in addition to SPL concepts, application feature priorities to drive the adaptability. A tool, based on that feature model, is presented to model the mobile application features and to derive the SPL members. A mobile framework, built on top of OSGI framework to dynamically adapt the application at runtime is also described.},
  comment       = {16},
  doi           = {https://doi.org/10.1016/j.eswa.2014.05.049},
  keywords      = {Mobile devices, SPL, Multi-view variability model, Feature priority, Runtime adaptability},
  url           = {http://www.sciencedirect.com/science/article/pii/S0957417414003364},
}

@Article{Bertolino2015,
  author        = {Antonia Bertolino and Said Daoudagh and Donia El Kateb and Christopher Henard and Yves Le Traon and Francesca Lonetti and Eda Marchetti and Tejeddine Mouelhi and Mike Papadakis},
  title         = {Similarity testing for access control},
  journal       = {Information and Software Technology},
  year          = {2015},
  volume        = {58},
  pages         = {355 - 372},
  issn          = {0950-5849},
  __markedentry = {[mac:]},
  abstract      = {Context
Access control is among the most important security mechanisms, and XACML is the de facto standard for specifying, storing and deploying access control policies. Since it is critical that enforced policies are correct, policy testing must be performed in an effective way to identify potential security flaws and bugs. In practice, exhaustive testing is impossible due to budget constraints. Therefore the tests need to be prioritized so that resources are focused on their most relevant subset.
Objective
This paper tackles the issue of access control test prioritization. It proposes a new approach for access control test prioritization that relies on similarity.
Method
The approach has been applied to several policies and the results have been compared to random prioritization (as a baseline). To assess the different prioritization criteria, we use mutation analysis and compute the mutation scores reached by each criterion. This helps assessing the rate of fault detection.
Results
The empirical results indicate that our proposed approach is effective and its rate of fault detection is higher than that of random prioritization.
Conclusion
We conclude that prioritization of access control test cases can be usefully based on similarity criteria.},
  comment       = {18},
  doi           = {https://doi.org/10.1016/j.infsof.2014.07.003},
  keywords      = {Similarity, Test prioritization, Security policies},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584914001578},
}

@Article{Jordan2015,
  author        = {Howell Jordan and Goetz Botterweck and John Noll and Andrew Butterfield and Rem Collier},
  title         = {A feature model of actor, agent, functional, object, and procedural programming languages},
  journal       = {Science of Computer Programming},
  year          = {2015},
  volume        = {98},
  pages         = {120 - 139},
  issn          = {0167-6423},
  note          = {Special Issue on Programming Based on Actors, Agents and Decentralized Control},
  __markedentry = {[mac:]},
  abstract      = {The number of programming languages is large and steadily increasing. However, little structured information and empirical evidence is available to help software engineers assess the suitability of a language for a particular development project or software architecture. We argue that these shortages are partly due to a lack of high-level, objective programming language feature assessment criteria: existing advice to practitioners is often based on ill-defined notions of â€˜paradigmsâ€™ [3, p. xiii] and â€˜orientationâ€™, while researchers lack a shared common basis for generalisation and synthesis of empirical results. This paper presents a feature model constructed from the programmer's perspective, which can be used to precisely compare general-purpose programming languages in the actor-oriented, agent-oriented, functional, object-oriented, and procedural categories. The feature model is derived from the existing literature on general concepts of programming, and validated with concrete mappings of well-known languages in each of these categories. The model is intended to act as a tool for both practitioners and researchers, to facilitate both further high-level comparative studies of programming languages, and detailed investigations of feature usage and efficacy in specific development contexts.},
  comment       = {19},
  doi           = {https://doi.org/10.1016/j.scico.2014.02.009},
  keywords      = {Programming languages, Programming language constructs, Agent-oriented programming, Functional programming, Object-oriented programming},
  url           = {http://www.sciencedirect.com/science/article/pii/S0167642314000501},
}

@Article{Groener2013,
  author        = {Gerd GrÃ¶ner and Marko BoÅ¡koviÄ‡ and Fernando Silva Parreiras and Dragan GaÅ¡eviÄ‡},
  title         = {Modeling and validation of business process families},
  journal       = {Information Systems},
  year          = {2013},
  volume        = {38},
  number        = {5},
  pages         = {709 - 726},
  issn          = {0306-4379},
  __markedentry = {[mac:]},
  abstract      = {Process modeling is an expensive task that needs to encompass requirements of different stakeholders, assure compliance with different standards, and enable the flexible adaptivity to newly emerging requirements in today's dynamic global market. Identifying reusability of process models is a promising direction towards reducing the costs of process modeling. Recent research has offered several solutions. Such solutions promote effective and formally sound methods for variability modeling and configuration management. However, ensuring behavioral validity of reused process models with respect to the original process models (often referred to as reference process models) is still an open research challenge. To address this challenge, in this paper, we propose the notion of business process families by building upon the well-known software engineering disciplineâ€”software product line engineering. Business process families comprise (i) a variability modeling perspective, (ii) a process model template (or reference model), and (iii) mappings between (i) and (ii). For business process families, we propose a correct validation algorithm ensuring that each member of a business process family adheres to the core intended behavior that is specified in the process model template. The proposed validation approach is based on the use of Description Logics, variability is represented by using the well-known Feature Models and behavior of process models is considered in terms of control flow patterns. The paper also reports on the experience gained in two external trial cases and results obtained by measuring the tractability of the implementation of the proposed validation approach.},
  comment       = {18},
  doi           = {https://doi.org/10.1016/j.is.2012.11.010},
  keywords      = {Business process families, Control flow relations, Validation, Process model variability, Process model configuration},
  url           = {http://www.sciencedirect.com/science/article/pii/S0306437912001524},
}

@Article{Shabah2015,
  author        = {Abdo Shabah},
  title         = {HUMANIT3D for Disaster Response: An Assessment of Mass Customization on Organizational Performance Under Turbulent Environments},
  journal       = {Procedia Engineering},
  year          = {2015},
  volume        = {107},
  pages         = {223 - 236},
  issn          = {1877-7058},
  note          = {Humanitarian Technology: Science, Systems and Global Impact 2015, HumTech2015},
  __markedentry = {[mac:]},
  abstract      = {Mass customization aims to produce customized goods (allowing economies of scope) at lower cost (to achieve economies of scale) using multiple strategies (modularization and postponement). Mass customization in software and hardware design is becoming more popular for users and researchers. Through a simulation experiment of emergency response organizations under turbulent environment, we aim to compare standardization and mass customization of services and assess the impact of different forms of mass customization (early and late postponement) on performance, quality and consumer satisfaction, on the use of modular dynamic ecosystem based on HUMANIT3D, an integrated collaborative ecosystem composed of UAV management system, data collection system, and 3D Geographic Information System. Our hypothesis is that mass customization performs better and achieves better quality in turbulent environment than standardization, but only when using early postponement strategies. Using mixed methods study, we try to confirm our hypothesis.},
  comment       = {14},
  doi           = {https://doi.org/10.1016/j.proeng.2015.06.077},
  keywords      = {mass customization, postponement, experiment, performance, quality, satisfaction, 3D GIS, UAV, mobile ecosystem},
  url           = {http://www.sciencedirect.com/science/article/pii/S1877705815010309},
}

@Article{Ubayashi2013,
  author        = {Naoyasu Ubayashi and Shin Nakajima and Masayuki Hirayama},
  title         = {Context-dependent product line engineering with lightweight formal approaches},
  journal       = {Science of Computer Programming},
  year          = {2013},
  volume        = {78},
  number        = {12},
  pages         = {2331 - 2346},
  issn          = {0167-6423},
  note          = {Special Section on International Software Product Line Conference 2010 and Fundamentals of Software Engineering (selected papers of FSEN 2011)},
  __markedentry = {[mac:]},
  abstract      = {This paper proposes a new style of product line engineering methods. It focuses on constructing embedded systems that take into account the contexts such as the external physical environments. In current product line development projects, Feature Analysis is mainly conducted from the viewpoint of system configurations: how hardware and software components are configured to constitute a system. In most cases, contexts are not considered explicitly. As a result, unexpected and unfavorable behavior might emerge in a system if a developer does not recognize any possible conflicting combinations between the system and contexts. To deal with this problem, this paper provides the notion of a context-dependent product line, which is composed of the system and context lines. The former is obtained by analyzing a family of systems. The latter is obtained by analyzing features of contexts associated to the systems. The system and context lines contain reusable core assets. The configuration of selected system components and contexts can be formally checked at the specification level. In this paper, we show a development process that includes the creation of both product line assets as well as context assets.},
  comment       = {16},
  doi           = {https://doi.org/10.1016/j.scico.2012.06.006},
  keywords      = {Product line engineering, Context analysis, Formal methods},
  url           = {http://www.sciencedirect.com/science/article/pii/S0167642312001177},
}

@Article{Heider2010,
  author        = {Wolfgang Heider and Roman Froschauer and Paul GrÃ¼nbacher and Rick Rabiser and Deepak Dhungana},
  title         = {Simulating evolution in model-based product line engineering},
  journal       = {Information and Software Technology},
  year          = {2010},
  volume        = {52},
  number        = {7},
  pages         = {758 - 769},
  issn          = {0950-5849},
  __markedentry = {[mac:]},
  abstract      = {Context
Numerous approaches are available for modeling product lines and their variability. However, the long-term impacts of model-based development on maintenance effort and model complexity can hardly be investigated due to a lack of empirical data. Conducting empirical research in product line engineering is difficult as companies are typically reluctant to provide access to data from their product lines. Also, many benefits of product lines can be measured only in longitudinal studies, which are difficult to perform in most environments.
Objective
In this paper, we thus aim to explore the benefit of simulation to investigate the evolution of model-based product lines.
Method
We present a simulation approach for exploring the effects of product line evolution on model complexity and maintenance effort. Our simulation considers characteristics of product lines (e.g., size, dependencies in models) and we experiment with different evolution profiles (e.g., technical refactoring vs. placement of new products).
Results
We apply the approach in a simulation experiment that uses data from real-world product lines from the domain of industrial automation systems to demonstrate its feasibility.
Conclusion
Our results demonstrate that simulation contributes to understanding the effects of maintenance and evolution in model-based product lines.},
  comment       = {12},
  doi           = {https://doi.org/10.1016/j.infsof.2010.03.007},
  keywords      = {Product line engineering, Maintenance and evolution, Model-based development, Simulation, Industrial automation systems},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584910000479},
}

@Article{Cunha2016,
  author        = {JÃ¡come Cunha and JoÃ£o Paulo Fernandes and Pedro Martins and Jorge Mendes and Rui Pereira and JoÃ£o Saraiva},
  title         = {Evaluating refactorings for spreadsheet models},
  journal       = {Journal of Systems and Software},
  year          = {2016},
  volume        = {118},
  pages         = {234 - 250},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {Software refactoring is a well-known technique that provides transformations on software artifacts with the aim of improving their overall quality. We have previously proposed a catalog of refactorings for spreadsheet models expressed in the ClassSheets modeling language, which allows us to specify the business logic of a spreadsheet in an object-oriented fashion. Reasoning about spreadsheets at the model level enhances a model-driven spreadsheet environment where a ClassSheet model and its conforming instance (spreadsheet data) automatically co-evolves after applying a refactoring at the model level. Research motivation was to improve the model and its conforming instance: the spreadsheet data. In this paper we define such refactorings using previously proposed evolution steps for models and instances. We also present an empirical study we designed and conducted in order to confirm our original intuition that these refactorings have a positive impact on end-user productivity, both in terms of effectiveness and efficiency. The results are not only presented in terms of productivity changes between refactored and non-refactored scenarios, but also the overall user satisfaction, relevance, and experience. In almost all cases the refactorings improved end-users productivity. Moreover, in most cases users were more engaged with the refactored version of the spreadsheets they worked with.},
  comment       = {17},
  doi           = {https://doi.org/10.1016/j.jss.2016.04.043},
  keywords      = {Software refactoring, Model-driven engineering, Spreadsheets, Empirical study,},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121216300280},
}

@Article{Parizi2015,
  author        = {Reza Meimandi Parizi and Abdul Azim Abdul Ghani and Sai Peck Lee},
  title         = {Automated test generation technique for aspectual features in AspectJ},
  journal       = {Information and Software Technology},
  year          = {2015},
  volume        = {57},
  pages         = {463 - 493},
  issn          = {0950-5849},
  __markedentry = {[mac:]},
  abstract      = {Context
Aspect-oriented programming (AOP) has been promoted as a means for handling the modularization of software systems by raising the abstraction level and reducing the scattering and tangling of crosscutting concerns. Studies from literature have shown the usefulness and application of AOP across various fields of research and domains. Despite this, research shows that AOP is currently used in a cautious way due to its natural impact on testability and maintainability.
Objective
To realize the benefits of AOP and to increase its adoption, aspects developed using AOP should be subjected to automated testing. Automated testing, as one of the most pressing needs of the software industry to reduce both effort and costs in assuring correctness, is a delicate issue in testing aspect-oriented programs that still requires advancement and has a way to go before maturity.
Method
Previous attempts and studies in automated test generation process for aspect-oriented programs have been very limited. This paper proposes a rigorous automated test generation technique, called RAMBUTANS, with its tool support based on guided random testing for the AspectJ programs.
Results
The paper reports the results of a thorough empirical study of 9 AspectJ benchmark programs, including non-trivial and larger software, by means of mutation analysis to compare RAMBUTANS and the four existing automated AOP testing approaches for testing aspects in terms of fault detection effectiveness and test effort efficiency. The results of the experiment and statistical tests supplemented by effect size measures presented evidence of the effectiveness and efficiency of the proposed technique at 99% confidence level (i.e. p<0.01).
Conclusion
The study showed that the resulting randomized tests were reasonably good for AOP testing, thus the proposed technique could be worth using as an effective and efficient AOP-specific automated test generation technique.},
  comment       = {31},
  doi           = {https://doi.org/10.1016/j.infsof.2014.05.020},
  keywords      = {Software testing, Automated test generation, Testing tool, AspectJ, Empirical study},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584914001372},
}

@Article{Liu2017,
  author        = {Yuzhou Liu and Lei Liu and Huaxiao Liu and Xiaoyu Wang and Hongji Yang},
  title         = {Mining domain knowledge from app descriptions},
  journal       = {Journal of Systems and Software},
  year          = {2017},
  volume        = {133},
  pages         = {126 - 144},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {Domain analysis aims at gaining knowledge to a particular domain in the early stage of software development. A key challenge in domain analysis is to extract features automatically from related product artifacts. Compared with other kinds of artifacts, high volume of descriptions can be collected from App marketplaces (such as Google Play and Apple Store) easily when developing a new mobile application (App), so it is essential for the success of domain analysis to gain features and relationships from them using data analysis techniques. In this paper, we propose an approach to mine domain knowledge from App descriptions automatically, where the information of features in a single App description is firstly extracted and formally described by a Concern-based Description Model (CDM), which is based on predefined rules of feature extraction and a modified topic modeling method; then the overall knowledge in the domain is identified by classifying, clustering and merging the knowledge in the set of CDMs and topics, and the results are formalized by a Data-based Raw Domain Model (DRDM). Furthermore, we propose a quantified evaluation method for prioritizing the knowledge in DRDM. The proposed approach is validated by a series of experiments.},
  comment       = {19},
  doi           = {https://doi.org/10.1016/j.jss.2017.08.024},
  keywords      = {Domain analysis, Feature extraction, App descriptions, Data analysis},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121217301784},
}

@Article{Erdweg2015,
  author        = {Sebastian Erdweg and Tijs van der Storm and Markus VÃ¶lter and Laurence Tratt and Remi Bosman and William R. Cook and Albert Gerritsen and Angelo Hulshout and Steven Kelly and Alex Loh and GabriÃ«l Konat and Pedro J. Molina and Martin Palatnik and Risto Pohjonen and Eugen Schindler and Klemens Schindler and Riccardo Solmi and Vlad Vergu and Eelco Visser and Kevin van der Vlist and Guido Wachsmuth and Jimi van der Woning},
  title         = {Evaluating and comparing language workbenches: Existing results and benchmarks for the future},
  journal       = {Computer Languages, Systems \& Structures},
  year          = {2015},
  volume        = {44},
  pages         = {24 - 47},
  issn          = {1477-8424},
  note          = {Special issue on the 6th and 7th International Conference on Software Language Engineering (SLE 2013 and SLE 2014)},
  __markedentry = {[mac:]},
  abstract      = {Language workbenches are environments for simplifying the creation and use of computer languages. The annual Language Workbench Challenge (LWC) was launched in 2011 to allow the many academic and industrial researchers in this area an opportunity to quantitatively and qualitatively compare their approaches. We first describe all four LWCs to date, before focussing on the approaches used, and results generated, during the third LWC. We give various empirical data for ten approaches from the third LWC. We present a generic feature model within which the approaches can be understood and contrasted. Finally, based on our experiences of the existing LWCs, we propose a number of benchmark problems for future LWCs.},
  comment       = {24},
  doi           = {https://doi.org/10.1016/j.cl.2015.08.007},
  keywords      = {Language workbenches, Domain-specific languages, Questionnaire language, Survey, Benchmarks},
  url           = {http://www.sciencedirect.com/science/article/pii/S1477842415000573},
}

@Article{Tsai2016,
  author        = {Wei-Tek Tsai and Peide Zhong and Yinong Chen},
  title         = {Tenant-centric Sub-Tenancy Architecture in Software-as-a-Service},
  journal       = {CAAI Transactions on Intelligence Technology},
  year          = {2016},
  volume        = {1},
  number        = {2},
  pages         = {150 - 161},
  issn          = {2468-2322},
  __markedentry = {[mac:]},
  abstract      = {Multi-tenancy architecture (MTA) is often used in Software-as-a-Service (SaaS) and the central idea is that multiple tenant applications can be developed using components stored in the SaaS infrastructure. Recently, MTA has been extended to allow a tenant application to have its own sub-tenants, where the tenant application acts like a SaaS infrastructure. In other words, MTA is extended to STA (Sub-Tenancy Architecture). In STA, each tenant application needs not only to develop its own functionalities, but also to prepare an infrastructure to allow its sub-tenants to develop customized applications. This paper applies Crowdsourcing as the core to STA component in the development life cycle. In addition, to discovering adequate fit tenant developers or components to help build and compose new components, dynamic and static ranking models are proposed. Furthermore, rank computation architecture is presented to deal with the case when the number of tenants and components becomes huge. Finally, experiments are performed to demonstrate that the ranking models and the rank computation architecture work as design.},
  comment       = {12},
  doi           = {https://doi.org/10.1016/j.trit.2016.08.002},
  keywords      = {SaaS, MTA, STA, Tenant, Sub-tenant, Crowdsourcing, Ranking},
  url           = {http://www.sciencedirect.com/science/article/pii/S2468232216300154},
}

@Article{Colanzi2016,
  author        = {Thelma Elita Colanzi and Silvia Regina Vergilio},
  title         = {A feature-driven crossover operator for multi-objective and evolutionary optimization of product line architectures},
  journal       = {Journal of Systems and Software},
  year          = {2016},
  volume        = {121},
  pages         = {126 - 143},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {The optimization of a Product Line Architecture (PLA) design can be modeled as a multi-objective problem, influenced by many factors, such as feature modularization, extensibility and other design principles. Due to this it has been properly solved in the Search Based Software Engineering (SBSE) field. However, previous empirical studies optimized PLA design using the multi-objective and evolutionary algorithm NSGA-II, without applying one of the most important genetic operators: the crossover. To overcome this limitation, this paper presents a feature-driven crossover operator that aims at improving feature modularization in PLA design. The proposed operator was applied in two empirical studies using NSGA-II in comparison with another version of NSGA-II that uses only mutation operators. The results show the usefulness and applicability of the proposed operator. The NSGA-II version that applies the feature-driven crossover found a greater diversity of solutions (potential PLA designs), with higher feature-based cohesion, and less feature scattering and tangling.},
  comment       = {18},
  doi           = {https://doi.org/10.1016/j.jss.2016.02.026},
  keywords      = {Product line architecture design, Multi-objective genetic algorithm, Crossover operator, Empirical study},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121216000583},
}

@Article{Wnuk2013,
  author        = {Krzysztof Wnuk and Tony Gorschek and Showayb Zahda},
  title         = {Obsolete software requirements},
  journal       = {Information and Software Technology},
  year          = {2013},
  volume        = {55},
  number        = {6},
  pages         = {921 - 940},
  issn          = {0950-5849},
  __markedentry = {[mac:]},
  abstract      = {Context
Coping with rapid requirements change is crucial for staying competitive in the software business. Frequently changing customer needs and fierce competition are typical drivers of rapid requirements evolution resulting in requirements obsolescence even before project completion.
Objective
Although the obsolete requirements phenomenon and the implications of not addressing them are known, there is a lack of empirical research dedicated to understanding the nature of obsolete software requirements and their role in requirements management.
Method
In this paper, we report results from an empirical investigation with 219 respondents aimed at investigating the phenomenon of obsolete software requirements.
Results
Our results contain, but are not limited to, defining the phenomenon of obsolete software requirements, investigating how they are handled in industry today and their potential impact.
Conclusion
We conclude that obsolete software requirements constitute a significant challenge for companies developing software intensive products, in particular in large projects, and that companies rarely have processes for handling obsolete software requirements. Further, our results call for future research in creating automated methods for obsolete software requirements identification and management, methods that could enable efficient obsolete software requirements management in large projects.},
  comment       = {20},
  doi           = {https://doi.org/10.1016/j.infsof.2012.12.001},
  keywords      = {Requirements management, Obsolete requirements, Survey, Empirical study, Market driven requirements engineering, Change impact analysis},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584912002364},
}

@Article{Tibermacine2016,
  author        = {Chouki Tibermacine and Salah Sadou and Minh Tu Ton That and Christophe Dony},
  title         = {Software architecture constraint reuse-by-composition},
  journal       = {Future Generation Computer Systems},
  year          = {2016},
  volume        = {61},
  pages         = {37 - 53},
  issn          = {0167-739X},
  __markedentry = {[mac:]},
  abstract      = {Architecture constraints are specifications which enable developers to formalize design rules that architectures should respect, like the topological conditions of a given architecture pattern or style. These constraints can serve as a documentation to better understand an existing architecture description, or can serve as invariants that can be checked after the application of an architecture change to see whether design rules still hold. Like any specifications, architecture constraints are frequently subject to reuse. Besides, these constraints are specified and checked during architecture design time, when component descriptions are specified or selected from repositories, then instantiated and connected together to define architecture descriptions. These two facts (being subject to reuse and instantiation/connection) make architecture constraints good candidates for component-based design within a unified environment. In this paper, we propose a component model for specifying architecture constraints. This model has been implemented as an extension to an ADL that we have developed, which is called CLACS. The obtained process advocates the idea of specifying architecture constraints using the same paradigm of component-based development as for architecture description. To evaluate the component model, we conducted an experiment with a catalog of constraints formalizing the topological conditions of architecture patterns. The results of this experiment showed that constraint specification is improved by this reuse-by-composition model.},
  comment       = {17},
  doi           = {https://doi.org/10.1016/j.future.2016.02.006},
  keywords      = {Architecture constraint, Software component, Architecture description, OCL},
  url           = {http://www.sciencedirect.com/science/article/pii/S0167739X1630019X},
}

@Article{Vogel-Heuser2015,
  author        = {Birgit Vogel-Heuser and Alexander Fay and Ina Schaefer and Matthias Tichy},
  title         = {Evolution of software in automated production systems: Challenges and research directions},
  journal       = {Journal of Systems and Software},
  year          = {2015},
  volume        = {110},
  pages         = {54 - 84},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {Abstract

Coping with evolution in automated production systems implies a cross-disciplinary challenge along the system's life-cycle for variant-rich systems of high complexity. The authors from computer science and automation provide an interdisciplinary survey on challenges and state of the art in evolution of automated production systems. Selected challenges are illustrated on the case of a simple pick and place unit. In the first part of the paper, we discuss the development process of automated production systems as well as the different type of evolutions during the system's life-cycle on the case of a pick and place unit. In the second part, we survey the challenges associated with evolution in the different development phases and a couple of cross-cutting areas and review existing approaches addressing the challenges. We close with summarizing future research directions to address the challenges of evolution in automated production systems.},
  comment       = {30},
  doi           = {https://doi.org/10.1016/j.jss.2015.08.026},
  keywords      = {Evolution, Automation, Automated production systems, Software engineering},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121215001818},
}

@Article{Ryssel2012,
  author        = {Uwe Ryssel and Joern Ploennigs and Klaus Kabitzsch},
  title         = {Automatic library migration for the generation of hardware-in-the-loop models},
  journal       = {Science of Computer Programming},
  year          = {2012},
  volume        = {77},
  number        = {2},
  pages         = {83 - 95},
  issn          = {0167-6423},
  note          = {Special Issue on Automatic Program Generation for Embedded Systems},
  __markedentry = {[mac:]},
  abstract      = {Embedded systems are widely used in several applications nowadays. As they integrate hard- and software elements, their functionality and reliability are often tested by hardware-in-the-loop methods, in which the system under test runs in a simulated environment. Due to the rising complexity of the embedded functions, performance limitations and practicability reasons, the simulations are often specialized to test specific aspects of the embedded system and develop a high diversity by themselves. This diversity is difficult to manage for a user and results in erroneously selected test components and compatibility problems in the test configuration. This paper presents a generative programming approach that handles the diversity of test libraries. Compatibility issues are explicitly evaluated by a new interface concept. Furthermore, a novel model analyzer facilitates the efficient application in practice by migrating existing libraries. The approach is evaluated for an example from the automotive domain using MATLAB/Simulink.},
  comment       = {13},
  doi           = {https://doi.org/10.1016/j.scico.2010.06.005},
  keywords      = {Generative programming, Function-block-based design, Library migration, Structural comparison},
  url           = {http://www.sciencedirect.com/science/article/pii/S0167642310001115},
}

@Article{Paeivaerinta2015,
  author        = {Tero PÃ¤ivÃ¤rinta and Kari Smolander},
  title         = {Theorizing about software development practices},
  journal       = {Science of Computer Programming},
  year          = {2015},
  volume        = {101},
  pages         = {124 - 135},
  issn          = {0167-6423},
  note          = {Towards general theories of software engineering},
  __markedentry = {[mac:]},
  abstract      = {The paper focuses on the challenge of generating theoretical support for software development, especially when human software developers are involved in the software development process. We outline a model, â€œCoat Hangerâ€, for theorizing about development practices. The model focuses on the intended rationale for the actual realization and resulting impacts of using particular practices in varying contexts. To illustrate the use of the model, we have studied recent practice-oriented articles in the journal Science of Computer Programming. A survey of articles in the journal between 2010 and 2013 showed that out of 371 articles, only four studied software development in professional organizations with actual software practitioners as informants. The Coat Hanger model was then used to identify the theoretical strengths and weaknesses of these four practice descriptions. The analysis is used as the basis to declare the potential of our model as a conceptual aid for more structured theorizing about software development practices. The contribution of the model is the introduction of a concretization of how theorizing can be done through reflection-in-action, instead of regarding research on software practices plainly from the prevailing viewpoint of technical rationality.},
  comment       = {12},
  doi           = {https://doi.org/10.1016/j.scico.2014.11.012},
  keywords      = {Software development, Practice, Theory, Theorizing, Reflection-in-action},
  url           = {http://www.sciencedirect.com/science/article/pii/S0167642314005449},
}

@Article{Kulk2008,
  author        = {G.P. Kulk and C. Verhoef},
  title         = {Quantifying requirements volatility effects},
  journal       = {Science of Computer Programming},
  year          = {2008},
  volume        = {72},
  number        = {3},
  pages         = {136 - 175},
  issn          = {0167-6423},
  __markedentry = {[mac:]},
  abstract      = {In an organization operating in the bancassurance sector we identified a low-risk IT subportfolio of 84 IT projects comprising together 16,500 function points, each project varying in size and duration, for which we were able to quantify its requirements volatility. This representative portfolio stems from a much larger portfolio of IT projects. We calculated the volatility from the function point countings that were available to us. These figures were aggregated into a requirements volatility benchmark. We found that maximum requirements volatility rates depend on size and duration, which refutes currently known industrial averages. For instance, a monthly growth rate of 5% is considered a critical failure factor, but in our low-risk portfolio we found more than 21% of successful projects with a volatility larger than 5%. We proposed a mathematical model taking size and duration into account that provides a maximum healthy volatility rate that is more in line with the reality of low-risk IT portfolios. Based on the model, we proposed a tolerance factor expressing the maximal volatility tolerance for a project or portfolio. For a low-risk portfolio its empirically found tolerance is apparently acceptable, and values exceeding this tolerance are used to trigger IT decision makers. We derived two volatility ratios from this model, the Ï€-ratio and the Ï-ratio. These ratios express how close the volatility of a project has approached the danger zone when requirements volatility reaches a critical failure rate. The volatility data of a governmental IT portfolio were juxtaposed to our bancassurance benchmark, immediately exposing a problematic project, which was corroborated by its actual failure. When function points are less common, e.g.Â in the embedded industry, we used daily source code size measures and illustrated how to govern the volatility of a software product line of a hardware manufacturer. With the three real-world portfolios we illustrated that our results serve the purpose of an early warning system for projects that are bound to fail due to excessive volatility. Moreover, we developed essential requirements volatility metrics that belong on an IT governance dashboard and presented such a volatility dashboard.},
  comment       = {40},
  doi           = {https://doi.org/10.1016/j.scico.2008.04.003},
  keywords      = {Requirements volatility, IT portfolio management, Quantitative IT portfolio management, Volatility benchmark, IT dashboard, Requirements metric, Requirements creep, Scope creep, Requirements scrap, Requirements churn, Compound monthly growth rate, Volatility tolerance factor, -ratio, -ratio, Requirements volatility dashboard},
  url           = {http://www.sciencedirect.com/science/article/pii/S0167642308000464},
}

@Article{Lee2010,
  author        = {Jaejoon Lee and Dirk Muthig and Matthias Naab},
  title         = {A feature-oriented approach for developing reusable product line assets of service-based systems},
  journal       = {Journal of Systems and Software},
  year          = {2010},
  volume        = {83},
  number        = {7},
  pages         = {1123 - 1136},
  issn          = {0164-1212},
  note          = {SPLC 2008},
  __markedentry = {[mac:]},
  abstract      = {Service orientation (SO) is a relevant promising candidate for accommodating rapidly changing user needs and expectations. One of the goals of adopting SO is the improvement of reusability, however, the development of service-based system in practice has uncovered several challenging issues, such as how to identify reusable services, how to determine configurations of services that are relevant to usersâ€™ current product configuration and context, and how to maintain service validity after configuration changes. In this paper, we propose a method that addresses these issues by adapting a feature-oriented product line engineering approach. The method is notable in that it guides developers to identify reusable services at the right level of granularity and to map usersâ€™ context to relevant service configuration, and it also provides a means to check the validity of services at runtime in terms of invariants and pre/post-conditions of services. Moreover, we propose a heterogeneous style based architecture model for developing such systems.},
  comment       = {14},
  doi           = {https://doi.org/10.1016/j.jss.2010.01.048},
  keywords      = {Software product line engineering, Feature-oriented, Service-based systems, Software architecture, Software architecture styles},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121210000324},
}

@Article{Kosar2016,
  author        = {TomaÅ¾ Kosar and Sudev Bohra and Marjan Mernik},
  title         = {Domain-Specific Languages: A Systematic Mapping Study},
  journal       = {Information and Software Technology},
  year          = {2016},
  volume        = {71},
  pages         = {77 - 91},
  issn          = {0950-5849},
  __markedentry = {[mac:]},
  abstract      = {Context: In this study we report on a Systematic Mapping Study (SMS) for Domain-Specific Languages (DSLs), based on an automatic search including primary studies from journals, conferences, and workshops during the period from 2006 until 2012. Objective: The main objective of the described work was to perform an SMS on DSLs to better understand the DSL research field, identify research trends, and any possible open issues. The set of research questions was inspired by a DSL survey paper published in 2005. Method: We conducted a SMS over 5 stages: defining research questions, conducting the search, screening, classifying, and data extraction. Our SMS included 1153 candidate primary studies from the ISI Web of Science and ACM Digital Library, 390 primary studies were classified after screening. Results: This SMS discusses two main research questions: research space and trends/demographics of the literature within the field of DSLs. Both research questions are further subdivided into several research sub-questions. The results from the first research question clearly show that the DSL community focuses more on the development of new techniques/methods rather than investigating the integrations of DSLs with other software engineering processes or measuring the effectiveness of DSL approaches. Furthermore, there is a clear lack of evaluation research. Amongst different DSL development phases more attention is needed in regard to domain analysis, validation, and maintenance. The second research question revealed that the number of publications remains stable, and has not increased over the years. Top cited papers and venues are mentioned, as well as identifying the more active institutions carrying DSL research. Conclusion: The statistical findings regarding research questions paint an interesting picture about the mainstreams of the DSL community, as well as open issues where researchers can improve their research in their future work.},
  comment       = {15},
  doi           = {https://doi.org/10.1016/j.infsof.2015.11.001},
  keywords      = {Domain-Specific Languages, Systematic Mapping Study, Systematic Review},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584915001858},
}

@Article{Mahdavi-Hezavehi2013,
  author        = {Sara Mahdavi-Hezavehi and Matthias Galster and Paris Avgeriou},
  title         = {Variability in quality attributes of service-based software systems: A systematic literature review},
  journal       = {Information and Software Technology},
  year          = {2013},
  volume        = {55},
  number        = {2},
  pages         = {320 - 343},
  issn          = {0950-5849},
  note          = {Special Section: Component-Based Software Engineering (CBSE), 2011},
  __markedentry = {[mac:]},
  abstract      = {Context
Variability is the ability of a software artifact (e.g., a system, component) to be adapted for a specific context, in a preplanned manner. Variability not only affects functionality, but also quality attributes (e.g., security, performance). Service-based software systems consider variability in functionality implicitly by dynamic service composition. However, variability in quality attributes of service-based systems seems insufficiently addressed in current design practices.
Objective
We aim at (a) assessing methods for handling variability in quality attributes of service-based systems, (b) collecting evidence about current research that suggests implications for practice, and (c) identifying open problems and areas for improvement.
Method
A systematic literature review with an automated search was conducted. The review included studies published between the year 2000 and 2011. We identified 46 relevant studies.
Results
Current methods focus on a few quality attributes, in particular performance and availability. Also, most methods use formal techniques. Furthermore, current studies do not provide enough evidence for practitioners to adopt proposed approaches. So far, variability in quality attributes has mainly been studied in laboratory settings rather than in industrial environments.
Conclusions
The product line domain as the domain that traditionally deals with variability has only little impact on handling variability in quality attributes. The lack of tool support, the lack of practical research and evidence for the applicability of approaches to handle variability are obstacles for practitioners to adopt methods. Therefore, we suggest studies in industry (e.g., surveys) to collect data on how practitioners handle variability of quality attributes in service-based systems. For example, results of our study help formulate hypotheses and questions for such surveys. Based on needs in practice, new approaches can be proposed.},
  comment       = {24},
  doi           = {https://doi.org/10.1016/j.infsof.2012.08.010},
  keywords      = {Variability, Service-based systems, Quality attributes, Systematic literature review},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584912001772},
}

@Article{Salvaneschi2015,
  author        = {Guido Salvaneschi and Carlo Ghezzi and Matteo Pradella},
  title         = {ContextErlang: A language for distributed context-aware self-adaptive applications},
  journal       = {Science of Computer Programming},
  year          = {2015},
  volume        = {102},
  pages         = {20 - 43},
  issn          = {0167-6423},
  __markedentry = {[mac:]},
  abstract      = {Self-adaptive software modifies its behavior at run time to satisfy changing requirements in a dynamic environment. Context-oriented programming (COP) has been recently proposed as a specialized programming paradigm for context-aware and adaptive systems. COP mostly focuses on run time adaptation of the application's behavior by supporting modular descriptions of behavioral variations. However, self-adaptive applications must satisfy additional requirements, such as distribution and concurrency, support for unforeseen changes and enforcement of correct behavior in the presence of dynamic change. Addressing these issues at the language level requires a holistic design that covers all aspects and takes into account the possibly cumbersome interaction of those features, for example concurrency and dynamic change. We present ContextErlang, a COP programming language in which adaptive abstractions are seamlessly integrated with distribution and concurrency. We define ContextErlang's formal semantics, validated through an executable prototype, and we show how it supports formal proofs that the language design ensures satisfaction of certain safety requirements. We provide empirical evidence that ContextErlang is an effective solution through case studies and a performance assessment. We also show how the same design principles that lead to the development of ContextErlang can be followed to systematically design contextual extensions of other languages. A concrete example is presented concerning ContextScala.},
  comment       = {24},
  doi           = {https://doi.org/10.1016/j.scico.2014.11.016},
  keywords      = {Context-oriented programming, Context, Self-adaptive software, Concurrency, Distribution},
  url           = {http://www.sciencedirect.com/science/article/pii/S0167642314005577},
}

@Article{Korhonen2004,
  author        = {Mika Korhonen and Tommi Mikkonen},
  title         = {Assessing systems adaptability to a product family},
  journal       = {Journal of Systems Architecture},
  year          = {2004},
  volume        = {50},
  number        = {7},
  pages         = {383 - 392},
  issn          = {1383-7621},
  note          = {Adaptable System/Software Architectures},
  __markedentry = {[mac:]},
  abstract      = {In many cases, product families are established on top of a successful pilot product. While this approach provides an option to measure many concrete attributes like performance and memory footprint, adequateness and adaptability of the architecture of the pilot cannot be fully verified. Yet, these properties are crucial business enablers for the whole product family. In this paper, we discuss an architectural assessment of one such seminal system, intended for monitoring electronic subsystems of a mobile machine, which is to be extended to support a wide range of different types of products. This paper shows how well the assessment reveals possible problems and existing flexibilities in assessed system, and this way helps different stakeholders in their further decisions.},
  comment       = {10},
  doi           = {https://doi.org/10.1016/j.sysarc.2003.08.011},
  keywords      = {Software product lines, Product line architecture, Adaptability, Assessment},
  url           = {http://www.sciencedirect.com/science/article/pii/S1383762103001619},
}

@Article{Ullah2010,
  author        = {Muhammad Irfan Ullah and GÃ¼nther Ruhe and Vahid Garousi},
  title         = {Decision support for moving from a single product to a product portfolio in evolving software systems},
  journal       = {Journal of Systems and Software},
  year          = {2010},
  volume        = {83},
  number        = {12},
  pages         = {2496 - 2512},
  issn          = {0164-1212},
  note          = {TAIC PART 2009 - Testing: Academic \& Industrial Conference - Practice And Research Techniques},
  __markedentry = {[mac:]},
  abstract      = {Successful software systems continuously evolve to accommodate ever-changing needs of customers. Accommodating the feature requests of all the customers in a single product increases the risks and costs of software maintenance. A possible approach to mitigate these risks is to transition the evolving software system (ESS) from a single system to a portfolio of related product variants, each addressing a specific customersâ€™ segment. This evolution should be conducted such that the extent of modifications required in ESS's structure is reduced. The proposed method COPE+ uses preferences of customers on product features to generate multiple product portfolios each containing one product variant per segment of customers. Recommendations are given to the decision maker to update the product portfolios based on structural analysis of ESS. Product portfolios are compared with the ESS using statechart representations to identify the level of similarity in their behaviors. A proof of concept is presented by application to an open-source text editing system. Structural and behavioral analysis of candidate portfolios helped the decision maker to select one portfolio out of three candidates.},
  comment       = {17},
  doi           = {https://doi.org/10.1016/j.jss.2010.07.049},
  keywords      = {Software product lines, Software product evolution, Software product management, Decision support, Behavioral analysis, Open-source systems},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121210002062},
}

@Article{Stol2011,
  author        = {Klaas-Jan Stol and Muhammad Ali Babar and Paris Avgeriou and Brian Fitzgerald},
  title         = {A comparative study of challenges in integrating Open Source Software and Inner Source Software},
  journal       = {Information and Software Technology},
  year          = {2011},
  volume        = {53},
  number        = {12},
  pages         = {1319 - 1336},
  issn          = {0950-5849},
  __markedentry = {[mac:]},
  abstract      = {Context
Several large software-developing organizations have adopted Open Source Software development (OSSD) practices to develop in-house components that are subsequently integrated into products. This phenomenon is also known as â€œInner Sourceâ€. While there have been several reports of successful cases of this phenomenon, little is known about the challenges that practitioners face when integrating software that is developed in such a setting.
Objective
The objective of this study was to shed light on challenges related to building products with components that have been developed within an Inner Source development environment.
Method
Following an initial systematic literature review to generate seed category data constructs, we performed an in-depth exploratory case study in an organization that has a significant track record in the implementation of Inner Source. Data was gathered through semi-structured interviews with participants from a range of divisions across the organization. Interviews were transcribed and analyzed using qualitative data analysis techniques.
Results
We have identified a number of challenges and approaches to address them, and compared the findings to challenges related to development with OSS products reported in the literature. We found that many challenges identified in the case study could be mapped to challenges related to integration of OSS.
Conclusion
The results provide important insights into common challenges of developing with OSS and Inner Source and may help organizations to understand how to improve their software development practices by adopting certain OSSD practices. The findings also identify the areas that need further research.},
  comment       = {18},
  doi           = {https://doi.org/10.1016/j.infsof.2011.06.007},
  keywords      = {Open Source Software, Inner Source, Software development, Challenges, Case study, Empirical studies},
  url           = {http://www.sciencedirect.com/science/article/pii/S095058491100142X},
}

@Article{AlmeidaMaia2013,
  author        = {Marcelo de Almeida Maia and Raquel Fialho LafetÃ¡},
  title         = {On the impact of trace-based feature location in the performance of software maintainers},
  journal       = {Journal of Systems and Software},
  year          = {2013},
  volume        = {86},
  number        = {4},
  pages         = {1023 - 1037},
  issn          = {0164-1212},
  note          = {SI : Software Engineering in Brazil: Retrospective and Prospective Views},
  __markedentry = {[mac:]},
  abstract      = {Software maintainers frequently strive to locate source code related to specific software features. This situation is mostly observable when features are scattered in the code. Considering this problem, several approaches for feature location using execution traces have been developed. Nonetheless, the practice of post-mortem analysis based on execution traces is not fully incorporated in the daily practice of software maintainers. Empirical studies that reveal strengths and weaknesses on the use of execution traces in maintenance activities could better explain the role of execution traces in software maintenance. This study reports on a controlled experiment conducted with maintainers performing actual maintenance activities on systems of different sizes unknown to them. There are benefits from systematic use of execution traces: the reduction of the maintenance activity time and greater accuracy of the activity outcome. Other qualitative observations were the lower level of activity difficulty perceived by the participants that used execution trace information and that this kind of information seems to be less useful in maintenance activities where the problem of feature scattering does not occur clearly.},
  comment       = {15},
  doi           = {https://doi.org/10.1016/j.jss.2012.12.032},
  keywords      = {Empirical assessment, Execution traces, Feature location, Software maintenance},
  url           = {http://www.sciencedirect.com/science/article/pii/S016412121200341X},
}

@Article{Hall2000,
  author        = {Robert J. Hall},
  title         = {Feature combination and interaction detection via foreground/background models},
  journal       = {Computer Networks},
  year          = {2000},
  volume        = {32},
  number        = {4},
  pages         = {449 - 469},
  issn          = {1389-1286},
  __markedentry = {[mac:]},
  abstract      = {One approach to building complex software product families is to partition the possible functions of the system into conceptual chunks called features. Ideally, system instances are rapidly assembled by combining features desired by the particular customer. Unfortunately, features often interact, meaning their combination causes unintended undesirable behavior even though in isolation the features work fine. This paper describes an approach to feature combination and interaction detection via foreground/background models, which allows expressing features as augmentations to the behavior of a base model. It also classifies interactions into three categories, based on how they can be detected, and describes implemented tools which can detect interactions from the three categories. I show why this approach avoids falsely detecting the spurious Type I interactions to which many existing approaches are prone. The tools and methodology, as well as the prevalence of spurious interactions in existing approaches, are illustrated through application to telephony features from the feature interaction contest associated with FIW'98. This data provides evidence that the foreground/background approach catches more nonspurious interactions, with less human effort, than competing approaches.},
  comment       = {21},
  doi           = {https://doi.org/10.1016/S1389-1286(00)00010-4},
  keywords      = {Feature interaction, Software validation, Formal methods},
  url           = {http://www.sciencedirect.com/science/article/pii/S1389128600000104},
}

@Article{Karatas2013,
  author        = {Ahmet Serkan KarataÅŸ and Halit OÄŸuztÃ¼zÃ¼n and Ali DoÄŸru},
  title         = {From extended feature models to constraint logic programming},
  journal       = {Science of Computer Programming},
  year          = {2013},
  volume        = {78},
  number        = {12},
  pages         = {2295 - 2312},
  issn          = {0167-6423},
  note          = {Special Section on International Software Product Line Conference 2010 and Fundamentals of Software Engineering (selected papers of FSEN 2011)},
  __markedentry = {[mac:]},
  abstract      = {Since feature models for realistic product families may be quite complicated, the automated analysis of feature models is desirable. Although several approaches reported in the literature address this issue, complex cross-tree relationships involving attributes in extended feature models have not been handled. In this article, we introduce a mapping from extended feature models to constraint logic programming over finite domains. This mapping is used to translate into constraint logic programs; basic, cardinality-based and extended feature models, which can include complex cross-tree relationships involving attributes. This translation enables the use of off-the-shelf constraint solvers for the automated analysis of extended feature models involving such complex relationships. We also present the performance results of some well-known analysis operations on an example translated model.},
  comment       = {18},
  doi           = {https://doi.org/10.1016/j.scico.2012.06.004},
  keywords      = {Variability modeling, Extended feature model, Feature attribute, Constraint logic programming},
  url           = {http://www.sciencedirect.com/science/article/pii/S0167642312001153},
}

@Article{Sierla2014,
  author        = {Seppo Sierla and Bryan M. Oâ€™Halloran and Heikki Nikula and Nikolaos Papakonstantinou and Irem Y. Tumer},
  title         = {Safety analysis of mechatronic product lines},
  journal       = {Mechatronics},
  year          = {2014},
  volume        = {24},
  number        = {3},
  pages         = {231 - 240},
  issn          = {0957-4158},
  __markedentry = {[mac:]},
  abstract      = {Most methodologies for the design and analysis of mechatronic systems target a single product. From a business perspective, successful product development requires shortening development times, reducing engineering costs and offering a greater variety of product options for customers. In software engineering, the software product line (SPL) technology has been developed to meet these conflicting goals, and several major companies have reported success stories resulting from SPL adoption. In mechanical engineering, similar methodologies have been developed under the name of product platforms. Methodologies for analyzing product qualities such as safety or reliability have been introduced for both SPL and product platforms. The problem with these methodologies is that they consider either software or mechanical product design, so they do not guide developers to find the best balance between the controller and the equipment to be controlled. Several system properties of a mechatronic product line should be investigated with mechatronic analysis methodologies before the development process branches to software, electronic and mechanical design. In particular, safety is one system property that can only be analyzed by considering both the equipment and its controller, so mechatronic methodologies early in the design are advantageous for discovering safety-related design constraints before costly design commitments are made. This paper extends the Functional Failure Identification and Propagation (FFIP) framework to the safety analysis of a mechatronic product line with options in software signal connections and equipment. The result of applying FFIP is that unsafe combinations of options are removed from the product line.},
  comment       = {10},
  doi           = {https://doi.org/10.1016/j.mechatronics.2014.02.003},
  keywords      = {Functional Failure Identification and Propagation, Safety, Risk analysis, Product line, Product platform},
  url           = {http://www.sciencedirect.com/science/article/pii/S0957415814000312},
}

@Article{Ouni2017,
  author        = {Ali Ouni and Raula Gaikovina Kula and Marouane Kessentini and Takashi Ishio and Daniel M. German and Katsuro Inoue},
  title         = {Search-based software library recommendation using multi-objective optimization},
  journal       = {Information and Software Technology},
  year          = {2017},
  volume        = {83},
  pages         = {55 - 75},
  issn          = {0950-5849},
  __markedentry = {[mac:]},
  abstract      = {Context: Software library reuse has significantly increased the productivity of software developers, reduced time-to-market and improved software quality and reusability. However, with the growing number of reusable software libraries in code repositories, finding and adopting a relevant software library becomes a fastidious and complex task for developers. Objective: In this paper, we propose a novel approach called LibFinder to prevent missed reuse opportunities during software maintenance and evolution. The goal is to provide a decision support for developers to easily find â€œusefulâ€ third-party libraries to the implementation of their software systems. Method: To this end, we used the non-dominated sorting genetic algorithm (NSGA-II), a multi-objective search-based algorithm, to find a trade-off between three objectives : 1) maximizing co-usage between a candidate library and the actual libraries used by a given system, 2) maximizing the semantic similarity between a candidate library and the source code of the system, and 3) minimizing the number of recommended libraries. Results: We evaluated our approach on 6083 different libraries from Maven Central super repository that were used by 32,760 client systems obtained from Github super repository. Our results show that our approach outperforms three other existing search techniques and a state-of-the art approach, not based on heuristic search, and succeeds in recommending useful libraries at an accuracy score of 92%, precision of 51% and recall of 68%, while finding the best trade-off between the three considered objectives. Furthermore, we evaluate the usefulness of our approach in practice through an empirical study on two industrial Java systems with developers. Results show that the top 10 recommended libraries was rated by the original developers with an average of 3.25 out of 5. Conclusion: This study suggests that (1) library usage history collected from different client systems and (2) library semantics/content embodied in library identifiers should be balanced together for an efficient library recommendation technique.},
  comment       = {21},
  doi           = {https://doi.org/10.1016/j.infsof.2016.11.007},
  keywords      = {Search-based software engineering, Software library, Software reuse, Multi-objective optimization},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584916303652},
}

@Article{Kuhrmann2016,
  author        = {Marco Kuhrmann and Thomas TernitÃ© and Jan Friedrich and Andreas Rausch and Manfred Broy},
  title         = {Flexible software process lines in practice: A metamodel-based approach to effectively construct and manage families of software process models},
  journal       = {Journal of Systems and Software},
  year          = {2016},
  volume        = {121},
  pages         = {49 - 71},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {Process flexibility and adaptability is a frequently discussed topic in literature, and several approaches propose techniques to improve and optimize software processes for a given organization- or project context. A software process line (SPrL) is an instrument to systematically construct and manage variable software processes, by combining pre-defined and standardized process assets that can be reused, modified, and extended using a well-defined customization approach. Hence, process engineers can ground context-specific process variants in a standardized or domain-specific reference model that can be adapted to the respective context. In this article, we present an approach to construct flexible software process lines and show its practical application in the German V-ModellÂ XT. The presented approach emerges from a 10-year research endeavor and was used to enhance the metamodel of the V-ModellÂ XT and to allow for improved process variability and lifecycle management. Practical dissemination and complementing empirical research show the suitability of the concept. We therefore contribute a proven approach that is presented as metamodel fragment for reuse and implementation in further process modeling approaches.},
  comment       = {23},
  doi           = {https://doi.org/10.1016/j.jss.2016.07.031},
  keywords      = {Software process metamodel, Software process, Software process lines, Process design, Process realisation, V-Modell XT metamodel},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121216301236},
}

@Article{Hartmann2012,
  author        = {Herman Hartmann and Tim Trew and Jan Bosch},
  title         = {The changing industry structure of software development for consumer electronics and its consequences for software architectures},
  journal       = {Journal of Systems and Software},
  year          = {2012},
  volume        = {85},
  number        = {1},
  pages         = {178 - 192},
  issn          = {0164-1212},
  note          = {Dynamic Analysis and Testing of Embedded Software},
  __markedentry = {[mac:]},
  abstract      = {During the last decade the structure of the consumer electronics industry has been changing profoundly. Current consumer electronics products are built using components from a large variety of specialized firms, whereas previously each product was developed by a single, vertically integrated company. Taking a software development perspective, we analyze the transition in the consumer electronics industry using case studies from digital televisions and mobile phones. We introduce a model consisting of five industry structure types and describe the forces that govern the transition between types and we describe the consequences for software architectures. We conclude that, at this point in time, software supply chains are the dominant industry structure for developing consumer electronics products. This is because the modularization of the architecture is limited, due to the lack of industry-wide standards and because resource constrained devices require variants of supplied software that are optimized for different hardware configurations. Due to these characteristics open ecosystems have not been widely adopted. The model and forces can serve the decision making process for individual companies that consider the transition to a different type of industry structure as well as provide a framework for researchers studying the software-intensive industries.},
  comment       = {15},
  doi           = {https://doi.org/10.1016/j.jss.2011.08.007},
  keywords      = {Industry structures, Ecosystems, Software supply chains, Case study, Software architecture, Software management, Software evolution, Embedded systems, Mobile phones, Consumer electronics},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121211002081},
}

@Article{Alferez2014a,
  author        = {G.H. AlfÃ©rez and V. Pelechano and R. Mazo and C. Salinesi and D. Diaz},
  title         = {Dynamic adaptation of service compositions with variability models},
  journal       = {Journal of Systems and Software},
  year          = {2014},
  volume        = {91},
  pages         = {24 - 47},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {Web services run in complex contexts where arising events may compromise the quality of the whole system. Thus, it is desirable to count on autonomic mechanisms to guide the self-adaptation of service compositions according to changes in the computing infrastructure. One way to achieve this goal is by implementing variability constructs at the language level. However, this approach may become tedious, difficult to manage, and error-prone. In this paper, we propose a solution based on a semantically rich variability model to support the dynamic adaptation of service compositions. When a problematic event arises in the context, this model is leveraged for decision-making. The activation and deactivation of features in the variability model result in changes in a composition model that abstracts the underlying service composition. These changes are reflected into the service composition by adding or removing fragments of Business Process Execution Language (WS-BPEL) code, which can be deployed at runtime. In order to reach optimum adaptations, the variability model and its possible configurations are verified at design time using Constraint Programming. An evaluation demonstrates several benefits of our approach, both at design time and at runtime.},
  comment       = {24},
  doi           = {https://doi.org/10.1016/j.jss.2013.06.034},
  keywords      = {Variability, Models at runtime, Autonomic computing, Dynamic adaptation, Dynamic software product line, Web service composition, Constraint programming, Verification},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121213001465},
}

@Article{Hurtado2013a,
  author        = {Julio Ariel Hurtado and MarÃ­a Cecilia Bastarrica and Sergio F. Ochoa and Jocelyn Simmonds},
  title         = {MDE software process lines in small companies},
  journal       = {Journal of Systems and Software},
  year          = {2013},
  volume        = {86},
  number        = {5},
  pages         = {1153 - 1171},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {Software organizations specify their software processes so that process knowledge can be systematically reused across projects. However, different projects may require different processes. Defining a separate process for each potential project context is expensive and error-prone, since these processes must simultaneously evolve in a consistent manner. Moreover, an organization cannot envision all possible project contexts in advance because several variables may be involved, and these may also be combined in different ways. This problem is even worse in small companies since they usually cannot afford to define more than one process. Software process lines are a specific type of software product lines, in the software process domain. A benefit of software process lines is that they allow software process customization with respect to a context. In this article we propose a model-driven approach for software process lines specification and configuration. The article also presents two industrial case studies carried out at two small Chilean software development companies. Both companies have benefited from applying our approach to their processes: new projects are now developed using custom processes, process knowledge is systematically reused, and the total time required to customize a process is much shorter than before.},
  comment       = {19},
  doi           = {https://doi.org/10.1016/j.jss.2012.09.033},
  keywords      = {Software process lines, Model-driven engineering, Process asset reuse},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121212002749},
}

@Article{Vogel-Heuser2017,
  author        = {Birgit Vogel-Heuser and Juliane Fischer and Stefan Feldmann and Sebastian Ulewicz and Susanne RÃ¶sch},
  title         = {Modularity and architecture of PLC-based software for automated production Systems: An analysis in industrial companies},
  journal       = {Journal of Systems and Software},
  year          = {2017},
  volume        = {131},
  pages         = {35 - 62},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {Adaptive and flexible production systems require modular and reusable software especially considering their long-term life cycle of up to 50 years. SWMAT4aPS, an approach to measure Software Maturity for automated Production Systems is introduced. The approach identifies weaknesses and strengths of various companiesâ€™ solutions for modularity of software in the design of automated Production Systems (aPS). At first, a self-assessed questionnaire is used to evaluate a large number of companies concerning their software maturity. Secondly, we analyze PLC code, architectural levels, workflows and abilities to configure code automatically out of engineering information in four selected companies. In this paper, the questionnaire results from 16 German world-leading companies in machine and plant manufacturing and four case studies validating the results from the detailed analyses are introduced to prove the applicability of the approach and give a survey of the state of the art in industry.},
  comment       = {28},
  doi           = {https://doi.org/10.1016/j.jss.2017.05.051},
  keywords      = {Factory automation, Automated production systems, Maturity, Modularity, Control software, Programmable logic controller},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121217300985},
}

@Article{Reinhartz-Berger2017,
  author        = {Iris Reinhartz-Berger and Kathrin Figl and Ã˜ystein Haugen},
  title         = {Investigating styles in variability modeling: Hierarchical vs. constrained styles},
  journal       = {Information and Software Technology},
  year          = {2017},
  volume        = {87},
  pages         = {81 - 102},
  issn          = {0950-5849},
  __markedentry = {[mac:]},
  abstract      = {Context
A common way to represent product lines is with variability modeling. Yet, there are different ways to extract and organize relevant characteristics of variability. Comprehensibility of these models and the ease of creating models are important for the efficiency of any variability management approach.
Objective
The goal of this paper is to investigate the comprehensibility of two common styles to organize variability into models â€“ hierarchical and constrained â€“ where the dependencies between choices are specified either through the hierarchy of the model or as cross-cutting constraints, respectively.
Method
We conducted a controlled experiment with a sample of 90 participants who were students with prior training in modeling. Each participant was provided with two variability models specified in Common Variability Language (CVL) and was asked to answer questions requiring interpretation of provided models. The models included 9â€“20 nodes and 8â€“19 edges and used the main variability elements. After answering the questions, the participants were asked to create a model based on a textual description.
Results
The results indicate that the hierarchical modeling style was easier to comprehend from a subjective point of view, but there was also a significant interaction effect with the degree of dependency in the models, that influenced objective comprehension. With respect to model creation, we found that the use of a constrained modeling style resulted in higher correctness of variability models.
Conclusions
Prior exposure to modeling style and the degree of dependency among elements in the model determine what modeling style a participant chose when creating the model from natural language descriptions. Participants tended to choose a hierarchical style for modeling situations with high dependency and a constrained style for situations with low dependency. Furthermore, the degree of dependency also influences the comprehension of the variability model.},
  comment       = {22},
  doi           = {https://doi.org/10.1016/j.infsof.2017.01.012},
  keywords      = {Variability modeling, Feature modeling, Comprehensibility, Hierarchical modeling, Textual constraints, Cognitive aspects, Empirical research, Product line engineering},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584917300800},
}

@Article{Chien2007,
  author        = {Shih-Wen Chien and Shu-Ming Tsaur},
  title         = {Investigating the success of ERP systems: Case studies in three Taiwanese high-tech industries},
  journal       = {Computers in Industry},
  year          = {2007},
  volume        = {58},
  number        = {8},
  pages         = {783 - 793},
  issn          = {0166-3615},
  __markedentry = {[mac:]},
  abstract      = {The measurement of enterprise resource planning (ERP) systems success or effectiveness is critical to our understanding of the value and efficacy of ERP investment and managerial actions. Whether traditional information systems success models can be extended to investigating ERP systems success is yet to be investigated. This paper proposes a partial extension and respecification of the DeLone and MacLean model of IS success to ERP systems. The purpose of the present research is to re-examine the updated DeLone and McLean model [W. DeLone, E. McLean, The DeLone McLean model of information system success: a ten-year update, Journal of Management Information Systems 19 (4) (2003) 3â€“9] of ERP systems success. The updated DeLone and McLean model was applied to collect data from the questionnaires answered by 204 users of ERP systems at three high-tech firms in Taiwan. Finally, this study suggests that system quality, service quality, and information quality are most important successful factors.},
  comment       = {11},
  doi           = {https://doi.org/10.1016/j.compind.2007.02.001},
  keywords      = {ERP success model, DeLone and McLean model, High-tech firms},
  url           = {http://www.sciencedirect.com/science/article/pii/S0166361507000188},
}

@Article{MotaSilveiraNeto2013,
  author        = {Paulo Anselmo da Mota Silveira Neto and JoÃ¡s Sousa Gomes and Eduardo Santana de Almeida and Jair Cavalcanti Leite and Thais Vasconcelos Batista and Larissa Leite},
  title         = {25 years of software engineering in Brazil: Beyond an insider's view},
  journal       = {Journal of Systems and Software},
  year          = {2013},
  volume        = {86},
  number        = {4},
  pages         = {872 - 889},
  issn          = {0164-1212},
  note          = {SI : Software Engineering in Brazil: Retrospective and Prospective Views},
  __markedentry = {[mac:]},
  abstract      = {The software engineering area is facing a growing number of challenges due to the continuing increase in software size and complexity. The challenges are addressed by the very relevant and high quality publications of the Brazilian Symposium on Software Engineering (SBES), in the past 25 editions. This article summarizes the findings from two different mapping studies about these 25 SBES editions. It also reports the results of an expert opinion survey with the most important Brazilian researchers in the software engineering (SE) area. The survey reinforces the findings of the mapping studies. It also provides guidance for future research. In addition, the studies report several findings that confirmed the validity of the research methods applied. All of these findings are important input to the current Brazilian SE scenario. Our findings also suggest that greater attention should be given to the SE area, by improving researchersâ€™ interaction with industry and increasing collaboration between researchers, especially internationally.},
  comment       = {18},
  doi           = {https://doi.org/10.1016/j.jss.2012.10.041},
  keywords      = {Mapping study, Expert opinion survey, Software engineering},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121212002981},
}

@Article{Folmer2004,
  author        = {Eelke Folmer and Jan Bosch},
  title         = {Architecting for usability: a survey},
  journal       = {Journal of Systems and Software},
  year          = {2004},
  volume        = {70},
  number        = {1},
  pages         = {61 - 78},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {Over the years the software engineering community has increasingly realized the important role software architecture plays in fulfilling the quality requirements of a system. The quality attributes of a software system are, to a large extent determined by the systemâ€™s software architecture. In recent years, the software engineering community has developed various tools and techniques that allow for design for quality attributes, such as performance or maintainability, at the software architecture level. We believe this design approach can be applied not only to â€œtraditionalâ€ quality attributes such as performance or maintainability but also to usability. This survey explores the feasibility of such a design approach. Current practice is surveyed from the perspective of a software architect. Are there any design methods that allow for design for usability at the architectural level? Are there any evaluation tools that allow assessment of architectures for their support of usability? What is usability? A framework is presented which visualizes these three research questions. Usability should drive design at all stages, but current usability engineering practice fails to fully achieve this goal. Our survey shows that there are no design techniques or assessment tools that allow for design for usability at the architectural level.},
  comment       = {18},
  doi           = {https://doi.org/10.1016/S0164-1212(02)00159-0},
  keywords      = {Software architecture, Usability, Design for quality attributes},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121202001590},
}

@Article{Guinea2016,
  author        = {Alejandro SÃ¡nchez Guinea and GrÃ©gory Nain and Yves Le Traon},
  title         = {A systematic review on the engineering of software for ubiquitous systems},
  journal       = {Journal of Systems and Software},
  year          = {2016},
  volume        = {118},
  pages         = {251 - 276},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {Context: Software engineering for ubiquitous systems has experienced an important and rapid growth, however the vast research corpus makes it difficult to obtain valuable information from it. Objective: To identify, evaluate, and synthesize research about the most relevant approaches addressing the different phases of the software development life cycle for ubiquitous systems. Method: We conducted a systematic literature review of papers presenting and evaluating approaches for the different phases of the software development life cycle for ubiquitous systems. Approaches were classified according to the phase of the development cycle they addressed, identifying their main concerns and limitations. Results: We identified 128 papers reporting 132 approaches addressing issues related to different phases of the software development cycle for ubiquitous systems. Most approaches have been aimed at addressing the implementation, evolution/maintenance, and feedback phases, while others phases such as testing need more attention from researchers. Conclusion: We recommend to follow existing guidelines when conducting case studies to make the studies more reproducible and closer to real life cases. While some phases of the development cycle have been extensively explored, there is still room for research in other phases, toward a more agile and integrated cycle, from requirements to testing and feedback.},
  comment       = {26},
  doi           = {https://doi.org/10.1016/j.jss.2016.05.024},
  keywords      = {EmpiricalÂ softwareÂ engineering, Evidence-basedÂ softwareÂ engineering, SystematicÂ review, ResearchÂ synthesis, SoftwareÂ developmentÂ cycle, UbiquitousÂ systems, DevelopmentÂ methods, PervasiveÂ systems},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121216300553},
}

@Article{Fontana2015,
  author        = {Rafaela Mantovani Fontana and Victor Meyer and Sheila Reinehr and Andreia Malucelli},
  title         = {Progressive Outcomes: A framework for maturing in agile software development},
  journal       = {Journal of Systems and Software},
  year          = {2015},
  volume        = {102},
  pages         = {88 - 108},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {Maturity models are used to guide improvements in the software engineering field and a number of maturity models for agile methods have been proposed in the last years. These models differ in their underlying structure prescribing different possible paths to maturity in agile software development, neglecting the fact that agile teams struggle to follow prescribed processes and practices. Our objective, therefore, was to empirically investigate how agile teams evolve to maturity, as a means to conceive a theory for agile software development evolvement that considers agile teams nature. The complex adaptive systems theory was used as a lens for analysis and four case studies were conducted to collect qualitative and quantitative data. As a result, we propose the Progressive Outcomes framework to describe the agile software development maturing process. It is a framework in which people have the central role, ambidexterity is a key ability to maturity, and improvement is guided by outcomes agile teams pursue, instead of prescribed practices.},
  comment       = {21},
  doi           = {https://doi.org/10.1016/j.jss.2014.12.032},
  keywords      = {Maturity, Agile software development, Software process improvement, Complex adaptive systems, Ambidexterity},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121214002908},
}

@Article{Cavalcanti2016,
  author        = {YguaratÃ£ Cerqueira Cavalcanti and Ivan do Carmo Machado and Paulo Anselmo da Motal S. Neto and Eduardo Santana de Almeida},
  title         = {Towards semi-automated assignment of software change requests},
  journal       = {Journal of Systems and Software},
  year          = {2016},
  volume        = {115},
  pages         = {82 - 101},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {Change Requests (CRs) are key elements to software maintenance and evolution. Finding the appropriate developer to a CR is crucial for obtaining the lowest, economically feasible, fixing time. Nevertheless, assigning CRs is a labor-intensive and time consuming task. In this paper, we report on a questionnaire-based survey with practitioners to understand the characteristics of CR assignment, and on a semi-automated approach for CR assignment which combines rule-based and machine learning techniques. In accordance with the results of the survey, the proposed approach emphasizes the use of contextual information, essential to effective assignments, and puts the development team in control of the assignment rules, toward making its adoption easier. The assignment rules can be either extracted from the assignment history or created from scratch. An empirical validation was performed through an offline experiment with CRs from a large software project. The results pointed out that the approach is up to 46.5% more accurate than other approaches which relying solely on machine learning techniques. This indicates that a rule-based approach is a viable and simple method to leverage CR assignments.},
  comment       = {20},
  doi           = {https://doi.org/10.1016/j.jss.2016.01.038},
  keywords      = {Software maintenance and evolution, Change request management, Automatic change request assignment, Bug triage},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121216000352},
}

@Article{Dang2014,
  author        = {Han-Hing Dang and Roland GlÃ¼ck and Bernhard MÃ¶ller and Patrick Roocks and Andreas Zelend},
  title         = {Exploring modal worlds},
  journal       = {Journal of Logical and Algebraic Methods in Programming},
  year          = {2014},
  volume        = {83},
  number        = {2},
  pages         = {135 - 153},
  issn          = {2352-2208},
  note          = {Festschrift in Honour of Gunther Schmidt on the Occasion of his 75th Birthday},
  __markedentry = {[mac:]},
  abstract      = {Modal idempotent semirings cover a large set of different applications. The paper presents a small collection of these, ranging from algebraic logics for program correctness over bisimulation refinement, formal concept analysis, database preferences to feature oriented software development. We provide new results and/or views on these domains; the modal semiring setting allows a concise and unified treatment, while being more general than, e.g., standard relation algebra.},
  comment       = {19},
  doi           = {https://doi.org/10.1016/j.jlap.2014.02.004},
  keywords      = {Bisimulation, Formal concept analysis, Pareto front, Rectangles, Separation logic, Software product lines},
  url           = {http://www.sciencedirect.com/science/article/pii/S1567832614000058},
}

@Article{Tarhan2014,
  author        = {Ayca Tarhan and Seda Gunes Yilmaz},
  title         = {Systematic analyses and comparison of development performance and product quality of Incremental Process and Agile Process},
  journal       = {Information and Software Technology},
  year          = {2014},
  volume        = {56},
  number        = {5},
  pages         = {477 - 494},
  issn          = {0950-5849},
  note          = {Performance in Software Development},
  __markedentry = {[mac:]},
  abstract      = {Context
Although Agile software development models have been widely used as a base for the software project life-cycle since 1990s, the number of studies that follow a sound empirical method and quantitatively reveal the effect of using these models over Traditional models is scarce.
Objective
This article explains the empirical method of and the results from systematic analyses and comparison of development performance and product quality of Incremental Process and Agile Process adapted in two projects of a middle-size, telecommunication software development company. The Incremental Process is an adaption of the Waterfall Model whereas the newly introduced Agile Process is a combination of the Unified Software Development Process, Extreme Programming, and Scrum.
Method
The method followed to perform the analyses and comparison is benefited from the combined use of qualitative and quantitative methods. It utilizes; GQM Approach to set measurement objectives, CMMI as the reference model to map the activities of the software development processes, and a pre-defined assessment approach to verify consistency of process executions and evaluate measure characteristics prior to quantitative analysis.
Results
The results of the comparison showed that the Agile Process had performed better than the Incremental Process in terms of productivity (79%), defect density (57%), defect resolution effort ratio (26%), Test Execution V&V Effectiveness (21%), and effort prediction capability (4%). These results indicate that development performance and product quality achieved by following the Agile Process was superior to those achieved by following the Incremental Process in the projects compared.
Conclusion
The acts of measurement, analysis, and comparison enabled comprehensive review of the two development processes, and resulted in understanding their strengths and weaknesses. The comparison results constituted objective evidence for organization-wide deployment of the Agile Process in the company.},
  comment       = {18},
  doi           = {https://doi.org/10.1016/j.infsof.2013.12.002},
  keywords      = {Empirical method, Quantitative analysis, Qualitative analysis, Software measurement, Process performance, Agile development},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584913002310},
}

@Article{Spek2011,
  author        = {Pieter van der Spek and Steven Klusener},
  title         = {Applying a dynamic threshold to improve cluster detection of LSI},
  journal       = {Science of Computer Programming},
  year          = {2011},
  volume        = {76},
  number        = {12},
  pages         = {1261 - 1274},
  issn          = {0167-6423},
  note          = {Special Issue on Software Evolution, Adaptability and Variability},
  __markedentry = {[mac:]},
  abstract      = {Latent Semantic Indexing (LSI) is a standard approach for extracting and representing the meaning of words in a large set of documents. Recently it has been shown that it is also useful for identifying concerns in source code. The tree cutting strategy plays an important role in obtaining the clusters, which identify the concerns. In this contribution the authors compare two tree cutting strategies: the Dynamic Hybrid cut and the commonly used fixed height threshold. Two case studies have been performed on the source code of Philips Healthcare to compare the results using both approaches. While some of the settings are particular to the Philips-case, the results show that applying a dynamic threshold, implemented by the Dynamic Hybrid cut, is an improvement over the fixed height threshold in the detection of clusters representing relevant concerns. This makes the approach as a whole more usable in practice.},
  comment       = {14},
  doi           = {https://doi.org/10.1016/j.scico.2010.12.004},
  keywords      = {Feature extraction, Clustering, Reverse engineering, Software architecture, Latent Semantic Indexing},
  url           = {http://www.sciencedirect.com/science/article/pii/S0167642310002297},
}

@Article{Taibi2017,
  author        = {Davide Taibi and Andrea Janes and Valentina Lenarduzzi},
  title         = {How developers perceive smells in source code: A replicated study},
  journal       = {Information and Software Technology},
  year          = {2017},
  volume        = {92},
  pages         = {223 - 235},
  issn          = {0950-5849},
  __markedentry = {[mac:]},
  abstract      = {Context. In recent years, smells, also referred to as bad smells, have gained popularity among developers. However, it is still not clear how harmful they are perceived from the developersâ€™ point of view. Many developers talk about them, but only few know what they really are, and even fewer really take care of them in their source code. Objective. The goal of this work is to understand the perceived criticality of code smells both in theory, when reading their description, and in practice. Method. We executed an empirical study as a differentiated external replication of two previous studies. The studies were conducted as surveys involving only highly experienced developers (63 in the first study and 41 in the second one). First the perceived criticality was analyzed by proposing the description of the smells, then different pieces of code infected by the smells were proposed, and finally their ability to identify the smells in the analyzed code was tested. Results. According to our knowledge, this is the largest study so far investigating the perception of code smells with professional software developers. The results show that developers are very concerned about code smells in theory, nearly always considering them as harmful or very harmful (17 out of 23 smells). However, when they were asked to analyze an infected piece of code, only few infected classes were considered harmful and even fewer were considered harmful because of the smell. Conclusions. The results confirm our initial hypotheses that code smells are perceived as more critical in theory but not as critical in practice.},
  comment       = {13},
  doi           = {https://doi.org/10.1016/j.infsof.2017.08.008},
  keywords      = {Software maintenance, Code smells, Bad smells, Antipatterns, Refactoring},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584916304128},
}

@Article{Paz2016,
  author        = {AndrÃ©s Paz and Hugo Arboleda},
  title         = {A Model to Guide Dynamic Adaptation Planning in Self-Adaptive Systems},
  journal       = {Electronic Notes in Theoretical Computer Science},
  year          = {2016},
  volume        = {321},
  pages         = {67 - 88},
  issn          = {1571-0661},
  note          = {CLEI 2015, the XLI Latin American Computing Conference},
  __markedentry = {[mac:]},
  abstract      = {Self-adaptive enterprise applications have the ability to continuously reconfigure themselves according to changes in their execution contexts or user requirements. The infrastructure managing such systems is based on IBM's MAPE-K reference model: a Monitor and an Analyzer to sense and interpret context data, a Planner and an Executor to create and apply structural adaptation plans, and a Knowledge manager to share relevant information. In this paper we present a formal model, built on the principles of constraint satisfaction, to address dynamic adaptation planning for self-adaptive enterprise applications. We formalize, modify and extend the approach presented in [H. Arboleda, J. F. DÃ­az, V. Vargas, and J.-C. Royer, â€œAutomated reasoning for derivation of modeldriven spls,â€ in SPLC'10 MAPLE'10, 2010, pp. 181â€“188] for working with self-adaptation infrastructures in order to provide automated reasoning on the dynamic creation of structural adaptation plans. We use a running example to demonstrate the applicability of such model, even in situations where complex interactions arise between context elements and the target self-adaptive enterprise application.},
  comment       = {22},
  doi           = {https://doi.org/10.1016/j.entcs.2016.02.005},
  keywords      = {Self-Adaptive Enterprise Applications, Dynamic Adaptation Planning, Automated Reasoning},
  url           = {http://www.sciencedirect.com/science/article/pii/S1571066116300056},
}

@Article{Ahmed2017,
  author        = {Bestoun S. Ahmed and Luca M. Gambardella and Wasif Afzal and Kamal Z. Zamli},
  title         = {Handling constraints in combinatorial interaction testing in the presence of multi objective particle swarm and multithreading},
  journal       = {Information and Software Technology},
  year          = {2017},
  volume        = {86},
  pages         = {20 - 36},
  issn          = {0950-5849},
  __markedentry = {[mac:]},
  abstract      = {Context
Combinatorial testing strategies have lately received a lot of attention as a result of their diverse applications. In its simple form, a combinatorial strategy can reduce several input parameters (configurations) of a system into a small set based on their interaction (or combination). In practice, the input configurations of software systems are subjected to constraints, especially in case of highly configurable systems. To implement this feature within a strategy, many difficulties arise for construction. While there are many combinatorial interaction testing strategies nowadays, few of them support constraints.
Objective
This paper presents a new strategy, to construct combinatorial interaction test suites in the presence of constraints.
Method
The design and algorithms are provided in detail. To overcome the multi-judgement criteria for an optimal solution, the multi-objective particle swarm optimisation and multithreading are used. The strategy and its associated algorithms are evaluated extensively using different benchmarks and comparisons.
Results
Our results are promising as the evaluation results showed the efficiency and performance of each algorithm in the strategy. The benchmarking results also showed that the strategy can generate constrained test suites efficiently as compared to state-of-the-art strategies.
Conclusion
The proposed strategy can form a new way for constructing of constrained combinatorial interaction test suites. The strategy can form a new and effective base for future implementations.},
  comment       = {17},
  doi           = {https://doi.org/10.1016/j.infsof.2017.02.004},
  keywords      = {Constrained combinatorial interaction, Multi-objective particle swarm optimisation, Test generation tools, Search-based software engineering, Test case design techniques},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584917301349},
}

@Article{Hoda2017a,
  author        = {Rashina Hoda and Norsaremah Salleh and John Grundy and Hui Mien Tee},
  title         = {Systematic literature reviews in agile software development: A tertiary study},
  journal       = {Information and Software Technology},
  year          = {2017},
  volume        = {85},
  pages         = {60 - 70},
  issn          = {0950-5849},
  __markedentry = {[mac:]},
  abstract      = {Context
A number of systematic literature reviews and mapping studies (SLRs) covering numerous primary research studies on various aspects of agile software development (ASD) exist.
Objective
The aim of this paper is to provide an overview of the SLRs on ASD research topics for software engineering researchers and practitioners.
Method
We followed the tertiary study guidelines by Kitchenham etÂ al. to find SLRs published between late 1990s to December 2015.
Results
We found 28 SLRs focusing on ten different ASD research areas: adoption, methods, practices, human and social aspects, CMMI, usability, global software engineering (GSE), organizational agility, embedded systems, and software product line engineering. The number of SLRs on ASD topics, similar to those on software engineering (SE) topics in general, is on the rise. A majority of the SLRs applied standardized guidelines and the quality of these SLRs on ASD topics was found to be slightly higher for journal publications than for conferences. While some individuals and institutions seem to lead this area, the spread of authors and institutions is wide. With respect to prior review recommendations, significant progress was noticed in the area of connecting agile to established domains such as usability, CMMI, and GSE; and considerable progress was observed in focusing on management-oriented approaches as Scrum and sustaining ASD in different contexts such as embedded systems.
Conclusion
SLRs of ASD studies are on the rise and cover a variety of ASD aspects, ranging from early adoption issues to newer applications of ASD such as in product line engineering. ASD research can benefit from further primary and secondary studies on evaluating benefits and challenges of ASD methods, agile hybrids in large-scale setups, sustainability, motivation, teamwork, and project management; as well as a fresh review of empirical studies in ASD to cover the period post 2008.},
  comment       = {11},
  doi           = {https://doi.org/10.1016/j.infsof.2017.01.007},
  keywords      = {Agile software development, Tertiary study, Systematic literature reviews, Mapping study},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584917300538},
}

@Article{Kaur2017,
  author        = {Loveleen Kaur and Ashutosh Mishra},
  title         = {Software component and the semantic Web: An in-depth content analysis and integration history},
  journal       = {Journal of Systems and Software},
  year          = {2017},
  volume        = {125},
  pages         = {152 - 169},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {With the advent of Component-based software engineering (CBSE), large software systems are being built by integrating pre-built software components. The Semantic Web in association with CBSE has shown to offer powerful representation facilities and reasoning techniques to enhance and support querying, reasoning, discovery, etc. of software components. The goal of this paper is to research the applicability of Semantic Web technologies in performing the various tasks of CBSE and review the experimental results of the same in an easy and effective manner. To the best of our knowledge, this is the first study which provides an extensive review of the application of Semantic Web in CBSE from different perspectives. A systematic literature review of the Semantic Web approaches, employed for use in CBSE, reported from 2001 until 2015, is conducted in this research article. Empirical results have been drawn through the question-answer based analysis of the research, which clearly tells the year wise trend of the research articles, with the possible justification of the usage of Semantic Web technology and tools for a particular phase of CBSE. To conclude, gaps in the current research and potential future prospects have been discussed.},
  comment       = {18},
  doi           = {https://doi.org/10.1016/j.jss.2016.11.028},
  keywords      = {Component-based software engineering, Semantic Web, Ontology, Reasoners, Web services, Linked Data},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121216302308},
}

@Article{Graves2012,
  author        = {Daniel Graves and Joost Noppen and Witold Pedrycz},
  title         = {Clustering with proximity knowledge and relational knowledge},
  journal       = {Pattern Recognition},
  year          = {2012},
  volume        = {45},
  number        = {7},
  pages         = {2633 - 2644},
  issn          = {0031-3203},
  __markedentry = {[mac:]},
  abstract      = {In this article, a proximity fuzzy framework for clustering relational data is presented, where the relationships between the entities of the data are given in terms of proximity values. We offer a comprehensive and in-depth comparison of our clustering framework with proximity relational knowledge to clustering with distance relational knowledge, such as the well known relational Fuzzy C-Means (FCM). We conclude that proximity can provide a richer description of the relationships among the data and this offers a significant advantage when realizing clustering. We further motivate clustering relational proximity data and provide both synthetic and real-world experiments to demonstrate both the usefulness and advantage offered by clustering proximity data. Finally, a case study of relational clustering is introduced where we apply proximity fuzzy clustering to the problem of clustering a set of trees derived from software requirements engineering. The relationships between trees are based on the degree of closeness in both the location of the nodes in the trees and the semantics associated with the type of connections between the nodes.},
  comment       = {12},
  doi           = {https://doi.org/10.1016/j.patcog.2011.12.019},
  keywords      = {Relational clustering, Fuzzy clustering, Proximity, Knowledge representation, Software requirements},
  url           = {http://www.sciencedirect.com/science/article/pii/S0031320311005206},
}

@Article{Strasunskas2012,
  author        = {Darijus Strasunskas and Sari E. Hakkarainen},
  title         = {Domain model-driven software engineering: A method for discovery of dependency links},
  journal       = {Information and Software Technology},
  year          = {2012},
  volume        = {54},
  number        = {11},
  pages         = {1239 - 1249},
  issn          = {0950-5849},
  __markedentry = {[mac:]},
  abstract      = {Context
Dependency management often suffers from labor intensity and complexity in creating and maintaining the dependency relations in practice. This is even more critical in a distributed development, in which developers are geographically distributed and a wide variety of tools is used. In those settings, different interpretations of software requirements or usage of different terminologies make it challenging to predict the change impact.
Objective
is (a) to describe a method facilitating change management in geographically distributed software engineering by effective discovery and establishment of dependency links using domain models; (b) to evaluate the effectiveness of the proposed method.
Method
A domain model, providing a common reference point, is used to manage development objects and to automatically support dependency discovery. We propose to associate (annotate) development objects with the concepts from the model. These associations are used to compute dependency among development objects, and are stepwise refined to direct dependency links (i.e. enabling product traceability). To evaluate the method, we conducted a laboratory-based randomized experiment on two real cases. Six participants were using an implemented prototype and two comparable tools to perform simulated tasks.
Results
In the paper we elaborate on the proposed method discussing its functional steps. Results from the experiment show that the method can be effectively used to assist in discovery of dependency links. Users have discovered on average fourteen percent more dependency links than by using the comparable tools.
Conclusions
The proposed method advocates the use of domain models throughout the whole development life-cycle and is apt to facilitate multi-site software engineering. The experimental study and results suggest that the method is effective in the discovery of dependencies among development objects.},
  comment       = {11},
  doi           = {https://doi.org/10.1016/j.infsof.2012.06.004},
  keywords      = {Information systems development, Domain model-based information system design, Dependency management, Software engineering, Randomized experiment},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584912001073},
}

@Article{Lopez-Herrejon2015,
  author        = {Roberto E. Lopez-Herrejon and Lukas Linsbauer and JosÃ© A. Galindo and JosÃ© A. Parejo and David Benavides and Sergio Segura and Alexander Egyed},
  title         = {An assessment of search-based techniques for reverse engineering feature models},
  journal       = {Journal of Systems and Software},
  year          = {2015},
  volume        = {103},
  pages         = {353 - 369},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {Successful software evolves from a single system by adding and changing functionality to keep up with usersâ€™ demands and to cater to their similar and different requirements. Nowadays it is a common practice to offer a system in many variants such as community, professional, or academic editions. Each variant provides different functionality described in terms of features. Software Product Line Engineering (SPLE) is an effective software development paradigm for this scenario. At the core of SPLE is variability modelling whose goal is to represent the combinations of features that distinguish the system variants using feature models, the de facto standard for such task. As SPLE practices are becoming more pervasive, reverse engineering feature models from the feature descriptions of each individual variant has become an active research subject. In this paper we evaluated, for this reverse engineering task, three standard search based techniques (evolutionary algorithms, hill climbing, and random search) with two objective functions on 74 SPLs. We compared their performance using precision and recall, and found a clear trade-off between these two metrics which we further reified into a third objective function based on FÎ², an information retrieval measure, that showed a clear performance improvement. We believe that this work sheds light on the great potential of search-based techniques for SPLE tasks.},
  comment       = {17},
  doi           = {https://doi.org/10.1016/j.jss.2014.10.037},
  keywords      = {Feature model, Reverse engineering, Search Based Software Engineering},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121214002349},
}

@Article{Mohagheghi2009,
  author        = {Parastoo Mohagheghi and Vegard Dehlen and Tor Neple},
  title         = {Definitions and approaches to model quality in model-based software development â€“ A review of literature},
  journal       = {Information and Software Technology},
  year          = {2009},
  volume        = {51},
  number        = {12},
  pages         = {1646 - 1669},
  issn          = {0950-5849},
  note          = {Quality of UML Models},
  __markedentry = {[mac:]},
  abstract      = {More attention is paid to the quality of models along with the growing importance of modelling in software development. We performed a systematic review of studies discussing model quality published since 2000 to identify what model quality means and how it can be improved. From forty studies covered in the review, six model quality goals were identified; i.e., correctness, completeness, consistency, comprehensibility, confinement and changeability. We further present six practices proposed for developing high-quality models together with examples of empirical evidence. The contributions of the article are identifying and classifying definitions of model quality and identifying gaps for future research.},
  comment       = {24},
  doi           = {https://doi.org/10.1016/j.infsof.2009.04.004},
  keywords      = {Systematic review, Modelling, Model quality, Model-driven development, UML},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584909000457},
}

@Article{Reinhartz-Berger2015,
  author        = {Iris Reinhartz-Berger and Ora Wulf-Hadash},
  title         = {Improving the management of product lines by performing domain knowledge extraction and cross product line analysis},
  journal       = {Information and Software Technology},
  year          = {2015},
  volume        = {59},
  pages         = {191 - 204},
  issn          = {0950-5849},
  __markedentry = {[mac:]},
  abstract      = {Context
Increase in market competition is one of the main reasons for developing and maintaining families of systems, termed Product Lines (PLs). Managing those PLs is challenging, let alone the management of several related PLs. Currently, those PLs are managed separately or their relations are analyzed assuming explicit specification of dependencies or use of an underlying terminology. Such assumptions may not hold when developing the PLs in different departments or companies applying various engineering processes.
Objective
In this work we call for utilizing the knowledge gained from developing and maintaining different PLs in the same domain in order to recommend on improvements to the management of PLs.
Method
The suggested approach conducts domain knowledge extraction and cross PL analysis on feature diagrams â€“ the main aid for modeling PL variability. The domain knowledge is extracted by applying similarity metrics, clustering, and mining techniques. Based on the created domain models, the approach performs cross PL analysis that examines relations in the domain models and generates improvement recommendations to existing PLs and overall management recommendations (e.g., merging or splitting PLs).
Results
The approach outcomes were evaluated by humans in a domain of mobile phones. The evaluation results may provide evidence that the outcomes of the approach in general and its recommendations in particular meet human perception of the given domain.
Conclusion
We conclude that through domain knowledge extraction and cross PL analysis the suggested approach may generate recommendations useful to the management of individual PLs, as well as to the overall management of different PLs in the same domain.},
  comment       = {14},
  doi           = {https://doi.org/10.1016/j.infsof.2014.11.007},
  keywords      = {Software product line engineering, Variability management, Domain analysis, Feature modeling},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584914002535},
}

@Article{Laitenberger2000,
  author        = {Oliver Laitenberger and Jean-Marc DeBaud},
  title         = {An encompassing life cycle centric survey of software inspection},
  journal       = {Journal of Systems and Software},
  year          = {2000},
  volume        = {50},
  number        = {1},
  pages         = {5 - 31},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {This paper contributes an integrated survey of the work in the area of software inspection. It consists of two main sections. The first one introduces a detailed description of the core concepts and relationships that together define the field of software inspection. The second one elaborates a taxonomy that uses a generic development life-cycle to contextualize software inspection in detail. After Fagan's seminal work presented in 1976, the body of work in software inspection has greatly increased and reached measured maturity. Yet, there is still no encompassing and systematic view of this research body driven from a life-cycle perspective. This perspective is important since inspection methods and refinements are most often aligned to particular life-cycle artifacts. It also provides practitioners with a roadmap available in their terms. To provide a systematic and encompassing view of the research and practice body in software inspection, the contribution of this survey is, in a first step, to introduce in detail the core concepts and relationships that together embody the field of software inspection. This lays out the field key ideas and benefits and elicits a common vocabulary. There, we make a strong effort to unify the relevant vocabulary used in available literature sources. In a second step, we use this vocabulary to build a contextual map of the field in the form of a taxonomy indexed by the different development stages of a generic process. This contextual map can guide practitioners and focus their attention on the inspection work most relevant to the introduction or development of inspections at the level of their particular development stage; or to help motivate the use of software inspection earlier in their development cycle. Our work provides three distinct, practical benefits: First, the index taxonomy can help practitioners identify inspection experience directly related to a particular life-cycle stage. Second, our work allows structuring of the large amount of published inspection work. Third, such taxonomy can help researchers compare and assess existing inspection methods and refinements to identify fruitful areas of future work.},
  comment       = {27},
  doi           = {https://doi.org/10.1016/S0164-1212(99)00073-4},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121299000734},
}

@Article{Cacho2014,
  author        = {Nelio Cacho and Claudio Santâ€™anna and Eduardo Figueiredo and Francisco Dantas and Alessandro Garcia and Thais Batista},
  title         = {Blending design patterns with aspects: A quantitative study},
  journal       = {Journal of Systems and Software},
  year          = {2014},
  volume        = {98},
  pages         = {117 - 139},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {Design patterns often need to be blended (or composed) when they are instantiated in a software system. The composition of design patterns consists of assigning multiple pattern elements into overlapping sets of classes in a software system. Whenever the modularity of each design pattern is not preserved in the source code, their implementation becomes tangled with each other and with the classesâ€™ core responsibilities. As a consequence, the change or removal of each design pattern will be costly or prohibitive as the software system evolves. In fact, composing design patterns is much harder than instantiating them in an isolated manner. Previous studies have found design pattern implementations are naturally crosscutting in object-oriented systems, thereby making it difficult to modularly compose them. Therefore, aspect-oriented programming (AOP) has been pointed out as a natural alternative for modularizing and blending design patterns. However, there is little empirical knowledge on how AOP models influence the composability of widely used design patterns. This paper investigates the influence of using AOP models for composing the Gang-of-Four design patterns. Our study categorizes different forms of pattern composition and studies the benefits and drawbacks of AOP in these contexts. We performed assessments of several pair-wise compositions taken from 3 medium-sized systems implemented in Java and two AOP models, namely, AspectJ and Compose*. We also considered complex situations where more than two patterns involved in each composition, and the patterns were interacting with other aspects implementing other crosscutting concerns of the system. In general, we observed two dominant factors impacting the pattern composability with AOP: (i) the category of the pattern composition, and (ii) the AspectJ idioms used to implement the design patterns taking part in the composition.},
  comment       = {23},
  doi           = {https://doi.org/10.1016/j.jss.2014.08.041},
  keywords      = {Design patterns, Aspect-oriented programming, Composability, Empirical studies, Metrics},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121214001885},
}

@Article{Manikas2016,
  author        = {Konstantinos Manikas},
  title         = {Revisiting software ecosystems Research: A longitudinal literature study},
  journal       = {Journal of Systems and Software},
  year          = {2016},
  volume        = {117},
  pages         = {84 - 103},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {â€˜Software ecosystemsâ€™ is argued to first appear as a concept more than 10 years ago and software ecosystem research started to take off in 2010. We conduct a systematic literature study, based on the most extensive literature review in the field up to date, with two primarily aims: (a) to provide an updated overview of the field and (b) to document evolution in the field. In total, we analyze 231 papers from 2007 until 2014 and provide an overview of the research in software ecosystems. Our analysis reveals a field that is rapidly growing, both in volume and empirical focus, while becoming more mature. We identify signs of field maturity from the increase in: (i) the number of journal articles, (ii) the empirical models within the last two years, and (iii) the number of ecosystems studied. However, we note that the field is far from mature and identify a set of challenges that are preventing the field from evolving. We propose means for future research and the community to address them. Finally, our analysis shapes the view of the field having evolved outside the existing definitions of software ecosystems and thus propose the update of the definition of software ecosystems.},
  comment       = {20},
  doi           = {https://doi.org/10.1016/j.jss.2016.02.003},
  keywords      = {Software ecosystems, Longitudinal literature study, Software ecosystem maturity},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121216000406},
}

@Article{Dallal2008,
  author        = {Jehad Al Dallal and Paul Sorenson},
  title         = {Estimating the coverage of the framework application reusable cluster-based test cases},
  journal       = {Information and Software Technology},
  year          = {2008},
  volume        = {50},
  number        = {6},
  pages         = {595 - 604},
  issn          = {0950-5849},
  __markedentry = {[mac:]},
  abstract      = {Object-oriented frameworks support both software code and design reusability. In addition, it is found that providing class-based tests with the framework reduces considerably the class-based testing time and effort of the applications developed using the frameworks. Similarly, reusable cluster-based test cases can be generated using the framework hooks, and they, too, can be provided with the framework to reduce the cluster testing time and effort of the framework applications. In this paper, we introduce a methodology to estimate the possible coverage of the cluster-based reusable test cases for framework applications prior to suggesting and applying a specific technique to produce the test cases. An experimental case study is conducted to demonstrate the practical issues in applying the introduced methodology and to give insights on the possible coverage of the framework reusable cluster-based test cases. The results of applying the methodology on five framework applications show that, on average, the reusable cluster-based test cases cover at least one-third of the cluster testing areas of the interface classes created during the framework application engineering stage.},
  comment       = {10},
  doi           = {https://doi.org/10.1016/j.infsof.2007.07.006},
  keywords      = {Case study, Cluster testing, Framework interface classes, Hooks, Object-oriented framework, Object-oriented framework application, Reusable test cases},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584907000808},
}

@Article{Hervieu2016,
  author        = {Aymeric Hervieu and Dusica Marijan and Arnaud Gotlieb and Benoit Baudry},
  title         = {Practical minimization of pairwise-covering test configurations using constraint programming},
  journal       = {Information and Software Technology},
  year          = {2016},
  volume        = {71},
  pages         = {129 - 146},
  issn          = {0950-5849},
  __markedentry = {[mac:]},
  abstract      = {Context: Testing highly-configurable software systems is challenging due to a large number of test configurations that have to be carefully selected in order to reduce the testing effort as much as possible, while maintaining high software quality. Finding the smallest set of valid test configurations that ensure sufficient coverage of the systemâ€™s feature interactions is thus the objective of validation engineers, especially when the execution of test configurations is costly or time-consuming. However, this problem is NP-hard in general and approximation algorithms have often been used to address it in practice. Objective: In this paper, we explore an alternative exact approach based on constraint programming that will allow engineers to increase the effectiveness of configuration testing while keeping the number of configurations as low as possible. Method: Our approach consists in using a (time-aware) minimization algorithm based on constraint programming. Given the amount of time, our solution generates a minimized set of valid test configurations that ensure coverage of all pairs of feature values (a.k.a. pairwise coverage). The approach has been implemented in a tool called PACOGEN. Results: PACOGEN was evaluated on 224 feature models in comparison with the two existing tools that are based on a greedy algorithm. For 79% of 224 feature models, PACOGEN generated up to 60% fewer test configurations than the competitor tools. We further evaluated PACOGEN in the case study of an industrial video conferencing product line with a feature model of 169 features, and found 60% fewer configurations compared with the manual approach followed by test engineers. The set of test configurations generated by PACOGEN decreased the time required by test engineers in manual test configuration by 85%, increasing the feature-pairs coverage at the same time. Conclusion: Our experimental evaluation concluded that optimal time-aware minimization of pairwise-covering test configurations is efficiently addressed using constraint programming techniques.},
  comment       = {18},
  doi           = {https://doi.org/10.1016/j.infsof.2015.11.007},
  keywords      = {Variability testing, Highly-configurable software systems, Constraint programming},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584915002013},
}

@Article{Catala2013,
  author        = {Alejandro Catala and Patricia Pons and Javier Jaen and Jose A. Mocholi and Elena Navarro},
  title         = {A meta-model for dataflow-based rules in smart environments: Evaluating user comprehension and performance},
  journal       = {Science of Computer Programming},
  year          = {2013},
  volume        = {78},
  number        = {10},
  pages         = {1930 - 1950},
  issn          = {0167-6423},
  note          = {Special section on Language Descriptions Tools and Applications (LDTAâ€™08 \& â€™09) \& Special section on Software Engineering Aspects of Ubiquitous Computing and Ambient Intelligence (UCAmI 2011)},
  __markedentry = {[mac:]},
  abstract      = {A considerable part of the behavior in smart environments relies on event-driven and rule specification. Rules are the mechanism most often used to enable user customization of the environment. However, the expressiveness of the rules available to users in editing and other tools is usually either limited or the available rule editing interfaces are not designed for end-users with low skills in programming. This means we have to look for interaction techniques and new ways to define user customization rules. This paper describes a generic and flexible meta-model to support expressive rules enhanced with data flow expressions that will graphically support the definition of rules without writing code. An empirical study was conducted on the ease of understanding of the visual data flow expressions, which are the key elements in our rule proposal. The visual dataflow language was compared to its corresponding textual version in terms of comprehension and ease of learning by teenagers in exercises involving calculations, modifications, writing and detecting equivalences in expressions in both languages. Although the subjects had some previous experience in editing mathematical expressions on spreadsheets, the study found their performance with visual dataflows to be significantly better in calculation and modification exercises. This makes our dataflow approach a promising mechanism for expressing user-customized reactive behavior in Ambient Intelligence (AmI) environments. The performance of the rule matching processor was validated by means of two stress tests to ensure that the meta-model approach adopted would be able to scale up with the number of types and instances in the space.},
  comment       = {21},
  doi           = {https://doi.org/10.1016/j.scico.2012.06.010},
  keywords      = {Ambient intelligence, Customization, Dataflow, Visual language, Rule, Event based, Non-expert programmer, Smart home},
  url           = {http://www.sciencedirect.com/science/article/pii/S0167642312001232},
}

@Article{Poort2012,
  author        = {Eltjo R. Poort and Hans van Vliet},
  title         = {RCDA: Architecting as a risk- and cost management discipline},
  journal       = {Journal of Systems and Software},
  year          = {2012},
  volume        = {85},
  number        = {9},
  pages         = {1995 - 2013},
  issn          = {0164-1212},
  note          = {Selected papers from the 2011 Joint Working IEEE/IFIP Conference on Software Architecture (WICSA 2011)},
  __markedentry = {[mac:]},
  abstract      = {We propose to view architecting as a risk- and cost management discipline. This point of view helps architects identify the key concerns to address in their decision making, by providing a simple, relatively objective way to assess architectural significance. It also helps business stakeholders to align the architect's activities and results with their own goals. We examine the consequences of this point of view on the architecture process. The point of view is the basis of RCDA, the Risk- and Cost Driven Architecture approach. So far, more than 150 architects have received RCDA training. For a majority of the trainees, RCDA has a significant positive impact on their architecting work.},
  comment       = {19},
  doi           = {https://doi.org/10.1016/j.jss.2012.03.071},
  keywords      = {Software architecture, Risk Management, Cost management},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121212000994},
}

@Article{Franco-Bedoya2017,
  author        = {Oscar Franco-Bedoya and David Ameller and Dolors Costal and Xavier Franch},
  title         = {Open source software ecosystems: A Systematic mapping},
  journal       = {Information and Software Technology},
  year          = {2017},
  volume        = {91},
  pages         = {160 - 185},
  issn          = {0950-5849},
  __markedentry = {[mac:]},
  abstract      = {Context: Open source software (OSS) and software ecosystems (SECOs) are two consolidated research areas in software engineering. OSS influences the way organizations develop, acquire, use and commercialize software. SECOs have emerged as a paradigm to understand dynamics and heterogeneity in collaborative software development. For this reason, SECOs appear as a valid instrument to analyze OSS systems. However, there are few studies that blend both topics together. Objective: The purpose of this study is to evaluate the current state of the art in OSS ecosystems (OSSECOs) research, specifically: (a) what the most relevant definitions related to OSSECOs are; (b) what the particularities of this type of SECO are; and (c) how the knowledge about OSSECO is represented. Method: We conducted a systematic mapping following recommended practices. We applied automatic and manual searches on different sources and used a rigorous method to elicit the keywords from the research questions and selection criteria to retrieve the final papers. As a result, 82 papers were selected and evaluated. Threats to validity were identified and mitigated whenever possible. Results: The analysis allowed us to answer the research questions. Most notably, we did the following: (a) identified 64 terms related to the OSSECO and arranged them into a taxonomy; (b) built a genealogical tree to understand the genesis of the OSSECO term from related definitions; (c) analyzed the available definitions of SECO in the context of OSS; and (d) classified the existing modelling and analysis techniques of OSSECOs. Conclusion: As a summary of the systematic mapping, we conclude that existing research on several topics related to OSSECOs is still scarce (e.g., modelling and analysis techniques, quality models, standard definitions, etc.). This situation calls for further investigation efforts on how organizations and OSS communities actually understand OSSECOs.},
  comment       = {26},
  doi           = {https://doi.org/10.1016/j.infsof.2017.07.007},
  keywords      = {Software ecosystem, Open source software, Systematic mapping, Literature review, OSS, SECO, OSSECO},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584917304512},
}

@Article{Kaiya2017,
  author        = {Haruhiko Kaiya and Ryohei Sato and Atsuo Hazeyama and Shinpei Ogata and Takao Okubo and Takafumi Tanaka and Nobukazu Yoshioka and Hironori Washizaki},
  title         = {Preliminary Systematic Literature Review of Software and Systems Traceability},
  journal       = {Procedia Computer Science},
  year          = {2017},
  volume        = {112},
  pages         = {1141 - 1150},
  issn          = {1877-0509},
  note          = {Knowledge-Based and Intelligent Information \& Engineering Systems: Proceedings of the 21st International Conference, KES-20176-8 September 2017, Marseille, France},
  __markedentry = {[mac:]},
  abstract      = {Traceability is important knowledge for improving the artifacts of software and systems and processes related to them. Even in a single system, various kinds of artifacts exist. Various kinds of processes also exist, and each of them relates to different kinds of artifacts. Traceability over them has thus large diversity. In addition, developers in each process have different types of purposes to improve their artifacts and process. Research results in traceability have to be categorized and analyzed so that such a developer can choose one of them to achieve his/her purposes. In this paper, we report on the results of Systematic Literature Review (SLR) related to software and systems traceability. Our SLR is preliminary one because we only analyzed articles in ACM digital library and IEEE computer society digital library. We found several interesting trends in traceability research. For example, researches related to creating or maintaining traceability are larger than those related to using it or thinking its strategy. Various kinds of traceability purposes are addressed or assumed in many researches, but some researches do not specify purposes. Purposes related to changes and updates are dominant.},
  comment       = {10},
  doi           = {https://doi.org/10.1016/j.procs.2017.08.152},
  keywords      = {Systematic Literature Review, Software, Systems Traceability, IEEE CPS, ACM},
  url           = {http://www.sciencedirect.com/science/article/pii/S1877050917315090},
}

@Article{Nasir2015,
  author        = {S. Zafar Nasir and Tariq Mahmood and M. Shahid Shaikh and Zubair A. Shaikh},
  title         = {Fault-tolerant context development and requirement validation in ERP systems},
  journal       = {Computer Standards \& Interfaces},
  year          = {2015},
  volume        = {37},
  pages         = {9 - 19},
  issn          = {0920-5489},
  __markedentry = {[mac:]},
  abstract      = {This paper presents context development and requirement validation to overcome maintenance problems in Enterprise Resource Planning (ERP) systems. Using ERP data of a local petroleum firm, we employ knowledge integration to dynamically validate users' requirements, and to gather, analyze, and represent context through knowledge models. We also employ context-awareness to model the ERP context, along with a user requirement model. We employ context affinity to determine impact of these models on requirements' validation. We apply fault-tolerance on these models by using data mining to pre-identify delays in delivery of petroleum products, and to predict faulty contextual ERP product configuration.},
  comment       = {11},
  doi           = {https://doi.org/10.1016/j.csi.2014.05.001},
  keywords      = {Context development, Requirement validation, Enterprise Resource Planning, Fault tolerance, Data Mining},
  url           = {http://www.sciencedirect.com/science/article/pii/S0920548914000695},
}

@Article{April2009,
  author        = {Alain April and Alain Abran},
  title         = {A Software Maintenance Maturity Model (S3M): Measurement Practices at Maturity Levels 3 and 4},
  journal       = {Electronic Notes in Theoretical Computer Science},
  year          = {2009},
  volume        = {233},
  pages         = {73 - 87},
  issn          = {1571-0661},
  note          = {Proceedings of the International Workshop on Software Quality and Maintainability (SQM 2008)},
  __markedentry = {[mac:]},
  abstract      = {Evaluation and continuous improvement of software maintenance are key contributors to improving software quality. The software maintenance function suffers from a scarcity of the management models that would facilitate these functions. This paper presents an overview of the measurement practices that are being introduced for level 3 and higher to the Software Maintenance Maturity Model (S3M).},
  comment       = {15},
  doi           = {https://doi.org/10.1016/j.entcs.2009.02.062},
  keywords      = {Software Maintenance, Maturity Model, Process Improvement, Process Assessment, Product Assessment},
  url           = {http://www.sciencedirect.com/science/article/pii/S157106610900067X},
}

@Article{Silva2009a,
  author        = {Bruno Carreiro da Silva and Eduardo Figueiredo and Alessandro Garcia and Daltro Nunes},
  title         = {Refactoring of Crosscutting Concerns with Metaphor-Based Heuristics},
  journal       = {Electronic Notes in Theoretical Computer Science},
  year          = {2009},
  volume        = {233},
  pages         = {105 - 125},
  issn          = {1571-0661},
  note          = {Proceedings of the International Workshop on Software Quality and Maintainability (SQM 2008)},
  __markedentry = {[mac:]},
  abstract      = {It has been advocated that Aspect-Oriented Programming (AOP) is an effective technique to improve software maintainability through explicit support for modularising crosscutting concerns. However, in order to take the advantages of AOP, there is a need for supporting the systematic refactoring of crosscutting concerns to aspects. Existing techniques for aspect-oriented refactoring are too fine-grained and do not take the concern structure into consideration. This paper presents two categories towards a metaphor-based classification of crosscutting concerns driven by their manifested shapes through a system's modular structure. The proposed categories provide an intuitive and fundamental terminology for detecting concern-oriented design flaws and identifying refactorings in terms of recurring crosscutting structures. On top of this classification, we define a suite of metaphor-based refactorings to guide the â€œaspectisationâ€ of each concern category. We evaluate our technique by classifying concerns of 23 design patterns and by proposing refactorings to aspectise them according to observations made in previous empirical studies. Based on our experience, we also determine a catalogue of potential additional categories and heuristics for refactoring of crosscutting concerns.},
  comment       = {21},
  doi           = {https://doi.org/10.1016/j.entcs.2009.02.064},
  keywords      = {Refactoring, Aspect-oriented programming, Crosscutting concerns, Metaphor-based classification, Design heuristics},
  url           = {http://www.sciencedirect.com/science/article/pii/S1571066109000693},
}

@Article{Dittrich2014,
  author        = {Yvonne Dittrich},
  title         = {Software engineering beyond the project â€“ Sustaining software ecosystems},
  journal       = {Information and Software Technology},
  year          = {2014},
  volume        = {56},
  number        = {11},
  pages         = {1436 - 1456},
  issn          = {0950-5849},
  note          = {Special issue on Software Ecosystems},
  __markedentry = {[mac:]},
  abstract      = {Context
The main part of software engineering methods, tools and technologies has developed around projects as the central organisational form of software development. A project organisation depends on clear bounds regarding scope, participants, development effort and lead-time. What happens when these conditions are not given? The article claims that this is the case for software product specific ecosystems. As software is increasingly developed, adopted and deployed in the form of customisable and configurable products, software engineering as a discipline needs to take on the challenge to support software ecosystems.
Objective
The article provides a holistic understanding of the observed and reported practices as a starting point to device specific support for the development in software ecosystems.
Method
A qualitative interview study was designed based on previous long-term ethnographical inspired research.
Results
The analysis results in a set of common features of product development and evolution despite differences in size, kind of software and business models. Design is distributed and needs to be coordinated across heterogeneous design constituencies that, together with the software, build a product specific socio-technical ecosystem. The technical design has to support the deference of part of the development not only to 3rd-party developers but also to local designers tailoring the software in the use organisation. The technical interfaces that separate the work of different design constituencies are contested and need to be maintained permanently. Development takes place as cycles within cycles â€“ overlaying development cycles with different rhythms to accommodate different evolution drivers.
Conclusion
The reported practices challenge some of the very core assumptions of traditional software engineering, but makes perfect sense, considering that the frame of reference for product development is not a project but continuous innovation across the respective ecosystem. The article provides a number of concrete points for further research.},
  comment       = {21},
  doi           = {https://doi.org/10.1016/j.infsof.2014.02.012},
  keywords      = {Software ecosystems, Software product development, Qualitative empirical research},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584914000652},
}

@Article{Lung2016,
  author        = {Chung-Horng Lung and Xu Zhang and Pragash Rajeswaran},
  title         = {Improving software performance and reliability in a distributed and concurrent environment with an architecture-based self-adaptive framework},
  journal       = {Journal of Systems and Software},
  year          = {2016},
  volume        = {121},
  pages         = {311 - 328},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {More and more, modern software systems in a distributed and parallel environment are becoming highly complex and difficult to manage. A self-adaptive approach that integrates monitoring, analyzing, and actuation functionalities has the potential to accommodate an ever dynamically changing environment. This paper proposes an architecture-level self-adaptive framework with the aim of improving performance and reliability. To meet such a goal, this paper presents a Self-Adaptive Framework for Concurrency Architectures (SAFCA) that consists of multiple well-documented architectural patterns in addition to monitoring and adaptive capabilities. With this framework, a system using an architectural alternative can activate another alternative at runtime to cope with increasing demands or to recover from failure. Five adaptation mechanisms have been developed for concept demonstration and evaluation; four focus on performance improvement and one deals with failover and reliability enhancement. We have performed a number of experiments with this framework. The experimental results demonstrate that the proposed adaptive framework can mitigate the over-provisioning method commonly used in practice. As a result, resource usage becomes more efficient for most normal conditions, while the system is still able to effectively handle bursty or growing demands using an adaptive mechanism. The performance of SAFCA is also better than systems using only standalone architectural alternatives without an adaptation scheme. Moreover, the experimental results show that a fast recovery can be realized in the case of failure by conducting an architecture switchover to maintain the desired service.},
  comment       = {18},
  doi           = {https://doi.org/10.1016/j.jss.2016.06.102},
  keywords      = {Autonomic computing, Software architecture, Performance, Reliability, Patterns, Distributed and concurrent architecture, Elastic computing},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121216300991},
}

@Article{Coallier2005,
  author        = {FranÃ§ois Coallier and Roger Champagne},
  title         = {A Product Line engineering practices model},
  journal       = {Science of Computer Programming},
  year          = {2005},
  volume        = {57},
  number        = {1},
  pages         = {73 - 87},
  issn          = {0167-6423},
  note          = {System and Software Architectures},
  __markedentry = {[mac:]},
  abstract      = {This paper describes work in progress towards the elaboration of a Product Line practices model that combines concepts proposed by various authors. The strengths of existing Product Line frameworks and models are summarized and a new model is proposed in the form of 31 Product Line practice areas, grouped in five categories. An important objective of this Product Line practices model is that it should be easily incorporated into existing development methodologies, while remaining aligned with existing systems engineering standards.},
  comment       = {15},
  doi           = {https://doi.org/10.1016/j.scico.2004.10.006},
  keywords      = {Modeling, Product Lines, Software engineering, System analysis and design},
  url           = {http://www.sciencedirect.com/science/article/pii/S0167642304001923},
}

@Article{Amoui2012,
  author        = {Mehdi Amoui and Mahdi Derakhshanmanesh and JÃ¼rgen Ebert and Ladan Tahvildari},
  title         = {Achieving dynamic adaptation via management and interpretation of runtime models},
  journal       = {Journal of Systems and Software},
  year          = {2012},
  volume        = {85},
  number        = {12},
  pages         = {2720 - 2737},
  issn          = {0164-1212},
  note          = {Self-Adaptive Systems},
  __markedentry = {[mac:]},
  abstract      = {In this article, we present a generic model-centric approach for realizing fine-grained dynamic adaptation in software systems by managing and interpreting graph-based models of software at runtime. We implemented this approach as the Graph-based Runtime Adaptation Framework (GRAF), which is particularly tailored to facilitate and simplify the process of evolving and adapting current software towards runtime adaptivity. As a proof of concept, we present case study results that show how to achieve runtime adaptivity with GRAF and sketch the framework's capabilities for facilitating the evolution of real-world applications towards self-adaptive software. The case studies also provide some details of the GRAF implementation and examine the usability and performance of the approach.},
  comment       = {18},
  doi           = {https://doi.org/10.1016/j.jss.2012.05.033},
  keywords      = {Adaptation framework, Runtime adaptivity, Self-adaptive software, Model transformation, Models at runtime},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121212001458},
}

@Article{Alami2016,
  author        = {Adam Alami},
  title         = {Why Do Information Technology Projects Fail?},
  journal       = {Procedia Computer Science},
  year          = {2016},
  volume        = {100},
  pages         = {62 - 71},
  issn          = {1877-0509},
  note          = {International Conference on ENTERprise Information Systems/International Conference on Project MANagement/International Conference on Health and Social Care Information Systems and Technologies, CENTERIS/ProjMAN / HCist 2016},
  __markedentry = {[mac:]},
  abstract      = {With developing technological possibilities, IT projects are becoming increasingly ambitious in both goals and scale. Although technology itself is enabling easy management of project execution, failure can still occur, particularly with respect to an ample number of unique projects. It is argued here that the ampleness and uniqueness of projects provide criteria for such projects to be treated differently from smaller-scale enterprises of the same type. Gaps can be identified in the literature with regards to exact definitions of project success and failure. It is proposed that three main issues can impact a project's ecosystem and determine its failure, namely, uncertainty, volatility, and unknowns. Based on these aspects, future project performance can be estimated and correspondingly managed. At the same time, retrospective assessment of success or failure may be made rigorous based on exact definitions. Two case studies of major technological projects are presented and discussed here as examples of theory application.},
  comment       = {10},
  doi           = {https://doi.org/10.1016/j.procs.2016.09.124},
  keywords      = {project management, IT project failure, UK e-borders, LAUSD},
  url           = {http://www.sciencedirect.com/science/article/pii/S1877050916322918},
}

@Article{Shatnawi2017a,
  author        = {Anas Shatnawi and Abdelhak-Djamel Seriai and Houari Sahraoui and Zakarea Alshara},
  title         = {Reverse engineering reusable software components from object-oriented APIs},
  journal       = {Journal of Systems and Software},
  year          = {2017},
  volume        = {131},
  pages         = {442 - 460},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {Object-oriented Application Programing Interfaces (APIs) support software reuse by providing pre-implemented functionalities. Due to the huge number of included classes, reusing and understanding large APIs is a complex task. Otherwise, software components are accepted to be more reusable and understandable entities than object-oriented ones. Thus, in this paper, we propose an approach for reengineering object-oriented APIs into component-based ones. We mine components as a group of classes based on the frequency they are used together and their ability to form a quality-centric component. To validate our approach, we experimented on 100 Java applications that used four APIs.},
  comment       = {19},
  doi           = {https://doi.org/10.1016/j.jss.2016.06.101},
  keywords      = {Software reuse, Software component, Object-oriented, API, Reverse engineering, Frequent usage pattern},
  url           = {http://www.sciencedirect.com/science/article/pii/S016412121630098X},
}

@Article{Pinto2011,
  author        = {MÃ³nica Pinto and Lidia Fuentes and JosÃ© MarÃ­a Troya},
  title         = {Specifying aspect-oriented architectures in AO-ADL},
  journal       = {Information and Software Technology},
  year          = {2011},
  volume        = {53},
  number        = {11},
  pages         = {1165 - 1182},
  issn          = {0950-5849},
  note          = {AMOST 2010},
  __markedentry = {[mac:]},
  abstract      = {Context
Architecture description languages (ADLs) are a well-accepted approach to software architecture representation. The majority of well-known ADLs are defined by means of components and connectors. Architectural connectors are mainly used to model interactions among components, specifying component communication and coordination separately. However, there are other properties that cut across several components and also affect component interactions (e.g. security).
Objective
It seems reasonable therefore to model how such crosscutting properties affect component interactions as part of connectors.
Method
Using an aspect-oriented approach, the AO-ADL architecture description language extends the classical connector semantics with enough expressiveness to model the influences of such crosscutting properties on component interactions (defined as â€˜aspectual compositionsâ€™ in connectors).
Results
This paper describes the AO-ADL language putting special emphasis on the extended connectors used to specify aspectual and non-aspectual compositions between concrete components. The contributions of AO-ADL are validated using concern-oriented metrics available in the literature.
Conclusion
The measured indicators show that using AO-ADL it is possible to specify more reusable and scalable software architectures.},
  comment       = {18},
  doi           = {https://doi.org/10.1016/j.infsof.2011.04.003},
  keywords      = {Software Engineering, Software Architectures, Languages, Aspect-Oriented Software Development, Metrics},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584911001005},
}

@Article{Brosch2010,
  author        = {Franz Brosch and Ralf Gitzel and Heiko Koziolek and Simone Krug},
  title         = {Combining Architecture-based Software Reliability Predictions with Financial Impact Calculations},
  journal       = {Electronic Notes in Theoretical Computer Science},
  year          = {2010},
  volume        = {264},
  number        = {1},
  pages         = {3 - 17},
  issn          = {1571-0661},
  note          = {Proceedings of the 7th International Workshop on Formal Engineering approaches to Software Components and Architectures (FESCA 2010)},
  __markedentry = {[mac:]},
  abstract      = {Software failures can lead to substantial costs for the user. Existing models for software reliability prediction do not provide much insight into this financial impact. Our approach presents a first step towards the integration of reliability prediction from the IT perspective and the business perspective. We show that failure impact should be taken into account not only at their date of occurrence but already in the design stage of the development. First we model cost relevant business processes as well as the associated IT layer and then connect them to failure probabilities. Based on this we conduct a reliability and cost estimation. The method is illustrated by a case study.},
  comment       = {15},
  doi           = {https://doi.org/10.1016/j.entcs.2010.07.002},
  keywords      = {Software failure, Reliability, Cost},
  url           = {http://www.sciencedirect.com/science/article/pii/S1571066110000617},
}

@Article{Shin2007,
  author        = {Michael E. Shin and Hassan Gomaa},
  title         = {Software requirements and architecture modeling for evolving non-secure applications into secure applications},
  journal       = {Science of Computer Programming},
  year          = {2007},
  volume        = {66},
  number        = {1},
  pages         = {60 - 70},
  issn          = {0167-6423},
  note          = {Special Issue on the 5th International Workshop on System/Software Architectures (IWSSAâ€™06)},
  __markedentry = {[mac:]},
  abstract      = {This paper describes an approach to modeling the evolution of non-secure applications into secure applications in terms of the software requirements model and software architecture model. The requirements for security services are captured separately from application requirements, and the security services are encapsulated in connectors in the software architecture, separately from the components providing functional services. The enterprise architecture is described in terms of use case models, static models, and dynamic models. The software architecture is described in terms of components and connectors, which can be deployed to distributed configurations. By separating application concerns from security concerns, the evolution from a non-secure application to a secure application can be achieved with less impact on the application. An electronic commerce system is described to illustrate the approach.},
  comment       = {11},
  doi           = {https://doi.org/10.1016/j.scico.2006.10.009},
  keywords      = {Evolution, Security, Application system, Software requirements, Software architecture},
  url           = {http://www.sciencedirect.com/science/article/pii/S0167642306002486},
}

@Article{Mannaert2011,
  author        = {Herwig Mannaert and Jan Verelst and Kris Ven},
  title         = {The transformation of requirements into software primitives: Studying evolvability based on systems theoretic stability},
  journal       = {Science of Computer Programming},
  year          = {2011},
  volume        = {76},
  number        = {12},
  pages         = {1210 - 1222},
  issn          = {0167-6423},
  note          = {Special Issue on Software Evolution, Adaptability and Variability},
  __markedentry = {[mac:]},
  abstract      = {Evolvability is widely considered to be a crucial characteristic of software architectures, particularly in the area of information systems. Although many approaches have been proposed for improving evolvability, most indications are that it remains challenging to deliver the required levels of evolvability. In this paper, we present a theoretical approach to how the concept of systems theoretic stability can be applied to the evolvability of software architectures of information systems. We define and formalize the transformation of a set of basic functional requirements into a set of instantiations of software constructs. We define this transformation using both a static and a dynamic perspective. In the latter perspective, we formulate the postulate that information systems should be stable against new requirements. Based on this postulate, we derive a number of design theorems for software implementation. Using this transformation we use theoretical arguments to derive that these theorems contribute to achieving stability.},
  comment       = {13},
  doi           = {https://doi.org/10.1016/j.scico.2010.11.009},
  keywords      = {Systems theory, Normalized systems, Stability},
  url           = {http://www.sciencedirect.com/science/article/pii/S016764231000208X},
}

@Article{Wehrmeister2014,
  author        = {Marco AurÃ©lio Wehrmeister and Edison Pignaton de Freitas and AlÃ©cio Pedro Delazari Binotto and Carlos Eduardo Pereira},
  title         = {Combining aspects and object-orientation in model-driven engineering for distributed industrial mechatronics systems},
  journal       = {Mechatronics},
  year          = {2014},
  volume        = {24},
  number        = {7},
  pages         = {844 - 865},
  issn          = {0957-4158},
  note          = {1. Model-Based Mechatronic System Design 2. Model Based Engineering},
  __markedentry = {[mac:]},
  abstract      = {Recent advances in technology enable the creation of complex industrial systems comprising mechanical, electrical, and logical â€“ software â€“ components. It is clear that new project techniques are demanded to support the design of such systems. At design phase, it is extremely important to raise abstraction level in earlier stages of product development in order to deal with such a complexity in an efficient way. This paper discusses Model Driven Engineering (MDE) applied to design industrial mechatronics systems. An aspect-oriented MDE approach is presented by means of a real-world case study, comprising requirements engineering up to code generation. An assessment of two well-known high-level paradigms, namely Aspect- and Object-Oriented paradigms, is deeply presented. Their concepts are applied at every design step of an embedded and real-time mechatronics system, specifically for controlling a product assembler industrial cell. The handling of functional and non-functional requirements (at modeling level) using aspects and objects is further emphasized. Both designs are compared using a set of software engineering metrics, which were adapted to be applied at modeling level. Particularly, the achieved results show the suitability of each paradigm for the system specification in terms of reusability quality of model elements. Focused on the generated code for each case study, statistics depicted an improvement in number of lines using aspects.},
  comment       = {22},
  doi           = {https://doi.org/10.1016/j.mechatronics.2013.12.008},
  keywords      = {Model-Driven Engineering (MDE), Aspect Oriented Software Development (AOSD), Embedded and real-time system, Industrial mechatronics system, Design automation, Code generation},
  url           = {http://www.sciencedirect.com/science/article/pii/S0957415813002420},
}

@Article{Neto2013,
  author        = {Alberto Costa Neto and Rodrigo BonifÃ¡cio and MÃ¡rcio Ribeiro and Carlos Eduardo Pontual and Paulo Borba and Fernando Castor},
  title         = {A design rule language for aspect-oriented programming},
  journal       = {Journal of Systems and Software},
  year          = {2013},
  volume        = {86},
  number        = {9},
  pages         = {2333 - 2356},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {Aspect-oriented programming is known as a technique for modularizing crosscutting concerns. However, constructs aimed to support crosscutting modularity might actually break class modularity. As a consequence, class developers face changeability, parallel development and comprehensibility problems, because they must be aware of aspects whenever they develop or maintain a class. At the same time, aspects are vulnerable to changes in classes, since there is no contract specifying the points of interaction amongst these elements. These problems can be mitigated by using adequate design rules between classes and aspects. We present a design rule specification language and explore its benefits since the initial phases of the development process, specially with the aim of supporting modular development of classes and aspects. We discuss how our language improves crosscutting modularity without breaking class modularity. We evaluate it using a real case study and compare it with other approaches.},
  comment       = {24},
  doi           = {https://doi.org/10.1016/j.jss.2013.03.104},
  keywords      = {Aspect-oriented programming, Design rules, Modularity},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121213000861},
}

@Article{Arcaini2017a,
  author        = {Paolo Arcaini and Angelo Gargantini and Elvinia Riccobene and Paolo Vavassori},
  title         = {A novel use of equivalent mutants for static anomaly detection in software artifacts},
  journal       = {Information and Software Technology},
  year          = {2017},
  volume        = {81},
  pages         = {52 - 64},
  issn          = {0950-5849},
  __markedentry = {[mac:]},
  abstract      = {Context: In mutation analysis, a mutant of a software artifact, either a program or a model, is said equivalent if it leaves the artifact meaning unchanged. Equivalent mutants are usually seen as an inconvenience and they reduce the applicability of mutation analysis. Objective: Instead, we here claim that equivalent mutants can be useful to define, detect, and remove static anomalies, i.e., deficiencies of given qualities: If an equivalent mutant has a better quality value than the original artifact, then an anomaly has been found and removed. Method: We present a process for detecting static anomalies based on mutation, equivalence checking, and quality measurement. Results: Our proposal and the originating technique are applicable to different kinds of software artifacts. We present anomalies and conduct several experiments in different contexts, at specification, design, and implementation level. Conclusion: We claim that in mutation analysis a new research direction should be followed, in which equivalent mutants and operators generating them are welcome.},
  comment       = {13},
  doi           = {https://doi.org/10.1016/j.infsof.2016.01.019},
  keywords      = {Equivalent mutant, Static anomaly, Quality measure},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584916300180},
}

@Article{Lee2013,
  author        = {Seonah Lee and Sungwon Kang},
  title         = {Clustering navigation sequences to create contexts for guiding code navigation},
  journal       = {Journal of Systems and Software},
  year          = {2013},
  volume        = {86},
  number        = {8},
  pages         = {2154 - 2165},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {To guide programmer code navigation, previous approaches such as TeamTracks recommend pieces of code to visit by mining the associations between pieces of code in programmer interaction histories. However, these result in low recommendation accuracy. To create more accurate recommendations, we propose NavClus an approach that clusters navigation sequences from programmer interaction histories. NavClus automatically forms collections of code that are relevant to the tasks performed by programmers, and then retrieves the collections best matched to a programmer's current navigation path. This makes it possible to recommend the collections of code that are relevant to the programmer's given task. We compare NavClusâ€™ recommendation accuracy with TeamTracksâ€™ by simulating recommendations using 4397 interaction histories. The comparative experiment shows that the recommendation accuracy of NavClus is twice as high as that of TeamTracks.},
  comment       = {12},
  doi           = {https://doi.org/10.1016/j.jss.2013.03.103},
  keywords      = {Code navigation, Programmer interaction histories, Data clustering techniques, Data stream mining, Context aware code recommender},
  url           = {http://www.sciencedirect.com/science/article/pii/S016412121300085X},
}

@Article{Fleischmann2016,
  author        = {Marvin Fleischmann and Miglena Amirpur and Tillmann Grupp and Alexander Benlian and Thomas Hess},
  title         = {The role of software updates in information systems continuance â€” An experimental study from a user perspective},
  journal       = {Decision Support Systems},
  year          = {2016},
  volume        = {83},
  pages         = {83 - 96},
  issn          = {0167-9236},
  __markedentry = {[mac:]},
  abstract      = {Although software updates are a ubiquitous phenomenon in professional and private IT usage, they have to date received little attention in the IS post-adoption literature. Drawing on expectationâ€“confirmation theory and the IS continuance literature, we investigate whether, when and how software updates affect users' continuance intentions (CI). Based on a controlled laboratory experiment, we find a positive effect of feature updates on users' CI. According to this effect, software vendors can increase their users' CI by delivering features through updates after a software has been released and is already used by customers. We also find that users prefer frequent feature updates over less frequent update packages that bundle several features in one update. However, the positive effect from updates occurs only with functional feature updates and not with technical non-feature updates, disclosing update frequency and update type as crucial moderators to this effect. Furthermore, we unveil that this beneficial effect of feature updates operates through positive disconfirmation of expectations, resulting in increased perceived usefulness and satisfaction. Implications for research and practice as well as directions for future research are discussed.},
  comment       = {14},
  doi           = {https://doi.org/10.1016/j.dss.2015.12.010},
  keywords      = {Software updates, IT features, IS continuance, IS post-adoption, Expectationâ€“confirmation theory},
  url           = {http://www.sciencedirect.com/science/article/pii/S0167923616000026},
}

@Article{Guizzo2017,
  author        = {Giovani Guizzo and Silvia R. Vergilio and Aurora T.R. Pozo and Gian M. Fritsche},
  title         = {A multi-objective and evolutionary hyper-heuristic applied to the Integration and Test Order Problem},
  journal       = {Applied Soft Computing},
  year          = {2017},
  volume        = {56},
  pages         = {331 - 344},
  issn          = {1568-4946},
  __markedentry = {[mac:]},
  abstract      = {The field of Search-Based Software Engineering (SBSE) has widely utilized Multi-Objective Evolutionary Algorithms (MOEAs) to solve complex software engineering problems. However, the use of such algorithms can be a hard task for the software engineer, mainly due to the significant range of parameter and algorithm choices. To help in this task, the use of Hyper-heuristics is recommended. Hyper-heuristics can select or generate low-level heuristics while optimization algorithms are executed, and thus can be generically applied. Despite their benefits, we find only a few works using hyper-heuristics in the SBSE field. Considering this fact, we describe HITO, a Hyper-heuristic for the Integration and Test Order Problem, to adaptively select search operators while MOEAs are executed using one of the selection methods: Choice Function and Multi-Armed Bandit. The experimental results show that HITO can outperform the traditional MOEAs NSGA-II and MOEA/DD. HITO is also a generic algorithm, since the user does not need to select crossover and mutation operators, nor adjust their parameters.},
  comment       = {14},
  doi           = {https://doi.org/10.1016/j.asoc.2017.03.012},
  keywords      = {Metaheuristic, Hyper-heuristic, Multi-objective algorithm, Search-Based Software Engineering, Software testing},
  url           = {http://www.sciencedirect.com/science/article/pii/S1568494617301357},
}

@Article{Reinhartz-Berger2013,
  author        = {Iris Reinhartz-Berger and Arnon Sturm and Yair Wand},
  title         = {Comparing functionality of software systems: An ontological approach},
  journal       = {Data \& Knowledge Engineering},
  year          = {2013},
  volume        = {87},
  pages         = {320 - 338},
  issn          = {0169-023X},
  __markedentry = {[mac:]},
  abstract      = {Organizations can reduce the costs and enhance the quality of required software by adapting existing software systems. Software adaptation decisions often involve comparing alternatives on two criteria: (1) how well a system meets users' requirements and (2) the effort required for adapting the system. These criteria reflect two points of view â€” of users and of developers. Common to both views is the notion of functionality, which software developers have traditionally used for effort estimation utilizing concepts such as function points. However, users involved in selecting systems are not necessarily familiar with such concepts. We propose an approach for comparing software functionality from users' point of view. The approach employs ontological concepts to define functionality in terms of system behaviors. To evaluate whether or not the approach is also usable by software developers, we conducted an exploratory experiment. In the experiment, software engineering students ranked descriptions of software systems on the amount of changes needed to adapt the systems to given requirements. The results demonstrated that the ontological approach was usable after a short training and provided results comparable to ranking done by expert software developers. We also compared the ontological approach to a method which employed function point concepts. The results showed no statistically significant differences in performance, but there seemed to be an advantage to the ontological approach for cases that were difficult to analyze. Moreover, it took less time to apply the ontological approach than the function point-based approach, and the difference was statistically significant.},
  comment       = {19},
  doi           = {https://doi.org/10.1016/j.datak.2012.09.005},
  keywords      = {Software comparison, Variability management, Ontologies, Requirements engineering, Development effort estimation, Function point analysis},
  url           = {http://www.sciencedirect.com/science/article/pii/S0169023X12001073},
}

@Article{Oliveira2011,
  author        = {Toacy C. Oliveira and Paulo Alencar and Don Cowan},
  title         = {ReuseToolâ€”An extensible tool support for object-oriented framework reuse},
  journal       = {Journal of Systems and Software},
  year          = {2011},
  volume        = {84},
  number        = {12},
  pages         = {2234 - 2252},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {Object-oriented frameworks have become a popular paradigm used to improve the software development lifecycle. They promote reuse by providing a semi-complete architecture that can be extended through an instantiation process to integrate the needs of the new software application. Instantiation processes are typically enacted in an ad-hoc manner, which may lead to tedious and error-prone procedures. This work leverages our previous work on the definition of RDL, a language to facilitate the description of instantiation process, and describe the ReuseTool, which is an extensible tool to execute RDL programs and assist framework reuse by manipulating UML Diagrams. The ReuseTool integrates a RDL Compiler and a Workflow Engine to control most of the activities required to extend a framework design and, therefore, incorporates application-specific needs. This work also describes how the tool can be extended to incorporate new reuse activities and provides information of its use based on an exploratory Case Study.},
  comment       = {19},
  doi           = {https://doi.org/10.1016/j.jss.2011.06.030},
  keywords      = {UML, Object-oriented framework, Software process, Software reuse},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121211001531},
}

@Article{Palviainen2014,
  author        = {Marko Palviainen and Jarkko KuusijÃ¤rvi and Eila Ovaska},
  title         = {A semi-automatic end-user programming approach for smart space application development},
  journal       = {Pervasive and Mobile Computing},
  year          = {2014},
  volume        = {12},
  pages         = {17 - 36},
  issn          = {1574-1192},
  __markedentry = {[mac:]},
  abstract      = {This article describes a semi-automatic end-user programming approach that: (i) assists in the creation of easy-to-apply Semantic End-User Application Programming Interfaces(S-APIs) for the APIs of legacy software components; and (ii) enables the usage of S-APIs in command-oriented and goal-oriented end-user application programming. Furthermore, a reference implementation is presented for the approach that provides visual programming tools and an agent-based execution environment for smart space applications. The use of the approach is exemplified and tested in a case study in which S-APIs are created for a home automation system and for a personal assistant application, and then utilized in end-user programming performed in desktop and mobile environments.},
  comment       = {20},
  doi           = {https://doi.org/10.1016/j.pmcj.2013.04.002},
  keywords      = {Command-oriented end-user programming, Goal-oriented end-user programming, Smart Modeler, Smart space application, Ontology},
  url           = {http://www.sciencedirect.com/science/article/pii/S157411921300062X},
}

@Article{Binkley2015,
  author        = {Dave Binkley and Dawn Lawrie and Christopher Uehlinger and Daniel Heinz},
  title         = {Enabling improved IR-based feature location},
  journal       = {Journal of Systems and Software},
  year          = {2015},
  volume        = {101},
  pages         = {30 - 42},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {Recent solutions to software engineering problems have incorporated tools and techniques from information retrieval (IR). The use of IR requires choosing an appropriate retrieval model and deciding on a query that best captures a particular information need. Taking feature location as a representative example, three research questions are investigated: (1) the impact of query preprocessing, (2) the impact that different scraping techniques for queries have on retrieval performance, (3) the performance impact that the underlying retrieval model has on identifying the correct source-code functions (the correct documents). These research questions are addressed using the five open source projects released as part of the SEMERU dataset. In the experiments, five methods of scraping queries from modification requests and seven retrieval model instances are considered. Using the standard evaluation metric Mean Reciprocal Rank (MRR), the experimental analysis reveals that better retrieval models are not the ones commonly used by software engineering researchers. Results find that models based on query-likelihood perform about twice as well as models in common use in software engineering such as LSI and thus deserve greater attention. Furthermore, corpus preprocessing has a significant impact as the top performing setting is over 100% better than the average.},
  comment       = {13},
  doi           = {https://doi.org/10.1016/j.jss.2014.11.013},
  keywords      = {Information retrieval models, Query formulation, Feature location},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121214002428},
}

@Article{Baumann2015,
  author        = {Thomas Baumann and Sarah Harfst and Alice Swanger and Deborah Bayer and Amy Cell and William Boswell},
  title         = {Managing Successful Project Teams in a Diverse Stakeholder Environment: Merging Industry Best Practices with an Education System to Address Critical Human Factors},
  journal       = {Procedia - Social and Behavioral Sciences},
  year          = {2015},
  volume        = {194},
  pages         = {20 - 32},
  issn          = {1877-0428},
  note          = {Proceedings of the 2014 IPMA World Congress (Sept 29-Oct 1 â€“ Rotterdam, Netherlands)},
  __markedentry = {[mac:]},
  abstract      = {Across the United States, industry, education and government stakeholders are redefining their partnerships in order to address and reduce critical skill gaps in the current workforce. This represents a paradigm shift in technical and professional education, as well as the collaborative processes utilized in creating and maintaining complex multi-stakeholder talent development systems. In doing so, however, this paradigm shift also presents a huge challenge: education and public sectors are typically not familiar with matured product development and (project) management principles and often do not apply proven industry practices to the definition, design, delivery and improvement of their educational products. The evaluation, acceptance and ultimate implementation of those principles departs from the traditional (United States) culture of education, as it applies new instructional design processes, change implementation processes, and feedback/assessment models. Therefore, critical review of such methods, innovative development of transfer options and human factors management in successfully achieving the required systems change were identified, and include explanation, understanding, acceptance, personal development, and trust. Using a case study approach, this paper will analyze several highly visible and innovative adaptations of industry and educational standards which were accepted and released by all relevant stakeholders. By leveraging subject matter experts from both the industry and academic settings, Michigan Advanced Technician Training (MAT2) formed organizational and working teams comprised of the primary government, industry, and academic stakeholder groups and established a workable context for a knowledge transfer of best practice industry standards. Said standards were applied bi-directionally by academic providers and partnering manufacturing enterprises, and now serve as best practice examples for post-secondary systems of apprenticeship education.},
  comment       = {13},
  doi           = {https://doi.org/10.1016/j.sbspro.2015.06.140},
  keywords      = {Project management, stakeholder management, public projects, educational projects, quality and education, partnership management},
  url           = {http://www.sciencedirect.com/science/article/pii/S1877042815036198},
}

@Article{Clemente2011,
  author        = {Pedro J. Clemente and Juan HernÃ¡ndez and JosÃ© M. Conejero and Guadalupe Ortiz},
  title         = {Managing crosscutting concerns in component based systems using a model driven development approach},
  journal       = {Journal of Systems and Software},
  year          = {2011},
  volume        = {84},
  number        = {6},
  pages         = {1032 - 1053},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {Abstract
In the last few years, Model-Driven Development (MDD), Aspect-Oriented Software Development (AOSD), and Component-Based Software Development (CBSD) have become interesting alternatives for the design and construction of complex distributed applications. Although these methodological approaches share the principle of separation of concerns and their further integration as key factors to obtaining high-quality and evolvable large software systems, they usually each address this principle from their own particular perspective. In the present work, we combine Component-Based and Aspect-Oriented Software Developments in a Model Driven software process targeted at the development of complex systems. This process constitutes an enhancement of the separation of concerns by allowing the isolation of crosscutting concerns in both Platform Independent and Platform Specific models. Following a pure MDD philosophy, a set of model transformations are used to generate the system, from preliminary models to the final source code for the Corba Component Model platform. A twofold empirical analysis was used to evaluate the approachâ€™s benefits in terms of two internal quality attributes: modularity and complexity. Conclusions were drawn from this evaluation regarding other quality attributes correlated with these two â€“ stability, changeability, error-proneness, and reusability. An Eclipse plug-in was developed to drive the development of the entire system from early modeling to late deployment stages.},
  comment       = {22},
  doi           = {https://doi.org/10.1016/j.jss.2011.01.053},
  keywords      = {Model Driven Development (MDD), Crosscutting concerns, Aspect Oriented Software Development (AOSD), Component Based Software Development (CBSD), CORBA Component Model (CCM), Transformation models},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121211000434},
}

@Article{Neto2016,
  author        = {PlÃ¡cido A. Souza Neto and Genoveva Vargas-Solar and Umberto Souza da Costa and Martin A. Musicante},
  title         = {Designing service-based applications in the presence of non-functional properties: A mapping study},
  journal       = {Information and Software Technology},
  year          = {2016},
  volume        = {69},
  pages         = {84 - 105},
  issn          = {0950-5849},
  __markedentry = {[mac:]},
  abstract      = {Context
The development of distributed software systems has become an important problem for the software engineering community. Service-based applications are a common solution for this kind of systems. Services provide a uniform mechanism for discovering, integrating and using these resources. In the development of service based applications not only the functionality of services and compositions should be considered, but also conditions in which the system operates. These conditions are called non-functional requirements (NFR). The conformance of applications to NFR is crucial to deliver software that meets the expectations of its users.
Objective
This paper presents the results of a systematic mapping carried out to analyze how NFR have been addressed in the development of service-based applications in the last years, according to different points of view.
Method
Our analysis applies the systematic mapping approach. It focuses on the analysis of publications organized by categories called facets, which are combined to answer specific research questions. The facets compose a classification schema which is part of the contribution and results.
Results
This paper presents our findings on how NFR have been supported in the development of service-based applications by proposing a classification scheme consisting in five facets: (i) programming paradigm (object/service oriented); (ii) contribution (methodology, system, middleware); (iii) software process phase; (iv) technique or mathematical model used for expressing NFR; and (v) the types of NFR addressed by the papers, based on the classification proposed by the ISO/IEC 9126 specification. The results of our systematic mapping are presented as bubble charts that provide a quantitative analysis to show the frequencies of publications for each facet. The paper also proposes a qualitative analysis based on these plots. This analysis discusses how NFR (quality properties) have been addressed in the design and development of service-based applications, including methodologies, languages and tools devised to support different phases of the software process.
Conclusion
This systematic mapping showed that NFR are not fully considered in all software engineering phases for building service based applications. The study also let us conclude that work has been done for providing models and languages for expressing NFR and associated middleware for enforcing them at run time. An important finding is that NFR are not fully considered along all software engineering phases and this opens room for proposing methodologies that fully model NFR. The data collected by our work and used for this systematic mapping are available in https://github.com/placidoneto/systematic-mapping_service-based-app_nfr.},
  comment       = {22},
  doi           = {https://doi.org/10.1016/j.infsof.2015.09.004},
  keywords      = {Non-functional requirements, Service-based software process, Systematic mapping},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584915001573},
}

@Article{Pascual2015a,
  author        = {Gustavo G. Pascual and MÃ³nica Pinto and Lidia Fuentes},
  title         = {Self-adaptation of mobile systems driven by the Common Variability Language},
  journal       = {Future Generation Computer Systems},
  year          = {2015},
  volume        = {47},
  pages         = {127 - 144},
  issn          = {0167-739X},
  note          = {Special Section: Advanced Architectures for the Future Generation of Software-Intensive Systems},
  __markedentry = {[mac:]},
  abstract      = {The execution context in which pervasive systems or mobile computing run changes continually. Hence, applications for these systems require support for self-adaptation to the continual context changes. Most of the approaches for self-adaptive systems implement a reconfiguration service that receives as input the list of all possible configurations and the plans to switch between them. In this paper we present an alternative approach for the automatic generation of application configurations and the reconfiguration plans at runtime. With our approach, the generated configurations are optimal as regards different criteria, such as functionality or resource consumption (e.g. battery or memory). This is achieved by: (1) modelling architectural variability at design-time using the Common Variability Language (CVL), and (2) using a genetic algorithm that finds nearly-optimal configurations at run-time using the information provided by the variability model. We also specify a case study and we use it to evaluate our approach, showing that it is efficient and suitable for devices with scarce resources.},
  comment       = {18},
  doi           = {https://doi.org/10.1016/j.future.2014.08.015},
  keywords      = {Architectural variability, CVL, Dynamic reconfiguration, Genetic algorithm, Context, Pervasive systems},
  url           = {http://www.sciencedirect.com/science/article/pii/S0167739X14001630},
}

@Article{Bettini2013,
  author        = {Lorenzo Bettini and Ferruccio Damiani and Ina Schaefer and Fabio Strocco},
  title         = {TraitRecordJ: A programming language with traits and records},
  journal       = {Science of Computer Programming},
  year          = {2013},
  volume        = {78},
  number        = {5},
  pages         = {521 - 541},
  issn          = {0167-6423},
  note          = {Special section: Principles and Practice of Programming in Java 2009/2010 \& Special section: Self-Organizing Coordination},
  __markedentry = {[mac:]},
  abstract      = {Traits have been designed as units for fine-grained reuse of behavior in the object-oriented paradigm. Records have been devised to complement traits for fine-grained reuse of state. In this paper, we present the language TraitRecordJ, a Java dialect with records and traits. Records and traits can be composed by explicit linguistic operations, allowing code manipulations to achieve fine-grained code reuse. Classes are assembled from (composite) records and traits and instantiated to generate objects. We introduce the language through examples and illustrate the prototypical implementation of TraitRecordJ using Xtext, an Eclipse framework for the development of programming languages as well as other domain-specific languages. Our implementation comprises an Eclipse-based editor for TraitRecordJ with typical IDE functionalities, and a stand-alone compiler, which translates TraitRecordJ programs into standard Java programs. As a case study, we present the TraitRecordJ implementation of a part of the software used in a web-based information system previously implemented in Java.},
  comment       = {21},
  doi           = {https://doi.org/10.1016/j.scico.2011.06.007},
  keywords      = {Java, Trait, Type system, Implementation, Eclipse},
  url           = {http://www.sciencedirect.com/science/article/pii/S0167642311001572},
}

@Article{Lytra2015,
  author        = {Ioanna Lytra and Huy Tran and Uwe Zdun},
  title         = {Harmonizing architectural decisions with component view models using reusable architectural knowledge transformations and constraints},
  journal       = {Future Generation Computer Systems},
  year          = {2015},
  volume        = {47},
  pages         = {80 - 96},
  issn          = {0167-739X},
  note          = {Special Section: Advanced Architectures for the Future Generation of Software-Intensive Systems},
  __markedentry = {[mac:]},
  abstract      = {Architectural design decisions (ADDs) have been used in recent years for capturing design rationale and documenting architectural knowledge (AK). However, various architectural design views still provide the most common means for describing and communicating architectural design. The evolution of software systems requires that both ADDs and architectural design views are documented and maintained, which is a tedious and time-consuming task in the long run. Also, in lack of a systematic and automated support for bridging between ADDs and architectural design views, decisions and designs tend to become inconsistent over time. In our proposal, we introduce a reusable AK transformation language for supporting the automated transformation of reusable AK knowledge to component-and-connector models, the architectural design view used most commonly today. In addition, reusable consistency checking rules verify the consistency between decisions and designs. We evaluate our approach in an industrial case study and show that it offers high reusability, provides automation, and can, in principle, deal with large numbers of recurring decisions.},
  comment       = {17},
  doi           = {https://doi.org/10.1016/j.future.2014.11.010},
  keywords      = {Architectural decisions, Architectural design, Architectural knowledge, AK transformation language, Consistency checking},
  url           = {http://www.sciencedirect.com/science/article/pii/S0167739X14002441},
}

@Article{Lucena2013,
  author        = {Carlos Lucena and Ingrid Nunes},
  title         = {Contributions to the emergence and consolidation of Agent-oriented Software Engineering},
  journal       = {Journal of Systems and Software},
  year          = {2013},
  volume        = {86},
  number        = {4},
  pages         = {890 - 904},
  issn          = {0164-1212},
  note          = {SI : Software Engineering in Brazil: Retrospective and Prospective Views},
  __markedentry = {[mac:]},
  abstract      = {Many of the issues addressed with multi-agent approaches, such as distributed coordination and self-organization, are now becoming part of industrial and business systems. However, Multiagent Systems (MASs) are still not widely adopted in industry owing to the lack of a connection between MAS and software engineering. Since 2000, there is an effort to bridge this gap and to produce software engineering techniques for agent-based systems that guide the processes of design, development and maintenance. In Brazil, Agent-oriented Software Engineering (AOSE) was first investigated by the research group in the Software Engineering Laboratory (LES) at PUC-Rio, which after one decade of study in this area has built an AOSE community. This paper presents the history of AOSE at LES by discussing the sub-areas of MAS Software Engineering research and development that have been focus of the LES research group. We give examples of relevant results and present a subset of the extensive literature the group has produced during the last decade. We also report how we faced the challenges that emerged from our research by organizing and developing a research community at the intersection of software engineering, programming and MASs with a concern for scalability of solutions.},
  comment       = {15},
  doi           = {https://doi.org/10.1016/j.jss.2012.09.016},
  keywords      = {Multiagent systems, Agent-oriented Software Engineering, LES, PUC-Rio, SBES 25 years},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121212002567},
}

@Article{Turner2014,
  author        = {Hamilton Turner and Brian Dougherty and Jules White and Russell Kegley and Jonathan Preston and Douglas C. Schmidt and Aniruddha Gokhale},
  title         = {DRE system performance optimization with the SMACK cache efficiency metric},
  journal       = {Journal of Systems and Software},
  year          = {2014},
  volume        = {98},
  pages         = {25 - 43},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {System performance improvements are critical for the resource-limited environment of multiple integrated applications executing inside a single distributed real-time and embedded (DRE) system, such as integrated avionics platform or vehtronics systems. While processor caches can effectively reduce execution time there are several factors, such as cache size, system data sharing, and task execution schedule, which make it hard to quantify, predict, and optimize the cache usage of a DRE system. This article presents SMACK, a novel heuristic for estimating the hardware cache usage of a DRE system, and describes a method of varying the runtime behavior of DRE system software without (1) requiring extensive safety recertification or (2) violating the real-time scheduling deadlines. By using SMACK as a maximization target, we were able to reduce integrated DRE system execution time by an average of 2.4% and a maximum of 4.34%.},
  comment       = {18},
  doi           = {https://doi.org/10.1016/j.jss.2014.08.031},
  keywords      = {DRE, Deployment, Optimization, Heuristic, Cache},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121214001836},
}

@Article{Guo2016,
  author        = {Hai-Feng Guo},
  title         = {A semantic approach for automated test oracle generation},
  journal       = {Computer Languages, Systems \& Structures},
  year          = {2016},
  volume        = {45},
  pages         = {204 - 219},
  issn          = {1477-8424},
  __markedentry = {[mac:]},
  abstract      = {This paper presents the design, implementation, and applications of a software testing tool, TAO, which allows users to specify and generate test cases and oracles in a declarative way. Extended from its previous grammar-based test generation tool, TAO provides a declarative notation for defining denotational semantics on each productive grammar rule, such that when a test case is generated, its expected semantics will be evaluated automatically as well, serving as its test oracle. TAO further provides a simple tagging mechanism to embed oracles into test cases for bridging the automation between test case generation and software testing. Two practical case studies are used to illustrate how automated oracle generation can be effectively integrated with grammar-based test generation in different testing scenarios: locating fault-inducing input patterns on Java applications; and Selenium-based automated web testing.},
  comment       = {16},
  doi           = {https://doi.org/10.1016/j.cl.2016.01.006},
  keywords      = {Software testing, Test case generation, Test oracle, Denotational semantics},
  url           = {http://www.sciencedirect.com/science/article/pii/S147784241530021X},
}

@Article{Lochau2014,
  author        = {Malte Lochau and Sascha Lity and Remo Lachmann and Ina Schaefer and Ursula Goltz},
  title         = {Delta-oriented model-based integration testing of large-scale systems},
  journal       = {Journal of Systems and Software},
  year          = {2014},
  volume        = {91},
  pages         = {63 - 84},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {Software architecture specifications are of growing importance for coping with the complexity of large-scale systems. They provide an abstract view on the high-level structural system entities together with their explicit dependencies and build the basis for ensuring behavioral conformance of component implementations and interactions, e.g., using model-based integration testing. The increasing inherent diversity of such large-scale variant-rich systems further complicates quality assurance. In this article, we present a combination of architecture-driven model-based testing principles and regression-inspired testing strategies for efficient, yet comprehensive variability-aware conformance testing of variant-rich systems. We propose an integrated delta-oriented architectural test modeling and testing approach for component as well as integration testing that allows the generation and reuse of test artifacts among different system variants. Furthermore, an automated derivation of retesting obligations based on accurate delta-oriented architectural change impact analysis is provided. Based on a formal conceptual framework that guarantees stable test coverage for every system variant, we present a sample implementation of our approach and an evaluation of the validity and efficiency by means of a case study from the automotive domain.},
  comment       = {22},
  doi           = {https://doi.org/10.1016/j.jss.2013.11.1096},
  keywords      = {Large-scale systems, Model-based testing, Regression testing, Variable software architectures},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121213002781},
}

@Article{Eichelberger2009,
  author        = {Holger Eichelberger and Klaus Schmid},
  title         = {Guidelines on the aesthetic quality of UML class diagrams},
  journal       = {Information and Software Technology},
  year          = {2009},
  volume        = {51},
  number        = {12},
  pages         = {1686 - 1698},
  issn          = {0950-5849},
  note          = {Quality of UML Models},
  __markedentry = {[mac:]},
  abstract      = {In the past, formatting guidelines have proved to be a successful method to improve the readability of source code. With the increasing success of visual specification languages such as UML for model-driven software engineering visual guidelines are needed to standardize the presentation and the exchange of modeling diagrams with respect to human communication, understandability and readability. In this article, we introduce a new and encompassing taxonomy of visual guidelines capturing the aesthetic quality of UML class diagrams. We propose these guidelines as a framework to improve the aesthetic quality and thus the understandability of UML class diagrams. To validate this claim, we describe in detail a controlled experiment carried out as a pilot study to gather preliminary insights on the effects of some of the guideline rules on the understandability of UML class diagrams.},
  comment       = {13},
  doi           = {https://doi.org/10.1016/j.infsof.2009.04.008},
  keywords      = {Layout guidelines, Aesthetic quality, UML class diagrams, Software engineering, Modeling tools, Automatic layout},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584909000421},
}

@Article{Karus2011,
  author        = {Siim Karus and Marlon Dumas},
  title         = {Predicting the maintainability of XSL transformations},
  journal       = {Science of Computer Programming},
  year          = {2011},
  volume        = {76},
  number        = {12},
  pages         = {1161 - 1176},
  issn          = {0167-6423},
  note          = {Special Issue on Software Evolution, Adaptability and Variability},
  __markedentry = {[mac:]},
  abstract      = {XSLT is a popular language for implementing both presentation templates in Web applications as well as document and message converters in enterprise applications. The widespread adoption and popularity of XSLT raises the challenge of efficiently managing the evolution of significant amounts of XSLT code. This challenge calls for guidelines and tool support for developing maintainable XSLT code. In this setting, this paper addresses the following question: Can the maintainability of XSL transformations, measured in terms of code churn in the next revision of a transformation, be predicted using a combination of simple metrics? This question is studied using a dataset extracted from open-source software project repositories. An outcome of this empirical study is a set of statistical models for predicting the maintainability of XSL transformations with relatively high accuracy. In addition, by analyzing the major influencers of code churn in these models, the paper identifies guidelines for designing XSL transformations with reduced future churn.},
  comment       = {16},
  doi           = {https://doi.org/10.1016/j.scico.2010.12.006},
  keywords      = {Software maintenance, XML, XSLT, Software metrics},
  url           = {http://www.sciencedirect.com/science/article/pii/S0167642310002315},
}

@Article{Gerostathopoulos2016,
  author        = {Ilias Gerostathopoulos and Tomas Bures and Petr Hnetynka and Jaroslav Keznikl and Michal Kit and Frantisek Plasil and NoÃ«l Plouzeau},
  title         = {Self-adaptation in software-intensive cyberâ€“physical systems: From system goals to architecture configurations},
  journal       = {Journal of Systems and Software},
  year          = {2016},
  volume        = {122},
  pages         = {378 - 397},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {Design of self-adaptive software-intensive cyberâ€“physical systems (siCPS) operating in dynamic environments is a significant challenge when a sufficient level of dependability is required. This stems partly from the fact that the concerns of self-adaptivity and dependability are to an extent contradictory. In this paper, we introduce IRM-SA (Invariant Refinement Method for Self-Adaptation)â€”a design method and associated formally grounded model targeting siCPSâ€”that addresses self-adaptivity and supports dependability by providing traceability between system requirements, distinct situations in the environment, and predefined configurations of system architecture. Additionally, IRM-SA allows for architecture self-adaptation at runtime and integrates the mechanism of predictive monitoring that deals with operational uncertainty. As a proof of concept, it was implemented in DEECo, a component framework that is based on dynamic ensembles of components. Furthermore, its feasibility was evaluated in experimental settings assuming decentralized system operation.},
  comment       = {20},
  doi           = {https://doi.org/10.1016/j.jss.2016.02.028},
  keywords      = {Cyberâ€“physical systems, Self-adaptivity, Dependability},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121216000601},
}

@Article{Colombo2014,
  author        = {Massimo G. Colombo and Evila Piva and Cristina Rossi-Lamastra},
  title         = {Open innovation and within-industry diversification in small and medium enterprises: The case of open source software firms},
  journal       = {Research Policy},
  year          = {2014},
  volume        = {43},
  number        = {5},
  pages         = {891 - 902},
  issn          = {0048-7333},
  note          = {Open Innovation: New Insights and Evidence},
  __markedentry = {[mac:]},
  abstract      = {This paper examines the within-industry diversification of software small and medium enterprises that collaborate with the open source software community (OSS SMEs). In doing so, it offers new insights into the association between open innovation and diversification. We rely on arguments inspired by the literature and evidence collected through interviews with OSS SMEsâ€™ top managers to investigate factors that favor or hinder within-industry diversification. First, in line with the mainstream diversification literature, we focus attention on the role of firm size. Second, in the spirit of the open innovation research, we concentrate on the mechanisms that OSS SMEs put in place to get access to the external resources of the OSS community. Econometric evidence on 100 European OSS SMEs shows that firm size is negatively associated to within-industry diversification, while OSS SMEs that have contributed to a larger number of OSS projects have a more diversified portfolio of software products. Furthermore, we provide preliminary evidence that the practice of authorizing firm programmers to contribute autonomously to OSS projects of their own choice during working hours may be positively associated to within-industry diversification only if OSS SMEs possess adequate internal technological resources.},
  comment       = {12},
  doi           = {https://doi.org/10.1016/j.respol.2013.08.015},
  keywords      = {Open innovation, Within-industry diversification, Small and medium enterprises, Open Source community},
  url           = {http://www.sciencedirect.com/science/article/pii/S0048733313001601},
}

@Article{Sinnema2008,
  author        = {Marco Sinnema and Sybren Deelstra},
  title         = {Industrial validation of COVAMOF},
  journal       = {Journal of Systems and Software},
  year          = {2008},
  volume        = {81},
  number        = {4},
  pages         = {584 - 600},
  issn          = {0164-1212},
  note          = {Selected papers from the 10th Conference on Software Maintenance and Reengineering (CSMR 2006)},
  __markedentry = {[mac:]},
  abstract      = {COVAMOF is a variability management framework for product families that was developed to reduce the number of iterations required during product derivation and to reduce the dependency on experts. In this paper, we present the results of an experiment with COVAMOF in industry. The results show that with COVAMOF, engineers that are not involved in the product family were now capable of deriving the products in 100% of the cases, compared to 29% of the cases without COVAMOF. For experts, the use of COVAMOF reduced the number of iterations by 42%, and the total derivation time by 38%.},
  comment       = {17},
  doi           = {https://doi.org/10.1016/j.jss.2007.06.002},
  keywords      = {Product family engineering, Industrial validation, Software Variability Management},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121207001422},
}

@Article{Qu2014,
  author        = {Wei Qu and Yuanyuan Jia and Michael Jiang},
  title         = {Pattern mining of cloned codes in software systems},
  journal       = {Information Sciences},
  year          = {2014},
  volume        = {259},
  pages         = {544 - 554},
  issn          = {0020-0255},
  __markedentry = {[mac:]},
  abstract      = {Pattern mining of cloned codes in software systems is a challenging task due to various modifications and the large size of software codes. Most existing approaches adopt a token-based software representation and use sequential analysis for pattern mining of cloned codes. Due to the intrinsic limitations of such spatial space analysis, these methods have difficulties handling statement reordering, insertion and control replacement. Recently, graph-based models such as program dependent graph have been exploited to solve these issues. Although they can improve the performance in terms of accuracy, they introduce additional problems. Their computational complexity is very high and dramatically increases with the software size, thus limiting their applications in practice. In this paper, we propose a novel pattern mining framework for cloned codes in software systems. It efficiently exploits softwareâ€™s spatial space information as well as graph space information and thus can mine accurate patterns of cloned codes for software systems. Preliminary experimental results have demonstrated the superior performance of the proposed approach compared with other methods.},
  comment       = {0},
  doi           = {https://doi.org/10.1016/j.ins.2010.04.022},
  keywords      = {Pattern mining, Software clone detection, Software reuse detection, Software engineering},
  url           = {http://www.sciencedirect.com/science/article/pii/S0020025510001787},
}

@Article{Ferrari2013,
  author        = {Fabiano Cutigi Ferrari and Awais Rashid and JosÃ© Carlos Maldonado},
  title         = {Towards the practical mutation testing of AspectJ programs},
  journal       = {Science of Computer Programming},
  year          = {2013},
  volume        = {78},
  number        = {9},
  pages         = {1639 - 1662},
  issn          = {0167-6423},
  __markedentry = {[mac:]},
  abstract      = {Mutation testing is a test selection criterion that relies on the assumption that test cases which can reveal artificial faults in the software are also good to reveal the real ones. It helps to expose faults which would go otherwise unnoticed. This criterion has been shown to be a promising means to deal with testing-related specificities of contemporary programming techniques such as Aspect-Oriented Programming. However, to date the few initiatives for customising mutation testing for aspect-oriented (AO) programs show either limited coverage with respect to the range of simulated faults, or a need for both adequate tool support and proper evaluation in regard to properties like application cost and effectiveness. This article tackles these limitations by describing a comprehensive mutation-based testing approach for programs written in AspectJ, which represents the most investigated AO programming language to date. The approach encompasses the definition of a set of mutation operators for AspectJ-specific constructs and the implementation of a tool that automates the approach. The results of a preliminary evaluation study show that the mutation operators are able to simulate faults that may not be revealed by pre-existing, non-mutation-based test suites. The results also suggest that the approach seems not to overwhelm the testers and hence represents a step towards the practical fault-based testing of AspectJ-like programs.},
  comment       = {24},
  doi           = {https://doi.org/10.1016/j.scico.2013.02.011},
  keywords      = {Mutation testing, Aspect-oriented programming, AspectJ, Testing AspectJ programs, Test evaluation},
  url           = {http://www.sciencedirect.com/science/article/pii/S0167642313000713},
}

@Article{Pedrycz2011,
  author        = {Witold Pedrycz and Barbara Russo and Giancarlo Succi},
  title         = {A model of job satisfaction for collaborative development processes},
  journal       = {Journal of Systems and Software},
  year          = {2011},
  volume        = {84},
  number        = {5},
  pages         = {739 - 752},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {Modern software development relies on collaborative work as a means for sharing knowledge, distributing tasks and responsibilities, reducing risk of failures, and increasing the overall quality of the software product. Such objectives are achieved with a continuous share of the programmersâ€™ daily working life that inevitably influences the programmersâ€™ job satisfaction. One of the major challenges in process management is to determine the causes of this satisfaction. Traditional research models job satisfaction with social aspects of collaborative work like communication, work sustainability, and work environment. This study reflects on existing models of job satisfaction in collaborative environments, creates one for modern software development processes, and validates it with a retrospective comparative survey run on a sample of 108 respondents. In addition, the work investigates the impact on job satisfaction and its model of the agile practice of Pair Programming that pushes job sharing to the extreme. With this intent, the questionnaire also collected feedback from pair programmers whose responses were used for a comparative analysis. The results demonstrate that Pair Programming has actually a strong positive effect on satisfaction, work sustainability, and communication.},
  comment       = {14},
  doi           = {https://doi.org/10.1016/j.jss.2010.12.018},
  keywords      = {Job satisfaction, Pair programming, Log linear model},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121210003407},
}

@Article{Wallin2012,
  author        = {Peter Wallin and Stig Larsson and Joakim FrÃ¶berg and Jakob Axelsson},
  title         = {Problems and their mitigation in system and software architecting},
  journal       = {Information and Software Technology},
  year          = {2012},
  volume        = {54},
  number        = {7},
  pages         = {686 - 700},
  issn          = {0950-5849},
  __markedentry = {[mac:]},
  abstract      = {Context
Today, software and embedded systems act as enablers for developing new functionality in traditional industries such as the automotive, process automation, and manufacturing automation domains. This differs from 25â€“30years ago when these systems where based on electronics and electro-mechanical solutions. The architecture of the embedded system and of the software is important to ensure the qualities of these applications. However, the effort of designing and evolving the architecture is in practice often neglected during system development, whilst development efforts are centered on implementing new functionality.
Objective
We present problems and success factors that are central to the architectural development of software intensive systems in the domain of automotive and automation products as judged by practitioners.
Method
The method consisted of three steps. First, we used semi-structured interviews to collect data in an exploratory manner. As a second step, a survey based on problems extracted from the interview data was used to investigate the occurrence of these problems at a wider range of organizations. In order to identify and suggest how to mitigate the problems that were considered important, we finally performed root cause analysis workshops, and from these a number of success factors were elicited.
Results
A total of 21 problems have been identified based on the interview data, and these are related to the technical, organizational, project, and agreement processes. Based on the survey results, the following four problems were selected for a root cause analysis: (1) there is a lack of process for architecture development, (2) there is a lack of method or model to evaluate the business value when choosing the architecture, (3) there is a lack of clear long-term architectural strategy, and (4) processes and methods are less valued than knowledge and competence of individuals.
Conclusion
In conclusion, the following identified success factors are crucial components to be successful in developing software intensive systems: (1) define an architectural strategy, (2) implement a process for architectural work, (3) ensure authority for architects, (4) clarify the business impact of the architecture, and (5) optimize on the project portfolio level instead of optimizing each project.},
  comment       = {15},
  doi           = {https://doi.org/10.1016/j.infsof.2012.01.004},
  keywords      = {System and software architecture, Experience from practice, Success factors, Embedded systems},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584912000158},
}

@Article{Unphon2010,
  author        = {Hataichanok Unphon and Yvonne Dittrich},
  title         = {Software architecture awareness in long-term software product evolution},
  journal       = {Journal of Systems and Software},
  year          = {2010},
  volume        = {83},
  number        = {11},
  pages         = {2211 - 2226},
  issn          = {0164-1212},
  note          = {Interplay between Usability Evaluation and Software Development},
  __markedentry = {[mac:]},
  abstract      = {Software architecture has been established in software engineering for almost 40 years. When developing and evolving software products, architecture is expected to be even more relevant compared to contract development. However, the research results seem not to have influenced the development practice around software products very much. The architecture often only exists implicitly in discussions that accompany the development. Nonetheless many of the software products have been used for over 10, or even 20 years. How do development teams manage to accommodate changing needs and at the same time maintain the quality of the product? In order to answer this question, grounded theory study based on 15 semi-structured interviews was conducted in order to find out about the wide spectrum of architecture practices in software product developing organisations. Our results indicate that a chief architect or central developer acts as a â€˜walking architectureâ€™ devising changes and discussing local designs while at the same time updating his own knowledge about problematic aspects that need to be addressed. Architecture documentation and representations might not be used, especially if they replace the feedback from on-going developments into the â€˜architecturingâ€™ practices. Referring to results from Computer Supported Cooperative Work, we discuss how explicating the existing architecture needs to be complemented by social protocols to support the communication and knowledge sharing processes of the â€˜walking architectureâ€™.},
  comment       = {16},
  doi           = {https://doi.org/10.1016/j.jss.2010.06.043},
  keywords      = {Software products, Long-term evolution, Cooperative and human aspects, Software architecture, Architecture knowledge management, Qualitative empirical studies},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121210001743},
}

@Article{Rosemann2007,
  author        = {M. Rosemann and W.M.P. van der Aalst},
  title         = {A configurable reference modelling language},
  journal       = {Information Systems},
  year          = {2007},
  volume        = {32},
  number        = {1},
  pages         = {1 - 23},
  issn          = {0306-4379},
  __markedentry = {[mac:]},
  abstract      = {Enterprise Systems (ES) are comprehensive off-the-shelf packages that have to be configured to suit the requirements of an organization. Most ES solutions provide reference models that describe the functionality and structure of the system. However, these models do not capture the potential configuration alternatives. This paper discusses the shortcomings of current reference modelling languages using Event-Driven Process Chains (EPCs) as an example. We propose Configurable EPCs (C-EPCs) as an extended reference modelling language which allows capturing the core configuration patterns. A formalization of this language as well as examples for typical configurations are provided. A program of further research including the identification of a comprehensive list of configuration patterns, deriving possible notations for reference model configurations and testing the quality of these proposed extensions in experiments and focus groups is presented.},
  comment       = {23},
  doi           = {https://doi.org/10.1016/j.is.2005.05.003},
  keywords      = {Reference model, Enterprise systems, Configuration, Event-Driven Process Chains},
  url           = {http://www.sciencedirect.com/science/article/pii/S0306437905000487},
}

@Article{Clarke2017,
  author        = {Roger Clarke},
  title         = {Can small users recover from the cloud?},
  journal       = {Computer Law \& Security Review},
  year          = {2017},
  volume        = {33},
  number        = {6},
  pages         = {754 - 767},
  issn          = {0267-3649},
  __markedentry = {[mac:]},
  abstract      = {Large numbers of small organisations and prosumers have shifted away from managing data on their own devices and are now heavily reliant on service-providers for both storage and processing of their data. Most such entities are also dependent on those service-providers to perform backups and enable data recovery. Prior work defining users' backup needs was applied to this context in order to establish specifications for appropriate backup arrangements. A sample of service-providers was assessed against those specifications. Their backup and recovery mechanisms were found to fall seriously short of the need.},
  comment       = {14},
  doi           = {https://doi.org/10.1016/j.clsr.2017.08.004},
  keywords      = {Backup, Recovery, Cloud computing, SaaS, Small business, Consumers},
  url           = {http://www.sciencedirect.com/science/article/pii/S0267364917302789},
}

@Article{Cohen-Boulakia2017,
  author        = {Sarah Cohen-Boulakia and Khalid Belhajjame and Olivier Collin and JÃ©rÃ´me Chopard and Christine Froidevaux and Alban Gaignard and Konrad Hinsen and Pierre Larmande and Yvan Le Bras and FrÃ©dÃ©ric Lemoine and Fabien Mareuil and HervÃ© MÃ©nager and Christophe Pradal and Christophe Blanchet},
  title         = {Scientific workflows for computational reproducibility in the life sciences: Status, challenges and opportunities},
  journal       = {Future Generation Computer Systems},
  year          = {2017},
  volume        = {75},
  pages         = {284 - 298},
  issn          = {0167-739X},
  __markedentry = {[mac:]},
  abstract      = {With the development of new experimental technologies, biologists are faced with an avalanche of data to be computationally analyzed for scientific advancements and discoveries to emerge. Faced with the complexity of analysis pipelines, the large number of computational tools, and the enormous amount of data to manage, there is compelling evidence that many if not most scientific discoveries will not stand the test of time: increasing the reproducibility of computed results is of paramount importance. The objective we set out in this paper is to place scientific workflows in the context of reproducibility. To do so, we define several kinds of reproducibility that can be reached when scientific workflows are used to perform experiments. We characterize and define the criteria that need to be catered for by reproducibility-friendly scientific workflow systems, and use such criteria to place several representative and widely used workflow systems and companion tools within such a framework. We also discuss the remaining challenges posed by reproducible scientific workflows in the life sciences. Our study was guided by three use cases from the life science domain involving in silico experiments.},
  comment       = {15},
  doi           = {https://doi.org/10.1016/j.future.2017.01.012},
  keywords      = {Reproducibility, Scientific workflows, Provenance, Packaging environments},
  url           = {http://www.sciencedirect.com/science/article/pii/S0167739X17300316},
}

@Article{Garcia-Galan2016,
  author        = {JesÃºs GarcÃ­a-GalÃ¡n and Pablo Trinidad and Omer F. Rana and Antonio Ruiz-CortÃ©s},
  title         = {Automated configuration support for infrastructure migration to the cloud},
  journal       = {Future Generation Computer Systems},
  year          = {2016},
  volume        = {55},
  pages         = {200 - 212},
  issn          = {0167-739X},
  __markedentry = {[mac:]},
  abstract      = {With an increasing number of cloud computing offerings in the market, migrating an existing computational infrastructure to the cloud requires comparison of different offers in order to find the most suitable configuration. Cloud providers offer many configuration options, such as location, purchasing mode, redundancy, and extra storage. Often, the information about such options is not well organised. This leads to large and unstructured configuration spaces, and turns the comparison into a tedious, error-prone search problem for the customers. In this work we focus on supporting customer decision making for selecting the most suitable cloud configurationâ€”in terms of infrastructural requirements and cost. We achieve this by means of variability modelling and analysis techniques. Firstly, we structure the configuration space of an IaaS using feature models, usually employed for the modelling of variability-intensive systems, and present the case study of the Amazon EC2. Secondly, we assist the configuration search process. Feature models enable the use of different analysis operations that, among others, automate the search of optimal configurations. Results of our analysis show how our approach, with a negligible analysis time, outperforms commercial approaches in terms of expressiveness and accuracy.},
  comment       = {13},
  doi           = {https://doi.org/10.1016/j.future.2015.03.006},
  keywords      = {EC2, Automated analysis, Cloud migration, Feature model, IaaS},
  url           = {http://www.sciencedirect.com/science/article/pii/S0167739X15000618},
}

@Article{Wang2014,
  author        = {Dan Wang and Chao Qi and Hongwei Wang},
  title         = {Improving emergency response collaboration and resource allocation by task network mapping and analysis},
  journal       = {Safety Science},
  year          = {2014},
  volume        = {70},
  pages         = {9 - 18},
  issn          = {0925-7535},
  __markedentry = {[mac:]},
  abstract      = {Efficient resource allocation and collaboration among involved agencies are two essential prerequisites for successful emergency response. In order to contribute to reasonable resource allocation and targeted collaboration, this paper proposes a method of generating task network for emergency response based on the snowball procedure and an associated method of analyzing task network based on social network analysis. Firstly, the criticality of a task is evaluated using the proposed Weighted Proximity Prestige (WPP) index, which takes into account both the network structure and the dynamic urgency levels of response goals. Secondly, by calculating the WPP and observing its changing trend with time, shared resources for all response goals are identified. Thirdly, for each task, relations sink to it are ranked according their relative importance to provide explicit collaboration guidance. A case study based on the Beijing Flood Emergency Response Plan (2012 Amendment) is carried out to verify the rationality and effectiveness of the proposed method. The case study reveals that the WPP index particularly emphasizes tasks having dominating influence over on-site rescue actions, such as order maintenance, traffic route designing, and transportation resource coordination, which do not attract sufficient attentions in emergency response practices. Resources under severe contention as transit-related and man-power related tasks are identified based on the WPP index. Ranking the relations sinking to each task on a local scale provides more accurate information of working focuses to the agencies responsible for the task.},
  comment       = {10},
  doi           = {https://doi.org/10.1016/j.ssci.2014.05.005},
  keywords      = {Emergency response, Task network analysis, Weighted Proximity Prestige, Shared resources, Relation importance},
  url           = {http://www.sciencedirect.com/science/article/pii/S0925753514001106},
}

@Article{Sanchez2012a,
  author        = {Pedro SÃ¡nchez and Diego Alonso and JosÃ© Miguel Morales and Pedro Javier Navarro},
  title         = {From Teleo-Reactive specifications to architectural components: A model-driven approach},
  journal       = {Journal of Systems and Software},
  year          = {2012},
  volume        = {85},
  number        = {11},
  pages         = {2504 - 2518},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {The Teleo-Reactive approach designed by N.J. Nilsson offers a high-level programming model that permits the development of reactive systems, such as robotic vehicles. Teleo-Reactive programs are written in a manner that allows engineers to define the behaviour of the system while taking into account goals and changes in the state of the environment. This article presents a systematic approach that makes it possible to derive architectural models, with structural descriptions and behaviour, from Teleo-Reactive Programs. The development of reactive systems can therefore benefit significantly from a combination of two approaches: (1) the Teleo-Reactive approach, which is oriented towards a description of the system from the standpoint of the goals identified and the state of the environment and (2) the architectural approach, which is oriented towards the design of component-based software, in which decisions are conditioned by the need to reuse already tested solutions. The integration of this work into a development environment that allows code to be generated via model transformations opens up new possibilities in the development of this type of systems. The proposal is validated through a case study that is representative of the domain, and a survey carried out with post-graduate students.},
  comment       = {15},
  doi           = {https://doi.org/10.1016/j.jss.2012.05.067},
  keywords      = {Teleo-Reactive programs, Component-based software development, Reactive systems, Robotics, Model-driven software development},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121212001537},
}

@Article{Ghanam2012,
  author        = {Yaser Ghanam and Frank Maurer and Pekka Abrahamsson},
  title         = {Making the leap to a software platform strategy: Issues and challenges},
  journal       = {Information and Software Technology},
  year          = {2012},
  volume        = {54},
  number        = {9},
  pages         = {968 - 984},
  issn          = {0950-5849},
  __markedentry = {[mac:]},
  abstract      = {Context
While there are many success stories of achieving high reuse and improved quality using software platforms, there is a need to investigate the issues and challenges organizations face when transitioning to a software platform strategy.
Objective
This case study provides a comprehensive taxonomy of the challenges faced when a medium-scale organization decided to adopt software platforms. The study also reveals how new trends in software engineering (i.e. agile methods, distributed development, and flat management structures) interplayed with the chosen platform strategy.
Method
We used an ethnographic approach to collect data by spending time at a medium-scale company in Scandinavia. We conducted 16in-depth interviews with representatives of eight different teams, three of which were working on three separate platforms. The collected data was analyzed using Grounded Theory.
Results
The findings identify four classes of challenges, namely: business challenges, organizational challenges, technical challenges, and people challenges. The article explains how these findings can be used to help researchers and practitioners identify practical solutions and required tool support.
Conclusion
The organizationâ€™s decision to adopt a software platform strategy introduced a number of challenges. These challenges need to be understood and addressed in order to reap the benefits of reuse. Researchers need to further investigate issues such as supportive organizational structures for platform development, the role of agile methods in software platforms, tool support for testing and continuous integration in the platform context, and reuse recommendation systems.},
  comment       = {17},
  doi           = {https://doi.org/10.1016/j.infsof.2012.03.005},
  keywords      = {Software platform, Software reuse, Platform challenges, Ethnographic study, Grounded Theory},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584912000547},
}

@Article{Heradio2016a,
  author        = {Ruben Heradio and Hector Perez-Morago and Mauricio AlfÃ©rez and David Fernandez-Amoros and GermÃ¡n H. AlfÃ©rez},
  title         = {Augmenting measure sensitivity to detect essential, dispensable and highly incompatible features in mass customization},
  journal       = {European Journal of Operational Research},
  year          = {2016},
  volume        = {248},
  number        = {3},
  pages         = {1066 - 1077},
  issn          = {0377-2217},
  __markedentry = {[mac:]},
  abstract      = {Mass customization is the new frontier in business competition for both manufacturing and service industries. To improve customer satisfaction, reduce lead-times and shorten costs, families of similar products are built jointly by combining reusable parts that implement the features demanded by the customers. To guarantee the validity of the products derived from mass customization processes, feature dependencies and incompatibilities are usually specified with a variability model. As market demand grows and evolves, variability models become increasingly complex. In such entangled models it is hard to identify which features are essential, dispensable, highly required by other features, or highly incompatible with the remaining features. This paper exposes the limitations of existing approaches to gather such knowledge and provides efficient algorithms to retrieve that information from variability models.},
  comment       = {12},
  doi           = {https://doi.org/10.1016/j.ejor.2015.08.005},
  keywords      = {Mass customization, Product platform, Variability modeling, Binary decision diagram},
  url           = {http://www.sciencedirect.com/science/article/pii/S0377221715007225},
}

@Article{Li2015,
  author        = {Zonghua Li and Xiaofeng Zhou and Aihua Gu and Qinfeng Li},
  title         = {A complete approach for CIM modelling and model formalising},
  journal       = {Information and Software Technology},
  year          = {2015},
  volume        = {65},
  pages         = {39 - 55},
  issn          = {0950-5849},
  __markedentry = {[mac:]},
  abstract      = {Context
Computation Independent Model (CIM) as a business model describes the requirements and environment of a business system and instructs the designing and development; it is a key to influencing software success. Although many studies currently focus on model driven development (MDD); those researches, to a large extent, study the PIM-level and PSM-level model, and few have dealt with CIM-level modelling for case in which the requirements are unclear or incomplete.
Objective
This paper proposes a CIM-level modelling approach, which applies a stepwise refinement approach to modelling the CIM-level model starting from a high-level goal model to a lower-level business process model. A key advantage of our approach is the combination of the requirement model with the business model, which helps software engineers to define business models exactly for cases in which the requirements are unclear or incomplete.
Method
This paper, based on the model driven approach, proposes a set of models at the CIM-level and model transformations to connect these models. Accordingly, the formalisation approach of this paper involves formalising the goal model using the category theory and the scenario model and business process model using Petri nets.
Results
We have defined a set of metamodels and transformation rules making it possible to obtain automatically a scenario model from the goal model and a business process model from the scenario model. At the same time, we have defined a mapping rule to formalise these models. Our proposed CIM modelling approach and formalisation approach are implemented with an MDA tool, and it has been empirically validated by a travel agency case study.
Conclusion
This study shows how a CIM modelling approach helps to build a complete and consistent model at the CIM level for cases in which the requirements are unclear or incomplete in advance.},
  comment       = {17},
  doi           = {https://doi.org/10.1016/j.infsof.2015.04.003},
  keywords      = {CIM, Model formalisation, Petri nets, Model transformations, Model consistency verification},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584915000786},
}

@Article{Hauksdottir2014,
  author        = {DagnÃ½ HauksdÃ³ttir and Niels Henrik Mortensen and Poul Erik Nielsen},
  title         = {Identified adjustability dimensions when generating a product specific requirements specification by requirements reuse},
  journal       = {Computers in Industry},
  year          = {2014},
  volume        = {65},
  number        = {6},
  pages         = {952 - 966},
  issn          = {0166-3615},
  __markedentry = {[mac:]},
  abstract      = {A requirements reuse setups typically includes reusable requirement set(s) containing a collection of reusable requirements and a number of product specific requirements sets which are drawn from the reusable set(s). The ideal scenario when reusing requirements is that all the product requirements can be drawn directly from the reusable set. However, this is rarely the case in product development as new requirements are likely to surface. A critical issue in requirements reuse therefore becomes how to enable products to efficiently reuse requirements as well incorporating changes to the product set. In this paper the objective is not to present a specific method for requirements reuse but to introduce and discuss the possible dimensions of adjustability when generating a product requirement set by reusing requirements from a reusable set. Six adjustability dimensions have been identified. An extensive state of the art is included to introduce the presented methods related to each adjustability dimensions. The options for implementing each adjustability dimensions in a requirement reuse approach are illustrated along with a discussion regarding the benefits and issues resulting from each option. This discussion should help practitioners to better understand the possible methods that can be implemented and to design a user friendly and sustainable approach. A case study, describing how the dimensions are incorporated in two requirements reuse approaches, for Danfoss Solar Inverters (SI) and Danfoss Frequency Drives is provided. As a result an overview of how each adjustability dimensions is implemented in each case is presented. The case study demonstrates that all the identified adjustability dimensions were important elements in requirements reuse implementation. The case study furthermore highlights the need, not only to understand the effects of each adjustability dimension but also of the dependencies to case specific criterions. The classification of adjustability dimensions in requirements reuse and the options for their implementation has not been outlined by previous research and should be a useful contribution both to researchers and practitioners working in the field of requirements reuse.},
  comment       = {15},
  doi           = {https://doi.org/10.1016/j.compind.2014.02.011},
  keywords      = {Requirements reuse, Requirements specification, Product development, Adjustability dimensions},
  url           = {http://www.sciencedirect.com/science/article/pii/S0166361514000451},
}

@Article{Olszak2012,
  author        = {Andrzej Olszak and Bo NÃ¸rregaard JÃ¸rgensen},
  title         = {Remodularizing Java programs for improved locality of feature implementations in source code},
  journal       = {Science of Computer Programming},
  year          = {2012},
  volume        = {77},
  number        = {3},
  pages         = {131 - 151},
  issn          = {0167-6423},
  note          = {Feature-Oriented Software Development (FOSD 2009)},
  __markedentry = {[mac:]},
  abstract      = {Explicit traceability between features and source code is known to help programmers to understand and modify programs during maintenance tasks. However, the complex relations between features and their implementations are not evident from the source code of object-oriented Java programs. Consequently, the implementations of individual features are difficult to locate, comprehend, and modify in isolation. In this paper, we present a novel remodularization approach that improves the representation of features in the source code of Java programs. Both forward and reverse restructurings are supported through on-demand bidirectional restructuring between feature-oriented and object-oriented decompositions. The approach includes a feature location phase based on tracing of program execution, a feature representation phase that reallocates classes into a new package structure based on single-feature and multi-feature packages, and an annotation-based reverse transformation of code. Case studies performed on two open-source projects indicate that our approach requires relatively little manual effort and reduces tangling and scattering of feature implementations in the source code.},
  comment       = {21},
  doi           = {https://doi.org/10.1016/j.scico.2010.10.007},
  keywords      = {Features, Remodularization, Feature location, Fragile decomposition problem},
  url           = {http://www.sciencedirect.com/science/article/pii/S0167642310001917},
}

@Article{Cunningham2006,
  author        = {H. Conrad Cunningham and Yi Liu and Cuihua Zhang},
  title         = {Using classic problems to teach Java framework design},
  journal       = {Science of Computer Programming},
  year          = {2006},
  volume        = {59},
  number        = {1},
  pages         = {147 - 169},
  issn          = {0167-6423},
  note          = {Special Issue on Principles and Practices of Programming in Java (PPPJ 2004)},
  __markedentry = {[mac:]},
  abstract      = {All programmers should understand the concept of software families and know the techniques for constructing them. This paper suggests that classic problems, such as well-known algorithms and data structures, are good sources for examples to use in a study of software family design. The paper describes two case studies that can be used to introduce students in a Java software design course to the construction of software families using software frameworks. The first is the family of programs that use the well-known divide and conquer algorithmic strategy. The second is the family of programs that carry out traversals of binary trees.},
  comment       = {23},
  doi           = {https://doi.org/10.1016/j.scico.2005.07.009},
  keywords      = {Software family, Software framework, Hot spot, Design pattern, Divide and conquer, Tree traversal},
  url           = {http://www.sciencedirect.com/science/article/pii/S0167642305000900},
}

@Article{Torrecilla-Salinas2016,
  author        = {C.J. Torrecilla-Salinas and J. SedeÃ±o and M.J. Escalona and M. MejÃ­as},
  title         = {Agile, Web Engineering and Capability Maturity Model Integration: A systematic literature review.},
  journal       = {Information and Software Technology},
  year          = {2016},
  volume        = {71},
  pages         = {92 - 107},
  issn          = {0950-5849},
  __markedentry = {[mac:]},
  abstract      = {Context
Agile approaches are an alternative for organizations developing software, particularly for those who develop Web applications. Besides, CMMI (Capability Maturity Model Integration) models are well-established approaches focused on assessing the maturity of an organization that develops software. Web Engineering is the field of Software Engineering responsible for analyzing and studying the specific characteristics of the Web. The suitability of an Agile approach to help organizations reach a certain CMMI maturity level in Web environments will be very interesting, as they will be able to keep the ability to quickly react and adapt to changes as long as their development processes get mature.
Objective
This paper responds to whether it is feasible or not, for an organization developing Web systems, to achieve a certain maturity level of the CMMI-DEV model using Agile methods.
Method
The proposal is analyzed by means of a systematic literature review of the relevant approaches in the field, defining a characterization schema in order to compare them to introduce the current state-of-the-art.
Results
The results achieved after the systematic literature review are presented, analyzed and compared against the defined schema, extracting relevant conclusions for the different dimensions of the problem: compatibility, compliance, experience, maturity and Web.
Conclusion
It is concluded that although the definition of an Agile approach to meet the different CMMI maturity levels goals could be possible for an organization developing Web systems, there is still a lack of detailed studies and analysis on the field.},
  comment       = {15},
  doi           = {https://doi.org/10.1016/j.infsof.2015.11.002},
  keywords      = {Agile, Scrum, Web Engineering, CMMI, Software Engineering},
  url           = {http://www.sciencedirect.com/science/article/pii/S095058491500186X},
}

@Article{Milani2016,
  author        = {Fredrik Milani and Marlon Dumas and Naved Ahmed and Raimundas MatuleviÄius},
  title         = {Modelling families of business process variants: A decomposition driven method},
  journal       = {Information Systems},
  year          = {2016},
  volume        = {56},
  pages         = {55 - 72},
  issn          = {0306-4379},
  __markedentry = {[mac:]},
  abstract      = {Business processes usually do not exist as singular entities that can be managed in isolation, but rather as families of business process variants. When modelling such families of variants, analysts are confronted with the choice between modelling each variant separately, or modelling multiple or all variants in a single model. Modelling each variant separately leads to a proliferation of models that share common parts, resulting in redundancies and inconsistencies. Meanwhile, modelling all variants together leads to less but more complex models, thus hindering on comprehensibility. This paper introduces a method for modelling families of process variants that addresses this trade-off. The key tenet of the method is to alternate between steps of decomposition (breaking down processes into sub-processes) and deciding which parts should be modelled together and which ones should be modelled separately. We have applied the method to two case studies: one concerning the consolidation of existing process models, and another dealing with green-field process discovery. In both cases, the method produced fewer models with respect to the baseline and reduced duplicity by up to 50% without significant impact on complexity.},
  comment       = {18},
  doi           = {https://doi.org/10.1016/j.is.2015.09.003},
  keywords      = {Business process modelling, Business process variant, Business process model consolidation},
  url           = {http://www.sciencedirect.com/science/article/pii/S0306437915001684},
}

@Article{Sobernig2016a,
  author        = {Stefan Sobernig and Bernhard Hoisl and Mark Strembeck},
  title         = {Extracting reusable design decisions for UML-based domain-specific languages: A multi-method study},
  journal       = {Journal of Systems and Software},
  year          = {2016},
  volume        = {113},
  pages         = {140 - 172},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {When developing domain-specific modeling languages (DSMLs), software engineers have to make a number of important design decisions on the DSML itself, or on the software-development process that is applied to develop the DSML. Thus, making well-informed design decisions is a critical factor in developing DSMLs. To support this decision-making process, the model-driven development community has started to collect established design practices in terms of patterns, guidelines, story-telling, and procedural models. However, most of these documentation practices do not capture the details necessary to reuse the rationale behind these decisions in other DSML projects. In this paper, we report on a three-year research effort to compile and to empirically validate a catalog of structured decision descriptions (decision records) for UML-based DSMLs. This catalog is based on design decisions extracted from 90 DSML projects. These projects were identifiedâ€”among othersâ€”via an extensive systematic literature review (SLR) for the years 2005â€“2012. Based on more than 8,000 candidate publications, we finally selected 84 publications for extracting design-decision data. The extracted data were evaluated quantitatively using a frequent-item-set analysis to obtain characteristic combinations of design decisions and qualitatively to document recurring documentation issues for UML-based DSMLs. We revised the collected decision records based on this evidence and made the decision-record catalog for developing UML-based DSMLs publicly available. Furthermore, our study offers insights into UML usage (e.g. diagram types) and into the adoption of UML extension techniques (e.g. metamodel extensions, profiles).},
  comment       = {33},
  doi           = {https://doi.org/10.1016/j.jss.2015.11.037},
  keywords      = {Domain-specific language, Unified modeling language, Design decision, Design rationale, Domain-specific modeling, Model-driven development},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121215002617},
}

@Article{Reinhartz-Berger2009,
  author        = {Iris Reinhartz-Berger and Arnon Sturm},
  title         = {Utilizing domain models for application design and validation},
  journal       = {Information and Software Technology},
  year          = {2009},
  volume        = {51},
  number        = {8},
  pages         = {1275 - 1289},
  issn          = {0950-5849},
  __markedentry = {[mac:]},
  abstract      = {Domain analysis enables identifying families of applications and capturing their terminology in order to assist and guide system developers to design valid applications in the domain. One major way of carrying out the domain analysis is modeling. Several studies suggest using metamodeling techniques, feature-oriented approaches, or architectural-based methods for modeling domains and specifying applications in those domains. However, these methods mainly focus on representing the domain knowledge, providing insufficient guidelines (if any) for creating application models that satisfy the domain rules and constraints. In particular, validation of the application models which include application-specific knowledge is insufficiently dealt. In order to fill these lacks, we propose a general approach, called Application-based DOmain Modeling (ADOM), which enables specifying domains and applications similarly, (re)using domain knowledge in application models, and validating the application models against the relevant domain models. In this paper we present the ADOM approach, demonstrating its application to UML 2.0 class and sequence diagrams.},
  comment       = {15},
  doi           = {https://doi.org/10.1016/j.infsof.2009.03.005},
  keywords      = {Domain engineering, Software product line engineering, Domain analysis, Metamodeling, Feature oriented, Variability management},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584909000366},
}

@Article{Apel2012,
  author        = {Sven Apel and Sergiy Kolesnikov and JÃ¶rg Liebig and Christian KÃ¤stner and Martin Kuhlemann and Thomas Leich},
  title         = {Access control in feature-oriented programming},
  journal       = {Science of Computer Programming},
  year          = {2012},
  volume        = {77},
  number        = {3},
  pages         = {174 - 187},
  issn          = {0167-6423},
  note          = {Feature-Oriented Software Development (FOSD 2009)},
  __markedentry = {[mac:]},
  abstract      = {In feature-oriented programming (FOP) a programmer decomposes a program in terms of features. Ideally, features are implemented modularly so that they can be developed in isolation. Access control mechanisms in the form of access or visibility modifiers are an important ingredient to attain feature modularity as they allow programmers to hide and expose internal details of a moduleâ€™s implementation. But developers of contemporary feature-oriented languages have not considered access control mechanisms so far. The absence of a well-defined access control model for FOP breaks encapsulation of feature code and leads to unexpected program behaviors and inadvertent type errors. We raise awareness of this problem, propose three feature-oriented access modifiers, and present a corresponding access modifier model. We offer an implementation of the model on the basis of a fully-fledged feature-oriented compiler. Finally, by analyzing ten feature-oriented programs, we explore the potential of feature-oriented modifiers in FOP.},
  comment       = {14},
  doi           = {https://doi.org/10.1016/j.scico.2010.07.005},
  keywords      = {Feature-oriented programming, Feature modularity, Access control, Access modifier model, },
  url           = {http://www.sciencedirect.com/science/article/pii/S0167642310001528},
}

@Article{Assuncao2014,
  author        = {Wesley Klewerton Guez AssunÃ§Ã£o and Thelma Elita Colanzi and Silvia Regina Vergilio and Aurora Pozo},
  title         = {A multi-objective optimization approach for the integration and test order problem},
  journal       = {Information Sciences},
  year          = {2014},
  volume        = {267},
  pages         = {119 - 139},
  issn          = {0020-0255},
  __markedentry = {[mac:]},
  abstract      = {A common problem found during the integration testing is to determine an order to integrate and test the units. Important factors related to stubbing costs and constraints regarding to the software development context must be considered. To solve this problem, the most promising results were obtained with multi-objective algorithms, however few algorithms and contexts have been addressed by existing works. Considering such fact, this paper aims at introducing a generic approach based on multi-objective optimization to be applied in different development contexts and with distinct multi-objective algorithms. The approach is instantiated in the object and aspect-oriented contexts, and evaluated with real systems and three algorithms: NSGA-II, SPEA2 and PAES. The algorithms are compared by using different number of objectives and four quality indicators. Results point out that the characteristics of the systems, the instantiation context and the number of objectives influence on the behavior of the algorithms. Although for more complex systems, PAES reaches better results, NSGA-II is more suitable to solve the referred problem in general cases, considering all systems and indicators.},
  comment       = {21},
  doi           = {https://doi.org/10.1016/j.ins.2013.12.040},
  keywords      = {Search-based algorithm, Integration testing, Multi-objective optimization},
  url           = {http://www.sciencedirect.com/science/article/pii/S0020025513008967},
}

@Article{Costa2016,
  author        = {Bruno Costa and Paulo F. Pires and FlÃ¡via C. Delicato and Paulo Merson},
  title         = {Evaluating REST architecturesâ€”Approach, tooling and guidelines},
  journal       = {Journal of Systems and Software},
  year          = {2016},
  volume        = {112},
  pages         = {156 - 180},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {Architectural decisions determine the ability of the implemented system to satisfy functional and quality attribute requirements. The Representational State Transfer (REST) architectural style has been extensively used recently for integrating services and applications. Its adoption to build SOA-based distributed systems brings several benefits, but also poses new challenges and risks. Particularly important among those risks are failures to effectively address quality attribute requirements such as security, reliability, and performance. A proved efficient technique to identify and help mitigate those risks is the architecture evaluation. In this paper we propose an approach, tooling, and guidelines to aid architecture evaluation activities in REST-based systems. These guidelines can be systematically used along with evaluation methods to reason about design considerations and tradeoffs. To demonstrate how the guidelines can help architecture evaluators, we present a proof of concept describing how to use the guidelines in an ATAM (Architecture Tradeoff Analysis Method) evaluation. We also present the results of a survey conducted with industry specialists who have performed architecture evaluations in real world REST-based systems in order to gauge the suitability and utility of the proposed guidelines. Finally, the paper describes a Web tool developed to facilitate the use of the evaluation guidelines.},
  comment       = {25},
  doi           = {https://doi.org/10.1016/j.jss.2015.09.039},
  keywords      = {Software architecture evaluation, Scenario-based evaluation guidelines, REST},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121215002150},
}

@Article{Garcia-Diaz2010,
  author        = {Vicente GarcÃ­a-DÃ­az and HÃ©ctor FernÃ¡ndez-FernÃ¡ndez and ElÃ­as Palacios-GonzÃ¡lez and B. Cristina Pelayo G-Bustelo and Oscar SanjuÃ¡n-MartÃ­nez and Juan Manuel Cueva Lovelle},
  title         = {TALISMAN MDE: Mixing MDE principles},
  journal       = {Journal of Systems and Software},
  year          = {2010},
  volume        = {83},
  number        = {7},
  pages         = {1179 - 1191},
  issn          = {0164-1212},
  note          = {SPLC 2008},
  __markedentry = {[mac:]},
  abstract      = {The Model-Driven Engineering approach is progressively gaining popularity in the software engineering community as it raises the level of abstraction in software development. In TALISMAN MDE framework, we combine the principles of the two most important initiatives, Model-Driven Architecture and Software Factories. Both have their pros and cons, and we select the best from each in TALISMAN MDE. To show the advantages of TALISMAN MDE, we have developed a systems generator and used it to create applications for controlling food traceability. The applications are being used in dairies with different manufacturing processes, using software developed specifically for each dairy by working only with models, without additional programming.},
  comment       = {13},
  doi           = {https://doi.org/10.1016/j.jss.2010.01.010},
  keywords      = {MDA, Software factory, TALISMAN, MDE, TMDE, Model-Driven},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121210000075},
}

@Article{Mohan2008,
  author        = {Kannan Mohan and Peng Xu and Lan Cao and Balasubramaniam Ramesh},
  title         = {Improving change management in software development: Integrating traceability and software configuration management},
  journal       = {Decision Support Systems},
  year          = {2008},
  volume        = {45},
  number        = {4},
  pages         = {922 - 936},
  issn          = {0167-9236},
  note          = {Information Technology and Systems in the Internet-Era},
  __markedentry = {[mac:]},
  abstract      = {Software configuration management (SCM) and traceability are two prominent practices that support change management in software development. While SCM helps manage the evolution of software artifacts and their documentation, traceability helps manage knowledge about the process of the development of software artifacts. In this paper, we present the integration of traceability and SCM to help change management during the development and evolution of software artifacts. We developed a traceability model using a case study conducted in a software development organization. This model represents knowledge elements that are essential to comprehensively manage changes tracked within the change management function of SCM tools. A tool that supports the integrated practice of SCM and traceability is also presented. We illustrate the usefulness of our model and tool using a change management scenario that was drawn from our case study. We also present a qualitative study towards empirically evaluating the usefulness of our approach.},
  comment       = {15},
  doi           = {https://doi.org/10.1016/j.dss.2008.03.003},
  keywords      = {Software configuration management, Traceability, Change management, Process knowledge},
  url           = {http://www.sciencedirect.com/science/article/pii/S0167923608000523},
}

@Article{Perez2008,
  author        = {Francisco PÃ©rez and Juan de Lara and Luis Conde and Manuel Alfonseca and Luis GalÃ¡n and David Raboso},
  title         = {CEST and MEST: Tools for the simulation of radio frequency electric discharges in waveguides},
  journal       = {Simulation Modelling Practice and Theory},
  year          = {2008},
  volume        = {16},
  number        = {9},
  pages         = {1438 - 1452},
  issn          = {1569-190X},
  __markedentry = {[mac:]},
  abstract      = {In this paper we present two software tools for the simulation of electron multiplication processes in radio frequency (RF) waveguides. The electric discharges are caused by the multiplication of a small initial number of electrons. These are accelerated by the RF field and produce new electrons either by collisions with the walls of the waveguide (ripping new electrons from them), or by ionization of the neutral atoms of a gas inside the device. MEST allows simulating the Multipactor effect, a discharge produced in vacuum and generated by the collision of the electrons with the walls. CEST simulates the discharge when in addition a neutral gas is present in the waveguide, at pressures lower than ground levels (often denominated Corona discharge). The main characteristic of both tools is that they implement individual-based, microscopic models, where every electron is individually represented and tracked. In the case of MEST, the simulation is discrete-event, as the trajectory of each electron can be computed analytically. In CEST we use a hybrid simulation approach. The trajectory of each electron is governed by the Langevin stochastic differential equations that take into account a deterministic RF electric force and the random interaction with the neutral atom background. In addition, wall and ionizing collisions are modelled as discrete events. The tools allow performing batches of simulations with different wall coating materials and gases, and have produced results in good agreement with experimental and theoretical data. The different output forms generated at run-time have proven to be very useful in order to analyze the different discharge processes. The tools are valuable for the selection of the most promising coating materials for the construction of the waveguide, as well as for the identification of safe operating parameters.},
  comment       = {15},
  doi           = {https://doi.org/10.1016/j.simpat.2008.08.002},
  keywords      = {Simulation tools, Electric discharges, Multipactor, Corona, Discrete-event simulation, Hybrid simulation},
  url           = {http://www.sciencedirect.com/science/article/pii/S1569190X08001457},
}

@Article{Sherif2003,
  author        = {Karma Sherif and Ajay Vinze},
  title         = {Barriers to adoption of software reuse: A qualitative study},
  journal       = {Information \& Management},
  year          = {2003},
  volume        = {41},
  number        = {2},
  pages         = {159 - 175},
  issn          = {0378-7206},
  __markedentry = {[mac:]},
  abstract      = {With economic pressures to deliver software applications at a faster rate and at lower cost, software reuse is becoming a significant technology for software development. This paper focuses on deriving a descriptive and explanatory theory concerning the individual and organizational barriers associated with the adoption of reuse. A case-study research method was used. A series of five cases were selected on the basis of theoretical replication. The findings, which indicate that barriers occur at both the individual and organizational level, suggest that those at the individual level are actually a consequence of the interaction of barriers caused at the organizational level.},
  comment       = {17},
  doi           = {https://doi.org/10.1016/S0378-7206(03)00045-4},
  keywords      = {Software reuse, Component development, Barriers to adoption, Qualitative research, Grounded theory methodology, Case studies},
  url           = {http://www.sciencedirect.com/science/article/pii/S0378720603000454},
}

@Article{Cengarle2006,
  author        = {MarÃ­a Victoria Cengarle and Peter Graubmann and Stefan Wagner},
  title         = {Semantics of UML 2.0 Interactions with Variabilities},
  journal       = {Electronic Notes in Theoretical Computer Science},
  year          = {2006},
  volume        = {160},
  pages         = {141 - 155},
  issn          = {1571-0661},
  note          = {Proceedings of the International Workshop on Formal Aspects of Component Software (FACS 2005)},
  __markedentry = {[mac:]},
  abstract      = {Means for the representation of variability in UML 2.0 interactions, as presented in a previous work, are further formalised and given a mathematically formal semantics. In this way, UML 2.0 interactions can be used in the conception and development of system families within domain and application engineering tasks. Following the transition from domain to application engineering as a configuration endeavour, resolution of the variability according to a given configuration is captured by a denotational semantics for plain interactions extended to the features for the specification of variability. An example based on a previous case study explicates the semantics hereby defined.},
  comment       = {15},
  doi           = {https://doi.org/10.1016/j.entcs.2006.05.020},
  keywords      = {UML interactions, variability, system families, product lines, formal semantics},
  url           = {http://www.sciencedirect.com/science/article/pii/S1571066106003823},
}

@Article{Fortier2010,
  author        = {AndrÃ©s Fortier and Gustavo Rossi and Silvia E. Gordillo and Cecilia Challiol},
  title         = {Dealing with variability in context-aware mobile software},
  journal       = {Journal of Systems and Software},
  year          = {2010},
  volume        = {83},
  number        = {6},
  pages         = {915 - 936},
  issn          = {0164-1212},
  note          = {Software Architecture and Mobility},
  __markedentry = {[mac:]},
  abstract      = {Mobile context-aware software pose a set of challenging requirements to developers as these applications exhibit novel features, such as handling varied sensing devices and dynamically adapting to the userâ€™s context (e.g. his or her location), and evolve quickly according to technological advances. In this paper, we discuss how to handle variability both across different domains and during the evolution of a single application. We present a set of design structures for solving different problems related with mobility (such as location sensing, behaviour adaptation, etc.), together with the design rationale underlying them, and show how these sound micro-architectural constructs impact on variability. Our presentation is illustrated with case studies in different domains.},
  comment       = {22},
  doi           = {https://doi.org/10.1016/j.jss.2009.11.002},
  keywords      = {Context-awareness, Mobile software, Architecture, Software variability},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121209002830},
}

@Article{Stoica2015,
  author        = {Anca-Juliana Stoica and Kristiaan Pelckmans and William Rowe},
  title         = {System components of a general theory of software engineering},
  journal       = {Science of Computer Programming},
  year          = {2015},
  volume        = {101},
  pages         = {42 - 65},
  issn          = {0167-6423},
  note          = {Towards general theories of software engineering},
  __markedentry = {[mac:]},
  abstract      = {The contribution of this paper to a general theory of software engineering is twofold: it presents the model system concept, and it integrates the software engineering design process into a decision making theory and a value-based decision-under-risk process. The model system concept is defined as a collection of interconnected and consistent components that work together for defining, developing, and delivering a software system. This model system concept is used to represent the multiple facets of a software engineering project such as stakeholders and models related to domain/environment, success, decision, product, process, and property. The model system concept is derived from software development practices in the industry and academia. The theoretical decision framework acts as a central governance component for a given software engineering project. Applying this decision framework allows for effectively managing risks and uncertainties related to success in the project building stage. Especially, this puts the design process in an economic perspective, where concepts such as value-of-waiting, value-of-information and possible outcomes can be coped with explicitly. In practice, the decision framework allows for the optimal control of modern adaptive software development. In particular, one can use dynamic programming to find the optimal sequence of decisions to be made considering a defined time horizon. In this way we can relate our contribution to a theory of software engineering to the well-studied areas of automatic control, optimization, decision theory and Bayesian analysis. Computational case studies exemplify the conceptual innovations proposed in this paper.},
  comment       = {24},
  doi           = {https://doi.org/10.1016/j.scico.2014.11.008},
  keywords      = {General theory of software engineering, Model systems, Theoretic Decision Framework, Optimal decision-under-risk process, Adaptive software development},
  url           = {http://www.sciencedirect.com/science/article/pii/S0167642314005401},
}

@Article{Kuroiwa2016,
  author        = {Takeru Kuroiwa and Yusuke Aoyama and Noriyuki Kushiro},
  title         = {Testing Environment for CPS by Cooperating Model Checking with Execution Testing},
  journal       = {Procedia Computer Science},
  year          = {2016},
  volume        = {96},
  pages         = {1341 - 1350},
  issn          = {1877-0509},
  note          = {Knowledge-Based and Intelligent Information \& Engineering Systems: Proceedings of the 20th International Conference KES-2016},
  __markedentry = {[mac:]},
  abstract      = {In this study, we propose a testing environment for cyber-physical systems (CPS). In system testing for CPS, many tests are difficult to design or implement because of these systemsâ€™ many product variations. The proposed environment executes the tests and guarantees that these systems operate reliably using two methods. The first method provides easy management of test cases by managing functions to be tested and configurations to be tested separately. The second method involves automatic testing of real devices based on model checking technologies. The authors have developed a horizontal prototype of the proposed environment and confirmed its feasibility and applicability.},
  comment       = {10},
  doi           = {https://doi.org/10.1016/j.procs.2016.08.179},
  keywords      = {Cyber-physical systems, System testing, Model cheking ;},
  url           = {http://www.sciencedirect.com/science/article/pii/S1877050916319895},
}

@Article{Strode2012,
  author        = {Diane E. Strode and Sid L. Huff and Beverley Hope and Sebastian Link},
  title         = {Coordination in co-located agile software development projects},
  journal       = {Journal of Systems and Software},
  year          = {2012},
  volume        = {85},
  number        = {6},
  pages         = {1222 - 1238},
  issn          = {0164-1212},
  note          = {Special Issue: Agile Development},
  __markedentry = {[mac:]},
  abstract      = {Agile software development provides a way to organise the complex task of multi-participant software development while accommodating constant project change. Agile software development is well accepted in the practitioner community but there is little understanding of how such projects achieve effective coordination, which is known to be critical in successful software projects. A theoretical model of coordination in the agile software development context is presented based on empirical data from three cases of co-located agile software development. Many practices in these projects act as coordination mechanisms, which together form a coordination strategy. Coordination strategy in this context has three components: synchronisation, structure, and boundary spanning. Coordination effectiveness has two components: implicit and explicit. The theoretical model of coordination in agile software development projects proposes that an agile coordination strategy increases coordination effectiveness. This model has application for practitioners who want to select appropriate practices from agile methods to ensure they achieve coordination coverage in their project. For the field of information systems development, this theory contributes to knowledge of coordination and coordination effectiveness in the context of agile software development.},
  comment       = {17},
  doi           = {https://doi.org/10.1016/j.jss.2012.02.017},
  keywords      = {Agile methods, Agile software development project, Coordination effectiveness, Coordination strategy, Coordination Theory, Extreme Programming, Scrum},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121212000465},
}

@Article{Alonso2012a,
  author        = {Diego Alonso and Juan Ãngel Pastor and Pedro SÃ¡nchez and BÃ¡rbara Ãlvarez and Cristina Vicente-Chicote},
  title         = {GeneraciÃ³n AutomÃ¡tica de Software para Sistemas de Tiempo Real: Un Enfoque basado en Componentes, Modelos y Frameworks},
  journal       = {Revista Iberoamericana de AutomÃ¡tica e InformÃ¡tica Industrial RIAI},
  year          = {2012},
  volume        = {9},
  number        = {2},
  pages         = {170 - 181},
  issn          = {1697-7912},
  __markedentry = {[mac:]},
  abstract      = {Resumen
Los Sistemas de Tiempo-Real poseen caracterÃ­sticas que los hacen particularmente sensibles a las decisiones arquitectÃ³nicas que se adopten. El uso de Frameworks y Componentes ha demostrado ser eficaz en la mejora de la productividad y calidad del software, sobre todo si se combina con enfoques de LÃ­neas de Productos. Sin embargo, los resultados en cuanto a reutilizaciÃ³n y estandarizaciÃ³n dejan patente la ausencia de portabilidad tanto de los diseÃ±os como las implementaciones basadas en componentes. Este artÃ­culo, fundamentado en el Desarrollo de Software Dirigido por Modelos, presenta un enfoque que separa la descripciÃ³n de aplicaciones de tiempoâ€“real basadas en componentes de sus posibles implementaciones para distintas plataformas. Esta separaciÃ³n viene soportada por la integraciÃ³n automÃ¡tica del cÃ³digo obtenido a partir de los modelos de entrada en frameworks implementados usando tecnologÃ­a orientada a objetos. Asimismo, se detallan las decisiones arquitectÃ³nicas adoptadas en la implementaciÃ³n de uno de estos frameworks, que se utilizarÃ¡ como caso de estudio para ilustrar los beneficios derivados del enfoque propuesto. Por Ãºltimo, se realiza una comparativa en tÃ©rminos de coste de desarrollo con otros enfoques alternativos.
Real-Time Systems have some characteristics that make them particularly sensitive to architectural decisions. The use of Frameworks and Components has proven effective in improving productivity and software quality, especially when combined with Software Product Line approaches. However, the results in terms of software reuse and standardization make the lack of portability of both the design and componentbased implementations clear. This article, based on the Model- Driven Software Development paradigm, presents an approach that separates the component-based description of real-time applications from their possible implementations on different platforms. This separation is supported by the automatic integration of the code obtained from the input models into object-oriented frameworks. The article also details the architectural decisions taken in the implementation of one of such frameworks, which is used as a case study to illustrate the proposed approach. Finally, a comparison with other alternative approaches is made in terms of development cost.},
  comment       = {12},
  doi           = {https://doi.org/10.1016/j.riai.2012.02.010},
  keywords      = {IngenierÃ­a del Software, Desarrollo de Software Basado en Componentes, Desarrollo de Software Dirigido por Modelos, Frameworks, Patrones de DiseÃ±o Software, Tiempo-Real, Software Engineering, Component-Based Software Development, Model-Driven Software Development, Framework, Software Design Patterns, Real-Time},
  url           = {http://www.sciencedirect.com/science/article/pii/S169779121200012X},
}

@Article{Wendler2012,
  author        = {Roy Wendler},
  title         = {The maturity of maturity model research: A systematic mapping study},
  journal       = {Information and Software Technology},
  year          = {2012},
  volume        = {54},
  number        = {12},
  pages         = {1317 - 1339},
  issn          = {0950-5849},
  note          = {Special Section on Software Reliability and Security},
  __markedentry = {[mac:]},
  abstract      = {Context
Maturity models offer organizations a simple but effective possibility to measure the quality of their processes. Emerged out of software engineering, the application fields have widened and maturity model research is becoming more important. During the last two decades the publication amount steadily rose as well. Until today, no studies have been available summarizing the activities and results of the field of maturity model research.
Objective
The objective of this paper is to structure and analyze the available literature of the field of maturity model research to identify the state-of-the-art research as well as research gaps.
Method
A systematic mapping study was conducted. It included relevant publications of journals and IS conferences. Mapping studies are a suitable method for structuring a broad research field concerning research questions about contents, methods, and trends in the available publications.
Results
The mapping of 237 articles showed that current maturity model research is applicable to more than 20 domains, heavily dominated by software development and software engineering. The study revealed that most publications deal with the development of maturity models and empirical studies. Theoretical reflective publications are scarce. Furthermore, the relation between conceptual and design-oriented maturity model development was analyzed, indicating that there is still a gap in evaluating and validating developed maturity models. Finally, a comprehensive research framework was derived from the study results and implications for further research are given.
Conclusion
The mapping study delivers the first systematic summary of maturity model research. The categorization of available publications helps researchers gain an overview of the state-of-the-art research and current research gaps. The proposed research framework supports researchers categorizing their own projects. In addition, practitioners planning to use a maturity model may use the study as starting point to identify which maturity models are suitable for their domain and where limitations exist.},
  comment       = {23},
  doi           = {https://doi.org/10.1016/j.infsof.2012.07.007},
  keywords      = {Maturity models, Software management, Design-oriented research, Systematic mapping study},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584912001334},
}

@Article{Dam2016,
  author        = {Hoa Khanh Dam and Alexander Egyed and Michael Winikoff and Alexander Reder and Roberto E. Lopez-Herrejon},
  title         = {Consistent merging of model versions},
  journal       = {Journal of Systems and Software},
  year          = {2016},
  volume        = {112},
  pages         = {137 - 155},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {While many engineering tasks can, and should be, manageable independently, it does place a great burden on explicit collaboration needsâ€”including the need for frequent and incremental merging of artifacts that software engineers manipulate using these tools. State-of-the-art merging techniques are often limited to textual artifacts (e.g., source code) and they are unable to discover and resolve complex merging issues beyond simple conflicts. This work focuses on the merging of models where we consider not only conflicts but also arbitrary syntactic and semantic consistency issues. Consistent artifacts are merged fully automatically and only inconsistent/conflicting artifacts are brought to the usersâ€™ attention, together with a systematic proposal of how to resolve them. Our approach is neutral with regard to who made the changes and hence reduces the bias caused by any individual engineerâ€™s limited point of view. Our approach also applies to arbitrary design or models, provided that they follow a well-defined metamodel with explicit constraintsâ€”the norm nowadays. The extensive empirical evaluation suggests that our approach scales to practical settings.},
  comment       = {19},
  doi           = {https://doi.org/10.1016/j.jss.2015.06.044},
  keywords      = {Model merging, Inconsistency management, Model versioning},
  url           = {http://www.sciencedirect.com/science/article/pii/S016412121500134X},
}

@Article{Moros2013,
  author        = {BegoÃ±a Moros and Ambrosio Toval and Francisca Rosique and Pedro SÃ¡nchez},
  title         = {Transforming and tracing reused requirements models to home automation models},
  journal       = {Information and Software Technology},
  year          = {2013},
  volume        = {55},
  number        = {6},
  pages         = {941 - 965},
  issn          = {0950-5849},
  __markedentry = {[mac:]},
  abstract      = {Context
Model-Driven Software Development (MDSD) has emerged as a very promising approach to cope with the inherent complexity of modern software-based systems. Furthermore, it is well known that the Requirements Engineering (RE) stage is critical for a projectâ€™s success. Despite the importance of RE, MDSD approaches commonly leave textual requirements specifications to one side.
Objective
Our aim is to integrate textual requirements specifications into the MDSD approach by using the MDSD techniques themselves, including metamodelling and model transformations. The proposal is based on the assumption that a reuse-based Model-Driven Requirements Engineering (MDRE) approach will improve the requirements engineering stage, the quality of the development models generated from requirements models, and will enable the traces from requirements to other development concepts (such as analysis or design) to be maintained.
Method
The approach revolves around the Requirements Engineering Metamodel, denominated as REMM, which supports the definition of the boilerplate based textual requirements specification languages needed for the definition of model transformation from application requirements models to platform-specific application models and code.
Results
The approach has been evaluated through its application to Home Automation (HA) systems. The HA Requirement Specification Language denominated as HAREL is used to define application requirements models which will be automatically transformed and traced to the application model conforming to the HA Domain Specific Language.
Conclusions
An anonymous online survey has been conducted to evaluate the degree of acceptance by both HA application developers and MDSD practitioners. The main conclusion is that 66.7% of the HA experts polled strongly agree that the automatic transformation of the requirements models to HA models improves the quality of the HA models. Moreover, 58.3% of the HA participants strongly agree with the usefulness of the traceability matrix which links requirements to HA functional units in order to discover which devices are related to a specific requirement. We can conclude that the experts we have consulted agree with the proposal we are presenting here, since the average mark given is 4 out of 5.},
  comment       = {25},
  doi           = {https://doi.org/10.1016/j.infsof.2012.12.003},
  keywords      = {Requirements metamodel, Requirements reuse, Requirements traceability, Models transformation, Model driven software development, Home automation models},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584912002388},
}

@Article{Parolia2011,
  author        = {Neeraj Parolia and James J. Jiang and Gary Klein and Tsong Shin Sheu},
  title         = {The contribution of resource interdependence to IT program performance: A social interdependence perspective},
  journal       = {International Journal of Project Management},
  year          = {2011},
  volume        = {29},
  number        = {3},
  pages         = {313 - 324},
  issn          = {0263-7863},
  __markedentry = {[mac:]},
  abstract      = {Combinations of multiple, related projects into interdependent programs are becoming common in the field of information technology. However, little work exists to study the impact of interdependence in the program environment to achieve collective success. Social interdependence theory provides a structure to examine whether collaborative efforts promote behaviors that result in higher levels of success. Using resource interdependence as an indicator of collaboration, a model of promotive behaviors is developed using the lens of social interdependence. Expectations are that resource interdependence conditions will promote more mutual support, effort, and communication. In turn, these behaviors will lead to an improvement in the business objectives and operational efficiency of the organization. A survey of program and project managers in information system vendors located in India support the model. The results provide support for the argument that programs are effective organization structures that capitalize on interdependencies and that the social interdependence theory provides a consistent model to explain the benefits of resource interdependence.},
  comment       = {12},
  doi           = {https://doi.org/10.1016/j.ijproman.2010.03.004},
  keywords      = {Program management, Operational effectiveness, Business objectives, Resource interdependence, Promotive interaction, Program performance},
  url           = {http://www.sciencedirect.com/science/article/pii/S0263786310000542},
}

@Article{Otaduy2017,
  author        = {I. Otaduy and O. Diaz},
  title         = {User acceptance testing for Agile-developed web-based applications: Empowering customers through wikis and mind maps},
  journal       = {Journal of Systems and Software},
  year          = {2017},
  volume        = {133},
  pages         = {212 - 229},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {User Acceptance Testing (UAT) involves validating software in a real setting by the intended audience. The aim is not so much to check the defined requirements but to ensure that the software satisfies the customerâ€™s needs. Agile methodologies put stringent demands on UAT, if only for the frequency at which it needs to be conducted due to the iterative development of small product releases. In this setting, traditional in-person meetings might not scale up well. Complementary ways are needed to reduce the costs of developer-customer collaboration during UAT. This work introduces a wiki-based approach where customers and developers asynchronously collaborate: developers set the UAT scaffolding that will later shepherd customers when testing. To facilitate understanding, mind maps are used to represent UAT sessions. To facilitate engagement, a popular mind map editor, FreeMind, is turned into an editor for FitNesse, the wiki engine in which these ideas are borne out. The approach is evaluated through a case study involving three real customers. First evaluations are promising. Though at different levels of completeness, the three customers were able to complete a UAT. Customers valued asynchronicity, mind map structuredness, and the transparent generation of documentation out of the UAT session.},
  comment       = {18},
  doi           = {https://doi.org/10.1016/j.jss.2017.01.002},
  keywords      = {Agile development, User acceptance testing, Test automation},
  url           = {http://www.sciencedirect.com/science/article/pii/S016412121730002X},
}

@Article{Yongsiriwit2016,
  author        = {Karn Yongsiriwit and Nour Assy and Walid Gaaloul},
  title         = {A semantic framework for configurable business process as a service in the cloud},
  journal       = {Journal of Network and Computer Applications},
  year          = {2016},
  volume        = {59},
  pages         = {168 - 184},
  issn          = {1084-8045},
  __markedentry = {[mac:]},
  abstract      = {With the advent of Cloud Computing, new opportunities for Business Process Outsourcing services have emerged. Business Process as a Service (BPaaS), a new cloud service model, has recently gained a great importance for outsourcing cloud-based business processes constructed for multi-tenancy. In such a multi-tenant environment, using configurable business process models enables the sharing of a reference process among different tenants that can be customized according to specific needs. With a large choice of configurable process modeling languages, different providers may deliver configurable processes with common functionalities but different representations which makes the process discovery and configuration a manual tedious task. This in turn creates cloud silos and vendors lock-in with non-reusable configurable BPaaS models. Therefore, with the aim of enabling the interoperability between multiple BPaaS providers, we propose in this paper a semantic framework for BPaaS configurable models. Taking advantage of Semantic Web technologies and data mining techniques, our framework allows for (1) an ontology-based high level abstract representation of BPaaS configurable models enriched with configuration guidelines and (2) an automated approach for extracting the configuration guidelines from existing process repositories. To show the feasibility and effectiveness of our approach, we extend Signavio with our semantic framework and conduct experiments on a dataset from SAP reference model.},
  comment       = {17},
  doi           = {https://doi.org/10.1016/j.jnca.2015.07.007},
  keywords      = {Cloud Computing, Business Process as a Service, BPaaS, Configurable process model, Semantic technology, Green IT},
  url           = {http://www.sciencedirect.com/science/article/pii/S1084804515001708},
}

@Article{Magdaleno2012,
  author        = {AndrÃ©a MagalhÃ£es Magdaleno and ClÃ¡udia Maria Lima Werner and Renata Mendes de Araujo},
  title         = {Reconciling software development models: A quasi-systematic review},
  journal       = {Journal of Systems and Software},
  year          = {2012},
  volume        = {85},
  number        = {2},
  pages         = {351 - 369},
  issn          = {0164-1212},
  note          = {Special issue with selected papers from the 23rd Brazilian Symposium on Software Engineering},
  __markedentry = {[mac:]},
  abstract      = {Purpose
The purpose of this paper is to characterize reconciliation among the plan-driven, agile, and free/open source software models of software development.
Design/methodology/approach
An automated quasi-systematic review identified 42 papers, which were then analyzed.
Findings
The main findings are: there exist distinct â€“ organization, group and process â€“ levels of reconciliation; few studies deal with reconciliation among the three models of development; a significant amount of work addresses reconciliation between plan-driven and agile development; several large organizations (such as Microsoft, Motorola, and Philips) are interested in trying to combine these models; and reconciliation among software development models is still an open issue, since it is an emerging area and research on most proposals is at an early stage.
Research limitations
Automated searches may not capture relevant papers in publications that are not indexed. Other data sources not amenable to execution of the protocol were not used. Data extraction was performed by only one researcher, which may increase the risk of threats to internal validity.
Implications
This characterization is important for practitioners wanting to be current with the state of research. This review will also assist the scientific community working with software development processes to build a common understanding of the challenges that must be faced, and to identify areas where research is lacking. Finally, the results will be useful to software industry that is calling for solutions in this area.
Originality/value
There is no other systematic review on this subject, and reconciliation among software development models is an emerging area. This study helps to identify and consolidate the work done so far and to guide future research. The conclusions are an important step towards expanding the body of knowledge in the field.},
  comment       = {19},
  doi           = {https://doi.org/10.1016/j.jss.2011.08.028},
  keywords      = {Systematic review, Software process, Reconciliation among development models, Plan-driven, Agile, Free/open source software},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121211002287},
}

@Article{Knodel2005,
  author        = {Jens Knodel and Michalis Anastasopolous and Thomas Forster and Dirk Muthig},
  title         = {An Efficient Migration to Model-driven Development (MDD)},
  journal       = {Electronic Notes in Theoretical Computer Science},
  year          = {2005},
  volume        = {137},
  number        = {3},
  pages         = {17 - 27},
  issn          = {1571-0661},
  note          = {Proceedings of the 2nd International Workshop on Metamodels, Schemas, and Grammars for Reverse Engineering (ateM 2004)},
  __markedentry = {[mac:]},
  abstract      = {Model-driven development envisions raising the abstraction level of software development. To fully realize this vision, technology-specific aspects must be completely hidden from developers. They produce only platform-independent models (PIM), which are automatically transformed into executable systems. To enable an efficient migration to MDD, we recommend taking advantage of concepts from software architectures, product line engineering and reverse engineering.},
  comment       = {11},
  doi           = {https://doi.org/10.1016/j.entcs.2005.07.002},
  keywords      = {architecture-driven migration, model-driven development, product lines, reverse engineering, PuLSE},
  url           = {http://www.sciencedirect.com/science/article/pii/S1571066105051029},
}

@Article{Escalona2011,
  author        = {M.J. Escalona and J.J. Gutierrez and M. MejÃ­as and G. AragÃ³n and I. Ramos and J. Torres and F.J. DomÃ­nguez},
  title         = {An overview on test generation from functional requirements},
  journal       = {Journal of Systems and Software},
  year          = {2011},
  volume        = {84},
  number        = {8},
  pages         = {1379 - 1393},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {Despite the fact that the test phase is described in the literature as one of the most relevant for quality assurance in software projects, this test phase is not usually developed, among others, with enough resources, time or suitable techniques. To offer solutions which supply the test phase, with appropriate tools for the automation of tests generation, or even, for their self-execution, could become a suitable way to improve this phase and reduce the cost constraints in real projects. This paper focuses on answering a concrete research question: is it possible to generate test cases from functional requirements described in an informal way? For this aim, it presents an overview of a set of relevant approaches that works in this field and offers a set of comparative analysis to determine which the state of the art is.},
  comment       = {15},
  doi           = {https://doi.org/10.1016/j.jss.2011.03.051},
  keywords      = {Testing, Early testing, Functional test generation},
  url           = {http://www.sciencedirect.com/science/article/pii/S016412121100080X},
}

@Article{Mosser2013,
  author        = {SÃ©bastien Mosser and Mireille Blay-Fornarino},
  title         = {â€œAdoreâ€, a logical meta-model supporting business process evolution},
  journal       = {Science of Computer Programming},
  year          = {2013},
  volume        = {78},
  number        = {8},
  pages         = {1035 - 1054},
  issn          = {0167-6423},
  note          = {Special section on software evolution, adaptability, and maintenance \& Special section on the Brazilian Symposium on Programming Languages},
  __markedentry = {[mac:]},
  abstract      = {The Service Oriented Architecture (Soa) paradigm supports the assembly of atomic services to create applications that implement complex business processes. Since â€œreal-lifeâ€ processes can be very complex, composition mechanisms inspired by the Separation of Concerns paradigm (e.g. features, aspects) are good candidates to support the definition and the upcoming evolutions of large systems. We propose Adore, â€œan Activity meta-moDel supOrting oRchestration Evolutionâ€ to address this issue. The Adore meta-model allows process designers to express in the same formalism business processes and fragments of processes. Such fragments define additional activities that aim to be integrated into other processes and adequately support their evolution. The underlying logical foundations of Adore allow the definition of interference detection rules as logical predicate, as well as the definition of consistency properties on Adore models. Consequently, the Adore framework supports process designers while they design and then apply evolutions on large processes, managing the detection of interferences among fragments and ensuring that the composed processes are consistent and do not depend on the order of the composition.},
  comment       = {20},
  doi           = {https://doi.org/10.1016/j.scico.2012.06.009},
  keywords      = {SOA, Business processes, Sep. of concerns, Logical composition},
  url           = {http://www.sciencedirect.com/science/article/pii/S0167642312001220},
}

@Article{Uzunov2015,
  author        = {Anton V. Uzunov and Eduardo B. Fernandez and Katrina Falkner},
  title         = {ASE: A comprehensive pattern-driven security methodology for distributed systems},
  journal       = {Computer Standards \& Interfaces},
  year          = {2015},
  volume        = {41},
  pages         = {112 - 137},
  issn          = {0920-5489},
  __markedentry = {[mac:]},
  abstract      = {Incorporating security features is one of the most important and challenging tasks in designing distributed systems. Over the last decade, researchers and practitioners have come to recognize that the incorporation of security features should proceed by means of a structured, systematic approach, combining principles from both software and security engineering. Such systematic approaches, particularly those implying some sort of process aligned with the development life-cycle, are termed security methodologies. There are a number of security methodologies in the literature, of which the most flexible and, according to a recent survey, most satisfactory from an industry-adoption viewpoint are methodologies that encapsulate their security solutions in some fashion, especially via the use of security patterns. While the literature does present several mature pattern-driven security methodologies with either a general or a highly specific system applicability, there are currently no (pattern-driven) security methodologies specifically designed for general distributed systems. Going further, there are also currently no methodologies with mixed specific applicability, e.g. for both general and peer-to-peer distributed systems. In this paper we aim to fill these gaps by presenting a comprehensive pattern-driven security methodology â€“ arrived at by applying a previously devised approach to engineering security methodologies â€“ specifically designed for general distributed systems, which is also capable of taking into account the specifics of peer-to-peer systems as needed. Our methodology takes the principle of encapsulation several steps further, by employing patterns not only for the incorporation of security features (via security solution frames), but also for the modeling of threats, and even as part of its process. We illustrate and evaluate the presented methodology in detail via a realistic example â€“ the development of a distributed system for file sharing and collaborative editing. In both the presentation of the methodology and example our focus is on the early life-cycle phases (analysis and design).},
  comment       = {26},
  doi           = {https://doi.org/10.1016/j.csi.2015.02.011},
  keywords      = {Secure software engineering, Security methodologies, Distributed systems security, Security patterns, Security solution frames},
  url           = {http://www.sciencedirect.com/science/article/pii/S0920548915000276},
}

@Article{Yaguee2016,
  author        = {Agustin YagÃ¼e and Juan Garbajosa and Jessica DÃ­az and Eloy GonzÃ¡lez},
  title         = {An exploratory study in communication in Agile Global Software Development},
  journal       = {Computer Standards \& Interfaces},
  year          = {2016},
  volume        = {48},
  pages         = {184 - 197},
  issn          = {0920-5489},
  note          = {Special Issue on Information System in Distributed Environment},
  __markedentry = {[mac:]},
  abstract      = {Global software development (GSD) is gaining ever more relevance. Although communication is key in the exchange of information between team members, multi-site software development has introduced additional obstacles (different time-zones and cultures, IT infrastructure, etc.) and delays into the act of communication, which is already problematic. Communication is even more critical in the case of Agile Global Software Development (AGSD) in which communication plays a primary role. This paper reports an exploratory study of the effects of tools supporting communication in AGSD. More precisely, this paper analyses the perception of team members about communication infrastructures in AGSD. The research question to which this study responds concerns how development teams perceive the communication infrastructure while developing products using agile methodologies. Most previous studies have dealt with communication support from a highly technological media tool perspective. In this research work, instead, observations were obtained from three perspectives: communication among team members, communication of the status of the development process, and communication of the status of the progress of the product under development. It has been possible to show that team members perceive advantages to using media tools that make them feel in practice that teams are co-located, such as smartboards supported by efficient video-tools, and combining media tools with centralized repository tools, with information from the process development and product characteristics, that allow distributed teams to effectively share information about the status of the project/process/product during the development process in order to overcome some of the still existing problems in communication in AGSD.},
  comment       = {14},
  doi           = {https://doi.org/10.1016/j.csi.2016.06.002},
  keywords      = {Global Distributed Software Development, Agile, Exploratory research, Tools and technologies, Infrastructure},
  url           = {http://www.sciencedirect.com/science/article/pii/S0920548916300381},
}

@Article{Khatchadourian2017,
  author        = {Raffi Khatchadourian and Awais Rashid and Hidehiko Masuhara and Takuya Watanabe},
  title         = {Detecting broken pointcuts using structural commonality and degree of interest},
  journal       = {Science of Computer Programming},
  year          = {2017},
  volume        = {150},
  pages         = {56 - 74},
  issn          = {0167-6423},
  __markedentry = {[mac:]},
  abstract      = {Pointcut fragility is a well-documented problem in Aspect-Oriented Programming; changes to the base-code can lead to join points incorrectly falling in or out of the scope of pointcuts. Deciding which pointcuts have broken due to base-code changes is a daunting venture, especially in large and complex systems. We present an automated approach that recommends pointcuts that are likely to require modification due to a particular base-code change, as well as ones that do not. Our hypothesis is that join points selected by a pointcut exhibit common structural characteristics. Patterns describing such commonality are used to recommend pointcuts that have potentially broken with a degree of confidence as the developer is typing. The approach is implemented as an extension to the popular Mylyn Eclipse IDE plug-in, which maintains focused contexts of entities relevant to the task at hand using a Degree of Interest (DOI) model. We show that it is accurate in revealing broken pointcuts by applying it to multiple versions of several open source projects and evaluating the quality of the recommendations produced against actual modifications. We found that our tool made broken pointcuts 2.14 times more interesting in the DOI model than unbroken ones, with a p-value under 0.1, indicating a significant difference in final DOI value between the two kinds of pointcuts (i.e., broken and unbroken).},
  comment       = {19},
  doi           = {https://doi.org/10.1016/j.scico.2017.06.011},
  keywords      = {Software development environments, Software maintenance, Software tools},
  url           = {http://www.sciencedirect.com/science/article/pii/S0167642317301326},
}

@Article{Delgado2016,
  author        = {A. Delgado and A. Estepa and J.A. Troyano and R. Estepa},
  title         = {Reusing UI elements with Model-Based User Interface Development},
  journal       = {International Journal of Human-Computer Studies},
  year          = {2016},
  volume        = {86},
  pages         = {48 - 62},
  issn          = {1071-5819},
  __markedentry = {[mac:]},
  abstract      = {This paper introduces the potential for reusing UI elements in the context of Model-Based UI Development (MBUID) and provides guidance for future MBUID systems with enhanced reutilization capabilities. Our study is based upon the development of six inter-related projects with a specific MBUID environment which supports standard techniques for reuse such as parametrization and sub-specification, inclusion or shared repositories. We analyze our experience and discuss the benefits and limitations of each technique supported by our MBUID environment. The system architecture, the structure and composition of UI elements and the models specification languages have a decisive impact on reusability. In our case, more than 40% of the elements defined in the UI specifications were reused, resulting in a reduction of 55% of the specification size. Inclusion, parametrization and sub-specification have facilitated modularity and internal reuse of UI specifications at development time, whereas the reuse of UI elements between applications has greatly benefited from sharing repositories of UI elements at run time.},
  comment       = {15},
  doi           = {https://doi.org/10.1016/j.ijhcs.2015.09.003},
  keywords      = {MBUID, Reuse, Software engineering, User Interface},
  url           = {http://www.sciencedirect.com/science/article/pii/S1071581915001470},
}

@Article{Haitzer2014,
  author        = {Thomas Haitzer and Uwe Zdun},
  title         = {Semi-automated architectural abstraction specifications for supporting software evolution},
  journal       = {Science of Computer Programming},
  year          = {2014},
  volume        = {90},
  pages         = {135 - 160},
  issn          = {0167-6423},
  note          = {Special Issue on Component-Based Software Engineering and Software Architecture},
  __markedentry = {[mac:]},
  abstract      = {In this paper we present an approach for supporting the semi-automated architectural abstraction of architectural models throughout the software life-cycle. It addresses the problem that the design and implementation of a software system often drift apart as software systems evolve, leading to architectural knowledge evaporation. Our approach provides concepts and tool support for the semi-automatic abstraction of architecture component and connector views from implemented systems and keeping the abstracted architecture models up-to-date during software evolution. In particular, we propose architecture abstraction concepts that are supported through a domain-specific language (DSL). Our main focus is on providing architectural abstraction specifications in the DSL that only need to be changed, if the architecture changes, but can tolerate non-architectural changes in the underlying source code. Once the software architect has defined an architectural abstraction in the DSL, we can automatically generate architectural component views from the source code using model-driven development (MDD) techniques and check whether architectural design constraints are fulfilled by these models. Our approach supports the automatic generation of traceability links between source code elements and architectural abstractions using MDD techniques to enable software architects to easily link between components and the source code elements that realize them. It enables software architects to compare different versions of the generated architectural component view with each other. We evaluate our research results by studying the evolution of architectural abstractions in different consecutive versions of five open source systems and by analyzing the performance of our approach in these cases.},
  comment       = {26},
  doi           = {https://doi.org/10.1016/j.scico.2013.10.004},
  keywords      = {Architectural abstraction, Architectural component and connector views, Software evolution, UML, Model transformation},
  url           = {http://www.sciencedirect.com/science/article/pii/S0167642313002542},
}

@Article{Behjati2013,
  author        = {Razieh Behjati and Tao Yue and Lionel Briand and Bran Selic},
  title         = {SimPL: A product-line modeling methodology for families of integrated control systems},
  journal       = {Information and Software Technology},
  year          = {2013},
  volume        = {55},
  number        = {3},
  pages         = {607 - 629},
  issn          = {0950-5849},
  note          = {Special Issue on Software Reuse and Product Lines},
  __markedentry = {[mac:]},
  abstract      = {Context
Integrated control systems (ICSs) are heterogeneous systems where software and hardware components are integrated to control and monitor physical devices and processes. A family of ICSs share the same software code base, which is configured differently for each product to form a unique installation. Due to the complexity of ICSs and inadequate automation support, product configuration in this context is typically error-prone and costly.
Objective
As a first step to overcome these challenges, we propose a UML-based product-line modeling methodology that provides a foundation for semi-automated product configuration in the specific context of ICSs.
Method
We performed a comprehensive domain analysis to identify characteristics of ICS families, and their configuration challenges. Based on this, we formulated the characteristics of an adequate configuration solution, and derived from them a set of modeling requirements for a model-based solution to configuration. The SimPL methodology is proposed to fulfill these requirements.
Results
To evaluate the ability of SimPL to fulfill the modeling requirements, we applied it to a large-scale industrial case study. Our experience with the case study shows that SimPL is adequate to provide a model of the product family that meets the modeling requirements. Further evaluation is still required to assess the applicability and scalability of SimPL in practice. Doing this requires conducting field studies with human subjects and is left for future work.
Conclusion
We conclude that configuration in ICSs requires better automation support, and UML-based approaches to product family modeling can be tailored to provide the required foundation.},
  comment       = {23},
  doi           = {https://doi.org/10.1016/j.infsof.2012.09.006},
  keywords      = {Product-line engineering, Variability modeling, Integrated control systems, UML, MARTE},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584912002005},
}

@Article{Mahdavi-Hezavehi2017,
  author        = {Sara Mahdavi-Hezavehi and Vinicius H.S. Durelli and Danny Weyns and Paris Avgeriou},
  title         = {A systematic literature review on methods that handle multiple quality attributes in architecture-based self-adaptive systems},
  journal       = {Information and Software Technology},
  year          = {2017},
  volume        = {90},
  pages         = {1 - 26},
  issn          = {0950-5849},
  __markedentry = {[mac:]},
  abstract      = {Context
Handling multiple quality attributes (QAs) in the domain of self-adaptive systems is an understudied research area. One well-known approach to engineer adaptive software systems and fulfill QAs of the system is architecture-based self-adaptation. In order to develop models that capture the required knowledge of the QAs of interest, and to investigate how these models can be employed at runtime to handle multiple quality attributes, we need to first examine current architecture-based self-adaptive methods.
Objective
In this paper we review the state-of-the-art of architecture-based methods for handling multiple QAs in self-adaptive systems. We also provide a descriptive analysis of the collected data from the literature.
Method
We conducted a systematic literature review by performing an automatic search on 28 selected venues and books in the domain of self-adaptive systems. As a result, we selected 54 primary studies which we used for data extraction and analysis.
Results
Performance and cost are the most frequently addressed set of QAs. Current self-adaptive systems dealing with multiple QAs mostly belong to the domain of robotics and web-based systems paradigm. The most widely used mechanisms/models to measure and quantify QAs sets are QA data variables. After QA data variables, utility functions and Markov chain models are the most common models which are also used for decision making process and selection of the best solution in presence of many alternatives. The most widely used tools to deal with multiple QAs are PRISM and IBM's autonomic computing toolkit. KLAPER is the only language that has been specifically developed to deal with quality properties analysis.
Conclusions
Our results help researchers to understand the current state of research regarding architecture-based methods for handling multiple QAs in self-adaptive systems, and to identity areas for improvement in the future. To summarize, further research is required to improve existing methods performing tradeoff analysis and preemption, and in particular, new methods may be proposed to make use of models to handle multiple QAs and to enhance and facilitate the tradeoffs analysis and decision making mechanism at runtime.},
  comment       = {26},
  doi           = {https://doi.org/10.1016/j.infsof.2017.03.013},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584917302860},
}

@Article{Wenzel2014,
  author        = {S. Wenzel and D. Poggenpohl and J. JÃ¼rjens and M. Ochoa},
  title         = {Specifying model changes with UMLchange to support security verification of potential evolution},
  journal       = {Computer Standards \& Interfaces},
  year          = {2014},
  volume        = {36},
  number        = {4},
  pages         = {776 - 791},
  issn          = {0920-5489},
  note          = {Security in Information Systems: Advances and new Challenges.},
  __markedentry = {[mac:]},
  abstract      = {In model-based development, quality properties such as consistency of security requirements are often verified prior to code generation. Changed models have to be re-verified before re-generation. If several alternative evolutions of a model are possible, each alternative has to be modeled and verified to find the best model for further development. We present a verification strategy to analyze whether evolution preserves given security properties. The UMLchange profile is used for specifying potential evolutions of a given model simultaneously. We present a tool that reads these annotations and computes a delta containing all possible evolution paths. The paths can be verified wrt. security properties, and for each successfully verified path a new model version is generated automatically.},
  comment       = {16},
  doi           = {https://doi.org/10.1016/j.csi.2013.12.011},
  keywords      = {Model evolution, Security verification, UML profile, Tool support},
  url           = {http://www.sciencedirect.com/science/article/pii/S0920548913001852},
}

@Article{Fontana2014,
  author        = {Rafaela Mantovani Fontana and Isabela Mantovani Fontana and Paula Andrea da Rosa Garbuio and Sheila Reinehr and Andreia Malucelli},
  title         = {Processes versus people: How should agile software development maturity be defined?},
  journal       = {Journal of Systems and Software},
  year          = {2014},
  volume        = {97},
  pages         = {140 - 155},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {Maturity in software development is currently defined by models such as CMMI-DEV and ISO/IEC 15504, which emphasize the need to manage, establish, measure and optimize processes. Teams that develop software using these models are guided by defined, detailed processes. However, an increasing number of teams have been implementing agile software development methods that focus on people rather than processes. What, then, is maturity for these agile teams that focus less on detailed, defined processes? This is the question we sought to answer in this study. To this end, we asked agile practitioners about their perception of the maturity level of a number of practices and how they defined maturity in agile software development. We used cluster analysis to analyze quantitative data and triangulated the results with content analysis of the qualitative data. We then proposed a new definition for agile software development maturity. The findings show that practitioners do not see maturity in agile software development as process definition or quantitative management capabilities. Rather, agile maturity means fostering more subjective capabilities, such as collaboration, communication, commitment, care, sharing and self-organization.},
  comment       = {16},
  doi           = {https://doi.org/10.1016/j.jss.2014.07.030},
  keywords      = {Maturity, Agile software development, Software process improvement},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121214001587},
}

@Article{Stuikys2016,
  author        = {Vytautas Å tuikys and Renata BurbaitÄ— and Kristina Bespalova and Giedrius Ziberkas},
  title         = {Model-driven processes and tools to design robot-based generative learning objects for computer science education},
  journal       = {Science of Computer Programming},
  year          = {2016},
  volume        = {129},
  pages         = {48 - 71},
  issn          = {0167-6423},
  note          = {Special issue on eLearning Software Architectures},
  __markedentry = {[mac:]},
  abstract      = {In this paper, we introduce a methodology to design robot-oriented generative learning objects (GLOs) that are, in fact, heterogeneous meta-programs to teach computer science (CS) topics such as programming. The methodology includes CS learning variability modelling using the feature-based approaches borrowed from the SW engineering domain. Firstly, we define the CS learning domain using the known educational framework TPACK (Technology, Pedagogy And Content Knowledge). By learning variability we mean the attributes of the framework extracted and represented as feature models with multiple values. Therefore, the CS learning variability represents the problem domain. Meta-programming is considered as a solution domain. Both are represented by feature models. The GLO design task is formulated as mapping the problem domain model on the solution domain model. Next, we present the design framework to design GLOs manually or semi-automatically. The multi-level separation of concepts, model representation and transformation forms the conceptual background. Its theoretical background includes: (a) a formal definition of feature-based models; (b) a graph-based and set-based definition of meta-programming concepts; (c) transformation rules to support the model mapping; (d) a computational Abstract State Machine model to define the processes and design tool for developing GLOs. We present the architecture and some characteristics of the tool. The tool enables to improve the GLO design process significantly (in terms of time and quality) and to achieve a higher quality and functionality of GLOs themselves (in terms of the parameter space enlargement for reuse and adaptation). We demonstrate the appropriateness of the methodology in the real teaching setting. In this paper, we present the case study that analyses three robot-oriented GLOs as the higher-level specifications. Then, using the meta-language processor, we are able to produce, from the specifications, the concrete robot control programs on demand automatically and to demonstrate teaching algorithms visually by robot's actions. We evaluate the approach from technological and pedagogical perspectives using the known structural metrics. Also, we indicate the merits and demerits of the approach. The main contribution and originality of the paper is the seamless integration of two known technologies (feature modelling and meta-programming) in designing robot-oriented GLOs and their supporting tools.},
  comment       = {24},
  doi           = {https://doi.org/10.1016/j.scico.2016.03.009},
  keywords      = {Feature models, Model transformation, Generative learning objects (GLOs), GLO design tool, Educational robots},
  url           = {http://www.sciencedirect.com/science/article/pii/S0167642316300247},
}

@Article{Postmus2008,
  author        = {Douwe Postmus and Theo Dirk Meijler},
  title         = {Aligning the economic modeling of software reuse with reuse practices},
  journal       = {Information and Software Technology},
  year          = {2008},
  volume        = {50},
  number        = {7},
  pages         = {753 - 762},
  issn          = {0950-5849},
  __markedentry = {[mac:]},
  abstract      = {In contrast to current practices where software reuse is applied recursively and reusable assets are tailored trough parameterization or specialization, existing reuse economic models assume that (i) the cost of reusing a software asset depends on its size and (ii) reusable assets are developed from scratch. The contribution of this paper is that it provides modeling elements and an economic model that is better aligned with current practices. The functioning of the model is illustrated in an example. The example also shows how the model can support practitioners in deciding whether it is economically feasible to apply software reuse recursively.},
  comment       = {10},
  doi           = {https://doi.org/10.1016/j.infsof.2007.07.009},
  keywords      = {Software reuse, Reuse economic model, Composition, Variation},
  url           = {http://www.sciencedirect.com/science/article/pii/S095058490700081X},
}

@Article{Kim2011,
  author        = {Sangsig Kim and Dae-Kyoo Kim and Lunjin Lu and Suntae Kim and Sooyong Park},
  title         = {A feature-based approach for modeling role-based access control systems},
  journal       = {Journal of Systems and Software},
  year          = {2011},
  volume        = {84},
  number        = {12},
  pages         = {2035 - 2052},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {Abstract
Role-based access control (RBAC) is a popular access control model for enterprise systems due to its flexibility and scalability. There are many RBAC features available, each providing a different function. Not all features are needed for an RBAC system. Depending on the requirements, one should be able to configure features on a need basis, which reduces development complexity and thus fosters development. However, there have not been suitable methods that enable systematic configuration of RBAC features for system development. This paper presents an approach for configuring RBAC features using a combination of feature modeling and UML modeling. Feature modeling is used for capturing the structure of features and configuration rules, and UML modeling is used for defining the semantics of features. RBAC features are defined based on design principles of partial inheritance and compatibility, which facilitates feature composition and verification. We demonstrate the approach using a banking application and present tool support developed for the approach.},
  comment       = {18},
  doi           = {https://doi.org/10.1016/j.jss.2011.03.084},
  keywords      = {Feature modeling, Role-based access control, UML},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121211000926},
}

@Article{Lakka2015,
  author        = {Spyridoula Lakka and Teta Stamati and Christos Michalakelis and Dimosthenis Anagnostopoulos},
  title         = {Cross-national analysis of the relation of eGovernment maturity and OSS growth},
  journal       = {Technological Forecasting and Social Change},
  year          = {2015},
  volume        = {99},
  pages         = {132 - 147},
  issn          = {0040-1625},
  __markedentry = {[mac:]},
  abstract      = {The aims of this research are to explore and evaluate the nature of the relationship between open source software (OSS) and eGovernment maturity, as well as the factors impacting their development at a national level. The study proposes a theoretical framework, under the prism of which socio-economic, technological and institutional factors critical to eGovernment and OSS are revealed. The hypotheses are evaluated by means of an econometric model of simultaneous equations. In order to better gauge the results of the hypotheses, the model is evaluated over economic environments at different stages of development. Social development and OSS growth were found to be the most important facilitators for eGovernment maturity, across countries of all stages of development. Institutional quality, technological openness, freedom in press and the macro-economic environment exerted different weights of importance across different country groupings. Findings also suggest that technological infrastructure and innovation are important drivers for OSS growth across countries at all stages of development. Research results can provide useful input for research in eGov, as they open up new directions in the study of the relation with OSS.},
  comment       = {16},
  doi           = {https://doi.org/10.1016/j.techfore.2015.06.024},
  keywords      = {eGovernment, Open source software, Simultaneous equations, Institutionalism, Endogenous growth theory, Exogenous growth theory},
  url           = {http://www.sciencedirect.com/science/article/pii/S0040162515001912},
}

@Article{Souza2015a,
  author        = {Ã‰rica Ferreira de Souza and Ricardo de Almeida Falbo and Nandamudi L. Vijaykumar},
  title         = {Knowledge management initiatives in software testing: A mapping study},
  journal       = {Information and Software Technology},
  year          = {2015},
  volume        = {57},
  pages         = {378 - 391},
  issn          = {0950-5849},
  __markedentry = {[mac:]},
  abstract      = {Context
Software testing is a knowledge intensive process, and, thus, Knowledge Management (KM) principles and techniques should be applied to manage software testing knowledge.
Objective
This study conducts a survey on existing research on KM initiatives in software testing, in order to identify the state of the art in the area as well as the future research. Aspects such as purposes, types of knowledge, technologies and research type are investigated.
Method
The mapping study was performed by searching seven electronic databases. We considered studies published until December 2013. The initial resulting set was comprised of 562 studies. From this set, a total of 13 studies were selected. For these 13, we performed snowballing and direct search to publications of researchers and research groups that accomplished these studies.
Results
From the mapping study, we identified 15 studies addressing KM initiatives in software testing that have been reviewed in order to extract relevant information on a set of research questions.
Conclusions
Although only a few studies were found that addressed KM initiatives in software testing, the mapping shows an increasing interest in the topic in the recent years. Reuse of test cases is the perspective that has received more attention. From the KM point of view, most of the studies discuss aspects related to providing automated support for managing testing knowledge by means of a KM system. Moreover, as a main conclusion, the results show that KM is pointed out as an important strategy for increasing test effectiveness, as well as for improving the selection and application of suited techniques, methods and test cases. On the other hand, inadequacy of existing KM systems appears as the most cited problem related to applying KM in software testing.},
  comment       = {14},
  doi           = {https://doi.org/10.1016/j.infsof.2014.05.016},
  keywords      = {Software testing, Knowledge management, Mapping study},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584914001335},
}

@Article{Magdaleno2015,
  author        = {AndrÃ©a MagalhÃ£es Magdaleno and Marcio de Oliveira Barros and ClÃ¡udia Maria Lima Werner and Renata Mendes de Araujo and Carlos Freud Alves Batista},
  title         = {Collaboration optimization in software process composition},
  journal       = {Journal of Systems and Software},
  year          = {2015},
  volume        = {103},
  pages         = {452 - 466},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {Purpose: The purpose of this paper is to describe an optimization approach to maximize collaboration in software process composition. The research question is: how to compose a process for a specific software development project context aiming to maximize collaboration among team members? The optimization approach uses heuristic search algorithms to navigate the solution space and look for acceptable solutions. Design/methodology/approach: The process composition approach was evaluated through an experimental study conducted in the context of a large oil company in Brazil. The objective was to evaluate the feasibility of composing processes for three software development projects. We have also compared genetic algorithm (GA) and hill climbing (HC) algorithms driving the optimization with a simple random search (RS) in order to determine which would be more effective in addressing the problem. In addition, human specialist point-of-view was explored to verify if the composed processes were in accordance with his/her expectations regarding size, complexity, diversity, and reasonable sequence of components. Findings: The main findings indicate that GA is more effective (best results regarding the fitness function) than HC and RS in the search of solutions for collaboration optimization in software process composition for large instances. However, all algorithms are competitive for small instances and even brute force can be a feasible alternative in such a context. These SBSE results were complemented by the feedback given by specialist, indicating his satisfaction with the correctness, diversity, adherence to the project context, and support to the project manager during the decision making in process composition. Research limitations: This work was evaluated in the context of a single company and used only three project instances. Due to confidentiality restrictions, the data describing these instances could not be disclosed to be used in other research works. The reduced size of the sample prevents generalization for other types of projects or different contexts. Implications: This research is important for practitioners who are facing challenges to handle diversity in software process definition, since it proposes an approach based on context, reuse and process composition. It also contributes to research on collaboration by presenting a collaboration management solution (COMPOOTIM) that includes both an approach to introduce collaboration in organizations through software processes and a collaboration measurement strategy. From the standpoint of software engineering looking for collaborative solutions in distributed software development, free/open source software, agile, and ecosystems initiatives, the results also indicate how to increase collaboration in software development. Originality/value: This work proposes a systematic strategy to manage collaboration in software development process composition. Moreover, it brings together a mix of computer-oriented and human-oriented studies on the search-based software engineering (SBSE) research area. Finally, this work expands the body of knowledge in SBSE to the field of software process which has not been properly explored by former research.},
  comment       = {14},
  doi           = {https://doi.org/10.1016/j.jss.2014.11.036},
  keywords      = {Collaboration, Software process, SBSE},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121214002672},
}

@Article{Bazi2017,
  author        = {Hamid reza Bazi and Alireza Hassanzadeh and Ali Moeini},
  title         = {A comprehensive framework for cloud computing migration using Meta-synthesis approach},
  journal       = {Journal of Systems and Software},
  year          = {2017},
  volume        = {128},
  pages         = {87 - 105},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {Migration to the cloud computing environment is a strategic organizational decision. Using a reliable framework for migration ensures managers to mitigate risks in the cloud computing technology. Therefore, organizations always search for cloud migration frameworks with dynamic nature as well as integrity beside their simplicity. In previous studies, these important features have received less attention and have not been achieved in an integrated and comprehensive way. The aim of this study is to use a meta-synthesis method for the first time for analysis and synthesis of previous published studies and suggests a comprehensive cloud migration framework. We review more than 657 papers from relevant journals and conference proceedings. The concepts which are extracted from these papers are classified to related sub-categories and categories. Then, our proposed framework based on these concepts and categories is developed. It includes seven main phases (categories) and fifteen sub-categories. To improve the migration process a maturity model called â€œClM3â€ is introduced. Finally, proposed framework and maturity model is evaluated by forming different focus group meetings and taking advantages of the cloud expertsâ€™ opinion. The results of this research can help managers have a safe and effective migration to cloud computing environment.},
  comment       = {19},
  doi           = {https://doi.org/10.1016/j.jss.2017.02.049},
  keywords      = {Cloud computing, Migration framework, Meta-synthesis, Process maturity model},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121217300456},
}

@Article{Koning2009,
  author        = {Michiel Koning and Chang-ai Sun and Marco Sinnema and Paris Avgeriou},
  title         = {VxBPEL: Supporting variability for Web services in BPEL},
  journal       = {Information and Software Technology},
  year          = {2009},
  volume        = {51},
  number        = {2},
  pages         = {258 - 269},
  issn          = {0950-5849},
  __markedentry = {[mac:]},
  abstract      = {Web services provide a way to facilitate the business integration over the Internet. Flexibility is an important and desirable property of Web service-based systems due to dynamic business environments. The flexibility can be provided or addressed by incorporating variability into a system. In this study, we investigate how variability can be incorporated into service-based systems. We propose a language, VxBPEL, which is an adaptation of an existing language, BPEL, and able to capture variability in these systems. We develop a prototype to interpret this language. Finally, we illustrate our method by using it to handle variability of an example.},
  comment       = {12},
  doi           = {https://doi.org/10.1016/j.infsof.2007.12.002},
  keywords      = {Variability, Web service, Service-based system, Business Process Execution Language},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584908000207},
}

@Article{Zamli2016,
  author        = {Kamal Z. Zamli and Basem Y. Alkazemi and Graham Kendall},
  title         = {A Tabu Search hyper-heuristic strategy for t-way test suite generation},
  journal       = {Applied Soft Computing},
  year          = {2016},
  volume        = {44},
  pages         = {57 - 74},
  issn          = {1568-4946},
  __markedentry = {[mac:]},
  abstract      = {This paper proposes a novel hybrid t-way test generation strategy (where t indicates interaction strength), called High Level Hyper-Heuristic (HHH). HHH adopts Tabu Search as its high level meta-heuristic and leverages on the strength of four low level meta-heuristics, comprising of Teaching Learning based Optimization, Global Neighborhood Algorithm, Particle Swarm Optimization, and Cuckoo Search Algorithm. HHH is able to capitalize on the strengths and limit the deficiencies of each individual algorithm in a collective and synergistic manner. Unlike existing hyper-heuristics, HHH relies on three defined operators, based on improvement, intensification and diversification, to adaptively select the most suitable meta-heuristic at any particular time. Our results are promising as HHH manages to outperform existing t-way strategies on many of the benchmarks.},
  comment       = {18},
  doi           = {https://doi.org/10.1016/j.asoc.2016.03.021},
  keywords      = {Software testing, Testing, Hyper-heuristic, Particle Swarm Optimization, Cuckoo Search Algorithm, Teaching Learning based Optimization, Global Neighborhood Algorithm},
  url           = {http://www.sciencedirect.com/science/article/pii/S1568494616301302},
}

@Article{Castro2012,
  author        = {Jaelson Castro and Marcia Lucena and Carla Silva and Fernanda Alencar and Emanuel Santos and JoÃ£o Pimentel},
  title         = {Changing attitudes towards the generation of architectural models},
  journal       = {Journal of Systems and Software},
  year          = {2012},
  volume        = {85},
  number        = {3},
  pages         = {463 - 479},
  issn          = {0164-1212},
  note          = {Novel approaches in the design and implementation of systems/software architecture},
  __markedentry = {[mac:]},
  abstract      = {Architectural design is an important activity, but the understanding of how it is related to requirements modeling is rather limited. It is worth noting that goal orientation is an increasingly recognized paradigm for eliciting, modeling, specifying, and analyzing software requirements. However, it is not clear how goal models are related to architectural models. In this paper we present an approach based on model transformations to derive architectural structural specifications from system goals. The source and target languages are respectively the i* (iStar) modeling language and the Acme architectural description language. A real case study is used to show the feasibility of our approach.},
  comment       = {17},
  doi           = {https://doi.org/10.1016/j.jss.2011.05.047},
  keywords      = {Requirements engineering, Architectural design, Model driven development, Model transformations},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121211001415},
}

@Article{Peng2013,
  author        = {Xin Peng and Zhenchang Xing and Xi Tan and Yijun Yu and Wenyun Zhao},
  title         = {Improving feature location using structural similarity and iterative graph mapping},
  journal       = {Journal of Systems and Software},
  year          = {2013},
  volume        = {86},
  number        = {3},
  pages         = {664 - 676},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {Locating program element(s) relevant to a particular feature is an important step in efficient maintenance of a software system. The existing feature location techniques analyse each feature independently and perform a one-time analysis after being provided an initial input. As a result, these techniques are sensitive to the quality of the input. In this paper, we propose to address the above issues in feature location using an iterative context-aware approach. The underlying intuition is that features are not independent of each other, and the structure of source code resembles the structure of features. The distinguishing characteristics of the proposed approach are: (1) it takes into account the structural similarity between a feature and a program element to determine feature-element relevance and (2) it employs an iterative process to propagate the relevance of the established mappings between a feature and a program element to the neighbouring features and program elements. We evaluate our approach using two different systems, DirectBank, a small-scale industry financial system, and Linux kernel, a large-scale open-source operating system. Our evaluation suggests that the proposed approach is more robust and can significantly increase the recall of feature location with only a minor decrease of precision.},
  comment       = {13},
  doi           = {https://doi.org/10.1016/j.jss.2012.10.270},
  keywords      = {Feature location, Traceability recovery, Information retrieval, Structural similarity},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121212003007},
}

@Article{Guerra2013,
  author        = {Eduardo Guerra and Felipe Alves and UirÃ¡ Kulesza and Clovis Fernandes},
  title         = {A reference architecture for organizing the internal structure of metadata-based frameworks},
  journal       = {Journal of Systems and Software},
  year          = {2013},
  volume        = {86},
  number        = {5},
  pages         = {1239 - 1256},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {Metadata-based frameworks enable behavior adaptation through the configuration of custom metadata in application classes. Most of the current frameworks used in the industry for building enterprise applications adopt this approach. However, there is a lack of proven techniques for building such kind of framework, allowing for a better organization of its internal structure. In this paper we propose a pattern language and a reference architecture for better organizing the internal structure of metadata-based frameworks, which were defined as a result of a pattern mining process applied to a set of existing open source frameworks. To evaluate the resulting structure generated by the reference architecture application, a case study examined three frameworks developed according to the proposed reference architecture, each one referring to a distinct application domain. The assessment was conducted by using a metrics suite, metrics thresholds derived from a large set of open source metadata-based frameworks, a process for automatic detection of design disharmonies and manual source code analysis. As a result of this study, framework developers can understand and use the proposed reference architecture to develop new frameworks and refactor existing ones. The assessment revealed that the organization provided by the reference architecture is suitable for metadata-based frameworks, helping in the division of responsibility and functionality among their classes.},
  comment       = {18},
  doi           = {https://doi.org/10.1016/j.jss.2012.12.024},
  keywords      = {Framework, Metadata, Metadata-based framework, Software architecture, Reference architecture, Pattern language},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121212003366},
}

@Article{Kwiatkowski2013,
  author        = {Å.M. Kwiatkowski and C. Verhoef},
  title         = {Recovering management information from source code},
  journal       = {Science of Computer Programming},
  year          = {2013},
  volume        = {78},
  number        = {9},
  pages         = {1368 - 1406},
  issn          = {0167-6423},
  __markedentry = {[mac:]},
  abstract      = {IT has become a production means for many organizations and an important element of business strategy. Even though its effective management is a must, reality shows that this area still remains in its infancy. IT management relies profoundly on relevant information which enables risk mitigation or cost control. However, the needed information is either missing or its gathering boils down to daunting tasks. We propose an approach to recovery of management information from the essence of IT; the softwareâ€™s source code. In this paper we show how to employ source code analysis techniques and recover management information. In our approach we exploit the potential of the concealed data which resides in the source code statements, source comments, and also compiler listings. We show how to depart from the raw sources, extract data, organize it, and eventually utilize so that the bit level data provides IT executives with support at the portfolio level. Our approach is pragmatic as we rely on real management questions, best practices in software engineering, and also IT market specifics. We enable, for instance, an assessment of the IT-portfolio market value, support for carrying out what-if scenarios, or identification and evaluation of the hidden risks for IT-portfolio maintainability. The study is based on a real-life IT-portfolio which supports business functions of an organization operating in the financial sector. The IT-portfolio comprises Cobol applications run on a mainframe with the total number of lines of code amounting to over 18 million. The approach we propose is suited for facilitation within a large organization. It provides for a fact-based support for strategic decision making at the portfolio level.},
  comment       = {39},
  doi           = {https://doi.org/10.1016/j.scico.2012.07.016},
  keywords      = {IT-portfolio management, Management information, Source code analysis, Lexical analysis, Latent Semantic Indexing, LSI, Source code comments, Compilers, Obsolete language constructs, Volatility, Vendor locks, Legacy systems, Operational risk, Technology risk, Risk mitigation, Cost control, Market value, Scenario analysis, IT assets, IT metrics, Automated data extraction, Information retrieval, Cobol, Case study},
  url           = {http://www.sciencedirect.com/science/article/pii/S016764231200144X},
}

@Article{Figueiredo2012,
  author        = {Eduardo Figueiredo and Claudio Santâ€™Anna and Alessandro Garcia and Carlos Lucena},
  title         = {Applying and evaluating concern-sensitive design heuristics},
  journal       = {Journal of Systems and Software},
  year          = {2012},
  volume        = {85},
  number        = {2},
  pages         = {227 - 243},
  issn          = {0164-1212},
  note          = {Special issue with selected papers from the 23rd Brazilian Symposium on Software Engineering},
  __markedentry = {[mac:]},
  abstract      = {Manifestation of crosscutting concerns in software systems is often an indicative of design modularity flaws and further design instabilities as those systems evolve. Without proper design evaluation mechanisms, the identification of harmful crosscutting concerns can become counter-productive and impractical. Nowadays, metrics and heuristics are the basic mechanisms to support their identification and classification either in object-oriented or aspect-oriented programs. However, conventional mechanisms have a number of limitations to support an effective identification and classification of crosscutting concerns in a software system. In this paper, we claim that those limitations are mostly caused by the fact that existing metrics and heuristics are not sensitive to primitive concern properties, such as either their degree of tangling and scattering or their specific structural shapes. This means that modularity assessment is rooted only at conventional attributes of modules, such as module cohesion, coupling and size. This paper proposes a representative suite of concern-sensitive heuristic rules. The proposed heuristics are supported by a prototype tool. The paper also reports an exploratory study to evaluate the accuracy of the proposed heuristics by applying them to seven systems. The results of this exploratory analysis give evidences that the heuristics offer support for: (i) addressing the shortcomings of conventional metrics-based assessments, (ii) reducing the manifestation of false positives and false negatives in modularity assessment, (iii) detecting sources of design instability, and (iv) finding the presence of design modularity flaws in both object-oriented and aspect-oriented programs. Although our results are limited to a number of decisions we made in this study, they indicate a promising research direction. Further analyses are required to confirm or refute our preliminary findings and, so, this study should be seen as a stepping stone on understanding how concerns can be useful assessment abstractions. We conclude this paper by discussing the limitations of this exploratory study focusing on some situations which hinder the accuracy of concern-sensitive heuristics.},
  comment       = {17},
  doi           = {https://doi.org/10.1016/j.jss.2011.09.060},
  keywords      = {Design heuristics, Modularity, Crosscutting concerns, Software metrics, Aspect-oriented software development},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121211002585},
}

@Article{Proenca2017,
  author        = {JosÃ© ProenÃ§a and Dave Clarke},
  title         = {Typed connector families and their semantics},
  journal       = {Science of Computer Programming},
  year          = {2017},
  volume        = {146},
  pages         = {28 - 49},
  issn          = {0167-6423},
  note          = {Special issue with extended selected papers from FACS 2015},
  __markedentry = {[mac:]},
  abstract      = {Typed models of connector/component composition specify interfaces describing ports of components and connectors. Typing ensures that these ports are plugged together appropriately, so that data can flow out of each output port and into an input port. These interfaces typically consider the direction of data flow and the type of values flowing. Components, connectors, and systems are often parameterised in such a way that the parameters affect the interfaces. Typing such connector families is challenging. This paper takes a first step towards addressing this problem by presenting a calculus of connector families with integer and boolean parameters. The calculus is based on monoidal categories, with a dependent type system that describes the parameterised interfaces of these connectors. We use families of Reo connectors as running examples, and show how this calculus can be applied to Petri Nets and to BIP systems. The paper focuses on the structure of connectorsâ€”well-connectednessâ€”and less on their behaviour, making it easily applicable to a wide range of coordination and component-based models. A type-checking algorithm based on constraints is used to analyse connector families, supported by a proof-of-concept implementation.},
  comment       = {22},
  doi           = {https://doi.org/10.1016/j.scico.2017.03.002},
  keywords      = {Calculus of connectors, Variability in connectors, Composition of families, Type system, Tile Model},
  url           = {http://www.sciencedirect.com/science/article/pii/S0167642317300515},
}

@Article{Kaur2015a,
  author        = {Ramandeep Kaur and Stuti Arora and P.C. Jha and Sushila Madan},
  title         = {Fuzzy Multi-criteria Approach for Component Selection of Fault Tolerant Software System under Consensus Recovery Block Scheme},
  journal       = {Procedia Computer Science},
  year          = {2015},
  volume        = {45},
  pages         = {842 - 851},
  issn          = {1877-0509},
  note          = {International Conference on Advanced Computing Technologies and Applications (ICACTA)},
  __markedentry = {[mac:]},
  abstract      = {The vibrant and advanced software development tools not only provide software with versatile functions for radical users but at the same time, an easy to use GUI for naive users. APS (Application Package Software) has provided a customised approach for developing independent software components which are ready to be integrated with existing software systems. The APS along with CBSE (Component Based Software Engineering) has an inordinate potential for reducing development time, cost and effort, which otherwise may extend beyond weeks or monthsâ€™ time for integration. Further, the CBSE approach promotes software reusability i.e. reusing the available components. A component can be reused after fabrication which will include the fabrication cost and time. For development of economical and reliable software, components can be procured in the form of Commercial off-The Shelf (COTS) components from the vendor or may be developed in-house or can be fabricated. This decision is based on several parameters. The aim of this paper is to select the suitable mix of components using Build-or-buy strategy or considering fabrication and to propose a multi-objective model for software modular system with objective of maximizing reliability while simultaneously minimizing the cost, execution time and Source Lines of Code (SLOC) using Consensus Recovery Block Scheme.},
  comment       = {10},
  doi           = {https://doi.org/10.1016/j.procs.2015.03.169},
  keywords      = {CBSE, Software Reusability, Fabrication, COTS, Build-or-Buy, CRB, Execution Time, SLOC ;},
  url           = {http://www.sciencedirect.com/science/article/pii/S1877050915004123},
}

@Article{Rabiser2017,
  author        = {Rick Rabiser and Sam Guinea and Michael Vierhauser and Luciano Baresi and Paul GrÃ¼nbacher},
  title         = {A comparison framework for runtime monitoring approaches},
  journal       = {Journal of Systems and Software},
  year          = {2017},
  volume        = {125},
  pages         = {309 - 321},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {The full behavior of complex software systems often only emerges during operation. They thus need to be monitored at run time to check that they adhere to their requirements. Diverse runtime monitoring approaches have been developed in various domains and for different purposes. Their sheer number and heterogeneity, however, make it hard to find the right approach for a specific application or purpose. The aim of our research therefore was to develop a comparison framework for runtime monitoring approaches. Our framework is based on an analysis of the literature and existing taxonomies for monitoring languages and patterns. We use examples from existing monitoring approaches to explain the framework. We demonstrate its usefulness by applying it to 32 existing approaches and by comparing 3 selected approaches in the light of different monitoring scenarios. We also discuss perspectives for researchers.},
  comment       = {13},
  doi           = {https://doi.org/10.1016/j.jss.2016.12.034},
  keywords      = {Runtime monitoring, Literature review, Comparison framework},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121216302618},
}

@Article{Succi2003,
  author        = {Giancarlo Succi and Witold Pedrycz and Milorad Stefanovic and James Miller},
  title         = {Practical assessment of the models for identification of defect-prone classes in object-oriented commercial systems using design metrics},
  journal       = {Journal of Systems and Software},
  year          = {2003},
  volume        = {65},
  number        = {1},
  pages         = {1 - 12},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {The goal of this paper is to investigate and assess the ability of explanatory models based on design metrics to describe and predict defect counts in an object-oriented software system. Specifically, we empirically evaluate the influence of design decisions to defect behavior of the classes in two products from the commercial software domain. Information provided by these models can help in resource allocation and serve as a base for assessment and future improvements. We use innovative statistical methods to deal with the peculiarities of the software engineering data, such as non-normally distributed count data. To deal with overdispersed data and excess of zeroes in the dependent variable, we use negative binomial (NB) and zero-inflated NB regression in addition to Poisson regression. Furthermore, we form a framework for comparison of modelsâ€™ descriptive and predictive ability. Predictive capability of the models to identify most critical classes in the system early in the software development process can help in allocation of resources and foster software quality improvement. In addition to the correlation coefficients, we use additional statistics to assess a modelsâ€™ ability to explain high variability in the data and Pareto analysis to assess a modelsâ€™ ability to identify the most critical classes in the system. Results indicate that design aspects related to communication between classes and inheritance can be used as indicators of the most defect-prone classes, which require the majority of resources in development and testing phases. The zero-inflated negative binomial regression model, designed to explicitly model the occurrence of zero counts in the dataset, provides the best results for this purpose.},
  comment       = {12},
  doi           = {https://doi.org/10.1016/S0164-1212(02)00024-9},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121202000249},
}

@Article{Garcia-Magarino2015,
  author        = {IvÃ¡n GarcÃ­a-MagariÃ±o and Inmaculada Plaza},
  title         = {FTS-SOCI: An agent-based framework for simulating teaching strategies with evolutions of sociograms},
  journal       = {Simulation Modelling Practice and Theory},
  year          = {2015},
  volume        = {57},
  pages         = {161 - 178},
  issn          = {1569-190X},
  __markedentry = {[mac:]},
  abstract      = {Teaching strategies have been proven to influence the academic performance of students, as well as group sociometrics have been proven to be directly related to group performance. Although in the literature there are examples of teaching strategies and the resulting sociograms, there is not any detailed technique, tool or simulator that predicts the influence of a new teaching strategy on group sociometrics. The current work is aimed at covering this gap in the literature, by providing a framework for programming teaching strategies to simulate their influence on group sociometrics. In particular, this framework is called FTS-SOCI (Framework for simulating Teaching Strategies with evolutions of SOCIograms). This framework includes an agent-based simulator that simulates the evolution of sociograms taking five models of the literature into account. In this framework, students and teacher are modelled as agents, and the teacher agent can have any teaching strategy defined by the user. In order to test the current approach, this work simulates existing teaching strategies in (1) nursing education and (2) sport lessons. The resulting sociograms of FTS-SOCI for these strategies have been compared with the corresponding real sociograms reported in the literature. This works shows that the group sociometric values provided by FTS-SOCI are quite similar to the real cases. For instance, the mean squared error of the group cohesion sociometric (i.e. IAg metric) is only 0.00024 and 0.00068 respectively for the teaching strategies of both fields.},
  comment       = {18},
  doi           = {https://doi.org/10.1016/j.simpat.2015.07.003},
  keywords      = {Agent-based simulator, Agent-based framework, Multi-agent system, Sociogram, Teaching strategy},
  url           = {http://www.sciencedirect.com/science/article/pii/S1569190X15001070},
}

@Article{Colanzi2013,
  author        = {Thelma Elita Colanzi and Silvia Regina Vergilio and Wesley Klewerton Guez AssunÃ§Ã£o and Aurora Pozo},
  title         = {Search Based Software Engineering: Review and analysis of the field in Brazil},
  journal       = {Journal of Systems and Software},
  year          = {2013},
  volume        = {86},
  number        = {4},
  pages         = {970 - 984},
  issn          = {0164-1212},
  note          = {SI : Software Engineering in Brazil: Retrospective and Prospective Views},
  __markedentry = {[mac:]},
  abstract      = {Search Based Software Engineering (SBSE) is the field of software engineering research and practice that applies search based techniques to solve different optimization problems from diverse software engineering areas. SBSE approaches allow software engineers to automatically obtain solutions for complex and labor-intensive tasks, contributing to reduce efforts and costs associated to the software development. The SBSE field is growing rapidly in Brazil. The number of published works and research groups has significantly increased in the last three years and a Brazilian SBSE community is emerging. This is mainly due to the Brazilian Workshop on Search Based Software Engineering (WOES), co-located with the Brazilian Symposium on Software Engineering (SBES). Considering these facts, this paper presents results of a mapping we have performed in order to provide an overview of the SBSE field in Brazil. The main goal is to map the Brazilian SBSE community on SBES by identifying the main researchers, focus of the published works, fora and frequency of publications. The paper also introduces SBSE concerns and discusses trends, challenges, and open research problems to this emergent area. We hope the work serves as a reference to this novel field, contributing to disseminate SBSE and to its consolidation in Brazil.},
  comment       = {15},
  doi           = {https://doi.org/10.1016/j.jss.2012.07.041},
  keywords      = {Search based algorithms, Metaheuristics, Software engineering},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121212002166},
}

@Article{Lumertz2016,
  author        = {Paulo Roberto Lumertz and Leila Ribeiro and Lucio Mauro Duarte},
  title         = {User interfaces metamodel based on graphs},
  journal       = {Journal of Visual Languages \& Computing},
  year          = {2016},
  volume        = {32},
  pages         = {1 - 34},
  issn          = {1045-926X},
  __markedentry = {[mac:]},
  abstract      = {Information systems are widely used in all business areas. These systems typically integrate a set of functionalities that implement business rules and maintain databases. Users interact with these systems and use these features through user interfaces (UI). Each UI is usually composed of menus where the user can select the desired functionality, thus accessing a new UI that corresponds to the desired feature. Hence, a system normally contains multiple UIs. However, keeping consistency between these UIs of a system from a visual (organisation, component style, etc.) and behavioral perspective is usually difficult. This problem also appears in software production lines, where it would be desirable to have patterns to guide the construction and maintenance of UIs. One possible way of defining such patterns is to use model-driven engineering (MDE). In MDE, models are defined at different levels, where the bottom level is called a metamodel. The metamodel determines the main characteristics of the models of the upper levels, serving as a guideline. Each new level must adhere to the rules defined by the lower levels. This way, if anything changes in a lower level, these changes are propagated to the levels above it. The goal of this work is to define and validate a metamodel that allows the modeling of UIs of software systems, thus allowing the definition of patterns of interface and supporting system evolution. To build this metamodel, we use a graph structure. This choice is due to the fact that a UI can be easily represented as a graph, where each UI component is a vertex and edges represent dependencies between these components. Moreover, graph theory provides support for a great number of operations and transformations that can be useful for UIs. The metamodel was defined based on the investigation of patterns that occur in UIs. We used a sample of information systems containing different types of UIs to obtain such patterns. To validate the metamodel, we built the complete UI models of one new system and of four existing real systems. This shows not only the expressive power of the metamodel, but also its versatility, since our validation was conducted using different types of systems (a desktop system, a web system, mobile system, and a multiplatform system). Moreover, it also demonstrated that the proposed approach can be used not only to build new models, but also to describe existing ones (by reverse engineering).},
  comment       = {34},
  doi           = {https://doi.org/10.1016/j.jvlc.2015.10.026},
  keywords      = {Metamodel, Graphs, Graph transformation, User interface, User interface patterns},
  url           = {http://www.sciencedirect.com/science/article/pii/S1045926X1500083X},
}

@Article{Ognjanovic2013,
  author        = {Ivana OgnjanoviÄ‡ and Dragan GaÅ¡eviÄ‡ and Ebrahim Bagheri},
  title         = {A stratified framework for handling conditional preferences: An extension of the analytic hierarchy process},
  journal       = {Expert Systems with Applications},
  year          = {2013},
  volume        = {40},
  number        = {4},
  pages         = {1094 - 1115},
  issn          = {0957-4174},
  __markedentry = {[mac:]},
  abstract      = {Representing and reasoning over different forms of preferences is of crucial importance to many different fields, especially where numerical comparisons need to be made between critical options. Focusing on the well-known Analytical Hierarchical Process (AHP) method, we propose a two-layered framework for addressing different kinds of conditional preferences which include partial information over preferences and preferences of a lexicographic kind. The proposed formal two-layered framework, called CS-AHP, provides the means for representing and reasoning over conditional preferences. The framework can also effectively order decision outcomes based on conditional preferences in a way that is consistent with well-formed preferences. Finally, the framework provides an estimation of the potential number of violations and inconsistencies within the preferences. We provide and report extensive performance analysis for the proposed framework from three different perspectives, namely time-complexity, simulated decision making scenarios, and handling cyclic and partially defined preferences.},
  comment       = {21},
  doi           = {https://doi.org/10.1016/j.eswa.2012.08.026},
  keywords      = {Conditional preferences, Comparative preferences, AHP method, S-AHP method, Lexicographic order, Well-formed preferences},
  url           = {http://www.sciencedirect.com/science/article/pii/S0957417412009876},
}

@Article{Mohan2007,
  author        = {Kannan Mohan and Balasubramaniam Ramesh},
  title         = {Traceability-based knowledge integration in group decision and negotiation activities},
  journal       = {Decision Support Systems},
  year          = {2007},
  volume        = {43},
  number        = {3},
  pages         = {968 - 989},
  issn          = {0167-9236},
  note          = {Integrated Decision Support},
  __markedentry = {[mac:]},
  abstract      = {Group decision and negotiation (GDN) in distributed collaborative environments involves the acquisition and use of extensive knowledge. Knowledge elements that play a critical role in guiding GDN activities are distributed across different work environments that are not seamlessly integrated with each other. We argue that integrating fragmented knowledge will improve the process of GDN in software development. In this paper, we present an approach to knowledge integration using traceability. Our approach comprises of: (a) a traceability framework that identifies the key knowledge elements that are to be integrated, and (b) a prototype system that supports the acquisition, integration, and use of knowledge elements represented by the traceability framework. We illustrate the usefulness of our approach with a case study in a software development organization.},
  comment       = {22},
  doi           = {https://doi.org/10.1016/j.dss.2005.05.026},
  keywords      = {Knowledge integration, Traceability, Collaborative software development, Decision making, Group decision and negotiation, Work processes},
  url           = {http://www.sciencedirect.com/science/article/pii/S0167923605000916},
}

@Article{Rosa2011,
  author        = {Marcello La Rosa and Marlon Dumas and Arthur H.M. ter Hofstede and Jan Mendling},
  title         = {Configurable multi-perspective business process models},
  journal       = {Information Systems},
  year          = {2011},
  volume        = {36},
  number        = {2},
  pages         = {313 - 340},
  issn          = {0306-4379},
  note          = {Special Issue: Semantic Integration of Data, Multimedia, and Services},
  __markedentry = {[mac:]},
  abstract      = {A configurable process model provides a consolidated view of a family of business processes. It promotes the reuse of proven practices by providing analysts with a generic modeling artifact from which to derive individual process models. Unfortunately, the scope of existing notations for configurable process modeling is restricted, thus hindering their applicability. Specifically, these notations focus on capturing tasks and control-flow dependencies, neglecting equally important ingredients of business processes such as data and resources. This research fills this gap by proposing a configurable process modeling notation incorporating features for capturing resources, data and physical objects involved in the performance of tasks. The proposal has been implemented in a toolset that assists analysts during the configuration phase and guarantees the correctness of the resulting process models. The approach has been validated by means of a case study from the film industry.},
  comment       = {28},
  doi           = {https://doi.org/10.1016/j.is.2010.07.001},
  keywords      = {Business process, Configurable process model, EPC},
  url           = {http://www.sciencedirect.com/science/article/pii/S0306437910000633},
}

@Article{Liaskos2012,
  author        = {Sotirios Liaskos and Shakil M. Khan and Marin Litoiu and Marina Daoud Jungblut and Vyacheslav Rogozhkin and John Mylopoulos},
  title         = {Behavioral adaptation of information systems through goal models},
  journal       = {Information Systems},
  year          = {2012},
  volume        = {37},
  number        = {8},
  pages         = {767 - 783},
  issn          = {0306-4379},
  note          = {Special Issue: Advanced Information Systems Engineering (CAiSE'11)},
  __markedentry = {[mac:]},
  abstract      = {Customizing software to perfectly fit individual needs is becoming increasingly important in information systems engineering. Users want to be able to customize software behavior through reference to terms familiar to their diverse needs and experience. We present a requirements-driven approach to behavioral customization of software systems. Goal models are constructed to represent alternative behaviors that users can exhibit to achieve their goals. Customization information is then added to restrict the space of possibilities to those that fit specific users, contexts, or situations. Meanwhile, elements of the goal models are mapped to units of source code. This way, customization preferences posed at the requirements level are directly translated into system customizations. Our approach, which we apply to an on-line shopping cart system and an automated teller machine simulator, does not assume adoption of a particular development methodology, platform, or variability implementation technique and keeps the reasoning computation overhead from interfering with the execution of the configured application.},
  comment       = {17},
  doi           = {https://doi.org/10.1016/j.is.2012.05.006},
  keywords      = {Information systems engineering, Goal modeling, Software customization, Adaptive systems},
  url           = {http://www.sciencedirect.com/science/article/pii/S0306437912000737},
}

@Article{Duque2012,
  author        = {Rafael Duque and MarÃ­a Luisa RodrÃ­guez and MarÃ­a VisitaciÃ³n Hurtado and Crescencio Bravo and Carlos RodrÃ­guez-DomÃ­nguez},
  title         = {Integration of collaboration and interaction analysis mechanisms in a concern-based architecture for groupware systems},
  journal       = {Science of Computer Programming},
  year          = {2012},
  volume        = {77},
  number        = {1},
  pages         = {29 - 45},
  issn          = {0167-6423},
  note          = {System and Software Solution Oriented Architectures},
  __markedentry = {[mac:]},
  abstract      = {Collaboration and interaction analysis allows for the characterization and study of the collaborative work performed by the users of a groupware system. The results of the analyzed processes allow problems in usersâ€™ collaborative work and shortcomings in the functionalities of the groupware system to be identified. Therefore, automating collaboration and interaction analysis enables usersâ€™ work to be assessed and groupware system support and behavior to be improved. This article proposes a concern-based architecture to be used by groupware developers as a guide to the integration of analysis subsystems into groupware systems. This architecture was followed to design the COLLECE groupware system, which supports collaborative programming practices and integrates an analysis subsystem that assesses different aspects of the work carried out by the programmers and adapts the functionality of the system under specific conditions.},
  comment       = {17},
  doi           = {https://doi.org/10.1016/j.scico.2010.05.003},
  keywords      = {Collaboration and interaction analysis, CSCW, Groupware, Software architectures},
  url           = {http://www.sciencedirect.com/science/article/pii/S0167642310000936},
}

@Article{Koehler2008,
  author        = {Christian Koehler and Alexander Lazovik and Farhad Arbab},
  title         = {Connector Rewriting with High-Level Replacement Systems},
  journal       = {Electronic Notes in Theoretical Computer Science},
  year          = {2008},
  volume        = {194},
  number        = {4},
  pages         = {77 - 92},
  issn          = {1571-0661},
  note          = {Proceedings of the 6th International Workshop on the Foundations of Coordination Languages and Software Architectures (FOCLASA 2007)},
  __markedentry = {[mac:]},
  abstract      = {Reo is a language for coordinating autonomous components in distributed environments. Coordination in Reo is performed by circuit-like connectors, which are constructed from primitive channels with well-defined behavior. These channels are mobile, i.e. can be dynamically created and reconfigured at run-time. Based on these language features, we introduce a high-level transformation system for Reo. We show how transformations of Reo connectors can be defined using the theory of high-level replacement (HLR) systems. This leads to a powerful notion of dynamic connector reconfiguration in Reo. Moreover, the rewrite rules are naturally expressed in Reo's visual syntax for connectors. Applications of this framework are manifold, due to the generality of the field of coordination. In this paper we provide an example from the area of Service-oriented Computing.},
  comment       = {16},
  doi           = {https://doi.org/10.1016/j.entcs.2008.03.100},
  keywords      = {Reo, coordination, high-level replacement systems, adhesive categories, model transformation, service composition},
  url           = {http://www.sciencedirect.com/science/article/pii/S1571066108002053},
}

@Article{Wang2013b,
  author        = {Renzhong Wang and Cihan H. Dagli},
  title         = {Developing a Holistic Modeling Approach for Search-based System Architecting},
  journal       = {Procedia Computer Science},
  year          = {2013},
  volume        = {16},
  pages         = {206 - 215},
  issn          = {1877-0509},
  note          = {2013 Conference on Systems Engineering Research},
  __markedentry = {[mac:]},
  abstract      = {This paper proposes a holistic modeling approach that combines the capabilities of Object Process Methodology (OPM), Colored Petri Net (CPN), and feature model. The resultant holistic model not only can capture the structural, behavioral, and dynamic aspects of a system, allowing simulation and strong analysis methods to be applied, it can also specify the architectural design space. This modeling approach is developed to facilitate the implementation of search-based system architecting where search algorithms are used to explore design trade space for good architecture alternatives. Such architecting approach integrates certain model construction, alternative generation, simulation, and assessment processes into a coherent and automated framework. Both the proposed holistic modeling approach and the search-based architecting framework are generic. They are targeted at systems that can be specified by conceptual models using object-oriented or process-oriented paradigms. The broad applicability of the proposed approach is demonstrated with the configuration of reconfigurable manufacturing systems (RMSs) under multi- objective optimization as an example. The test results showed that the proposed modeling approach could cover a huge number of architecture alternatives and supported the assessment of several performance measures. A set of quality results was obtained after running the optimization algorithm following the proposed search-based architecting framework.},
  comment       = {10},
  doi           = {https://doi.org/10.1016/j.procs.2013.01.022},
  keywords      = {Object oriented modeling, Simulation, System analysis and design, System architecting},
  url           = {http://www.sciencedirect.com/science/article/pii/S1877050913000239},
}

@Article{Nakagawa2011,
  author        = {Elisa Y. Nakagawa and Fabiano C. Ferrari and Mariela M.F. Sasaki and JosÃ© C. Maldonado},
  title         = {An aspect-oriented reference architecture for Software Engineering Environments},
  journal       = {Journal of Systems and Software},
  year          = {2011},
  volume        = {84},
  number        = {10},
  pages         = {1670 - 1684},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {Abstract
Reusable and evolvable Software Engineering Environments (SEEs) are essential to software production and have increasingly become a need. In another perspective, software architectures and reference architectures have played a significant role in determining the success of software systems. In this paper we present a reference architecture for SEEs, named RefASSET, which is based on concepts coming from the aspect-oriented approach. This architecture is specialized to the software testing domain and the development of tools for that domain is discussed. This and other case studies have pointed out that the use of aspects in RefASSET provides a better Separation of Concerns, resulting in reusable and evolvable SEEs.},
  comment       = {15},
  doi           = {https://doi.org/10.1016/j.jss.2011.04.052},
  keywords      = {Software Engineering Environment, Software architecture, Reference architecture, Aspect orientation, Software testing},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121211001038},
}

@Article{Abrantes2014,
  author        = {Rui Abrantes and JosÃ© Figueiredo},
  title         = {Feature based process framework to manage scope in dynamic NPD portfolios},
  journal       = {International Journal of Project Management},
  year          = {2014},
  volume        = {32},
  number        = {5},
  pages         = {874 - 884},
  issn          = {0263-7863},
  __markedentry = {[mac:]},
  abstract      = {The need to develop new products in increasingly frequent cycles of innovation drives organizations to form new product development (NPD) portfolios. In such dynamic environments, organizations need to reinforce their capabilities to deal with the simultaneity of multiple NPD projects, as well as with the frequent changes of the product scope. Many organizations, that have adopted the typical NPD process enforcing a streamlined product development process, are challenged beyond strict planning and rigorous control of their NPD projects. This paper identifies the challenges to manage the scope of a complete portfolio of NPD projects within the dynamic context that organizations face today, and using existing scope management practices. This paper suggests a novel approach to structuring the scope in dynamic NPD portfolios using feature modeling, and illustrates its use in an action-research case.},
  comment       = {11},
  doi           = {https://doi.org/10.1016/j.ijproman.2013.10.014},
  keywords      = {Project portfolio management, Scope management, New product development, Feature modeling, Process framework, Action research},
  url           = {http://www.sciencedirect.com/science/article/pii/S0263786313001531},
}

@Article{Stallinger2013,
  author        = {Fritz Stallinger and Robert Neumann},
  title         = {Enhancing ISO/IEC 15288 with reuse and product management: An add-on process reference model},
  journal       = {Computer Standards \& Interfaces},
  year          = {2013},
  volume        = {36},
  number        = {1},
  pages         = {21 - 32},
  issn          = {0920-5489},
  __markedentry = {[mac:]},
  abstract      = {To support the transformation of system engineering from the project-based development of highly customer-specific solutions to the reuse and customization of â€˜system productsâ€™, we integrate a process reference model for reuse- and product-oriented industrial engineering and a process reference model extending ISO/IEC 12207 on software life cycle processes with software- and system-level product management. We synthesize the key process elements of both models to enhance ISO/IEC 15288 on system life cycle processes with product- and reuse-oriented engineering and product management practices as an integrated framework for process assessment and improvement in contexts where systems are developed and evolved as products.},
  comment       = {12},
  doi           = {https://doi.org/10.1016/j.csi.2013.07.006},
  keywords      = {Systems engineering, Product management, Reuse, Process reference model, ISO/IEC 15288},
  url           = {http://www.sciencedirect.com/science/article/pii/S0920548913000718},
}

@Article{Segura2011a,
  author        = {Sergio Segura and Robert M. Hierons and David Benavides and Antonio Ruiz-CortÃ©s},
  title         = {Automated metamorphic testing on the analyses of feature models},
  journal       = {Information and Software Technology},
  year          = {2011},
  volume        = {53},
  number        = {3},
  pages         = {245 - 258},
  issn          = {0950-5849},
  __markedentry = {[mac:]},
  abstract      = {Context
A feature model (FM) represents the valid combinations of features in a domain. The automated extraction of information from FMs is a complex task that involves numerous analysis operations, techniques and tools. Current testing methods in this context are manual and rely on the ability of the tester to decide whether the output of an analysis is correct. However, this is acknowledged to be time-consuming, error-prone and in most cases infeasible due to the combinatorial complexity of the analyses, this is known as the oracle problem.
Objective
In this paper, we propose using metamorphic testing to automate the generation of test data for feature model analysis tools overcoming the oracle problem. An automated test data generator is presented and evaluated to show the feasibility of our approach.
Method
We present a set of relations (so-called metamorphic relations) between input FMs and the set of products they represent. Based on these relations and given a FM and its known set of products, a set of neighbouring FMs together with their corresponding set of products are automatically generated and used for testing multiple analyses. Complex FMs representing millions of products can be efficiently created by applying this process iteratively.
Results
Our evaluation results using mutation testing and real faults reveal that most faults can be automatically detected within a few seconds. Two defects were found in FaMa and another two in SPLOT, two real tools for the automated analysis of feature models. Also, we show how our generator outperforms a related manual suite for the automated analysis of feature models and how this suite can be used to guide the automated generation of test cases obtaining important gains in efficiency.
Conclusion
Our results show that the application of metamorphic testing in the domain of automated analysis of feature models is efficient and effective in detecting most faults in a few seconds without the need for a human oracle.},
  comment       = {14},
  doi           = {https://doi.org/10.1016/j.infsof.2010.11.002},
  keywords      = {Metamorphic testing, Test data generation, Mutation testing, Feature models, Automated analysis, Product lines},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584910001904},
}

@Article{Veerman2006,
  author        = {Niels Veerman},
  title         = {Automated mass maintenance of a software portfolio},
  journal       = {Science of Computer Programming},
  year          = {2006},
  volume        = {62},
  number        = {3},
  pages         = {287 - 317},
  issn          = {0167-6423},
  note          = {Special issue on Source code analysis and manipulation (SCAM 2005)},
  __markedentry = {[mac:]},
  abstract      = {This is an experience report on automated mass maintenance of a large Cobol software portfolio. A company in the financial services and insurance industry upgraded their database system to a new version, affecting their entire software portfolio. The database system was accessed by the portfolio of 45 systems, totalling nearly 3000 programs and covering over 4 million lines of Cobol code. We upgraded the programs to the new database version using several automatic tools, and we performed an automated analysis supporting further manual modifications by the system experts. The automatic tools were built using a combination of lexical and syntactic technology, and they were deployed in a mass update factory to allow large-scale application to the software portfolio. The updated portfolio has been accepted and taken into production by the company, serving over 600 employees with the new database version. In this paper, we discuss the automated upgrade from problem statement to project costs.},
  comment       = {31},
  doi           = {https://doi.org/10.1016/j.scico.2006.04.006},
  keywords      = {Software maintenance, Reengineering, Cobol, Software portfolio, Mass modification, Mass maintenance, Mass update, Automated maintenance, Tool-supported maintenance, Automatic transformations},
  url           = {http://www.sciencedirect.com/science/article/pii/S0167642306000827},
}

@Article{Ianzen2013,
  author        = {Andressa Ianzen and Everson Carlos Mauda and Marco AntÃ´nio Paludo and Sheila Reinehr and Andreia Malucelli},
  title         = {Software process improvement in a financial organization: An action research approach},
  journal       = {Computer Standards \& Interfaces},
  year          = {2013},
  volume        = {36},
  number        = {1},
  pages         = {54 - 65},
  issn          = {0920-5489},
  __markedentry = {[mac:]},
  abstract      = {In order to increase the quality of systems of a financial company, the process of a software development team has changed some times to get stabilized. This paper presents the action research steps that were conducted, the perceptions of the team about the process evolution and the solved problems. Also, a software process improvement assessment has been conducted in order to identify the success factors on this implementation and the result is analyzed and discussed through the Servqual method. Among other conclusions, the involvement of the team during the improvement process and future perspectives are crucial to achieve success.},
  comment       = {12},
  doi           = {https://doi.org/10.1016/j.csi.2013.07.002},
  keywords      = {Software process improvement, Systems development, Software engineering, Software process evaluation, Software quality improvement},
  url           = {http://www.sciencedirect.com/science/article/pii/S0920548913000676},
}

@Article{Bress2014,
  author        = {Sebastian BreÃŸ and Norbert Siegmund and Max Heimel and Michael Saecker and Tobias Lauer and Ladjel Bellatreche and Gunter Saake},
  title         = {Load-aware inter-co-processor parallelism in database query processing},
  journal       = {Data \& Knowledge Engineering},
  year          = {2014},
  volume        = {93},
  pages         = {60 - 79},
  issn          = {0169-023X},
  note          = {Selected Papers from the 17th East-Â¬-European Conference on Advances in Databases and Information Systems},
  __markedentry = {[mac:]},
  abstract      = {For a decade, the database community has been exploring graphics processing units and other co-processors to accelerate query processing. While the developed algorithms often outperform their CPU counterparts, it is not beneficial to keep processing devices idle while overutilizing others. Therefore, an approach is needed that efficiently distributes a workload on available (co-)processors while providing accurate performance estimates for the query optimizer. In this paper, we contribute heuristics that optimize query processing for response time and throughput simultaneously via inter-device parallelism. Our empirical evaluation reveals that the new approach achieves speedups up to 1.85 compared to state-of-the-art approaches while preserving accurate performance estimations. In a further series of experiments, we evaluate our approach on two new use cases: joining and sorting. Furthermore, we use a simulation to assess the performance of our approach for systems with multiple co-processors and derive some general rules that impact performance in those systems.},
  comment       = {20},
  doi           = {https://doi.org/10.1016/j.datak.2014.07.003},
  keywords      = {Co-processing, Query processing, Query optimization},
  url           = {http://www.sciencedirect.com/science/article/pii/S0169023X14000627},
}

@InCollection{Rico2008,
  author        = {David F. Rico and Hasan H. Sayani and Ralph F. Field},
  title         = {History of Computers, Electronic Commerce and Agile Methods},
  booktitle     = {Advances in COMPUTERS},
  publisher     = {Elsevier},
  year          = {2008},
  editor        = {Marvin V. Zelkowitz},
  volume        = {73},
  series        = {Advances in Computers},
  pages         = {1 - 55},
  __markedentry = {[mac:]},
  abstract      = {Abstract
The purpose of this chapter is to present a literature review relevant to a study of using agile methods to manage the development of Internet websites and their subsequent quality. This chapter places website quality within the context of the $2.4 trillion U.S. electronic commerce industry. Thus, this chapter provides a history of electronic computers, electronic commerce, software methods, software quality metrics, agile methods and studies on agile methods. None of these histories are without controversy. For instance, some scholars begin the study of the electronic computer by mentioning the emergence of the Sumerian text, Hammurabi code or the abacus. We, however, will align our history with the emergence of the modern electronic computer at the beginning of World War II. The history of electronic commerce also has poorly defined beginnings. Some studies of electronic commerce begin with the widespread use of the Internet in the early 1990s. However, electronic commerce cannot be appreciated without establishing a deeper context. Few scholarly studies, if any, have been performed on agile methods, which is the basic purpose of this literature review. That is, to establish the context to conduct scholarly research within the fields of agile methods and electronic commerce.},
  comment       = {55},
  doi           = {https://doi.org/10.1016/S0065-2458(08)00401-4},
  issn          = {0065-2458},
  url           = {http://www.sciencedirect.com/science/article/pii/S0065245808004014},
}

@Article{Zhang2010,
  author        = {Pengcheng Zhang and Henry Muccini and Bixin Li},
  title         = {A classification and comparison of model checking software architecture techniques},
  journal       = {Journal of Systems and Software},
  year          = {2010},
  volume        = {83},
  number        = {5},
  pages         = {723 - 744},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {Software architecture specifications are used for many different purposes, such as documenting architectural decisions, predicting architectural qualities before the system is implemented, and guiding the design and coding process. In these contexts, assessing the architectural model as early as possible becomes a relevant challenge. Various analysis techniques have been proposed for testing, model checking, and evaluating performance based on architectural models. Among them, model checking is an exhaustive and automatic verification technique, used to verify whether an architectural specification conforms to expected properties. While model checking is being extensively applied to software architectures, little work has been done to comprehensively enumerate and classify these different techniques. The goal of this paper is to investigate the state-of-the-art in model checking software architectures. For this purpose, we first define the main activities in a model checking software architecture process. Then, we define a classification and comparison framework and compare model checking software architecture techniques according to it.},
  comment       = {22},
  doi           = {https://doi.org/10.1016/j.jss.2009.11.709},
  keywords      = {Software architecture, Model checking},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121209003070},
}

@Article{Vilela2017,
  author        = {JÃ©ssyka Vilela and Jaelson Castro and Luiz Eduardo G. Martins and Tony Gorschek},
  title         = {Integration between requirements engineering and safety analysis: A systematic literature review},
  journal       = {Journal of Systems and Software},
  year          = {2017},
  volume        = {125},
  pages         = {68 - 92},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {Context: Safety-Critical Systems (SCS) require more sophisticated requirements engineering (RE) approaches as inadequate, incomplete or misunderstood requirements have been recognized as a major cause in many accidents and safety-related catastrophes. Objective: In order to cope with the complexity of specifying SCS by RE, we investigate the approaches proposed to improve the communication or integration between RE and safety engineering in SCS development. We analyze the activities that should be performed by RE during safety analysis, the hazard/safety techniques it could use, the relationships between safety information that it should specify, the tools to support safety analysis as well as integration benefits between these areas. Method: We use a Systematic Literature Review (SLR) as the basis for our work. Results: We developed four taxonomies to help RE during specification of SCS that classify: techniques used in (1) hazard analysis; (2) safety analysis; (3) safety-related information and (4) a detailed set of information regarding hazards specification. Conclusions: This paper is a step towards developing a body of knowledge in safety concerns necessary to RE in the specification of SCS that is derived from a large-scale SLR. We believe the results will benefit both researchers and practitioners.},
  comment       = {25},
  doi           = {https://doi.org/10.1016/j.jss.2016.11.031},
  keywords      = {Safety-critical systems, Requirements engineering, Safety analysis, Integration, Communication, Systematic literature review},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121216302333},
}

@Article{Fuentes-Fernandez2012,
  author        = {RubÃ©n Fuentes-FernÃ¡ndez and Juan PavÃ³n and Francisco Garijo},
  title         = {A model-driven process for the modernization of component-based systems},
  journal       = {Science of Computer Programming},
  year          = {2012},
  volume        = {77},
  number        = {3},
  pages         = {247 - 269},
  issn          = {0167-6423},
  note          = {Feature-Oriented Software Development (FOSD 2009)},
  __markedentry = {[mac:]},
  abstract      = {Software modernization is critical for organizations that need cost-effective solutions to deal with the rapid obsolescence of software and the increasing demand for new functionality. This paper presents the XIRUP modernization methodology, which proposes a highly iterative process, structured into four phases: preliminary evaluation, understanding, building and migration. This modernization process is feature-driven, component-based, focused on the early elicitation of key information, and relies on a model-driven approach with extensive use of experience from the previous projects. XIRUP has been defined in the European IST project MOMOCS, which has also built a suite of support tools. This paper introduces the process using a case study that illustrates its activities, related tools and results. The discussion highlights the specific characteristics of modernization projects and how a customized methodology can take advantage of them.},
  comment       = {23},
  doi           = {https://doi.org/10.1016/j.scico.2011.04.003},
  keywords      = {Modernization of software systems, Component, Software methodology, Software engineering, Agile process, Model-driven engineering},
  url           = {http://www.sciencedirect.com/science/article/pii/S0167642311001110},
}

@Article{Wang2007,
  author        = {Hai H. Wang and Yuan Fang Li and Jing Sun and Hongyu Zhang and Jeff Pan},
  title         = {Verifying feature models using OWL},
  journal       = {Journal of Web Semantics},
  year          = {2007},
  volume        = {5},
  number        = {2},
  pages         = {117 - 129},
  issn          = {1570-8268},
  note          = {Software Engineering and the Semantic Web},
  __markedentry = {[mac:]},
  abstract      = {Feature models are widely used in domain engineering to capture common and variant features among systems in a particular domain. However, the lack of a formal semantics and reasoning support of feature models has hindered the development of this area. Industrial experiences also show that methods and tools that can support feature model analysis are badly appreciated. Such reasoning tool should be fully automated and efficient. At the same time, the reasoning tool should scale up well since it may need to handle hundreds or even thousands of features a that modern software systems may have. This paper presents an approach to modeling and verifying feature diagrams using Semantic Web OWL ontologies. We use OWL DL ontologies to precisely capture the inter-relationships among the features in a feature diagram. OWL reasoning engines such as FaCT++ are deployed to check for the inconsistencies of feature configurations fully automatically. Furthermore, a general OWL debugger has been developed to tackle the disadvantage of lacking debugging aids for the current OWL reasoner and to complement our verification approach. We also developed a CASE tool to facilitate visual development, interchange and reasoning of feature diagrams in the Semantic Web environment.},
  comment       = {13},
  doi           = {https://doi.org/10.1016/j.websem.2006.11.006},
  keywords      = {Semantic Web, OWL, Ontologies, Feature modeling},
  url           = {http://www.sciencedirect.com/science/article/pii/S1570826807000042},
}

@Article{Demuth2016,
  author        = {Andreas Demuth and Markus Riedl-Ehrenleitner and Roberto E. Lopez-Herrejon and Alexander Egyed},
  title         = {Co-evolution of metamodels and models through consistent change propagation},
  journal       = {Journal of Systems and Software},
  year          = {2016},
  volume        = {111},
  pages         = {281 - 297},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {In model-driven engineering (MDE), metamodels and domain-specific languages are key artifacts as they are used to define syntax and static semantics of domain models. However, metamodels are evolving over time, requiring existing domain models to be co-evolved. Though approaches have been proposed for performing such co-evolution automatically, those approaches typically support only specific metamodel changes. In this paper, we present a vision of co-evolution between metamodels and models through consistent change propagation. The approach addresses co-evolution issues without being limited to specific metamodels or evolution scenarios. It relies on incremental management of metamodel-based constraints that are used to detect co-evolution failures (i.e., inconsistencies between metamodel and model). After failure detection, the approach automatically generates suggestions for correction (i.e., repairs for inconsistencies). A case study with the UML metamodel and 23 UML models shows that the approach is technically feasible and also scalable.},
  comment       = {17},
  doi           = {https://doi.org/10.1016/j.jss.2015.03.003},
  keywords      = {Metamodel co-evolution, Consistency checking, Consistent change propagation},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121215000564},
}

@Article{Gonzalez-Herrera2016,
  author        = {I. Gonzalez-Herrera and J. Bourcier and E. Daubert and W. Rudametkin and O. Barais and F. Fouquet and J.M. JÃ©zÃ©quel and B. Baudry},
  title         = {ScapeGoat: Spotting abnormal resource usage in component-based reconfigurable software systems},
  journal       = {Journal of Systems and Software},
  year          = {2016},
  volume        = {122},
  pages         = {398 - 415},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {Modern component frameworks support continuous deployment and simultaneous execution of multiple software components on top of the same virtual machine. However, isolation between the various components is limited. A faulty version of any one of the software components can compromise the whole system by consuming all available resources. In this paper, we address the problem of efficiently identifying faulty software components running simultaneously in a single virtual machine. Current solutions that perform permanent and extensive monitoring to detect anomalies induce high overhead on the system, and can, by themselves, make the system unstable. In this paper we present an optimistic adaptive monitoring system to determine the faulty components of an application. Suspected components are finely analyzed by the monitoring system, but only when required. Unsuspected components are left untouched and execute normally. Thus, we perform localized just-in-time monitoring that decreases the accumulated overhead of the monitoring system. We evaluate our approach on two case studies against a state-of-the-art monitoring system and show that our technique correctly detects faulty components, while reducing overhead by an average of 93%.},
  comment       = {18},
  doi           = {https://doi.org/10.1016/j.jss.2016.02.027},
  keywords      = {Resource monitoring, Component, Models@Run.Time},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121216000595},
}

@Article{Fabry2016,
  author        = {Johan Fabry and Coen De Roover and Carlos Noguera and Steffen Zschaler and Awais Rashid and Viviane Jonckers},
  title         = {AspectJ code analysis and verification with GASR},
  journal       = {Journal of Systems and Software},
  year          = {2016},
  volume        = {117},
  pages         = {528 - 544},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {Aspect-oriented programming languages extend existing languages with new features for supporting modularization of crosscutting concerns. These features however make existing source code analysis tools unable to reason over this code. Consequently, all code analysis efforts of aspect-oriented code that we are aware of have either built limited analysis tools or were performed manually. Given the significant complexity of building them or manual analysis, a lot of duplication of effort could have been avoided by using a general-purpose tool. To address this, in this paper we present Gasr: a source code analysis tool that reasons over AspectJ source code, which may contain metadata in the form of annotations. GASR provides multiple kinds of analyses that are general enough such that they are reusable, tailorable and can reason over annotations. We demonstrate the use of GASR in two ways: we first automate the recognition of previously identified aspectual source code assumptions. Second, we turn implicit assumptions into explicit assumptions through annotations and automate their verification. In both uses GASR performs detection and verification of aspect assumptions on two well-known case studies that were manually investigated in earlier work. GASR finds already known aspect assumptions and adds instances that had been previously overlooked.},
  comment       = {17},
  doi           = {https://doi.org/10.1016/j.jss.2016.04.014},
  keywords      = {Aspect oriented programming, Source code analysis, Logic program querying},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121216300279},
}

@Article{Rodriguez-Covili2013,
  author        = {Juan RodrÃ­guez-Covili and Sergio F. Ochoa},
  title         = {A lightweight and distributed middleware to provide presence awareness in mobile ubiquitous systems},
  journal       = {Science of Computer Programming},
  year          = {2013},
  volume        = {78},
  number        = {10},
  pages         = {2009 - 2025},
  issn          = {0167-6423},
  note          = {Special section on Language Descriptions Tools and Applications (LDTAâ€™08 \& â€™09) \& Special section on Software Engineering Aspects of Ubiquitous Computing and Ambient Intelligence (UCAmI 2011)},
  __markedentry = {[mac:]},
  abstract      = {Several researchers have identified the need to count on presence awareness in ubiquitous systems that support mobile activities, particularly when these systems are used to perform loosely-coupled mobile work. In such a work style, mobile users conduct face-to-face on-demand interactions, therefore counting on awareness information about the position and availability of potential collaborators becomes mandatory for these applications. Most proposed solutions that provide user presence awareness involve centralized components, have reusability limitations, or simply address a part of that service. This article presents a lightweight and fully distributed middleware named Moware, which allows developers to embed presence awareness services in mobile ubiquitous systems in a simple way. The article also describes the Moware architecture, its main components and strategies used to deal with several aspects of the presence awareness support. These design strategies can be reused by software designers to provide presence awareness capabilities into middleware and specific software applications. Moware services were embedded in a mobile ubiquitous system that supports inspectors during the construction inspection process. The preliminary results indicate that the middleware was easy to use for developers, and its services were useful for the end-users.},
  comment       = {17},
  doi           = {https://doi.org/10.1016/j.scico.2013.02.003},
  keywords      = {Presence awareness, Mobile ubiquitous computing, Middleware, Loosely-coupled mobile work},
  url           = {http://www.sciencedirect.com/science/article/pii/S0167642313000282},
}

@Article{Eklund2014,
  author        = {Ulrik Eklund and Jan Bosch},
  title         = {Architecture for embedded open software ecosystems},
  journal       = {Journal of Systems and Software},
  year          = {2014},
  volume        = {92},
  pages         = {128 - 142},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {Software is prevalent in embedded products and may be critical for the success of the products, but manufacturers may view software as a necessary evil rather than as a key strategic opportunity and business differentiator. One of the reasons for this can be extensive supplier and subcontractor relationships and the cost, effort or unpredictability of the deliverables from the subcontractors are experienced as a major problem. The paper proposes open software ecosystem as an alternative approach to develop software for embedded systems, and elaborates on the necessary quality attributes of an embedded platform underlying such an ecosystem. The paper then defines a reference architecture consisting of 17 key decisions together with four architectural patterns, and provides the rationale why they are essential for an open software ecosystem platform for embedded systems in general and automotive systems in particular. The reference architecture is validated through a prototypical platform implementation in an industrial setting, providing a deeper understanding of how the architecture could be realised in the automotive domain. Four potential existing platforms, all targeted at the embedded domain (Android, OKL4, AUTOSAR and Robocop), are evaluated against the identified quality attributes to see how they could serve as a basis for an open software ecosystem platform with the conclusion that while none of them is a perfect fit they all have fundamental mechanisms necessary for an open software ecosystem approach.},
  comment       = {15},
  doi           = {https://doi.org/10.1016/j.jss.2014.01.009},
  keywords      = {Software architecture, Embedded software, Software ecosystem},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121214000211},
}

@Article{Soumeya2014,
  author        = {Debboub Soumeya and Meslati Djamel},
  title         = {Study of advanced separation of concerns approaches using the GoF design patterns: A quantitative and qualitative comparison},
  journal       = {Information and Software Technology},
  year          = {2014},
  volume        = {56},
  number        = {10},
  pages         = {1345 - 1359},
  issn          = {0950-5849},
  __markedentry = {[mac:]},
  abstract      = {Context
Since the emergence of the aspect oriented paradigm, several studies have been conducted to test the contribution of this new paradigm compared to the object paradigm. However, in addition to this type of studies, we need also comparative studies that assess the aspect approaches mutually. The motivations of the latter include the enhancement of each aspect approach, devising hybrid approaches or merely helping developers choosing the suitable approach according to their needs. Comparing advanced separation of concerns approaches is the context of our work.
Objective
We aim at making an assessment of how the aspect approaches deal with crosscutting concerns. This assessment is based on quantitative attributes such as coupling and cohesion that evaluate the modularity as well as on qualitative observations.
Method
We selected three of well-known aspect approaches: AspectJ, JBoss AOP and CaesarJ, all the three based on Java. We conducted then, a comparative study using the GoF design patterns. In order to be fair we asked a group of Master students to achieve the implementation of all patterns with the three approaches. The use of these implementations as hypothetical benchmarks allowed us to achieve two kinds of comparison: a quantitative one based on structural and performance metrics, and qualitative one based on observations collected during the implementation phase.
Results
The quantitative comparison shows some advantages like the using of fewer components with AspectJ and the strong cohesion with CaesarJ and weaknesses, as the high internal coupling caused by the inner classes of CaesarJ. The qualitative comparison gives comments about the approach understandability and others qualitative concepts.
Conclusion
This comparison highlighted strengths and weaknesses of each approach, and provided a referential work that can help choosing the right approach during software development, enhancing aspect approaches or devising hybrid approaches that combine best features.},
  comment       = {15},
  doi           = {https://doi.org/10.1016/j.infsof.2014.04.015},
  keywords      = {Advanced separation of concerns, Aspect oriented programming, Empirical assessment, AspectJ, JBoss AOP, CaesarJ},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584914000962},
}

@Article{Schmoelzer2008,
  author        = {Gernot SchmÃ¶lzer and Egon Teiniker and Christian Kreiner},
  title         = {Model-typed component interfaces},
  journal       = {Journal of Systems Architecture},
  year          = {2008},
  volume        = {54},
  number        = {6},
  pages         = {551 - 561},
  issn          = {1383-7621},
  note          = {Selection of best papers from the 32nd EUROMICRO Conference on â€˜Software Engineering and Advanced Applicationsâ€™ (SEAA 2006)},
  __markedentry = {[mac:]},
  abstract      = {Component based software engineering (CBSE) allows to design and develop reusable software components that can be assembled to construct software systems via well defined interfaces. However, designing such reusable components for data intensive business logic often requires heavy data transfer between components over interfaces. Static interface definitions using basic data types or structures of such lead to large interfaces susceptible to modifications. The goal of this paper is to present model-typed interfaces based on generic interface parameters, which allows to transfer complex structured data between components. Providing such generic, model-defined types (MDT) with data models specifying the parameter structure supports compatibility checks of model-typed interfaces at platform independent system design time. The methodology is described platform independently and the coherency with our system development process is discussed. Moreover, a technology mapping to IDL and the CORBA component model (CCM) is illustrated.},
  comment       = {11},
  doi           = {https://doi.org/10.1016/j.sysarc.2008.01.006},
  keywords      = {Interfaces definition, Software engineering, Data modeling},
  url           = {http://www.sciencedirect.com/science/article/pii/S1383762108000246},
}

@Article{Lassing2002,
  author        = {Nico Lassing and PerOlof Bengtsson and Hans van Vliet and Jan Bosch},
  title         = {Experiences with ALMA: Architecture-Level Modifiability Analysis},
  journal       = {Journal of Systems and Software},
  year          = {2002},
  volume        = {61},
  number        = {1},
  pages         = {47 - 57},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {Modifiability is an important quality for software systems, because a large part of the costs associated with these systems is spent on modifications. The effort, and therefore cost, that is required for these modifications is largely determined by a system's software architecture. Analysis of software architectures is therefore an important technique to achieve modifiability and reduce maintenance costs. However, few techniques for software architecture analysis currently exist. Based on our experiences with software architecture analysis of modifiability, we have developed ALMA, an architecture-level modifiability analysis method consisting of five steps. In this paper we report on our experiences with ALMA. We illustrate our experiences with examples from two case studies of software architecture analysis of modifiability. These case studies concern a system for mobile positioning at Ericsson Software Technology AB and a system for freight handling at DFDS Fraktarna. Our experiences are related to each step of the analysis process. In addition, we made some observations on software architecture analysis of modifiability in general.},
  comment       = {11},
  doi           = {https://doi.org/10.1016/S0164-1212(01)00113-3},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121201001133},
}

@Article{Capilla2016,
  author        = {Rafael Capilla and Anton Jansen and Antony Tang and Paris Avgeriou and Muhammad Ali Babar},
  title         = {10 years of software architecture knowledge management: Practice and future},
  journal       = {Journal of Systems and Software},
  year          = {2016},
  volume        = {116},
  pages         = {191 - 205},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {The importance of architectural knowledge (AK) management for software development has been highlighted over the past ten years, where a significant amount of research has been done. Since the first systems using design rationale in the seventies and eighties to the more modern approaches using AK for designing software architectures, a variety of models, approaches, and research tools have leveraged the interests of researchers and practitioners in AK management (AKM). Capturing, sharing, and using AK has many benefits for software designers and maintainers, but the cost to capture this relevant knowledge hampers a widespread use by software companies. However, as the improvements made over the last decade didn't boost a wider adoption of AKM approaches, there is a need to identify the successes and shortcomings of current AK approaches and know what industry needs from AK. Therefore, as researchers and promoters of many of the AK research tools in the early stages where AK became relevant for the software architecture community, and based on our experience and observations, we provide in this research an informal retrospective analysis of what has been done and the challenges and trends for a future research agenda to promote AK use in modern software development practices.},
  comment       = {15},
  doi           = {https://doi.org/10.1016/j.jss.2015.08.054},
  keywords      = {Architectural knowledge management, Architectural design decisions, Agile development},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121215002034},
}

@Article{Siek2011,
  author        = {Jeremy G. Siek and Andrew Lumsdaine},
  title         = {A language for generic programming in the large},
  journal       = {Science of Computer Programming},
  year          = {2011},
  volume        = {76},
  number        = {5},
  pages         = {423 - 465},
  issn          = {0167-6423},
  note          = {Special Issue on Generative Programming and Component Engineering (Selected Papers from GPCE 2004/2005)},
  __markedentry = {[mac:]},
  abstract      = {Generic programming is an effective methodology for developing reusable software libraries. Many programming languages provide generics and have features for describing interfaces, but none completely support the idioms used in generic programming. To address this need we developed the language G. The central feature of G is the concept, a mechanism for organizing constraints on generics that is inspired by the needs of modern C++ libraries. G provides modular type checking and separate compilation (even of generics). These characteristics support modular software development, especially the smooth integration of independently developed components. In this article we present the rationale for the design of G and demonstrate the expressiveness of G with two case studies: porting the Standard Template Library and the Boost Graph Library from C++ to G. The design of G shares much in common with the concept extension proposed for the next C++ Standard (the authors participated in its design) but there are important differences described in this article.},
  comment       = {43},
  doi           = {https://doi.org/10.1016/j.scico.2008.09.009},
  keywords      = {Programming language design, Generic programming, Generics, Polymorphism, Concepts, Associated types, Software reuse, Type classes, Modules, Signatures, Functors, Virtual types},
  url           = {http://www.sciencedirect.com/science/article/pii/S0167642308001123},
}

@Article{Silva2017,
  author        = {Rodolfo Adamshuk Silva and Simone do Rocio Senger de Souza and Paulo SÃ©rgio Lopes de Souza},
  title         = {A systematic review on search based mutation testing},
  journal       = {Information and Software Technology},
  year          = {2017},
  volume        = {81},
  pages         = {19 - 35},
  issn          = {0950-5849},
  __markedentry = {[mac:]},
  abstract      = {Context
Search Based Software Testing refers to the use of meta-heuristics for the optimization of a task in the context of software testing. Meta-heuristics can solve complex problems in which an optimum solution must be found among a large amount of possibilities. The use of meta-heuristics in testing activities is promising because of the high number of inputs that should be tested. Previous studies on search based software testing have focused on the application of meta-heuristics for the optimization of structural and functional criteria. Recently, some researchers have proposed the use of SBST for mutation testing and explored solutions for the cost of application of this testing criterion.
Objective
The objective is to identify how SBST has been explored in the context of mutation testing, how fitness functions are defined and the challenges and research opportunities in the application of meta-heuristic search techniques.
Method
A systematic review involving 263 papers published between 1996 and 2014 examined the studies on the use of meta-heuristic search techniques for the optimization of mutation testing.
Results
The results show meta-heuristic search techniques have been applied for the optimization of test data generation, mutant generation and selection of effective mutation operators. Five meta-heuristic techniques, namely Genetic Algorithm, Ant Colony, Bacteriological Algorithm, Hill Climbing and Simulated Annealing have been used in search based mutation testing. The review addressed different fitness functions used to guide the search.
Conclusion
Search based mutation testing is a field of interest, however, some issues remain unexplored. For instance, the use of meta-heuristics for the selection of effective mutation operators was identified in only one study. The results have pointed a range of possibilities for new studies to be developed, i.e., identification of equivalent mutants, experimental studies and application to different domains, such as concurrent programs.},
  comment       = {17},
  doi           = {https://doi.org/10.1016/j.infsof.2016.01.017},
  keywords      = {Mutation testing, Search based software testing, Meta-heuristic},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584916300167},
}

@Article{Kuecuek2015,
  author        = {Dilek KÃ¼Ã§Ã¼k},
  title         = {A high-level electrical energy ontology with weighted attributes},
  journal       = {Advanced Engineering Informatics},
  year          = {2015},
  volume        = {29},
  number        = {3},
  pages         = {513 - 522},
  issn          = {1474-0346},
  __markedentry = {[mac:]},
  abstract      = {One of the significant application areas of domain ontologies is known to be text analysis applications like information extraction and text classification systems, and semantic portals. In this paper, we present a high-level ontology for the electrical energy domain. This domain ontology has weighted attributes to cover the inherent fuzziness in the textual representations of its concepts. Additionally, we have included in the ontology the necessary attributes to align the ontology concepts to on-line collaborative knowledge bases like Wikipedia and linked open data sources like DBpedia, other attributes to facilitate its use in multilingual applications, and concepts to hold the named entities in the domain. The ultimate ontology is aligned with the previously proposed ontologies for the energy-related subdomains after extending the latter ones with weighted attributes. We make the ultimate form of the electrical energy ontology, as well as the extended versions of the domain ontologies for the subdomains, available for research purposes. Also included in the paper are sample text analysis applications which mainly exploit the weighted attributes within the ontology.},
  comment       = {10},
  doi           = {https://doi.org/10.1016/j.aei.2015.04.002},
  keywords      = {Domain ontology, Electrical energy, Weighted attributes, Ontology learning, Wikipedia, Text analysis applications},
  url           = {http://www.sciencedirect.com/science/article/pii/S1474034615000385},
}

@Article{Verhoef2007,
  author        = {C. Verhoef},
  title         = {Quantifying the effects of IT-governance rules},
  journal       = {Science of Computer Programming},
  year          = {2007},
  volume        = {67},
  number        = {2},
  pages         = {247 - 277},
  issn          = {0167-6423},
  __markedentry = {[mac:]},
  abstract      = {Via quantitative analyses of large IT-portfolio databases, we detected unique data patterns pointing to certain IT-governance rules and styles, plus their sometimes nonintuitive and negative side-effects. We grouped the most important patterns in seven categories and highlighted them separately. These patterns relate to the five fundamental parameters for IT-governance: data, control, time, cost and functionality. We revealed patterns of overperfect and heterogeneous data signifying reporting anomalies or ambiguous IT-governance rules, respectively. We also detected patterns of overregulation and underregulation, portending bloated control or no IT-control at all, both with negative side-effects: productivity loss, and too costly IT-development. Uniform management on time, cost or functionality showed clear patterns in the time and cost case, and more diffuse combined patterns for functionality. For these in total seven types of patterns, it was possible to take corrective measures to reduce unwanted side-effects, and/or amplify the intended purpose of the underlying IT-governance rules. These modifications ranged from refinements and additions, to eradications of IT-governance rules. For each of the seven patterns we provided lessons learned and recommendations on how to recognize and remove unwanted effects. Some effects were dangerous, and addressing them led to significant risk reduction and cost savings.},
  comment       = {31},
  doi           = {https://doi.org/10.1016/j.scico.2007.01.010},
  keywords      = {IT-governance, IT-governance rules, IT-portfolio analysis, Quantitative IT-governance, Overperfect data, Heterogeneous data, Overregulation, Underregulation, Managing on time, Managing on budget, Managing on functionality, Time compression, Time decompression, Seasonality effects},
  url           = {http://www.sciencedirect.com/science/article/pii/S0167642307000780},
}

@Article{Walraven2014,
  author        = {Stefan Walraven and Dimitri Van Landuyt and Eddy Truyen and Koen Handekyn and Wouter Joosen},
  title         = {Efficient customization of multi-tenant Software-as-a-Service applications with service lines},
  journal       = {Journal of Systems and Software},
  year          = {2014},
  volume        = {91},
  pages         = {48 - 62},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {Application-level multi-tenancy is an architectural approach for Software-as-a-Service (SaaS) applications which enables high operational cost efficiency by sharing one application instance among multiple customer organizations (the so-called tenants). However, the focus on increased resource sharing typically results in a one-size-fits-all approach. In principle, the shared application instance satisfies only the requirements common to all tenants, without supporting potentially different and varying requirements of these tenants. As a consequence, multi-tenant SaaS applications are inherently limited in terms of flexibility and variability. This paper presents an integrated service engineering method, called service line engineering, that supports co-existing tenant-specific configurations and that facilitates the development and management of customizable, multi-tenant SaaS applications, without compromising scalability. Specifically, the method spans the design, implementation, configuration, composition, operations and maintenance of a SaaS application that bundles all variations that are based on a common core. We validate this work by illustrating the benefits of our method in the development of a real-world SaaS offering for document processing. We explicitly show that the effort to configure and compose an application variant for each individual tenant is significantly reduced, though at the expense of a higher initial development effort.},
  comment       = {15},
  doi           = {https://doi.org/10.1016/j.jss.2014.01.021},
  keywords      = {Multi-tenancy, SaaS, Variability},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121214000326},
}

@Article{Magdalenic2013,
  author        = {Ivan MagdaleniÄ‡ and Danijel RadoÅ¡eviÄ‡ and Tihomir OrehovaÄki},
  title         = {Autogenerator: Generation and execution of programming code on demand},
  journal       = {Expert Systems with Applications},
  year          = {2013},
  volume        = {40},
  number        = {8},
  pages         = {2845 - 2857},
  issn          = {0957-4174},
  __markedentry = {[mac:]},
  abstract      = {While generating program files that can be executed afterwards is widely established in Automatic programming, the generation of programming code and its execution on demand without creating program files is still a challenge. In the approach presented in this paper a generator entitled Autogenerator uses the ability of scripting languages to evaluate programming code from a variable. The main benefits of this approach lie in facilitating the application change during its execution on the one hand and in dependencies update on the other. Autogenerator can be useful in the development of a common Generative programming application for previewing the application before the generation of the final release. With the aim of examining specific facets of the autogeneration process, we also conducted performance tests. Finally, the presented model of Autogenerator is verified through the development of an application for the creation and handling of Universal Business Language documents.},
  comment       = {13},
  doi           = {https://doi.org/10.1016/j.eswa.2012.12.003},
  keywords      = {Autogenerator, Dynamic frames, Generation on demand},
  url           = {http://www.sciencedirect.com/science/article/pii/S0957417412012444},
}

@Article{Romero2017,
  author        = {L.F. Romero and A. Arce},
  title         = {Applying Value Stream Mapping in Manufacturing: A Systematic Literature Review},
  journal       = {IFAC-PapersOnLine},
  year          = {2017},
  volume        = {50},
  number        = {1},
  pages         = {1075 - 1086},
  issn          = {2405-8963},
  note          = {20th IFAC World Congress},
  __markedentry = {[mac:]},
  abstract      = {Value Stream Mapping is a critical tool when it comes to implement the lean approach and it has spanned to many sectors in industry. Although previous studies justify its use in manufacturing sector by identifying previous cases within the literature, none to the best of our knowledge has used our approach to explore the aspects covered it this review, yet the potential exists. Based on a systematic approach, we analyzed available literature published in refereed journals, providing academics and researchers with valuable findings related to the evolution, application and performance of the Value Stream Mapping in context of the manufacturing sector.},
  comment       = {12},
  doi           = {https://doi.org/10.1016/j.ifacol.2017.08.385},
  keywords      = {Value stream mapping, logistics in manufacturing, business process management systems, lean manufacturing, methodologies, tools for analysis of productive systems},
  url           = {http://www.sciencedirect.com/science/article/pii/S2405896317307292},
}

@Article{Zielinski2017,
  author        = {Cezary ZieliÅ„ski and Maciej StefaÅ„czyk and Tomasz Kornuta and Maksym Figat and Wojciech Dudek and Wojciech Szynkiewicz and WÅ‚odzimierz Kasprzak and Jan Figat and Marcin Szlenk and Tomasz Winiarski and Konrad Banachowicz and Teresa ZieliÅ„ska and Emmanouil G. Tsardoulias and Andreas L. Symeonidis and Fotis E. Psomopoulos and Athanassios M. Kintsakis and Pericles A. Mitkas and Aristeidis Thallas and Sofia E. Reppou and George T. Karagiannis and Konstantinos Panayiotou and Vincent Prunet and Manuel Serrano and Jean-Pierre Merlet and Stratos Arampatzis and Alexandros Giokas and Lazaros Penteridis and Ilias Trochidis and David Daney and Miren Iturburu},
  title         = {Variable structure robot control systems: The RAPP approach},
  journal       = {Robotics and Autonomous Systems},
  year          = {2017},
  volume        = {94},
  pages         = {226 - 244},
  issn          = {0921-8890},
  __markedentry = {[mac:]},
  abstract      = {This paper presents aÂ method of designing variable structure control systems for robots. As the on-board robot computational resources are limited, but in some cases the demands imposed on the robot by the user are virtually limitless, the solution is to produce aÂ variable structure system. The task dependent part has to be exchanged, however the task governs the activities of the robot. Thus not only exchange of some task-dependent modules is required, but also supervisory responsibilities have to be switched. Such control systems are necessary in the case of robot companions, where the owner of the robot may demand from it to provide many services.},
  comment       = {19},
  doi           = {https://doi.org/10.1016/j.robot.2017.05.002},
  keywords      = {Robot controllers, Variable structure controllers, Cloud robotics, RAPP},
  url           = {http://www.sciencedirect.com/science/article/pii/S0921889016306248},
}

@Article{Quintero2007,
  author        = {A.M. Reina Quintero and J. Torres Valderrama},
  title         = {Using Aspect-orientation Techniques to Improve Reuse of Metamodels},
  journal       = {Electronic Notes in Theoretical Computer Science},
  year          = {2007},
  volume        = {163},
  number        = {2},
  pages         = {29 - 43},
  issn          = {1571-0661},
  note          = {Proceedings of the Second International Workshop on Aspect-Based and Model-Based Separation of Concerns in Software Systems (ABMB 2006)},
  __markedentry = {[mac:]},
  abstract      = {Metamodelling is an activity that attracts attention of the research community dealing with the Model-Driven Development (MDD). To be reusable in different MDD approaches a metamodel should be unaware of being extended by another metamodel. This property of metamodel is called obliviousness. This paper shows that current techniques implementing metamodels do not maintain obliviousness when some elements of the extended metamodel and the elements of the original model have association relations. Three different approaches to reuse of metamodels are analyzed. One of the approaches uses traditional object-oriented techniques. Two other approaches use aspect-oriented techniques. The paper shows that the third approach, which considers relationships as first-class citizens at the implementation level by using relationship aspects, guarantees obliviousness of metamodels.},
  comment       = {15},
  doi           = {https://doi.org/10.1016/j.entcs.2006.10.014},
  keywords      = {Metamodelling, Aspect-Oriented Programming, Model-Driven Architecture},
  url           = {http://www.sciencedirect.com/science/article/pii/S1571066107001466},
}

@Article{Mohan2007a,
  author        = {Kannan Mohan and Radhika Jain and Balasubramaniam Ramesh},
  title         = {Knowledge networking to support medical new product development},
  journal       = {Decision Support Systems},
  year          = {2007},
  volume        = {43},
  number        = {4},
  pages         = {1255 - 1273},
  issn          = {0167-9236},
  note          = {Special Issue Clusters},
  __markedentry = {[mac:]},
  abstract      = {New product development (NPD) in the pharmaceutical industry is very knowledge intensive. Knowledge generated and used during medical NPD processes is fragmented and distributed across various phases and artifacts. Many challenges in medical NPD can be addressed by the integration of this fragmented knowledge. We propose the creation and use of knowledge networks to address these challenges. Based on a case study conducted in a leading pharmaceutical company, we have developed a knowledge framework that represents knowledge fragments that need to be integrated to support medical NPD. We have also developed a prototype system that supports knowledge integration using knowledge networks. We illustrate the capabilities of the system through scenarios drawn from the case study. Qualitative validation of our approach is also presented.},
  comment       = {19},
  doi           = {https://doi.org/10.1016/j.dss.2006.02.005},
  keywords      = {Knowledge integration, Knowledge networks, New product development, Pharmaceutical knowledge management, Healthcare},
  url           = {http://www.sciencedirect.com/science/article/pii/S0167923606000273},
}

@Article{Kuz2007,
  author        = {Ihor Kuz and Yan Liu and Ian Gorton and Gernot Heiser},
  title         = {CAmkES: A component model for secure microkernel-based embedded systems},
  journal       = {Journal of Systems and Software},
  year          = {2007},
  volume        = {80},
  number        = {5},
  pages         = {687 - 699},
  issn          = {0164-1212},
  note          = {Component-Based Software Engineering of Trustworthy Embedded Systems},
  __markedentry = {[mac:]},
  abstract      = {Component-based software engineering promises to provide structure and reusability to embedded-systems software. At the same time, microkernel-based operating systems are being used to increase the reliability and trustworthiness of embedded systems. Since the microkernel approach to designing systems is partially based on the componentisation of system services, component-based software engineering is a particularly attractive approach to developing microkernel-based systems. While a number of widely used component architectures already exist, they are generally targeted at enterprise computing rather than embedded systems. Due to the unique characteristics of embedded systems, a component architecture for embedded systems must have low overhead, be able to address relevant non-functional issues, and be flexible to accommodate application specific requirements. In this paper we introduce a component architecture aimed at the development of microkernel-based embedded systems. The key characteristics of the architecture are that it has a minimal, low-overhead, core but is highly modular and therefore flexible and extensible. We have implemented a prototype of this architecture and confirm that it has very low overhead and is suitable for implementing both system-level and application level services.},
  comment       = {13},
  doi           = {https://doi.org/10.1016/j.jss.2006.08.039},
  keywords      = {Component architecture, Microkernel, Embedded system},
  url           = {http://www.sciencedirect.com/science/article/pii/S016412120600224X},
}

@Article{Kilamo2012,
  author        = {Terhi Kilamo and Imed Hammouda and Tommi Mikkonen and Timo Aaltonen},
  title         = {From proprietary to open sourceâ€”Growing an open source ecosystem},
  journal       = {Journal of Systems and Software},
  year          = {2012},
  volume        = {85},
  number        = {7},
  pages         = {1467 - 1478},
  issn          = {0164-1212},
  note          = {Software Ecosystems},
  __markedentry = {[mac:]},
  abstract      = {In today's business and software arena, Free/Libre/Open Source Software has emerged as a promising platform for software ecosystems. Following this trend, more and more companies are releasing their proprietary software as open source, forming a software ecosystem of related development projects complemented with a social ecosystem of community members. Since the trend is relatively recent, there are few guidelines on how to create and maintain a sustainable open source ecosystem for a proprietary software. This paper studies the problem of building open source communities for industrial software that was originally developed as closed source. Supporting processes, guidelines and best practices are discussed and illustrated through an industrial case study. The research is paving the road for new directions in growing a thriving open source ecosystem.},
  comment       = {12},
  doi           = {https://doi.org/10.1016/j.jss.2011.06.071},
  keywords      = {Open source, Software ecosystem, Opening proprietary software, Open source engineering},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121211001683},
}

@Article{Vierhauser2016,
  author        = {Michael Vierhauser and Rick Rabiser and Paul GrÃ¼nbacher and Klaus Seyerlehner and Stefan Wallner and Helmut Zeisel},
  title         = {ReMinds : A flexible runtime monitoring framework for systems of systems},
  journal       = {Journal of Systems and Software},
  year          = {2016},
  volume        = {112},
  pages         = {123 - 136},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {Many software-intensive systems today can be characterized as systems of systems (SoS) comprising complex, interrelated, and heterogeneous systems. The behavior of SoS often only emerges at runtime due to complex interactions between the involved systems and their environment. It is thus necessary to determine unexpected behavior by monitoring SoS at runtime, i.e., collecting and analyzing events and data at different layers and levels of granularity. Existing monitoring approaches are often limited to individual systems, particular architectural styles, or technologies. In this paper we thus derive challenges for monitoring SoS based on an industrial case. We present a flexible framework adaptable to different system architectures and technologies. We discuss its capabilities for instrumenting systems, collecting and persisting events and data, checking constraints on events and data, and visualizing the systemsâ€™ behavior to users. We demonstrate the frameworkâ€™s flexibility by tailoring and applying it to an industrial SoS and assessing its performance and scalability. Our results show that the framework is flexible and scalable for monitoring an industrial SoS with realistic event loads.},
  comment       = {14},
  doi           = {https://doi.org/10.1016/j.jss.2015.07.008},
  keywords      = {System-of-systems architectures, Runtime monitoring, Framework},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121215001478},
}

@Article{Coelho2011a,
  author        = {Roberta Coelho and Arndt von Staa and UirÃ¡ Kulesza and Awais Rashid and Carlos Lucena},
  title         = {Unveiling and taming liabilities of aspects in the presence of exceptions: A static analysis based approach},
  journal       = {Information Sciences},
  year          = {2011},
  volume        = {181},
  number        = {13},
  pages         = {2700 - 2720},
  issn          = {0020-0255},
  note          = {Including Special Section on Databases and Software Engineering},
  __markedentry = {[mac:]},
  abstract      = {As aspects extend or replace existing functionality at specific join points in the code, their behavior may raise new exceptions, which can flow through the program execution in unexpected ways. Assuring the reliability of exception handling code in aspect-oriented (AO) systems is a challenging task. Testing the exception handling code is inherently difficult, since it is tricky to provoke all exceptions during tests, and the large number of different exceptions that can happen in a system may lead to the test-case explosion problem. Moreover, we have observed that some properties of AO programming (e.g., quantification, obliviousness) may conflict with characteristics of exception handling mechanisms, exacerbating existing problems (e.g., uncaught exceptions). The lack of verification approaches for exception handling code in AO systems stimulated the present work. This work presents a verification approach based on a static analysis tool, called SAFE, to check the reliability of exception handling code in AspectJ programs. We evaluated the effectiveness and feasibility of our approach in two complementary ways (i) by investigating if the SAFE tool is precise enough to uncover exception flow information and (ii) by applying the approach to three medium-sized ApectJ systems from different application domains.},
  comment       = {21},
  doi           = {https://doi.org/10.1016/j.ins.2010.06.002},
  keywords      = {Exception handling, Aspect-oriented programming, Static analysis, Exception flow analysis, Exception handling rules conformance},
  url           = {http://www.sciencedirect.com/science/article/pii/S0020025510002525},
}

@Article{Hoefner2016,
  author        = {Peter HÃ¶fner and Bernhard MÃ¶ller},
  title         = {Extended Feature Algebra},
  journal       = {Journal of Logical and Algebraic Methods in Programming},
  year          = {2016},
  volume        = {85},
  number        = {5, Part 2},
  pages         = {952 - 971},
  issn          = {2352-2208},
  note          = {Articles dedicated to Prof. J. N. Oliveira on the occasion of his 60th birthday},
  __markedentry = {[mac:]},
  abstract      = {Feature Algebra was introduced as an abstract framework for feature-oriented software development. One goal is to provide a common, clearly defined basis for the key ideas of feature-orientation. The algebra captures major aspects of feature-orientation, such as the hierarchical structure of features and feature composition. However, as we will show, it is not able to model aspects at the level of code, i.e., situations where code fragments of different features have to be merged. In other words, it does not reflect details of concrete implementations. In this paper we first present concrete models for the original axioms of Feature Algebra which represent the main concepts of feature-oriented programs. This shows that the abstract Feature Algebra can be interpreted in different ways. We then use these models to show that the axioms of Feature Algebra do not properly reflect all aspects of feature-orientation from the level of directory structures down to the level of actual code. This gives motivation to extend the abstract algebra, which is the second main contribution of the paper. We modify the axioms and introduce the concept of an Extended Feature Algebra. As third contribution, we introduce more operators to cover concepts like overriding in the abstract setting.},
  comment       = {20},
  doi           = {https://doi.org/10.1016/j.jlamp.2015.12.002},
  keywords      = {Feature-orientation, Feature Algebra, Algebraic characterisation of FOSD},
  url           = {http://www.sciencedirect.com/science/article/pii/S2352220815001480},
}

@Article{Varela-Vaca2013,
  author        = {Angel Jesus Varela-Vaca and Rafael M. Gasca},
  title         = {Towards the automatic and optimal selection of risk treatments for business processes using a constraint programming approach},
  journal       = {Information and Software Technology},
  year          = {2013},
  volume        = {55},
  number        = {11},
  pages         = {1948 - 1973},
  issn          = {0950-5849},
  __markedentry = {[mac:]},
  abstract      = {Context
The use of Business Process Management Systems (BPMS) has emerged in the IT arena for the automation of business processes. In the majority of cases, the issue of security is overlooked by default in these systems, and hence the potential cost and consequences of the materialization of threats could produce catastrophic loss for organizations. Therefore, the early selection of security controls that mitigate risks is a real and important necessity. Nevertheless, there exists an enormous range of IT security controls and their configuration is a human, manual, time-consuming and error-prone task. Furthermore, configurations are carried out separately from the organization perspective and involve many security stakeholders. This separation makes difficult to ensure the effectiveness of the configuration with regard to organizational requirements.
Objective
In this paper, we strive to provide security stakeholders with automated tools for the optimal selection of IT security configurations in accordance with a range of business process scenarios and organizational multi-criteria.
Method
An approach based on feature model analysis and constraint programming techniques is presented, which enable the automated analysis and selection of optimal security configurations.
Results
A catalogue of feature models is determined by analyzing typical IT security controls for BPMSs for the enforcement of the standard goals of security: integrity, confidentiality, availability, authorization, and authentication. These feature models have been implemented through constraint programs, and Constraint Programming techniques based on optimized and non-optimized searches are used to automate the selection and generation of configurations. In order to compare the results of the determination of configuration a comparative analysis is given.
Conclusion
In this paper, we present innovative tools based on feature models, Constraint Programming and multi-objective techniques that enable the agile, adaptable and automatic selection and generation of security configurations in accordance with the needs of the organization.},
  comment       = {26},
  doi           = {https://doi.org/10.1016/j.infsof.2013.05.007},
  keywords      = {Business process, Business process management systems, Security, Risk treatments, Constraint programming, Feature model},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584913001286},
}

@Article{Jarke2011,
  author        = {Matthias Jarke and Pericles Loucopoulos and Kalle Lyytinen and John Mylopoulos and William Robinson},
  title         = {The brave new world of design requirements},
  journal       = {Information Systems},
  year          = {2011},
  volume        = {36},
  number        = {7},
  pages         = {992 - 1008},
  issn          = {0306-4379},
  note          = {Special Issue: Advanced Information Systems Engineering (CAiSE'10)},
  __markedentry = {[mac:]},
  abstract      = {Despite its success over the last 30 years, the field of Requirements Engineering (RE) is still experiencing fundamental problems that indicate a need for a change of focus to better ground its research on issues underpinning current practices. We posit that these practices have changed significantly in recent years. To this end we explore changes in software system operational environments, targets, and the process of RE. Our explorations include a field study, as well as two workshops that brought together experts from academia and industry. We recognize that these changes influence the nature of central RE research questions. We identify four new principles that underlie contemporary requirements processes, namely: (1) intertwining of requirements with implementation and organizational contexts, (2) dynamic evolution of requirements, (3) emergence of architectures as a critical stabilizing force, and (4) need to recognize unprecedented levels of design complexity. We recommend a re-focus of RE research based on a review and analysis of these four principles, and identify several theoretical and practical implications that flow from this analysis.},
  comment       = {17},
  doi           = {https://doi.org/10.1016/j.is.2011.04.003},
  keywords      = {Requirements, Requirements principles, Requirements engineering, Architectures, Complexity, Evolution, Future of requirements engineering},
  url           = {http://www.sciencedirect.com/science/article/pii/S0306437911000548},
}

@Article{Tahir2013,
  author        = {Abbas Tahir and Davide Tosi and Sandro Morasca},
  title         = {A systematic review on the functional testing of semantic web services},
  journal       = {Journal of Systems and Software},
  year          = {2013},
  volume        = {86},
  number        = {11},
  pages         = {2877 - 2889},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {Semantic web services are gaining more attention as an important element of the emerging semantic web. Therefore, testing semantic web services is becoming a key concern as an essential quality assurance measure. The objective of this systematic literature review is to summarize the current state of the art of functional testing of semantic web services by providing answers to a set of research questions. The review follows a predefined procedure that involves automatically searching 5 well-known digital libraries. After applying the selection criteria to the results, a total of 34 studies were identified as relevant. Required information was extracted from the studies and summarized. Our systematic literature review identified some approaches available for deriving test cases from the specifications of semantic web services. However, many of the approaches are either not validated or the validation done lacks credibility. We believe that a substantial amount of work remains to be done to improve the current state of research in the area of testing semantic web services.},
  comment       = {13},
  doi           = {https://doi.org/10.1016/j.jss.2013.06.064},
  keywords      = {Functional testing, Semantic web services, Testing approach, Systematic literature review},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121213001659},
}

@Article{Viana2013,
  author        = {Matheus C. Viana and RosÃ¢ngela A.D. Penteado and AntÃ´nio F. do Prado},
  title         = {Domain-Specific Modeling Languages to improve framework instantiation},
  journal       = {Journal of Systems and Software},
  year          = {2013},
  volume        = {86},
  number        = {12},
  pages         = {3123 - 3139},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {Frameworks are reusable software composed of concrete and abstract classes that implement the functionality of a domain. Applications reuse frameworks to enhance quality and development efficiency. However, frameworks are hard to learn and reuse. Application developers must understand the complex class hierarchy of the framework to instantiate it properly. In this paper, we present an approach to build a Domain-Specific Modeling Language (DSML) of a framework and use it to facilitate framework reuse during application development. The DSML of a framework is built by identifying the features of this framework and the information required to instantiate them. Application generators transform models created with the DSML into application code, hiding framework complexities. In this paper, we illustrate the use of our approach in a framework for the domain of business resource transactions and a experiment that evaluated the efficiency obtained with our approach.},
  comment       = {17},
  doi           = {https://doi.org/10.1016/j.jss.2013.07.030},
  keywords      = {Framework, Domain-Specific Modeling Language, Reuse},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121213001775},
}

@Article{Schaetz2007,
  author        = {Bernhard SchÃ¤tz},
  title         = {Combining Product Lines and Model-Based Development},
  journal       = {Electronic Notes in Theoretical Computer Science},
  year          = {2007},
  volume        = {182},
  pages         = {171 - 186},
  issn          = {1571-0661},
  note          = {Proceedings of the Third International Workshop on Formal Aspects of Component Software (FACS 2006)},
  __markedentry = {[mac:]},
  abstract      = {Using model-based development has shown to increase efficiency and effectiveness of software production. However, with software as an integral part of products with customized functionalities, explicit treatment of product lines is increasingly becoming necessary to cope with this additional complexity. To combine these aspects, which are generally considered only in isolation, a conceptual model addressing both the aspects of product-line engineering as well as aspects of component systems is introduced, and the consequences concerning product line identification and instantiation are illustrated.},
  comment       = {16},
  doi           = {https://doi.org/10.1016/j.entcs.2006.09.038},
  keywords      = {Product line, variability, model-based, conceptual model, meta model, consistency},
  url           = {http://www.sciencedirect.com/science/article/pii/S1571066107003933},
}

@Article{Chabridon2013,
  author        = {Sophie Chabridon and Denis Conan and Zied Abid and Chantal Taconet},
  title         = {Building ubiquitous QoC-aware applications through model-driven software engineering},
  journal       = {Science of Computer Programming},
  year          = {2013},
  volume        = {78},
  number        = {10},
  pages         = {1912 - 1929},
  issn          = {0167-6423},
  note          = {Special section on Language Descriptions Tools and Applications (LDTAâ€™08 \& â€™09) \& Special section on Software Engineering Aspects of Ubiquitous Computing and Ambient Intelligence (UCAmI 2011)},
  __markedentry = {[mac:]},
  abstract      = {As every-day mobile devices can easily be equipped with multiple sensing capabilities, ubiquitous applications are expected to exploit the richness of the context information that can be collected by these devices in order to provide the service that is the most appropriate to the situation of the user. However, the design and implementation of such context-aware ubiquitous appplications remain challenging as there exist very few models and tools to guide application designers and developers in mastering the complexity of context information. This becomes even more crucial as context is by nature imperfect. One way to address this issue is to associate to context information meta-data representing its quality. We propose a generic and extensible design process for context-aware applications taking into account the quality of context (QoC). We demonstrate its use on a prototype application for sending flash sale offers to mobile users. We present extensive performance results in terms of memory and processing time of both elementary context management operations and the whole context policy implementing the Flash sale application. The cost of adding QoC management is also measured and appears to be limited to a few milliseconds. We show that a context policy with 120 QoC-aware nodes can be processed in less than 100 ms on a mobile phone. Moreover, a policy of almost 3000 nodes can be instantiated before exhausting the resources of the phone. This enables very rich application scenarios enhancing the user experience and will favor the development of new ubiquitous applications.},
  comment       = {18},
  doi           = {https://doi.org/10.1016/j.scico.2012.07.019},
  keywords      = {Model-driven software engineering, Context, Quality of context, Domain specific language, Ubiquitous computing, Pervasive computing},
  url           = {http://www.sciencedirect.com/science/article/pii/S0167642312001475},
}

@Article{Zhao2008,
  author        = {Liping Zhao and Linda Macaulay and Jonathan Adams and Paul Verschueren},
  title         = {A pattern language for designing e-business architecture},
  journal       = {Journal of Systems and Software},
  year          = {2008},
  volume        = {81},
  number        = {8},
  pages         = {1272 - 1287},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {The pattern language for e-business provides a holistic support for developing software architectures for the e-business domain. The pattern language contains four related pattern categories: Business Patterns, Integration Patterns, Application Patterns, and Runtime Patterns. These pattern categories organise an e-business architecture into three layersâ€”business interaction, application infrastructure and middleware infrastructureâ€”and provide reusable design solutions to these layers in a topâ€“down decomposition fashion. Business and Integration Patterns partition the business interaction layer into a set of subsystems; Application Patterns provide a high-level application infrastructure for these subsystems and separate business abstractions from their software solutions; Runtime Patterns then define a middleware infrastructure for the subsystems and shield design solutions from their implementations. The paper describes, demonstrates and evaluates this pattern language.},
  comment       = {16},
  doi           = {https://doi.org/10.1016/j.jss.2007.11.717},
  keywords      = {e-Business architecture, Architectural design, Pattern, Pattern languages, Software architecture},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121207003123},
}

@Article{Abate2012,
  author        = {Pietro Abate and Roberto Di Cosmo and Ralf Treinen and Stefano Zacchiroli},
  title         = {Dependency solving: A separate concern in component evolution management},
  journal       = {Journal of Systems and Software},
  year          = {2012},
  volume        = {85},
  number        = {10},
  pages         = {2228 - 2240},
  issn          = {0164-1212},
  note          = {Automated Software Evolution},
  __markedentry = {[mac:]},
  abstract      = {Maintenance of component-based software platforms often has to face rapid evolution of software components. Component dependencies, conflicts, and package managers with dependency solving capabilities are the key ingredients of prevalent software maintenance technologies that have been proposed to keep software installations synchronized with evolving component repositories. We review state-of-the-art package managers and their ability to keep up with evolution at the current growth rate of popular component-based platforms, and conclude that their dependency solving abilities are not up to the task. We show that the complexity of the underlying upgrade planning problem is NP-complete even for seemingly simple component models, and argue that the principal source of complexity lies in multiple available versions of components. We then discuss the need of expressive languages for user preferences, which makes the problem even more challenging. We propose to establish dependency solving as a separate concern from other upgrade aspects, and present CUDF as a formalism to describe upgrade scenarios. By analyzing the result of an international dependency solving competition, we provide evidence that the proposed approach is viable.},
  comment       = {13},
  doi           = {https://doi.org/10.1016/j.jss.2012.02.018},
  keywords      = {Component, Dependency solving, Software evolution, Package management, Open source, Competition},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121212000477},
}

@Article{Boerger2012,
  author        = {Egon BÃ¶rger and Antonio Cisternino and Vincenzo Gervasi},
  title         = {Ambient Abstract State Machines with applications},
  journal       = {Journal of Computer and System Sciences},
  year          = {2012},
  volume        = {78},
  number        = {3},
  pages         = {939 - 959},
  issn          = {0022-0000},
  note          = {In Commemoration of Amir Pnueli},
  __markedentry = {[mac:]},
  abstract      = {We define a flexible abstract ambient concept which turned out to support current programming practice, in fact can be instantiated to apparently any environment paradigm in use in frameworks for distributed computing with heterogeneous components. For the sake of generality and to also support rigorous high-level system design practice we give the definition in terms of Abstract State Machines. We show the definition to uniformly capture the common static and dynamic disciplines for isolating states or concurrent behavior (e.g. handling of multiple threads for Java) as well as for sharing memory, patterns of object-oriented programming (e.g. for delegation, incremental refinement, encapsulation, views) and agent mobility.},
  comment       = {21},
  doi           = {https://doi.org/10.1016/j.jcss.2011.08.004},
  keywords      = {Ambient concept, Abstract State Machines, Naming disciplines, Memory sharing disciplines, Object-oriented design patterns, Mobile agents},
  url           = {http://www.sciencedirect.com/science/article/pii/S0022000011000833},
}

@Article{Zhang2010a,
  author        = {Weishan Zhang and Klaus Marius Hansen and Thomas Kunz},
  title         = {Enhancing intelligence and dependability of a product line enabled pervasive middleware},
  journal       = {Pervasive and Mobile Computing},
  year          = {2010},
  volume        = {6},
  number        = {2},
  pages         = {198 - 217},
  issn          = {1574-1192},
  note          = {Context Modelling, Reasoning and Management},
  __markedentry = {[mac:]},
  abstract      = {To provide good support for user-centered application scenarios in pervasive computing environments, pervasive middleware must react to context changes and prepare services accordingly. At the same time, pervasive middleware should provide extended dependability via self-management capabilities, to conduct self-diagnosis of possible malfunctions using the current runtime context, and self-configuration and self-adaptation when there are service mismatches. In this article, we present an approach to combine the power of BDI practical reasoning and OWL/SWRL ontologies theoretical reasoning in order to improve the intelligence of pervasive middleware, supported by a set of Self-Management Pervasive Service (SeMaPS) ontologies featuring dynamic context, complex context, and self-management rules modeling. In this approach, belief sets are enriched with the results of OWL/SWRL theoretical reasoning to derive beliefs that cannot be obtained directly or explicitly. This is demonstrated with agents negotiating sports appointments. To cope with self-management, the corresponding monitoring, configuration, adaptation and diagnosis rules are developed based on OWL and SWRL utilizing SeMaPS ontologies. Evaluations show this combined reasoning approach can perform well, and that Semantic Web-based self-management is promising for pervasive computing environments.},
  comment       = {20},
  doi           = {https://doi.org/10.1016/j.pmcj.2009.07.002},
  keywords      = {BDI (Belief-Desire-Intention) agents, OWL (Web Ontology Language), Self-diagnosis, SWRL (Semantic Web Rule Language), XVCL (XML-based Variant Configuration Language), Self-management, Middleware},
  url           = {http://www.sciencedirect.com/science/article/pii/S1574119209000637},
}

@Article{Navas2013,
  author        = {Juan F. Navas and Jean-Philippe Babau and Jacques Pulou},
  title         = {Reconciling run-time evolution and resource-constrained embedded systems through a component-based development framework},
  journal       = {Science of Computer Programming},
  year          = {2013},
  volume        = {78},
  number        = {8},
  pages         = {1073 - 1098},
  issn          = {0167-6423},
  note          = {Special section on software evolution, adaptability, and maintenance \& Special section on the Brazilian Symposium on Programming Languages},
  __markedentry = {[mac:]},
  abstract      = {This paper deals with the evolution of embedded systems software at run-time. To accomplish such software evolution activities in resource-constrained embedded systems, we propose a component-based, execution time evolution infrastructure, that reconciles richness of evolution alternatives and performance requirements. The proposition is based on fine-grained optimization of embedded components, and on off-site component reifications called mirrors, which are representations of components that allow us to treat evolution concerns remotely and hence to reduce the memory footprint. An evaluation on a real-world evolution scenario shows the efficiency and relevance of our approach.},
  comment       = {26},
  doi           = {https://doi.org/10.1016/j.scico.2012.08.004},
  keywords      = {Embedded software, Components, Optimization, Evolution, Reifications, Architecture},
  url           = {http://www.sciencedirect.com/science/article/pii/S0167642312001633},
}

@Article{Autili2013,
  author        = {Marco Autili and Paolo Di Benedetto and Paola Inverardi},
  title         = {A hybrid approach for resource-based comparison of adaptable Java applications},
  journal       = {Science of Computer Programming},
  year          = {2013},
  volume        = {78},
  number        = {8},
  pages         = {987 - 1009},
  issn          = {0167-6423},
  note          = {Special section on software evolution, adaptability, and maintenance \& Special section on the Brazilian Symposium on Programming Languages},
  __markedentry = {[mac:]},
  abstract      = {During the last decade, context-awareness and adaptation have been receiving significant attention in many research areas. For application developers, the heterogeneity of resource-constrained mobile terminals creates serious problems for the development of mobile applications able to run properly on a large number of different devices. Thus, resource awareness plays a crucial role when developing such applications. It identifies the capability of being aware of the resources offered by an execution environment, in order to decide whether that environment is suited to receive and execute the application. Within this line of research, we propose Chameleon, a framework that provides both an integrated development environment and a proper context-aware support to adaptable Java applications for limited devices. In this paper we present the novel hybrid (from static to dynamic) analysis approach that Chameleon uses for inspecting (adaptable) Java programs with respect to their resource consumption in a given execution environment. This analysis permits to quantitatively compare alternative versions of the same program. The analysis is based on a resource model for specifying resource provisions and consumptions, and a parametric transition system that performs the actual analysis.},
  comment       = {23},
  doi           = {https://doi.org/10.1016/j.scico.2012.01.005},
  keywords      = {Adaptable applications, Analysis, Tool support},
  url           = {http://www.sciencedirect.com/science/article/pii/S0167642312000147},
}

@Article{Gurov2013,
  author        = {Dilian Gurov and Marieke Huisman},
  title         = {Reducing behavioural to structural properties of programs with procedures},
  journal       = {Theoretical Computer Science},
  year          = {2013},
  volume        = {480},
  pages         = {69 - 103},
  issn          = {0304-3975},
  __markedentry = {[mac:]},
  abstract      = {There is an intimate link between program structure and behaviour. Exploiting this link to phrase program correctness problems in terms of the structural properties of a program graph rather than in terms of its unfoldings is a useful strategy for making analyses more tractable. The present paper presents a characterisation of behavioural program properties through sets of structural properties by means of a translation. The characterisation is given in the context of a program model based on control flow graphs of sequential programs with procedures, abstracting away completely from program data, and properties expressed in a fragment of the modal Î¼-calculus with boxes and greatest fixed-points only. The property translation is based on a tableau construction that conceptually amounts to symbolic execution of the behavioural formula, collecting structural constraints along the way. By keeping track of the subformulae that have been examined, recursion in the structural constraints can be identified and captured by fixed-point formulae. The tableau construction terminates, and the characterisation is exact, i.e., the translation is sound and complete. A prototype implementation has been developed. In addition, we show how the translation can be extended beyond the basic flow graph model and safety logic to richer behavioural models (such as open programs) and richer program models (including Boolean programs), and discuss possible extensions for more complex logics. We present several applications of the characterisation, in particular sound and complete compositional verification for behavioural properties based on maximal models.},
  comment       = {35},
  doi           = {https://doi.org/10.1016/j.tcs.2013.02.006},
  keywords      = {Compositional reasoning, Control-flow behaviour, Control-flow structure, Modal -calculus, Program verification, Safety properties},
  url           = {http://www.sciencedirect.com/science/article/pii/S0304397513001151},
}

@Article{Beg2015,
  author        = {Azam Beg and Falah Awwad and Walid Ibrahim and Faheem Ahmed},
  title         = {On the reliability estimation of nano-circuits using neural networks},
  journal       = {Microprocessors and Microsystems},
  year          = {2015},
  volume        = {39},
  number        = {8},
  pages         = {674 - 685},
  issn          = {0141-9331},
  __markedentry = {[mac:]},
  abstract      = {As the integrated circuit geometries shrink, it becomes important for the designers to take into consideration the reliability of the circuits. Different techniques can be used for reliability calculation or estimation. Some of these techniques are accurate but time-consuming while others are quick but not accurate. For example, using a set of mathematical equations for reliability estimation is very fast but not precise enough for large systems. Alternatively, Monte Carlo simulations are highly accurate, but very time-intensive. This work presents three different neural network models for estimating circuit reliability. The models provide better prediction accuracies than the mathematical technique. A reasonably large number of combinational circuits were simulated over a wide range of device reliabilities to collect the training data for the models. Multiple slices of an ISCAS-85 benchmark circuit were used to validate the modelsâ€™ prediction results.},
  comment       = {12},
  doi           = {https://doi.org/10.1016/j.micpro.2015.09.008},
  keywords      = {Reliability, Modeling, Neural network, Digital circuit, Nano-electronics},
  url           = {http://www.sciencedirect.com/science/article/pii/S0141933115001507},
}

@Article{Cuesta2005,
  author        = {Carlos E. Cuesta and Pablo de la Fuente and Manuel Barrio-SolÃ³rzano and M. EncarnaciÃ³n Beato},
  title         = {An â€œabstract processâ€ approach to algebraic dynamic architecture description},
  journal       = {The Journal of Logic and Algebraic Programming},
  year          = {2005},
  volume        = {63},
  number        = {2},
  pages         = {177 - 214},
  issn          = {1567-8326},
  note          = {Special Issue on Process Algebra and System Architecture},
  __markedentry = {[mac:]},
  abstract      = {Current software development methodologies recognize the critical importance of the architectural concerns during the design phase. Software Architecture promises to be the solution for a number of recurring problems; but to do so, the first task is to be able to obtain a precise description of a system architecture. In late years, a number of specific architecture description languages (Adls) have been proposed in order to achieve the required precision. Most of them have solid formal foundations; among them, several process-algebraic Adls stand out for their popularity and expressive power. The algebraic approach to architecture description is probably the most successful in the field. There is a natural intuition relating the concepts of algebraic process and architectural component; anyway, none of the existing approaches seems to have found the right balance between them. This article explains what is the problem with them, and defines the informal concept of abstract process, trying to provide a reference for the right level of abstraction. After presenting the concept, the article presents a dynamic, reflective Adl named PiLar, which has been designed using this notion. The syntax and semantics of this language are briefly summarized and explained. Finally, the classic example of the Gas Station is described in terms of PiLar, and then compared to previous presentations in other Adls.},
  comment       = {38},
  doi           = {https://doi.org/10.1016/j.jlap.2004.05.003},
  keywords      = {Architecture description language, Process algebra, Dynamic software architecture, Abstract process, Reflection, -Calculus},
  url           = {http://www.sciencedirect.com/science/article/pii/S1567832604000360},
}

@Article{Horcas2016b,
  author        = {Jose-Miguel Horcas and MÃ³nica Pinto and Lidia Fuentes and Wissam Mallouli and Edgardo Montes de Oca},
  title         = {An approach for deploying and monitoring dynamic security policies},
  journal       = {Computers \& Security},
  year          = {2016},
  volume        = {58},
  pages         = {20 - 38},
  issn          = {0167-4048},
  __markedentry = {[mac:]},
  abstract      = {Security policies are enforced through the deployment of certain security functionalities within the applications. When the security policies dynamically change, the associated security functionalities currently deployed within the applications must be adapted at runtime in order to enforce the new security policies. INTER-TRUST is a framework for the specification, negotiation, deployment and dynamic adaptation of interoperable security policies, in the context of pervasive systems where devices are constantly exchanging critical information through the network. The dynamic adaptation of the security policies at runtime is addressed using Aspect-Oriented Programming (AOP) that allows enforcing security requirements by dynamically weaving security aspects into the applications. However, a mechanism to guarantee the correct adaptation of the functionality that enforces the changing security policies is needed. In this paper, we present an approach based on the combination of monitoring and detection techniques in order to maintain the correlation between the security policies and the associated functionality deployed using AOP, allowing the INTER-TRUST framework to automatically react when needed.},
  comment       = {19},
  doi           = {https://doi.org/10.1016/j.cose.2015.11.007},
  keywords      = {Aspect-oriented programming, Dynamic deployment, Monitoring, Security framework, Security policies},
  url           = {http://www.sciencedirect.com/science/article/pii/S0167404815001832},
}

@Article{Peng2012,
  author        = {Xin Peng and Bihuan Chen and Yijun Yu and Wenyun Zhao},
  title         = {Self-tuning of software systems through dynamic quality tradeoff and value-based feedback control loop},
  journal       = {Journal of Systems and Software},
  year          = {2012},
  volume        = {85},
  number        = {12},
  pages         = {2707 - 2719},
  issn          = {0164-1212},
  note          = {Self-Adaptive Systems},
  __markedentry = {[mac:]},
  abstract      = {Quality requirements of a software system cannot be optimally met, especially when it is running in an uncertain and changing environment. In principle, a controller at runtime can monitor the change impact on quality requirements of the system, update the expectations and priorities from the environment, and take reasonable actions to improve the overall satisfaction. In practice, however, existing controllers are mostly designed for tuning low-level performance indicators instead of high-level requirements. By maintaining a live goal model to represent runtime requirements and linking the overall satisfaction of quality requirements to an indicator of earned business value, we propose a control-theoretic self-tuning method that can dynamically tune the preferences of different quality requirements, and can autonomously make tradeoff decisions through our Preference-Based Goal Reasoning procedure. The reasoning procedure results in an optimal configuration of the variation points by selecting the right alternative of OR-decomposed goals and such a configuration is mapped onto corresponding system architecture reconfigurations. The effectiveness of our self-tuning method is evaluated by earned business value, comparing our results with those obtained using static and ad hoc methods.},
  comment       = {13},
  doi           = {https://doi.org/10.1016/j.jss.2012.04.079},
  keywords      = {Feedback control theory, Preference, Goal-oriented reasoning, Self-tuning, Earned business value},
  url           = {http://www.sciencedirect.com/science/article/pii/S016412121200132X},
}

@Article{Magill2016,
  author        = {Evan Magill and Jesse Blum},
  title         = {Exploring conflicts in rule-based sensor networks},
  journal       = {Pervasive and Mobile Computing},
  year          = {2016},
  volume        = {27},
  pages         = {133 - 154},
  issn          = {1574-1192},
  __markedentry = {[mac:]},
  abstract      = {This paper addresses rule conflicts within wireless sensor networks. The work is situated within psychiatric ambulatory assessment settings where patients are monitored in and around their homes. Detecting behaviours within these settings favours sensor networks, while scalability and resource concerns favour processing data on smart nodes incorporating rule engines. Such monitoring involves personalisation, thereby becoming important to program node rules on the fly. Since rules may originate from distinct sources and change over time, methods are required to maintain rule consistency. Drawing on lessons from Feature Interaction, the paper contributes novel approaches for detecting and resolving rule-conflict across sensor networks.},
  comment       = {22},
  doi           = {https://doi.org/10.1016/j.pmcj.2015.08.005},
  keywords      = {Rule-based systems, Run-time programming, Feature interactions, Rule conflict},
  url           = {http://www.sciencedirect.com/science/article/pii/S1574119215001650},
}

@Article{Fronza2013,
  author        = {Ilenia Fronza and Alberto Sillitti and Giancarlo Succi and Mikko Terho and Jelena Vlasenko},
  title         = {Failure prediction based on log files using Random Indexing and Support Vector Machines},
  journal       = {Journal of Systems and Software},
  year          = {2013},
  volume        = {86},
  number        = {1},
  pages         = {2 - 11},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {Research problem
The impact of failures on software systems can be substantial since the recovery process can require unexpected amounts of time and resources. Accurate failure predictions can help in mitigating the impact of failures. Resources, applications, and services can be scheduled to limit the impact of failures. However, providing accurate predictions sufficiently ahead is challenging. Log files contain messages that represent a change of system state. A sequence or a pattern of messages may be used to predict failures.
Contribution
We describe an approach to predict failures based on log files using Random Indexing (RI) and Support Vector Machines (SVMs).
Method
RI is applied to represent sequences: each operation is characterized in terms of its context. SVMs associate sequences to a class of failures or non-failures. Weighted SVMs are applied to deal with imbalanced datasets and to improve the true positive rate. We apply our approach to log files collected during approximately three months of work in a large European manufacturing company.
Results
According to our results, weighted SVMs sacrifice some specificity to improve sensitivity. Specificity remains higher than 0.80 in four out of six analyzed applications.
Conclusions
Overall, our approach is very reliable in predicting both failures and non-failures.},
  comment       = {10},
  doi           = {https://doi.org/10.1016/j.jss.2012.06.025},
  keywords      = {Failure prediction, Random Indexing, Support Vector Machine (SVM), Event sequence data, Log files},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121212001732},
}

@Article{Pillat2015,
  author        = {Raquel M. Pillat and Toacy C. Oliveira and Paulo S.C. Alencar and Donald D. Cowan},
  title         = {BPMNt: A BPMN extension for specifying software process tailoring},
  journal       = {Information and Software Technology},
  year          = {2015},
  volume        = {57},
  pages         = {95 - 115},
  issn          = {0950-5849},
  __markedentry = {[mac:]},
  abstract      = {Context
Although SPEM 2.0 has great potential for software process modeling, it does not provide concepts or formalisms for precise modeling of process behavior. Indeed, SPEM fails to address process simulation, execution, monitoring and analysis, which are important activities in process management. On the other hand, BPMN 2.0 is a widely used notation to model business processes that has associated tools and techniques to facilitate the aforementioned process management activities. Using BPMN to model software development processes can leverage BPMNâ€™s infrastructure to improve the quality of these processes. However, BPMN lacks an important feature to model software processes: a mechanism to represent process tailoring.
Objective
This paper proposes BPMNt, a conservative extension to BPMN that aims at creating a tailoring representation mechanism similar to the one found in SPEM 2.0.
Method
We have used the BPMN 2.0 extensibility mechanism to include the representation of specific tailoring relationships namely suppression, local contribution, and local replacement, which establish links between process elements (such as in the case of SPEM). Moreover, this paper also presents some rules to ensure the consistency of BPMN models when using tailoring relationships.
Results
In order to evaluate our proposal we have implemented a tool to support the BPMNt approach and have applied it for representing real process adaptations in the context of an academic management system development project. Results of this study showed that the approach and its support tool can successfully be used to adapt BPMN-based software processes in real scenarios.
Conclusion
We have proposed an approach to enable reuse and adaptation of BPMN-based software process models as well as derivation traceability between models through tailoring relationships. We believe that bringing such capabilities into BPMN will open new perspectives to software process management.},
  comment       = {20},
  doi           = {https://doi.org/10.1016/j.infsof.2014.09.004},
  keywords      = {Process modeling, Software process tailoring, BPMN},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584914002031},
}

@Article{Her2007,
  author        = {Jin Sun Her and Ji Hyeok Kim and Sang Hun Oh and Sung Yul Rhew and Soo Dong Kim},
  title         = {A framework for evaluating reusability of core asset in product line engineering},
  journal       = {Information and Software Technology},
  year          = {2007},
  volume        = {49},
  number        = {7},
  pages         = {740 - 760},
  issn          = {0950-5849},
  __markedentry = {[mac:]},
  abstract      = {Product line engineering (PLE) is a new effective approach to software reuse, where applications are generated by instantiating a core asset which is a large-grained reuse unit. Hence, a core asset is a key element of PLE, and therefore the reusability of the core asset largely determines the success of PLE projects. However, current quality models to evaluate reusability do not adequately address the unique characteristics of core assets in PLE. This paper proposes a comprehensive framework for evaluating the reusability of core assets. We first identify the key characteristics of core assets, and derive a set of quality attributes that characterizes the reusability of core assets. Then, we define metrics for each quality attribute and finally present practical guidelines for applying the evaluation framework in PLE projects. Using the proposed framework, the reusability of core assets can be more effectively and precisely evaluated.},
  comment       = {21},
  doi           = {https://doi.org/10.1016/j.infsof.2006.08.008},
  keywords      = {Reusability, Product line engineering, Quality model, Metric, Core asset},
  url           = {http://www.sciencedirect.com/science/article/pii/S095058490600111X},
}

@Article{Zhou2017,
  author        = {Feng Zhou and Jianxin Roger Jiao and Xi Jessie Yang and Baiying Lei},
  title         = {Augmenting feature model through customer preference mining by hybrid sentiment analysis},
  journal       = {Expert Systems with Applications},
  year          = {2017},
  volume        = {89},
  pages         = {306 - 317},
  issn          = {0957-4174},
  __markedentry = {[mac:]},
  abstract      = {A feature model is an essential tool to identify variability and commonality within a product line of an enterprise, assisting stakeholders to configure product lines and to discover opportunities for reuse. However, the number of product variants needed to satisfy individual customer needs is still an open question, as feature models do not incorporate any direct customer preference information. In this paper, we propose to incorporate customer preference information into feature models using sentiment analysis of user-generated online product reviews. The proposed sentiment analysis method is a hybrid combination of affective lexicons and a rough-set technique. It is able to predict sentence sentiments for individual product features with acceptable accuracy, and thus augment a feature model by integrating positive and negative opinions of the customers. Such opinionated customer preference information is regarded as one attribute of the features, which helps to decide the number of variants needed within a product line. Finally, we demonstrate the feasibility and potential of the proposed method via an application case of Kindle Fire HD tablets.},
  comment       = {12},
  doi           = {https://doi.org/10.1016/j.eswa.2017.07.021},
  keywords      = {Feature model, Customer preference mining, Sentiment analysis, Product line planning},
  url           = {http://www.sciencedirect.com/science/article/pii/S0957417417304980},
}

@Article{Han2015,
  author        = {Sangyup Han and Myungchul Kim and Ben Lee and Sungwon Kang},
  title         = {Fast Directional Handoff and lightweight retransmission protocol for enhancing multimedia quality in indoor WLANs},
  journal       = {Computer Networks},
  year          = {2015},
  volume        = {79},
  pages         = {133 - 147},
  issn          = {1389-1286},
  __markedentry = {[mac:]},
  abstract      = {More and more mobile devices such as smartphones are being used with IEEE 802.11 wireless LANs (WLANs or Wi-Fi). However, mobile users are still experiencing poor service quality on the move due to the large handoff delay and packet loss problem. In order to reduce the delay, a new handoff scheme using the geomagnetic sensor embedded in mobile devices is proposed in this paper. The proposed scheme predicts the movement direction of a Mobile Station (MS) from the currently associated Access Point (AP) and performs active scanning with a reduced number of channels. In terms of the packet loss, a lightweight retransmission protocol is also proposed to minimize lost packets on Wi-Fi without producing a lot of acknowledgement packets. The proposed approaches are implemented on Android smartphones, and their performance is evaluated in a real indoor WLAN environment. The evaluation results demonstrate that the proposed schemes maintain seamless quality for real-time video even in an environment with frequent handoffs. Note that the proposed schemes are a client-only solution and do not require modification of the existing APs, which renders them very practical.},
  comment       = {15},
  doi           = {https://doi.org/10.1016/j.comnet.2014.12.019},
  keywords      = {IEEE 802.11, Fast Directional Handoff, Geomagnetic sensor, Digital compass, Lightweight retransmission protocol},
  url           = {http://www.sciencedirect.com/science/article/pii/S1389128615000031},
}

@Article{Mernik2013,
  author        = {Marjan Mernik},
  title         = {An object-oriented approach to language compositions for software language engineering},
  journal       = {Journal of Systems and Software},
  year          = {2013},
  volume        = {86},
  number        = {9},
  pages         = {2451 - 2464},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {In this paper, it is shown that inheritance, a core concept from object-oriented programming, is a possible solution for realizing composition of computer languages. Language composability is a property of language descriptions, which can be further classified into informal (language syntax and semantics are hard-coded in compiler/interpreter) and formal language descriptions (syntax and semantics are formally specified with one of several formal methods for language definition). However, language composition is much easier to achieve with declarative formal language descriptions into which the notion of inheritance is introduced. Multiple attribute grammar inheritance, as implemented in the language implementation system LISA, can assist in realizing all of the different types of language compositions identified in Erdweg et al. (2012). Different examples are given throughout the paper using an easy to understand domain-specific language that describes simple robot movement.},
  comment       = {14},
  doi           = {https://doi.org/10.1016/j.jss.2013.04.087},
  keywords      = {Software language engineering, Language composition, Domain-specific languages},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121213001271},
}

@Article{Schwaegerl2015,
  author        = {Felix SchwÃ¤gerl and Sabrina Uhrig and Bernhard Westfechtel},
  title         = {A graph-based algorithm for three-way merging of ordered collections in EMF models},
  journal       = {Science of Computer Programming},
  year          = {2015},
  volume        = {113},
  pages         = {51 - 81},
  issn          = {0167-6423},
  note          = {Model Driven Development (Selected \& extended papers from MODELSWARD 2014)},
  __markedentry = {[mac:]},
  abstract      = {In EMF models, ordered collections appear as the values of multi-valued structural features. Traditional, text-based version control systems do not sufficiently support three-way merging of ordered collections inside EMF models since they cannot guarantee a consistent result. The operation three-way merging is defined as follows: based on a common base version b, two alternative versions a1 and a2 were developed by copying and modifying the base version. To reconcile these changes, a merged version m is to be created as a common successor of a1 and a2. In this paper, we present a graph algorithm to solve the problem of three-way merging of ordered collections in EMF models. Each version of a collection can be represented by means of a linearly ordered graph. To create the merged version, these graphs are combined to a merged collection graph using set formula. To create the merged collection, a generalized topological sort is performed on the merged collection graph. Conflicts occur in case the order of elements cannot be deduced automatically; these conflicts are resolved either interactively or by default rules. We have implemented the merge algorithm in our tool BTMerge, which performs a consistency-preserving three-way merge of versions of EMF models being instances of arbitrary Ecore models. Our implementation relies on an alternative form of representing multiple versions of a collection, namely a versioned collection graph which forms a superimposition of collection versions. The algorithm presented here is purely state-based. Matching and merging of collections are clearly separated sub-problems. Insertions and deletions performed on the elements of the collection are propagated into the merged version in a consistent way. Our algorithm makes only minimal assumptions with regard to the underlying product model and thus may be applied to ordered collections inside plain text or XML files. By taking arbitrary move operations into account, the algorithm considerably goes beyond the functionality of contemporary merge tools which cannot adequately handle move operations.},
  comment       = {31},
  doi           = {https://doi.org/10.1016/j.scico.2015.02.008},
  keywords      = {Model-driven software engineering, EMF models, Version control, Three-way merging, Graph algorithm},
  url           = {http://www.sciencedirect.com/science/article/pii/S0167642315000532},
}

@Article{Albert2015,
  author        = {Elvira Albert and JesÃºs Correas and GermÃ¡n Puebla and Guillermo RomÃ¡n-DÃ­ez},
  title         = {A multi-domain incremental analysis engine and its application to incremental resource analysis},
  journal       = {Theoretical Computer Science},
  year          = {2015},
  volume        = {585},
  pages         = {91 - 114},
  issn          = {0304-3975},
  note          = {Developments in Implicit Complexity},
  __markedentry = {[mac:]},
  abstract      = {The aim of incremental analysis is, given a program, its analysis results, and a series of changes to the program, to obtain the new analysis results as efficiently as possible and, ideally, without having to (re-)analyze fragments of code which are not affected by the changes. Incremental analysis can significantly reduce both the time and the memory requirements of analysis. The first contribution of this article is a multi-domain incremental fixed-point algorithm for a sequential Java-like language. The algorithm is multi-domain in the sense that it interleaves the (re-)analysis for multiple domains by taking into account dependencies among them. Importantly, this allows the incremental analyzer to invalidate only those analysis results previously inferred by certain dependent domains. The second contribution is an incremental resource usage analysis which, in its first phase, uses the multi-domain incremental fixed-point algorithm to carry out all global pre-analyses required to infer cost in an interleaved way. Such resource analysis is parametric on the cost metrics one wants to measure (e.g., number of executed instructions, number of objects created, etc.). Besides, we present a novel form of cost summaries which allows us to incrementally reconstruct only those components of cost functions affected by the changes. Experimental results in the costa system show that the proposed incremental analysis provides significant performance gains, ranging from a speedup of 1.48 up to 5.13 times faster than non-incremental analysis.},
  comment       = {24},
  doi           = {https://doi.org/10.1016/j.tcs.2015.03.002},
  keywords      = {Static analysis, Resource usage analysis, Cost analysis, Incremental analysis},
  url           = {http://www.sciencedirect.com/science/article/pii/S0304397515001954},
}

@Article{Koeksal2017,
  author        = {Ã–mer KÃ¶ksal and Bedir Tekinerdogan},
  title         = {Obstacles in Data Distribution Service Middleware: A Systematic Review},
  journal       = {Future Generation Computer Systems},
  year          = {2017},
  volume        = {68},
  pages         = {191 - 210},
  issn          = {0167-739X},
  __markedentry = {[mac:]},
  abstract      = {Context: Data Distribution Service (DDS) is a standard data-centric publishâ€“subscribe programming model and specification for distributed systems. DDS has been applied for the development of high performance distributed systems such as in the defense, finance, automotive, and simulation domains. Various papers have been written on the application of DDS, however, there has been no attempt to systematically review and categorize the identified obstacles. Objective: The overall objective of this paper is to identify the state of the art of DDS, and describe the main lessons learned and obstacles in applying DDS. In addition, we aim to identify the important open research issues. Method: A systematic literature review (SLR) is conducted by a multiphase study selection process using the published literature since the introduction of DDS in 2003. Results: We reviewed 468 papers that are discovered using a well-planned review protocol, and 34 of them were assessed as primary studies related to our research questions. Conclusions: We have identified 11 basic categories for describing the identified obstacles and the corresponding research challenges that can be used to depict the state-of-the-art in DDS and provide a vision for further research.},
  comment       = {20},
  doi           = {https://doi.org/10.1016/j.future.2016.09.020},
  keywords      = {Data Distribution Service (DDS), Middleware, Systematic literature review},
  url           = {http://www.sciencedirect.com/science/article/pii/S0167739X1630351X},
}

@Article{Karam2008,
  author        = {Marcel Karam and Sergiu Dascalu and Haidar Safa and Rami Santina and Zeina Koteich},
  title         = {A product-line architecture for web service-based visual composition of web applications},
  journal       = {Journal of Systems and Software},
  year          = {2008},
  volume        = {81},
  number        = {6},
  pages         = {855 - 867},
  issn          = {0164-1212},
  note          = {Agile Product Line Engineering},
  __markedentry = {[mac:]},
  abstract      = {A web service-based web application (WSbWA) is a collection of web services or reusable proven software parts that can be discovered and invoked using standard Internet protocols. The use of these web services in the development process of WSbWAs can help overcome many problems of software use, deployment and evolution. Although the cost-effective software engineering of WSbWAs is potentially a very rewarding area, not much work has been done to accomplish short time to market conditions by viewing and dealing with WSbWAs as software products that can be derived from a common infrastructure and assets with a captured specific abstraction in the domain. Both Product Line Engineering (PLE) and Agile Methods (AMs), albeit with different philosophies, are software engineering approaches that can significantly shorten the time to market and increase the quality of products. Using the PLE approach we built, at the domain engineering level, a WSbWA-specific lightweight product-line architecture and combined it, at the application engineering level, with an Agile Method that uses a domain-specific visual language with direct manipulation and extraction capabilities of web services to perform customization and calibration of a product or WSBWA for a specific customer. To assess the effectiveness of our approach we designed and implemented a tool that we used to investigate the return on investment of the activities related to PLE and AMs. Details of our proposed approach, the related tool developed, and the experimental study performed are presented in this article together with a discussion of planned directions of future work.},
  comment       = {13},
  doi           = {https://doi.org/10.1016/j.jss.2007.10.031},
  keywords      = {Product line engineering, Product line architecture, Agile methods, Web services, Visual languages},
  url           = {http://www.sciencedirect.com/science/article/pii/S016412120700252X},
}

@Article{Salvaneschi2012,
  author        = {Guido Salvaneschi and Carlo Ghezzi and Matteo Pradella},
  title         = {Context-oriented programming: A software engineering perspective},
  journal       = {Journal of Systems and Software},
  year          = {2012},
  volume        = {85},
  number        = {8},
  pages         = {1801 - 1817},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {The implementation of context-aware systems can be supported through the adoption of techniques at the architectural level such as middlewares or component-oriented architectures. It can also be supported by suitable constructs at the programming language level. Context-oriented programming (COP) is emerging as a novel paradigm for the implementation of this kind of software, in particular in the field of mobile and ubiquitous computing. The COP paradigm tackles the issue of developing context-aware systems at the language-level, introducing ad hoc language abstractions to manage adaptations modularization and their dynamic activation. In this paper we review the state of the art in the field of COP in the perspective of the benefits that this technique can provide to software engineers in the design and implementation of context-aware applications.},
  comment       = {17},
  doi           = {https://doi.org/10.1016/j.jss.2012.03.024},
  keywords      = {Context-oriented programming, Context, Context-awareness},
  url           = {http://www.sciencedirect.com/science/article/pii/S016412121200074X},
}

@Article{Huang2012,
  author        = {Ge Huang and Albert Y. Zomaya and FlÃ¡via C. Delicato and Paulo F. Pires},
  title         = {An accurate on-demand time synchronization protocol for wireless sensor networks},
  journal       = {Journal of Parallel and Distributed Computing},
  year          = {2012},
  volume        = {72},
  number        = {10},
  pages         = {1332 - 1346},
  issn          = {0743-7315},
  __markedentry = {[mac:]},
  abstract      = {Time synchronization is a critical component in any wireless sensor network (WSN). In terms of energy consumption, on-demand time synchronization is better than continuous synchronization. However, currently existing on-demand time synchronization protocols have a very low accuracy and very strong spatial accumulative effect. These features are not suitable for several types of WSN applications, such as applications with stringent temporal requirements, or applications that have a large spatial region of interest. In this paper, we propose an on-demand time synchronization protocol, named AOTSP (Accurate On-demand Time Synchronization Protocol), which differs from other protocols of the same category by having the following advantages, as shown in our theoretical analysis and simulation results: (1) weak spatial accumulative effect; (2) fairly low communication cost; (3) low computational complexity; (4) high accuracy; (5) high scalability. Such features make AOTSP a suitable time synchronization protocol for a broad range of WSN applications.},
  comment       = {15},
  doi           = {https://doi.org/10.1016/j.jpdc.2012.06.003},
  keywords      = {On-demand synchronization, Wireless sensor networks, Taylor expansion},
  url           = {http://www.sciencedirect.com/science/article/pii/S0743731512001426},
}

@Article{Smith2009,
  author        = {S. Smith and W. Yu},
  title         = {A document driven methodology for developing a high quality Parallel Mesh Generation Toolbox},
  journal       = {Advances in Engineering Software},
  year          = {2009},
  volume        = {40},
  number        = {11},
  pages         = {1155 - 1167},
  issn          = {0965-9978},
  __markedentry = {[mac:]},
  abstract      = {This paper motivates the value of using a document driven methodology to improve the quality of scientific computing applications by illustrating the design and documentation of a Parallel Mesh Generation Toolbox (PMGT). Formal mathematical specification is promoted for writing unambiguous requirements, which can be used to judge the correctness and thus the reliability of PMGT. Mathematics is also shown to improve understandability, reusability and maintainability through modelling software modules as finite state machines. The proposed methodology includes explicit traceability between requirements, design, implementation and test cases. Traceability improves the verification of completeness and consistency and it allows for proper change management. To improve the reliability of PMGT, given the challenge that the correct solution is unknown a priori, an automated testing approach is adopted to verify the known properties of a correct solution, such as conformality and counterclockwise vertex numbering.},
  comment       = {13},
  doi           = {https://doi.org/10.1016/j.advengsoft.2009.05.003},
  keywords      = {Mesh generation, Software engineering, Software quality, Scientific computing},
  url           = {http://www.sciencedirect.com/science/article/pii/S0965997809001306},
}

@Article{Gadeyne2014,
  author        = {Klaas Gadeyne and Gregory Pinte and Kristof Berx},
  title         = {Describing the design space of mechanical computational design synthesis problems},
  journal       = {Advanced Engineering Informatics},
  year          = {2014},
  volume        = {28},
  number        = {3},
  pages         = {198 - 207},
  issn          = {1474-0346},
  note          = {Multiview Modeling for Mechatronic Design},
  __markedentry = {[mac:]},
  abstract      = {An important challenge in mechatronic system design is to select a feasible system architecture that satisfies all requirements. This article describes (i) the necessary concepts that a system architect needs to be able to formally and declaratively describe the design space of mechanical design synthesis problems, thereby minimizing accidental complexity; (ii) how a Domain Specific Language based on the SysML modeling language and the Object Constraint Language (OCL) can be used to create this model of the design space; and (iii) an iterative process to come up with a formal model of the design space. This model describes the design space independent of any (knowledge of a) particular solving technology for the Design Space Exploration. Furthermore, the information in the model allows to select the most appropriate solving strategy for a particular design synthesis problem. The different concepts are illustrated on the example of automated synthesis of a gearbox.},
  comment       = {10},
  doi           = {https://doi.org/10.1016/j.aei.2014.03.004},
  keywords      = {Gearbox architecture, Design Space Exploration, Configuration design, Variant design, Computational design synthesis, Embodiment design},
  url           = {http://www.sciencedirect.com/science/article/pii/S1474034614000329},
}

@Article{Huang2008,
  author        = {Hui-Wen Huang and Chunkuan Shih and Swu Yih and Ming-Huei Chen},
  title         = {System-level hazard analysis using the sequence-tree method},
  journal       = {Annals of Nuclear Energy},
  year          = {2008},
  volume        = {35},
  number        = {3},
  pages         = {353 - 362},
  issn          = {0306-4549},
  __markedentry = {[mac:]},
  abstract      = {A system-level PHA using the sequence-tree method is presented to perform safety-related digital I&C system SSA. The conventional PHA involves brainstorming among experts on various portions of the system to identify hazards through discussions. However, since the conventional PHA is not a systematic technique, the analysis results depend strongly on the expertsâ€™ subjective opinions. The quality of analysis cannot be appropriately controlled. Therefore, this study presents a system-level sequence tree based PHA, which can clarify the relationship among the major digital I&C systems. This sequence-tree-based technique has two major phases. The first phase adopts a table to analyze each event in SAR Chapter 15 for a specific safety-related I&C system, such as RPS. The second phase adopts a sequence tree to recognize the I&C systems involved in the event, the working of the safety-related systems and how the backup systems can be activated to mitigate the consequence if the primary safety systems fail. The defense-in-depth echelons, namely the Control echelon, Reactor trip echelon, ESFAS echelon and Monitoring and indicator echelon, are arranged to build the sequence-tree structure. All the related I&C systems, including the digital systems and the analog back-up systems, are allocated in their specific echelons. This system-centric sequence-tree analysis not only systematically identifies preliminary hazards, but also vulnerabilities in a nuclear power plant. Hence, an effective simplified D3 evaluation can also be conducted.},
  comment       = {10},
  doi           = {https://doi.org/10.1016/j.anucene.2007.07.010},
  url           = {http://www.sciencedirect.com/science/article/pii/S0306454907001764},
}

@Article{Mohan2006a,
  author        = {Kannan Mohan and Peng Xu and Balasubramaniam Ramesh},
  title         = {Supporting dynamic group decision and negotiation processes: A traceability augmented peer-to-peer network approach},
  journal       = {Information \& Management},
  year          = {2006},
  volume        = {43},
  number        = {5},
  pages         = {650 - 662},
  issn          = {0378-7206},
  __markedentry = {[mac:]},
  abstract      = {Peer-to-peer (P2P) networks are gaining popularity in supporting group decision and negotiation (GDN) activities in which ad hoc, transient groups participate. In these, multiple stakeholders create and use knowledge that is fragmented and distributed across different locations. While P2P networks help establish physical links across participants, they lack the capability to integrate knowledge fragments embedded in documents and artifacts distributed across peers. We augmented the P2P architecture with traceability to provide a way of integrating distributed knowledge. We implemented this approach in a prototype system that used a P2P networking tool. Using a case study of software development outsourcing, we showed how our approach supported critical GDN activities. Qualitative evaluation of our approach in supporting GDN was also demonstrated.},
  comment       = {13},
  doi           = {https://doi.org/10.1016/j.im.2006.04.001},
  keywords      = {Group decision and negotiation, Peer-to-peer networks, Traceability, Knowledge integration},
  url           = {http://www.sciencedirect.com/science/article/pii/S0378720606000449},
}

@Article{Deb2016,
  author        = {Novarun Deb and Nabendu Chaki and Aditya Ghose},
  title         = {Extracting finite state models from i* models},
  journal       = {Journal of Systems and Software},
  year          = {2016},
  volume        = {121},
  pages         = {265 - 280},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {i* models are inherently sequence agnostic. This makes the process of cross-checking i* models against temporal properties quite impossible. There is an immediate industrial need to bridge the gap between such a sequence agnostic model and a standardized model verifier so that model checking can be performed in the requirement analysis phase itself. In this paper, we first spell out the Naive Algorithm that generates all possible finite state models corresponding to a given i* model. The growth of the finite state model space can be mapped to the problem of finding the number of possible paths between the Least Upper Bound (LUB) and the Greatest Lower Bound (GLB) of a k-dimensional hypercube lattice structure. The mathematics for doing a quantitative analysis of the space growth has also been presented. The Naive Algorithm has its main drawback in the hyperexponential growth of the model space. The Semantic Implosion Algorithm is proposed as a solution to the hyperexponential problem. This algorithm exploits the temporal information embedded within the i* model of an enterprise to reduce the rate of growth of the finite state model space. A comparative quantitative analysis between the two approaches concludes the superiority of the Semantic Implosion Algorithm.},
  comment       = {16},
  doi           = {https://doi.org/10.1016/j.jss.2016.03.038},
  keywords      = {i model, Model transformation, Model checking},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121216300048},
}

@Article{Zdun2006,
  author        = {Uwe Zdun},
  title         = {Tailorable language for behavioral composition and configuration of software components},
  journal       = {Computer Languages, Systems \& Structures},
  year          = {2006},
  volume        = {32},
  number        = {1},
  pages         = {56 - 82},
  issn          = {1477-8424},
  __markedentry = {[mac:]},
  abstract      = {Many software systems suffer from missing support for behavioral (runtime) composition and configuration of software components. The concern â€œbehavioral composition and configurationâ€ is not treated as a first-class entity, but instead it is hard-coded in different programming styles, leading to tangled composition and configuration code that is hard to understand and maintain. We propose to embed a dynamic language with a tailorable object and class concept into the host language in which the components are written, and use the tailorable language for behavioral composition and configuration tasks. Using this approach we can separate the concerns â€œbehavioral composition and configurationâ€ from the rest of the software system, leading to a more reusable, understandable, and maintainable composition and configuration of software components.},
  comment       = {27},
  doi           = {https://doi.org/10.1016/j.cl.2005.04.001},
  keywords      = {Software components, Component composition, Component configuration, Tailorable language},
  url           = {http://www.sciencedirect.com/science/article/pii/S1477842405000205},
}

@Article{Beek2016,
  author        = {Maurice H. ter Beek and Alessandro Fantechi and Stefania Gnesi and Franco Mazzanti},
  title         = {Modelling and analysing variability in product families: Model checking of modal transition systems with variability constraints},
  journal       = {Journal of Logical and Algebraic Methods in Programming},
  year          = {2016},
  volume        = {85},
  number        = {2},
  pages         = {287 - 315},
  issn          = {2352-2208},
  __markedentry = {[mac:]},
  abstract      = {We present the formal underpinnings of a modelling and analysis framework for the specification and verification of variability in product families. We address variability at the behavioural level by modelling the family behaviour by means of a Modal Transition System (MTS) with an associated set of variability constraints expressed over action labels. An MTS is a Labelled Transition System (LTS) which distinguishes between optional and mandatory transitions. Steered by the variability constraints, the inclusion or exclusion of labelled transitions in an LTS refining the MTS determines the family's possible product behaviour. We formalise this as a special-purpose refinement relation for MTSs, which differs fundamentally from the classical one, and show how to use it for the definition and derivation of valid product behaviour starting from product family behaviour. We also present a variability-aware action-based branching-time modal temporal logic to express properties over MTSs, and demonstrate a number of results regarding the preservation of logical properties from family to product behaviour. These results pave the way for the more efficient family-based analyses of MTSs, limiting the need for product-by-product analyses of LTSs. Finally, we define a high-level modal process algebra for the specification of MTSs. The complete framework is implemented in a model-checking tool: given the behaviour of a product family modelled as an MTS with an additional set of variability constraints, it allows the explicit generation of valid product behaviour as well as the efficient on-the-fly verification of logical properties over family and product behaviour alike.},
  comment       = {29},
  doi           = {https://doi.org/10.1016/j.jlamp.2015.11.006},
  keywords      = {Model checking, Modal transition systems, Temporal logic, Product families, Variability},
  url           = {http://www.sciencedirect.com/science/article/pii/S2352220815001431},
}

@Article{Zhao2015,
  author        = {Xin Zhao and Liwei Shen and Xin Peng and Wenyun Zhao},
  title         = {Toward SLA-constrained service composition: An approach based on a fuzzy linguistic preference model and an evolutionary algorithm},
  journal       = {Information Sciences},
  year          = {2015},
  volume        = {316},
  pages         = {370 - 396},
  issn          = {0020-0255},
  note          = {Nature-Inspired Algorithms for Large Scale Global Optimization},
  __markedentry = {[mac:]},
  abstract      = {In a market-oriented service computing environment, both back-end SLA (service level agreement) offers and front-end SLA requirements should be considered when performing service composition. In this paper, we address the optimization problem of SLA-constrained service composition and focus on the following issues: the difficulties related to preference definition and to weight assignment, the limitation of linear utility functions in identifying preferred skyline solutions, and the efficiency and scalability requirements of the optimization algorithm. We present a systematic approach based on a fuzzy preference model and on evolutionary algorithms. Specifically, we first model this multi-objective optimization problem using the weighted Tchebycheff distance rather than a linear utility function. We then present a fuzzy preference model for preference representation and weight assignment. In the model, a set of fuzzy linguistic preference terms and their properties are introduced for establishing consistent preference order of multiple QoS dimensions, and a weighting procedure is proposed to transform the preference into numeric weights. Finally, we present two evolutionary algorithms, i.e., single_EA and hybrid_EA, that implement different optimization objectives and that can be used in different SLA management scenarios for service composition. We conduct a set of experimental studies to evaluate the effectiveness of the proposed algorithms in determining the optimal solutions, and to evaluate their efficiency and scalability for different problem scales.},
  comment       = {27},
  doi           = {https://doi.org/10.1016/j.ins.2014.11.016},
  keywords      = {SLA, Service composition, Multi-objective optimization, Linguistic preference, Evolutionary algorithm, Weighted Tchebycheff distance},
  url           = {http://www.sciencedirect.com/science/article/pii/S002002551401086X},
}

@Article{Gomez-Abajo2017,
  author        = {Pablo GÃ³mez-Abajo and Esther Guerra and Juan de Lara},
  title         = {A domain-specific language for model mutation and its application to the automated generation of exercises},
  journal       = {Computer Languages, Systems \& Structures},
  year          = {2017},
  volume        = {49},
  pages         = {152 - 173},
  issn          = {1477-8424},
  __markedentry = {[mac:]},
  abstract      = {Model-Driven Engineering (MDE) is a software engineering paradigm that uses models as main assets in all development phases. While many languages for model manipulation exist (e.g., for model transformation or code generation), there is a lack of frameworks to define and apply model mutations. A model mutant is a variation of an original model, created by the application of specific model mutation operations. Model mutation has many applications, for instance, in the areas of model transformation testing, model-based testing or education. In this paper, we present a domain-specific language called Wodel for the specification and generation of model mutants. Wodel is domain-independent, as it can be used to generate mutants of models conformant to arbitrary meta-models. Its development environment is extensible, permitting the incorporation of post-processors for different applications. In particular, we describe Wodel-Edu, a post-processing extension directed to the automated generation of exercises for particular domains and their automated correction. We show the application of Wodel-Edu to the generation of exercises for deterministic automata, and report on an evaluation of the quality of the generated exercises, obtaining overall good results.},
  comment       = {22},
  doi           = {https://doi.org/10.1016/j.cl.2016.11.001},
  keywords      = {Model-Driven Engineering, Domain-Specific Languages, Model mutation, Education, Automatic exercise generation and correction},
  url           = {http://www.sciencedirect.com/science/article/pii/S147784241630094X},
}

@Article{Celik2013,
  author        = {Turgay Ã‡elik and Bedir Tekinerdogan},
  title         = {S-IDE: A tool framework for optimizing deployment architecture of High Level Architecture based simulation systems},
  journal       = {Journal of Systems and Software},
  year          = {2013},
  volume        = {86},
  number        = {10},
  pages         = {2520 - 2541},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {One of the important problems in High Level Architecture (HLA) based distributed simulation systems is the allocation of the different simulation modules to the available physical resources. Usually, the deployment of the simulation modules to the physical resources can be done in many different ways, and each deployment alternative will have a different impact on the performance. Although different algorithmic solutions have been provided to optimize the allocation with respect to the performance, the problem has not been explicitly tackled from an architecture design perspective. Moreover, for optimizing the deployment of the simulation system, tool support is largely missing. In this paper we propose a method for automatically deriving deployment alternatives for HLA based distributed simulation systems. The method extends the IEEE Recommended Practice for High Level Architecture Federation Development and Execution Process by providing an approach for optimizing the allocation at the design level. The method is realized by the tool framework, S-IDE (Simulation-IDE) that we have developed to provide an integrated development environment for deriving a feasible deployment alternative based on the simulation system and the available physical resources at the design phase. The method and the tool support have been validated using a case study for the development of a traffic simulation system.},
  comment       = {22},
  doi           = {https://doi.org/10.1016/j.jss.2013.03.013},
  keywords      = {Deployment model optimization, Metamodel based tool development, Distributed simulation, High Level Architecture (HLA), FEDEP, Software architecture, Model transformations, Metamodeling},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121213000599},
}

@Article{Huang2008a,
  author        = {Hui-Wen Huang and Chunkuan Shih and Swu Yih and Ming-Huei Chen},
  title         = {Integrated software safety analysis method for digital I\&C systems},
  journal       = {Annals of Nuclear Energy},
  year          = {2008},
  volume        = {35},
  number        = {8},
  pages         = {1471 - 1483},
  issn          = {0306-4549},
  __markedentry = {[mac:]},
  abstract      = {The digitalized Instrumentation and Control (I&C) system of Nuclear power plants can provide more powerful overall operation capability, and user friendly man-machine interface. The operator can obtain more information through digital I&C system. However, while I&C system being digitalized, three issues are encountered: (1) software common-cause failure, (2) the interaction failure between operator and digital instrumentation and control system interface, and (3) the non-detectability of software failure. These failures might defeat defense echelons, and make the Diversity and Defense-in-Depth (D3) analysis be more difficult. This work developed an integrated methodology to evaluate nuclear power plant safety effect by interactions between operator and digital I&C system, and then propose improvement recommendations. This integrated methodology includes component-level software fault tree, system-level sequence-tree method and nuclear power plant computer simulation analysis. Software fault tree can clarify the software failure structure in digital I&C systems. Sequence-tree method can identify the interaction process and relationship among operator and I&C systems in each D3 echelon in a design basis event. Nuclear power plant computer simulation analysis method can further analyze the available backup facilities and allowable manual action duration for the operator when the digital I&C fail to function. Applying this methodology to evaluate the performance of digital nuclear power plant D3 design, could promote the nuclear power plant operation safety. The operator can then trust the nuclear power plant than before, when operating the highly automatic digital I&C facilities.},
  comment       = {13},
  doi           = {https://doi.org/10.1016/j.anucene.2008.01.009},
  url           = {http://www.sciencedirect.com/science/article/pii/S0306454908000273},
}

@Article{Johnsen2015,
  author        = {Einar Broch Johnsen and Rudolf Schlatte and S. Lizeth Tapia Tarifa},
  title         = {Integrating deployment architectures and resource consumption in timed object-oriented models},
  journal       = {Journal of Logical and Algebraic Methods in Programming},
  year          = {2015},
  volume        = {84},
  number        = {1},
  pages         = {67 - 91},
  issn          = {2352-2208},
  note          = {Special Issue: The 23rd Nordic Workshop on Programming Theory (NWPT 2011) Special Issue: Domains X, International workshop on Domain Theory and applications, Swansea, 5-7 September, 2011},
  __markedentry = {[mac:]},
  abstract      = {Software today is often developed for many deployment scenarios; the software may be adapted to sequential, concurrent, distributed, and even virtualized architectures. Since software performance can vary significantly depending on the target architecture, design decisions need to address which features to include and what performance to expect for different architectures. To make use of formal methods for these design decisions, system models need to range over deployment scenarios. For this purpose, it is desirable to lift aspects of low-level deployment to the abstraction level of the modeling language. This paper proposes an integration of deployment architectures in the Real-Time ABS language, with restrictions on processing resources. Real-Time ABS is a timed, abstract and behavioral specification language with a formal semantics and a Java-like syntax, that targets concurrent, distributed and object-oriented systems. A separation of concerns between execution cost at the object level and execution capacity at the deployment level makes it easy to compare the timing and performance of different deployment scenarios already during modeling. The language and associated simulation tool is demonstrated on examples and its semantics is formalized.},
  comment       = {25},
  doi           = {https://doi.org/10.1016/j.jlamp.2014.07.001},
  keywords      = {Deployment architecture, Resource management, Object orientation, Formal methods, Performance, Real-Time ABS},
  url           = {http://www.sciencedirect.com/science/article/pii/S2352220814000479},
}

@Article{Esfahani2012,
  author        = {Naeem Esfahani and Sam Malek},
  title         = {Utilizing architectural styles to enhance the adaptation support of middleware platforms},
  journal       = {Information and Software Technology},
  year          = {2012},
  volume        = {54},
  number        = {7},
  pages         = {786 - 801},
  issn          = {0950-5849},
  __markedentry = {[mac:]},
  abstract      = {Context
Modern middleware platforms provide the applications deployed on top of them with facilities for their adaptation. However, the level of adaptation support provided by the state-of-the-art middleware solutions is often limited to dynamically loading and off-loading of software components. Therefore, it is left to the application developers to handle the details of change such that the systemâ€™s consistency is not jeopardized.
Objective
We aim to change the status quo by providing the middleware facilities necessary to ensure the consistency of software after adaptation. We would like these facilities to be reusable across different applications, such that the middleware can streamline the process of achieving safe adaptation.
Method
Our approach addresses the current shortcomings by utilizing the information encoded in a software systemâ€™s architectural style. This information drives the development of reusable adaptation patterns. The patterns specify both the exact sequence of changes and the time at which those changes need to occur. We use the patterns to provide advanced adaptation support on top of an existing architectural middleware platform.
Results
Our experience shows the feasibility of deriving detailed adaptation patterns for several architectural styles. Applying the middleware to adapt two real-world software systems shows the approach is effective in consistently adapting these systems without jeopardizing their consistency.
Conclusion
We conclude the approach is effective in alleviating the application developers from the responsibility of managing the adaptation process at the application-level. Moreover, we believe this study provides the foundation for changing the way adaptation support is realized in middleware solutions.},
  comment       = {16},
  doi           = {https://doi.org/10.1016/j.infsof.2012.02.001},
  keywords      = {Software architecture, Architectural style, Adaptation patterns, Middleware},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584912000328},
}

@Article{Haruvy2005,
  author        = {Ernan Haruvy and Ashutosh Prasad},
  title         = {Freeware as a competitive deterrent},
  journal       = {Information Economics and Policy},
  year          = {2005},
  volume        = {17},
  number        = {4},
  pages         = {513 - 534},
  issn          = {0167-6245},
  __markedentry = {[mac:]},
  abstract      = {Using perfect foresight and adaptive models, this paper examines the effect of competitor asymmetry, consumer sensitivity to incentives and adaptive processes on freeware strategies and competitive outcomes. Four roles played by freeware in competitive markets are identified â€“ it can be a mechanism to build or speed up the growth of a network without the need to lower prices on the commercial version, a deterrence mechanism, a hindrance to a rivalâ€™s network building efforts, and a coordination device in the presence of forward looking consumers. We determine the optimal prices of the commercial version, the decisions to introduce freeware and the freeware qualities for both competing firms.},
  comment       = {22},
  doi           = {https://doi.org/10.1016/j.infoecopol.2005.03.002},
  keywords      = {Freeware, Software, Product line design, Competitive strategy, Evolutionary dynamics},
  url           = {http://www.sciencedirect.com/science/article/pii/S0167624505000259},
}

@Article{Nakagawa2013,
  author        = {Elisa Y. Nakagawa and Pablo O. Antonino and Martin Becker and JosÃ© C. Maldonado and Holger Storf and Karina B. Villela and Dieter Rombach},
  title         = {Relevance and perspectives of AAL in Brazil},
  journal       = {Journal of Systems and Software},
  year          = {2013},
  volume        = {86},
  number        = {4},
  pages         = {985 - 996},
  issn          = {0164-1212},
  note          = {SI : Software Engineering in Brazil: Retrospective and Prospective Views},
  __markedentry = {[mac:]},
  abstract      = {Population aging has been taking place in many countries across the globe and more recently in emerging countries. In this context, Ambient Assisted Living (AAL) has become one focus of attention, including methods, products, services, and AAL software systems that support the everyday lives of elderly people, promoting mainly their independence and dignity. From the perspective of computer science, efforts are already being dedicated to adequately developing AAL systems. However, in spite of its relevance, AAL has not been properly investigated in emerging countries, including Brazil. Thus, the contribution of this paper is to present the main perspectives of research in AAL, in particular in the area of software engineering, considering that the Brazilian population is also subject to the aging process. The main intention of this paper is to raise the interest of Brazilian researchers, as well as government and industry, for this important area.},
  comment       = {12},
  doi           = {https://doi.org/10.1016/j.jss.2012.10.013},
  keywords      = {Ambient Assisted Living (AAL), AAL platform, Reference architecture, Population aging},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121212002841},
}

@Article{Atkinson2015,
  author        = {Colin Atkinson and Ralph Gerbig and Mathias Fritzsche},
  title         = {A multi-level approach to modeling language extension in the Enterprise Systems Domain},
  journal       = {Information Systems},
  year          = {2015},
  volume        = {54},
  pages         = {289 - 307},
  issn          = {0306-4379},
  __markedentry = {[mac:]},
  abstract      = {As the number and diversity of technologies involved in building enterprise systems continues to grow so does the importance of modeling tools that are able to present customized views of enterprise systems to different stakeholders according to their needs and skills. Moreover, since the range of required view types is continuously evolving, it must be possible to extend and enhance the languages and services offered by such tools on an ongoing basis. However, this can be difficult with today×³s modeling tools because the meta-models that define the languages, views and services they support are usually hardwired and thus not amenable to extensions. In practice, therefore, various workarounds have to be used to extend a tool×³s underlying meta-model. Some of these are built into the implemented modeling standards (e.g. UML 2, BPMN 2.0 and ArchiMate 2.0) while others have to be applied by complementary, external tools (e.g. annotation models). These techniques not only increase accidental complexity, they also reduce the ability of the modeling tool to ensure adherence to enterprise rules and constraints. In this paper we discuss the strengths and weaknesses of the various approaches for language extension and propose a modeling framework best able to support the main extension scenarios currently found in practice today.},
  comment       = {19},
  doi           = {https://doi.org/10.1016/j.is.2015.01.003},
  keywords      = {Multi-level modeling, Model language extension, Orthogonal classification architecture, Linguistic classification, Ontological classification},
  url           = {http://www.sciencedirect.com/science/article/pii/S0306437915000137},
}

@Article{Malhotra2017,
  author        = {Ruchika Malhotra and Megha Khanna and Rajeev R. Raje},
  title         = {On the application of search-based techniques for software engineering predictive modeling: A systematic review and future directions},
  journal       = {Swarm and Evolutionary Computation},
  year          = {2017},
  volume        = {32},
  pages         = {85 - 109},
  issn          = {2210-6502},
  __markedentry = {[mac:]},
  abstract      = {Software engineering predictive modeling involves construction of models, with the help of software metrics, for estimating quality attributes. Recently, the use of search-based techniques have gained importance as they help the developers and project-managers in the identification of optimal solutions for developing effective prediction models. In this paper, we perform a systematic review of 78 primary studies from January 1992 to December 2015 which analyze the predictive capability of search-based techniques for ascertaining four predominant software quality attributes, i.e., effort, defect proneness, maintainability and change proneness. The review analyses the effective use and application of search-based techniques by evaluating appropriate specifications of fitness functions, parameter settings, validation methods, accounting for their stochastic natures and the evaluation of developmental models with the use of well-known statistical tests. Furthermore, we compare the effectiveness of different models, developed using the various search-based techniques amongst themselves, and also with the prevalent machine learning techniques used in literature. Although there are very few studies which use search-based techniques for predicting maintainability and change proneness, we found that the results of the application of search-based techniques for effort estimation and defect prediction are encouraging. Hence, this comprehensive study and the associated results will provide guidelines to practitioners and researchers and will enable them to make proper choices for applying the search-based techniques to their specific situations.},
  comment       = {25},
  doi           = {https://doi.org/10.1016/j.swevo.2016.10.002},
  keywords      = {Search-based techniques, Change prediction, Defect prediction, Effort estimation, Maintainability prediction, Software quality},
  url           = {http://www.sciencedirect.com/science/article/pii/S2210650216303418},
}

@Article{Kim2006a,
  author        = {Jintae Kim and Minseong Kim and Sooyong Park},
  title         = {Goal and scenario based domain requirements analysis environment},
  journal       = {Journal of Systems and Software},
  year          = {2006},
  volume        = {79},
  number        = {7},
  pages         = {926 - 938},
  issn          = {0164-1212},
  note          = {Selected papers from the 11th Asia Pacific Software Engineering Conference (APSEC2004)},
  __markedentry = {[mac:]},
  abstract      = {Identifying and representing domain requirements among products in a product family are crucial activities for a successful software reuse. The domain requirements should be not only identified based on the business goal, which drives marketing plan, product plan, and differences among products, but also represented as familiar notations in order to support developing a particular product in the product family. Thus, our proposal is to identify the domain requirements through goals and scenarios, and represent them as variable use cases for a product family. Especially, for identification of the domain requirements, we propose four abstraction levels of requirements in a product family, and goal and scenario modeling. For representation of them, variable use case model is suggested, and also the use case transfer rules are proposed so as to bridge the gap between the identification and representation activity. The paper illustrates the application of the approach within a supporting tool using the HIS (Home Integration System) example.},
  comment       = {13},
  doi           = {https://doi.org/10.1016/j.jss.2005.06.046},
  keywords      = {Goal, Scenario, Use case, Domain requirements analysis},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121205001810},
}

@Article{Reinhartz-Berger2010,
  author        = {Iris Reinhartz-Berger},
  title         = {Towards automatization of domain modeling},
  journal       = {Data \& Knowledge Engineering},
  year          = {2010},
  volume        = {69},
  number        = {5},
  pages         = {491 - 515},
  issn          = {0169-023X},
  __markedentry = {[mac:]},
  abstract      = {A domain model, which captures the common knowledge and the possible variability allowed among applications in a domain, may assist in the creation of other valid applications in that domain. However, to create such domain models is not a trivial task: it requires expertise in the domain, reaching a very high level of abstraction, and providing flexible, yet formal, artifacts. In this paper an approach, called Semi-automated Domain Modeling (SDM), to create draft domain models from applications in those domains, is presented. SDM takes a repository of application models in a domain and matches, merges, and generalizes them into sound draft domain models that include the commonality and variability allowed in these domains. The similarity of the different elements is measured, with consideration of syntactic, semantic, and structural aspects. Unlike ontology and schema integration, these models capture both structural and behavioral aspects of the domain. Running SDM on small repositories of project management applications and scheduling systems, we found that the approach may provide reasonable draft domain models, whose comprehensibility, correctness, completeness, and consistency levels are satisfactory.},
  comment       = {25},
  doi           = {https://doi.org/10.1016/j.datak.2010.01.002},
  keywords      = {Domain engineering, Product line engineering, Domain analysis, Metamodeling, UML, DSL},
  url           = {http://www.sciencedirect.com/science/article/pii/S0169023X10000030},
}

@Article{Walther2016,
  author        = {Sven Walther and Heike Wehrheim},
  title         = {On-the-fly construction of provably correct service compositions â€“ templates and proofs},
  journal       = {Science of Computer Programming},
  year          = {2016},
  volume        = {127},
  pages         = {2 - 23},
  issn          = {0167-6423},
  note          = {Special issue of the 11th International Symposium on Formal Aspects of Component Software},
  __markedentry = {[mac:]},
  abstract      = {Today, service compositions often need to be assembled or changed on-the-fly, which leaves only little time for quality assurance. Moreover, quality assurance is complicated by service providers only giving information on their services in terms of domain specific concepts with only limited semantic meaning. In this paper, we propose a method for constructing service compositions based on pre-verified templates. Templates, given as workflow descriptions, are typed over a (domain-independent) template ontology defining concepts and predicates. Their meaning is defined by an abstract semantics, leaving the specific meaning of ontology concepts open, however, only up to given ontology rules. Templates are proven correct using a Hoare-style proof calculus, extended by a specific rule for service calls. Construction of service compositions amounts to instantiation of templates with domain-specific services. Correctness of an instantiation can then simply be checked by verifying that the domain ontology (a) adheres to the rules of the template ontology, and (b) fulfills the constraints of the employed template.},
  comment       = {22},
  doi           = {https://doi.org/10.1016/j.scico.2016.04.002},
  keywords      = {Verification, Hoare-calculus, Templates, Service compositions, Correctness by construction},
  url           = {http://www.sciencedirect.com/science/article/pii/S0167642316300028},
}

@Article{Veerman2005,
  author        = {Niels Veerman},
  title         = {Towards lightweight checks for mass maintenance transformations},
  journal       = {Science of Computer Programming},
  year          = {2005},
  volume        = {57},
  number        = {2},
  pages         = {129 - 163},
  issn          = {0167-6423},
  __markedentry = {[mac:]},
  abstract      = {We propose a lightweight, practical approach to check mass maintenance transformations. We present checks for both transformation tools and transformed source code, and illustrate them using examples of real-world transformations. Our approach is not a fully fledged, formal one but provides circumstantial evidence for transformation correctness, and has been applied to the mass maintenance of industrial Cobol systems.},
  comment       = {35},
  doi           = {https://doi.org/10.1016/j.scico.2005.01.001},
  keywords      = {Mass maintenance, Transformations, Lightweight checks},
  url           = {http://www.sciencedirect.com/science/article/pii/S0167642305000134},
}

@Article{Hofmeister2007,
  author        = {Christine Hofmeister and Philippe Kruchten and Robert L. Nord and Henk Obbink and Alexander Ran and Pierre America},
  title         = {A general model of software architecture design derived from five industrial approaches},
  journal       = {Journal of Systems and Software},
  year          = {2007},
  volume        = {80},
  number        = {1},
  pages         = {106 - 126},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {We compare five industrial software architecture design methods and we extract from their commonalities a general software architecture design approach. Using this general approach, we compare across the five methods the artifacts and activities they use or recommend, and we pinpoint similarities and differences. Once we get beyond the great variance in terminology and description, we find that the five approaches have a lot in common and match more or less the â€œidealâ€ pattern we introduced. From the ideal pattern we derive an evaluation grid that can be used for further method comparisons.},
  comment       = {21},
  doi           = {https://doi.org/10.1016/j.jss.2006.05.024},
  keywords      = {Software architecture, Software architecture design, Software architecture analysis, Architectural method},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121206001634},
}

@Article{Gonzalez-Perez2008,
  author        = {Cesar Gonzalez-Perez and Brian Henderson-Sellers},
  title         = {A work product pool approach to methodology specification and enactment},
  journal       = {Journal of Systems and Software},
  year          = {2008},
  volume        = {81},
  number        = {8},
  pages         = {1288 - 1305},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {Software development methodologies advocated and used today, whether traditional and plan-based or contemporary and agile, usually focus on process steps i.e. they start with requirements and iteratively describe what steps are necessary to move to the next stage or phase, until the software application is delivered to the end user. Such a process-oriented view of methodologies, based on the metaphor that human organizations are â€œmachinesâ€ that â€œexecuteâ€ processes, often results in methodologies that are too rigid and hard to follow, and most often than not end up being ignored or bypassed. Our proposal here is that, since the ultimate aim of software development is to provide a software product, software development methodologies should be described in terms of the intermediate products that are necessary to reach such a final product, plus the needed micro-processes that, as necessary evils, will be required to produce the appropriate work products from other, previously created ones. Using this product-oriented approach, software development methodologies can be specified that are, at least, as flexible as lightweight, agile approaches and, at the same time, as powerful and scalable as plan-oriented ones.},
  comment       = {18},
  doi           = {https://doi.org/10.1016/j.jss.2007.10.001},
  keywords      = {Software development methodologies, Enactment, Metamodelling, ISO/IEC 24744},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121207002439},
}

@Article{Walker2013,
  author        = {Martin Walker and Mark-Oliver Reiser and Sara Tucci-Piergiovanni and Yiannis Papadopoulos and Henrik LÃ¶nn and Chokri Mraidha and David Parker and DeJiu Chen and David Servat},
  title         = {Automatic optimisation of system architectures using EAST-ADL},
  journal       = {Journal of Systems and Software},
  year          = {2013},
  volume        = {86},
  number        = {10},
  pages         = {2467 - 2487},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {There are many challenges which face designers of complex system architectures, particularly safetyâ€“critical or real-time systems. The introduction of Architecture Description Languages (ADLs) has helped to meet these challenges by consolidating information about a system and providing a platform for modelling and analysis capabilities. However, managing this wealth of information can still be problematic, and evaluation of potential design decisions is still often performed manually. Automatic architectural optimisation can be used to assist this decision process, enabling designers to rapidly explore many different options and evaluate them according to specific criteria. In this paper, we present a multi-objective optimisation approach based on EAST-ADL, an ADL in the automotive domain, with the goal of combining the advantages of ADLs and architectural optimisation. The approach is designed to be extensible and leverages the capabilities of EAST-ADL to provide support for evaluation according to different factors, including dependability, timing/performance, and cost. The technique is applied to an illustrative example system featuring both hardware and software perspectives, demonstrating the potential benefits of this concept to the design of embedded system architectures.},
  comment       = {21},
  doi           = {https://doi.org/10.1016/j.jss.2013.04.001},
  keywords      = {Multi-objective optimisation, Architectural description languages, Dependability Analysis, Timing Analysis},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121213000885},
}

@Article{Hoffman2003,
  author        = {Daniel Hoffman and Paul Strooper},
  title         = {API documentation with executable examples},
  journal       = {Journal of Systems and Software},
  year          = {2003},
  volume        = {66},
  number        = {2},
  pages         = {143 - 156},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {The rise of component-based software development has created an urgent need for effective application program interface (API) documentation. Experience has shown that it is hard to create precise and readable documentation. Prose documentation can provide a good overview but lacks precision. Formal methods offer precision but the resulting documentation is expensive to develop. Worse, few developers have the skill or inclination to read formal documentation. We present a pragmatic solution to the problem of API documentation. We augment the prose documentation with executable test cases, including expected outputs, and use the prose plus the test cases as the documentation. With appropriate tool support, the test cases are easy to develop and read. Such test cases constitute a completely formal, albeit partial, specification of input/output behavior. Equally important, consistency between code and documentation is demonstrated by running the test cases. This approach provides an attractive bridge between formal and informal documentation. We also present a tool that supports compact and readable test cases, and generation of test drivers and documentation, and illustrate the approach with detailed case studies.},
  comment       = {14},
  doi           = {https://doi.org/10.1016/S0164-1212(02)00055-9},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121202000559},
}

@Article{Chen2017,
  author        = {Lianping Chen},
  title         = {Continuous Delivery: Overcoming adoption challenges},
  journal       = {Journal of Systems and Software},
  year          = {2017},
  volume        = {128},
  pages         = {72 - 86},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {Continuous Delivery (CD) is a relatively new software development approach. Companies that have adopted CD have reported significant benefits. Motivated by these benefits, many companies would like to adopt CD. However, adopting CD can be very challenging for a number of reasons, such as obtaining buy-in from a wide range of stakeholders whose goals may seemingly be different fromâ€”or even conflict withâ€”our own; gaining sustained support in a dynamic complex enterprise environment; maintaining an application development team's momentum when their application's migration to CD requires an additional strenuous effort over a long period of time; and so on. To help overcome the adoption challenges, I present six strategies: (1) selling CD as a painkiller; (2) establishing a dedicated team with multi-disciplinary members; (3) continuous delivery of continuous delivery; (4) starting with the easy but important applications; (5) visual CD pipeline skeleton; (6) expert drop. These strategies were derived from four years of experience in implementing CD at a multi-billion-euro company. Additionally, our experience led to the identification of eight further challenges for research. The information contributes toward building a body of knowledge for CD adoption.},
  comment       = {15},
  doi           = {https://doi.org/10.1016/j.jss.2017.02.013},
  keywords      = {Agile Software Development, Continuous Delivery, Continuous Deployment, Continuous Software Engineering, DevOps, Adoption},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121217300353},
}

@Article{Frantz2016,
  author        = {Rafael Z. Frantz and Rafael Corchuelo and Fabricia Roos-Frantz},
  title         = {On the design of a maintainable software development kit to implement integration solutions},
  journal       = {Journal of Systems and Software},
  year          = {2016},
  volume        = {111},
  pages         = {89 - 104},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {Companies typically rely on applications purchased from third parties or developed at home to support their business activities. It is not uncommon that these applications were not designed taking integration into account. Enterprise Application Integration provides methodologies and tools to design and implement integration solutions. Camel, Spring Integration, and Mule range amongst the most popular open-source tools that provide support to implement integration solutions. The adaptive maintenance of a software tool is very important for companies that need to reuse existing tools to build their own. We have analysed 25 maintainability measures on Camel, Spring Integration, and Mule. We have conducted a statistical analysis to confirm the results obtained with the maintainability measures, and it follows that these tools may have problems regarding maintenance. These problems increase the costs of the adaptation process. This motivated us to work on a new proposal that has been carefully designed in order to reduce maintainability efforts. GuaranÃ¡Â SDK is the software tool that we provide to implement integration solutions. We have also computed the maintainability measures regarding GuaranÃ¡Â SDK and the results suggest that maintaining it is easier than maintaining the others. Furthermore, we have conducted an industrial experience to demonstrate the application of our proposal in industry.},
  comment       = {16},
  doi           = {https://doi.org/10.1016/j.jss.2015.08.044},
  keywords      = {Enterprise Application Integration, Integration framework},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121215001880},
}

@Article{Sadat-Mohtasham2008,
  author        = {S. Hossein Sadat-Mohtasham and Ali A. Ghorbani},
  title         = {A language for high-level description of adaptive web systems},
  journal       = {Journal of Systems and Software},
  year          = {2008},
  volume        = {81},
  number        = {7},
  pages         = {1196 - 1217},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {Adaptive Web systems (AWS) are Web-based systems that can adapt their features such as, presentation, content, and structure, based on usersâ€™ behaviour and preferences, device capabilities, and environment attributes. A framework was developed in our research group to provide the necessary components and protocols for the development of adaptive Web systems; however, there were several issues and shortcomings (e.g. low productivity, lack of verification mechanisms, etc.) in using the framework that inspired the development of a domain-specific language for the framework. This paper focuses on the proposal, design, and implementation of AWL, the Adaptive Web Language, which is used to develop adaptive Web systems within our framework. Not only does AWL address the existing issues in the framework, but it also offers mechanisms to increase software quality attributes, especially, reusability. An example application named PENS (a personalized e-News system) is explained and implemented in AWL. AWL has been designed based on the analysis of the adaptive Web domain, having taken into account the principles of reuse-based software engineering (product-lines), domain-specific languages, and aspect-oriented programming. Specially, a novel design decision, inspired by aspect-oriented programming paradigm, allows separate specification of presentation features in an application from its adaptation features. The AWLâ€™s design decisions and their benefits are explained.},
  comment       = {22},
  doi           = {https://doi.org/10.1016/j.jss.2007.08.033},
  keywords      = {Adaptive web system, Domain-specific programming language, Aspect-oriented programming},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121207002233},
}

@Article{Weidlich2012,
  author        = {Matthias Weidlich and Jan Mendling and Mathias Weske},
  title         = {Propagating changes between aligned process models},
  journal       = {Journal of Systems and Software},
  year          = {2012},
  volume        = {85},
  number        = {8},
  pages         = {1885 - 1898},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {There is a wide variety of drivers for business process modelling initiatives, reaching from organisational redesign to the development of information systems. Consequently, a common business process is often captured in multiple models that overlap in content due to serving different purposes. Business process management aims at flexible adaptation to changing business needs. Hence, changes of business processes occur frequently and have to be incorporated in the respective process models. Once a process model is changed, related process models have to be updated accordingly, despite the fact that those process models may only be loosely coupled. In this article, we introduce an approach that supports change propagation between related process models. Given a change in one process model, we leverage the behavioural abstraction of behavioural profiles for corresponding activities in order to determine a change region in another model. Our approach is able to cope with changes in pairs of models that are not related by hierarchical refinement and show behavioural inconsistencies. We evaluate the applicability of our approach with two real-world process model collections. To this end, we either deduce change operations from different model revisions or rely on synthetic change operations.},
  comment       = {14},
  doi           = {https://doi.org/10.1016/j.jss.2012.02.044},
  keywords      = {Change propagation, Model synchronisation, Behavioural analysis, Process model alignment},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121212000672},
}

@Article{Fasquel2011,
  author        = {Jean-Baptiste Fasquel and Johan Moreau},
  title         = {A design pattern coupling role and component concepts: Application to medical software},
  journal       = {Journal of Systems and Software},
  year          = {2011},
  volume        = {84},
  number        = {5},
  pages         = {847 - 863},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {Abstract
One of the challenges in software development regards the appropriate coupling of separated code elements in order to correctly build initially expected high-level software functionalities. In this context, we address issues related to the dynamic composition of such code elements (i.e. how they are dynamically plugged together) as well as their collaboration (i.e. how they work together). We also consider the limitation of build-level dependencies, to avoid the entire re-compilation and re-deployment of a software when modifying it or integrating new functionalities. To solve these issues, we propose a new design pattern coupling role and component concepts and illustrate its relevance for medical software. Compared to most related works focusing on few role concepts while ignoring others, the proposed pattern integrates many role concepts as first-class entities, including in particular a refinement of the notion of collaboration. Another significant contribution of our proposal concerns the coupling of role and component concepts. Roles are related to the functional aspects of a target software program (composition and collaboration of functional units). Components correspond to the physical distribution of code elements with limited build-level dependencies. As illustrated in this paper, such a coupling enables to instantiate a software program using a generic main program together with a description file focusing on software functionalities only. Related code elements are transparently retrieved and composed at run-time before appropriately collaborating, regardless the specificity of their distribution over components.},
  comment       = {17},
  doi           = {https://doi.org/10.1016/j.jss.2011.01.026},
  keywords      = {Design pattern, Dynamic composition, Collaboration, Role, Component, Medical software},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121211000185},
}

@Article{Guillen2013,
  author        = {JoaquÃ­n GuillÃ©n and Javier Miranda and Juan Manuel Murillo and Carlos Canal},
  title         = {A service-oriented framework for developing cross cloud migratable software},
  journal       = {Journal of Systems and Software},
  year          = {2013},
  volume        = {86},
  number        = {9},
  pages         = {2294 - 2308},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {Whilst cloud computing has burst into the current scene as a technology that allows companies to access high computing rates at limited costs, cloud vendors have rushed to provide tools that allow developers to build software for their cloud platforms. The software developed with these tools is often tightly coupled to their services and restrictions. Consequently vendor lock in becomes a common problem which multiple cloud users have to tackle in order to exploit the full potential of cloud computing. A scenario where component-based applications are developed for being deployed across several clouds, and each component can independently be deployed in one cloud or another, remains fictitious due to the complexity and the cost of their development. This paper presents a cloud development framework for developing cloud agnostic applications that may be deployed indifferently across multiple cloud platforms. Information about cloud deployment and cloud integration is separated from the source code and managed by the framework. Interoperability between interdependent components deployed in different clouds is achieved by automatically generating services and service clients. This allows software developers to segment their applications into different modules that can easily be deployed and redistributed across heterogeneous cloud platforms.},
  comment       = {15},
  doi           = {https://doi.org/10.1016/j.jss.2012.12.033},
  keywords      = {Cloud framework, Adaptation, Cross-cloud applications},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121212003421},
}

@Article{Diaz2010,
  author        = {Oscar Diaz and Felipe M. Villoria},
  title         = {Generating blogs out of product catalogues: An MDE approach},
  journal       = {Journal of Systems and Software},
  year          = {2010},
  volume        = {83},
  number        = {10},
  pages         = {1970 - 1982},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {Blogs can be used as a conduit for customer opinions and, in so doing, building communities around products. We attempt to realise this vision by building blogs out of product catalogues. Unfortunately, the immaturity of blog engines makes this endeavour risky. This paper presents a model-driven approach to face this drawback. This implies the introduction of (meta)models: the catalogue model, based on the standard Open Catalog Format, and blog models, that elaborate on the use of blogs as conduits for virtual communities. Blog models end up being realised through blog engines. Specifically, we focus on two types of engines: a hosted blog platform and a standalone blog platform, both in Blojsom. However, the lack of standards in a broad and constantly evolving blog-engine space, hinders both the portability and the maintainability of the solution. Hence, we resort to the notion of â€œabstract platformâ€ as a way to depart from the peculiarities of specific blog engines. Additionally, the paper measures the reuse gains brought by MDE in comparison with the manual coding of blogs.},
  comment       = {13},
  doi           = {https://doi.org/10.1016/j.jss.2010.05.075},
  keywords      = {Model-Driven Engineering, PSM evolution, Blog, Catalogue},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121210001469},
}

@Article{Chen2014,
  author        = {Bihuan Chen and Xin Peng and Yijun Yu and Wenyun Zhao},
  title         = {Uncertainty handling in goal-driven self-optimization â€“ Limiting the negative effect on adaptation},
  journal       = {Journal of Systems and Software},
  year          = {2014},
  volume        = {90},
  pages         = {114 - 127},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {Goal-driven self-optimization through feedback loops has shown effectiveness in reducing oscillating utilities due to a large number of uncertain factors in the runtime environments. However, such self-optimization is less satisfactory when there contains uncertainty in the predefined requirements goal models, such as imprecise contributions and unknown quality preferences, or during the switches of goal solutions, such as lack of understanding about the time for the adaptation actions to take effect. In this paper, we propose to handle such uncertainty in goal-driven self-optimization without interrupting the services. Taking the monitored quality values as the feedback, and the estimated earned value as the global indicator of self-optimization, our approach dynamically updates the quantitative contributions from alternative functionalities to quality requirements, tunes the preferences of relevant quality requirements, and determines a proper timing delay for the last adaptation action to take effect. After applying these runtime measures to limit the negative effect of the uncertainty in goal models and their suggested switches, an experimental study on a real-life online shopping system shows the improvements over goal-driven self-optimization approaches without uncertainty handling.},
  comment       = {14},
  doi           = {https://doi.org/10.1016/j.jss.2013.12.033},
  keywords      = {Uncertainty, Goal-driven self-optimization, Requirements goal models},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121214000065},
}

@Article{Pinto2012,
  author        = {MÃ³nica Pinto and Lidia Fuentes and Luis FernÃ¡ndez},
  title         = {Deriving detailed design models from an aspect-oriented ADL using MDD},
  journal       = {Journal of Systems and Software},
  year          = {2012},
  volume        = {85},
  number        = {3},
  pages         = {525 - 545},
  issn          = {0164-1212},
  note          = {Novel approaches in the design and implementation of systems/software architecture},
  __markedentry = {[mac:]},
  abstract      = {Software architects can separate crosscutting concerns more appropriately by using an aspect-oriented ADL, concretely AO-ADL. This paper illustrates how aspect-orientation and model-driven development technologies can be used to enhance the system design phase; by automatically deriving detailed designs that take into account the â€œaspectsâ€ identified at the architectural level. Specifically, we have defined model-to-model transformation rules to automatically generate either aspect-oriented or object-oriented UML 2.0 models, closing the gap between ADLs and the notations used at the detailed design phase. By using AO-ADL it is possible to specify separately crosscutting concerns and base functionality. Another advantage of using AO-ADL is that it allows the specification of parameterizable architectures, promoting the definition of architectural templates. AO-ADL, then, enforces the specification of crosscutting concerns as separate architectural templates, which can be later instantiated and integrated with the core functionality of the system being developed. The AO-ADL language and the transformation rules from AO-ADL to UML 2.0 are available throughout the AO-ADL Tool Suite, which can be used to progressively refine and elaborate aspect-oriented software architectures. These refined architectures are the starting point of the detailed design phase. This means that our approach provides support to automatically generate a skeleton of the detailed design that preserves the information about the crosscutting and the non-crosscutting functionalities identified and modelled at the architecture level.},
  comment       = {21},
  doi           = {https://doi.org/10.1016/j.jss.2011.05.026},
  keywords      = {Aspect-oriented software development, Software architectures, AO-ADL, Theme/UML, UML 2.0, Model-driven development, ATL},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121211001269},
}

@Article{Meyers2011,
  author        = {Bart Meyers and Hans Vangheluwe},
  title         = {A framework for evolution of modelling languages},
  journal       = {Science of Computer Programming},
  year          = {2011},
  volume        = {76},
  number        = {12},
  pages         = {1223 - 1246},
  issn          = {0167-6423},
  note          = {Special Issue on Software Evolution, Adaptability and Variability},
  __markedentry = {[mac:]},
  abstract      = {In model-driven engineering, evolution is inevitable over the course of the complete life cycle of complex software-intensive systems and more importantly of entire product families. Not only instance models, but also entire modelling languages are subject to change. This is in particular true for domain-specific languages, whose language constructs are tightly coupled to an application domain. The most popular approach to evolution in the modelling domain is a manual process, with tedious and error-prone migration of artefacts such as instance models as a result. This paper provides a taxonomy for evolution of modelling languages and discusses the different evolution scenarios for various kinds of modelling artefacts, such as instance models, meta-models, and transformation models. Subsequently, the consequences of evolution and the required remedial actions are decomposed into primitive scenarios such that all possible evolutions can be covered exhaustively. These primitives are then used in a high-level framework for the evolution of modelling languages. We suggest that our structured approach enables the design of (semi-)automatic modelling language evolution solutions.},
  comment       = {24},
  doi           = {https://doi.org/10.1016/j.scico.2011.01.002},
  keywords      = {Evolution, Modelling languages, Language engineering, Model-driven engineering, Model transformation},
  url           = {http://www.sciencedirect.com/science/article/pii/S0167642311000141},
}

@Article{Soares2013,
  author        = {Gustavo Soares and Rohit Gheyi and Emerson Murphy-Hill and Brittany Johnson},
  title         = {Comparing approaches to analyze refactoring activity on software repositories},
  journal       = {Journal of Systems and Software},
  year          = {2013},
  volume        = {86},
  number        = {4},
  pages         = {1006 - 1022},
  issn          = {0164-1212},
  note          = {SI : Software Engineering in Brazil: Retrospective and Prospective Views},
  __markedentry = {[mac:]},
  abstract      = {Some approaches have been used to investigate evidence on how developers refactor their code, whether refactorings activities may decrease the number of bugs, or improve developersâ€™ productivity. However, there are some contradicting evidence in previous studies. For instance, some investigations found evidence that if the number of refactoring changes increases in the preceding time period the number of defects decreases, different from other studies. They have used different approaches to evaluate refactoring activities. Some of them identify committed behavior-preserving transformations in software repositories by using manual analysis, commit messages, or dynamic analysis. Others focus on identifying which refactorings are applied between two programs by using manual inspection or static analysis. In this work, we compare three different approaches based on manual analysis, commit message (Ratzinger's approach) and dynamic analysis (SafeRefactor's approach) to detect whether a pair of versions determines a refactoring, in terms of behavioral preservation. Additionally, we compare two approaches (manual analysis and Ref-Finder) to identify which refactorings are performed in each pair of versions. We perform both comparisons by evaluating their accuracy, precision, and recall in a randomly selected sample of 40 pairs of versions of JHotDraw, and 20 pairs of versions of Apache Common Collections. While the manual analysis presents the best results in both comparisons, it is not as scalable as the automated approaches. Ratzinger's approach is simple and fast, but presents a low recall; differently, SafeRefactor is able to detect most applied refactorings, although limitations in its test generation backend results for some kinds of subjects in low precision values. Ref-Finder presented a low precision and recall in our evaluation.},
  comment       = {17},
  doi           = {https://doi.org/10.1016/j.jss.2012.10.040},
  keywords      = {Refactoring, Repository, Manual analysis, Automated analysis},
  url           = {http://www.sciencedirect.com/science/article/pii/S016412121200297X},
}

@Article{Kumar2008,
  author        = {Nanda Kumar and Kannan Mohan and Richard Holowczak},
  title         = {Locking the door but leaving the computer vulnerable: Factors inhibiting home users' adoption of software firewalls},
  journal       = {Decision Support Systems},
  year          = {2008},
  volume        = {46},
  number        = {1},
  pages         = {254 - 264},
  issn          = {0167-9236},
  __markedentry = {[mac:]},
  abstract      = {In the new era of a ubiquitously networked world, security measures are only as good as their weakest link. Home computers with access to the Internet are one of the weaker links as they are typically not as well protected as computers in the corporate world. Malicious actors can not only target such computers but also use them to launch attacks against other systems connected to the Internet, thus posing severe threats to data and infrastructure as well as disrupting electronic commerce. This paper investigates the factors that affect the use of security protection strategies by home computer users in relation to a specific, but crucial security technology for home â€“ a software firewall. This paper proposes individuals' concern for privacy, awareness of common security measures, attitude towards security and privacy protection technologies, and computer anxiety as important antecedents that have an impact on the users' decision to adopt a software firewall. The results of our study suggest that attitude plays a more important role than perceived usefulness in shaping users' intention to use firewalls. We attribute this interesting finding to the non-functional nature of firewall systems that work best in the background with a complex relationship to users' productivity. Hence, the results add to our current understanding of Technology Acceptance Model vis-Ã -vis technologies that serve non-functional needs such as security. We then present a set of guidelines to home computer users, Internet Service Providers, e-commerce companies, and the government to increase home users' adoption rate of privacy and security protection technologies.},
  comment       = {11},
  doi           = {https://doi.org/10.1016/j.dss.2008.06.010},
  keywords      = {IS security, Firewalls, e-Commerce, Privacy, Security protection technologies},
  url           = {http://www.sciencedirect.com/science/article/pii/S0167923608001243},
}

@Article{Bassiliades2017,
  author        = {Nick Bassiliades and Moisis Symeonidis and Georgios Meditskos and Efstratios Kontopoulos and Panagiotis Gouvas and Ioannis Vlahavas},
  title         = {A semantic recommendation algorithm for the PaaSport platform-as-a-service marketplace},
  journal       = {Expert Systems with Applications},
  year          = {2017},
  volume        = {67},
  pages         = {203 - 227},
  issn          = {0957-4174},
  __markedentry = {[mac:]},
  abstract      = {Platform as a service (PaaS) is one of the Cloud computing services that provide a computing platform in the Cloud, allowing customers to develop, run, and manage web applications without the complexity of building and maintaining the infrastructure. The primary disadvantage for an SME to enter the emerging PaaS market is the possibility of being locked into a certain platform, mostly provided by the market's giants. The PaaSport project focuses on facilitating SMEs to deploy business applications on the best-matching Cloud PaaS offering and to seamlessly migrate these applications on demand, via a thin, non-intrusive Cloud-broker, in the form of a Cloud PaaS Marketplace. PaaSport enables PaaS provider SMEs to roll out semantically interoperable PaaS offerings, by annotating them using a unified PaaS semantic model that has been defined as an OWL ontology. In this paper we focus on the recommendation algorithm that has been developed on top of the ontology, for providing the application developer with recommendations about the best-matching Cloud PaaS offering. The algorithm consists of: a) a matchmaking part, where the functional parameters of the application are taken into account to rule out inconsistent offerings, and b) a ranking part, where the non-functional parameters of the application are considered to score and rank offerings. Î¤he algorithm is extensively evaluated showing linear scalability to the number of offerings and application requirements. Furthermore, it is extensible upon future semantic model extensions, because it is agnostic to domain specific concepts and parameters, using SPARQL template queries.},
  comment       = {25},
  doi           = {https://doi.org/10.1016/j.eswa.2016.09.032},
  keywords      = {Cloud computing, Platform-as-a-service, Semantic interoperability, Platform offering, Cloud application, Recommendation, Semantic matchmaking, Ranking},
  url           = {http://www.sciencedirect.com/science/article/pii/S0957417416305164},
}

@Article{Tekinerdogan2008,
  author        = {Bedir Tekinerdogan and Hasan Sozer and Mehmet Aksit},
  title         = {Software architecture reliability analysis using failure scenarios},
  journal       = {Journal of Systems and Software},
  year          = {2008},
  volume        = {81},
  number        = {4},
  pages         = {558 - 575},
  issn          = {0164-1212},
  note          = {Selected papers from the 10th Conference on Software Maintenance and Reengineering (CSMR 2006)},
  __markedentry = {[mac:]},
  abstract      = {With the increasing size and complexity of software in embedded systems, software has now become a primary threat for the reliability. Several mature conventional reliability engineering techniques exist in literature but traditionally these have primarily addressed failures in hardware components and usually assume the availability of a running system. Software architecture analysis methods aim to analyze the quality of software-intensive system early at the software architecture design level and before a system is implemented. We propose a Software Architecture Reliability Analysis Approach (SARAH) that benefits from mature reliability engineering techniques and scenario-based software architecture analysis to provide an early software reliability analysis at the architecture design level. SARAH defines the notion of failure scenario model that is based on the Failure Modes and Effects Analysis method (FMEA) in the reliability engineering domain. The failure scenario model is applied to represent so-called failure scenarios that are utilized to derive fault tree sets (FTS). Fault tree sets are utilized to provide a severity analysis for the overall software architecture and the individual architectural elements. Despite conventional reliability analysis techniques which prioritize failures based on criteria such as safety concerns, in SARAH failure scenarios are prioritized based on severity from the end-user perspective. SARAH results in a failure analysis report that can be utilized to identify architectural tactics for improving the reliability of the software architecture. The approach is illustrated using an industrial case for analyzing reliability of the software architecture of the next release of a Digital TV.},
  comment       = {18},
  doi           = {https://doi.org/10.1016/j.jss.2007.10.029},
  keywords      = {Reliability analysis, Scenario-based architectural evaluation, FMEA, Fault trees},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121207003032},
}

@Article{Axelsson2016,
  author        = {Jakob Axelsson and Mats Skoglund},
  title         = {Quality assurance in software ecosystems: A systematic literature mapping and research agenda},
  journal       = {Journal of Systems and Software},
  year          = {2016},
  volume        = {114},
  pages         = {69 - 81},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {Software ecosystems are becoming a common model for software development in which different actors cooperate around a shared platform. However, it is not clear what the implications are on software quality when moving from a traditional approach to an ecosystem, and this is becoming increasingly important as ecosystems emerge in critical domains such as embedded applications. Therefore, this paper investigates the challenges related to quality assurance in software ecosystems, and identifies what approaches have been proposed in the literature. The research method used is a systematic literature mapping, which however only resulted in a small set of six papers. The literature findings are complemented with a constructive approach where areas are identified that merit further research, resulting in a set of research topics that form a research agenda for quality assurance in software ecosystems. The agenda spans the entire system life-cycle, and focuses on challenges particular to an ecosystem setting, which are mainly the results of the interactions across organizational borders, and the dynamic system integration being controlled by the users.},
  comment       = {13},
  doi           = {https://doi.org/10.1016/j.jss.2015.12.020},
  keywords      = {Software ecosystems, Quality, Verification, Testing},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121215002861},
}

@Article{Nassif2013,
  author        = {Ali Bou Nassif and Danny Ho and Luiz Fernando Capretz},
  title         = {Towards an early software estimation using log-linear regression and a multilayer perceptron model},
  journal       = {Journal of Systems and Software},
  year          = {2013},
  volume        = {86},
  number        = {1},
  pages         = {144 - 160},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {Software estimation is a tedious and daunting task in project management and software development. Software estimators are notorious in predicting software effort and they have been struggling in the past decades to provide new models to enhance software estimation. The most critical and crucial part of software estimation is when estimation is required in the early stages of the software life cycle where the problem to be solved has not yet been completely revealed. This paper presents a novel log-linear regression model based on the use case point model (UCP) to calculate the software effort based on use case diagrams. A fuzzy logic approach is used to calibrate the productivity factor in the regression model. Moreover, a multilayer perceptron (MLP) neural network model was developed to predict software effort based on the software size and team productivity. Experiments show that the proposed approach outperforms the original UCP model. Furthermore, a comparison between the MLP and log-linear regression models was conducted based on the size of the projects. Results demonstrate that the MLP model can surpass the regression model when small projects are used, but the log-linear regression model gives better results when estimating larger projects.},
  comment       = {17},
  doi           = {https://doi.org/10.1016/j.jss.2012.07.050},
  keywords      = {Use case points, Log-linear regression model, Software effort estimation, Multilayer perceptron},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121212002221},
}

@Article{Kakarontzas2011,
  author        = {G. Kakarontzas and I.K. Savvas and I. Stamelos},
  title         = {Agents, clusters and components: A synergistic approach to the GSP},
  journal       = {Future Generation Computer Systems},
  year          = {2011},
  volume        = {27},
  number        = {8},
  pages         = {999 - 1010},
  issn          = {0167-739X},
  __markedentry = {[mac:]},
  abstract      = {Grids provide access to a vast amount of computational resources for the execution of demanding computations. These resources are geographically distributed, owned by different organizations and are vastly heterogeneous. The aforementioned factors introduce uncertainty in all phases of a Grid Scheduling Process (GSP). This work describes a synergistic multidisciplinary approach which aims at addressing this uncertainty. It proposes a network of resource representatives (RRs), which maintain the more or less static characteristics of available workers they represent. Clustering techniques are used for the efficient searching in the network of RRs by client agents. After the discovery of possibly suitable resources, client agents and resource agents negotiate directly for the selection of the best available resource set. Finally, according to the characteristics of the selected resource set and its current state, we propose a component-based application configuration approach based on component variants, that adjusts the application for the forthcoming execution phase in the selected resource set. We evaluate our approach using simulation and we show that it outperforms centralized index approaches for large computational grids.},
  comment       = {12},
  doi           = {https://doi.org/10.1016/j.future.2011.05.002},
  keywords      = {Grid scheduling process, Software agents, Clustering, Software components},
  url           = {http://www.sciencedirect.com/science/article/pii/S0167739X11000835},
}

@Article{Boudaa2017,
  author        = {Boudjemaa Boudaa and Slimane Hammoudi and Leila Amel Mebarki and Abdelkader Bouguessa and Mohammed Amine Chikh},
  title         = {An aspect-oriented model-driven approach for building adaptable context-aware service-based applications},
  journal       = {Science of Computer Programming},
  year          = {2017},
  volume        = {136},
  pages         = {17 - 42},
  issn          = {0167-6423},
  __markedentry = {[mac:]},
  abstract      = {Context
Context-aware service-based applications development has been considered among the most studied research fields in the last decade. The objective was to accompany the rapid technology evolution of mobile computing devices by providing customized services able to interact with different contextual situations of a pervasive environment. For this purpose, many research works have advocated Model-Driven Development (MDD) for building context-aware service-based applications. However, the proposed approaches have presented specific methodologies without using development standards, which may be followed by developers. In addition, most of them have ignored the dynamic adaptation aspect at runtime that should characterize such kind of applications and no adaptation strategy was considered in their proposals.
Objective
The current paper aims to propose a generic model-driven approach for context-aware service-based applications engineering with a software development methodology including a reconfiguration loop to achieve the dynamic adaptation of these applications.
Method
This approach focuses on the combination of MDD and Aspect Oriented Modelling (AOM) to take advantage of their benefits. AOM encapsulates different context-awareness logics separately in aspect models called ContextAspect that can be easily woven into the service's business logic according to the changing context over time. The proposed development methodology includes four phases (modelling, composition, transformation and adaptation) which act in conformance with the MDA technology.
Results
The main results gained by using the present approach are the possibility to combine the MDA technology with the aspect-oriented paradigm in a generic development methodology for context-aware service-based applications, and the handling of their dynamic adaptation at execution time according to the changes in the context.
Conclusion
The development of context-aware applications is a complex, cumbersome, and time-consuming task. However, the experience reached by implementing the proposed methodology leads us to believe that the involvement of MDD and AOM is significantly beneficial to overcome some recognised shortcomings of several existing approaches and to make this task simpler, easier and faster.},
  comment       = {26},
  doi           = {https://doi.org/10.1016/j.scico.2016.08.009},
  keywords      = {Context-aware service-based application, ContextAspect, Model-driven methodology, Aspect weaving, Dynamic adaptation},
  url           = {http://www.sciencedirect.com/science/article/pii/S0167642316301253},
}

@Article{Feitelson2012,
  author        = {Dror G. Feitelson},
  title         = {Perpetual development: A model of the Linux kernel life cycle},
  journal       = {Journal of Systems and Software},
  year          = {2012},
  volume        = {85},
  number        = {4},
  pages         = {859 - 875},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {Software evolution is widely recognized as an important and common phenomenon, whereby the system follows an ever-extending development trajectory with intermittent releases. Nevertheless there have been only few lifecycle models that attempt to portray such evolution. We use the evolution of the Linux kernel as the basis for the formulation of such a model, integrating the progress in time with growth of the codebase, and differentiating between development of new functionality and maintenance of production versions. A unique element of the model is the sequence of activities involved in releasing new production versions, and how this has changed with the growth of Linux. In particular, the release follow-up phase before the forking of a new development version, which was prominent in early releases of production versions, has been eliminated in favor of a concurrent merge window in the release of 2.6.x versions. We also show that a piecewise linear model with increasing slopes provides the best description of the growth of Linux. The perpetual development model is used as a framework in which commonly recognized benefits of incremental and evolutionary development may be demonstrated, and to comment on issues such as architecture, conservation of familiarity, and failed projects. We suggest that this model and variants thereof may apply to many other projects in addition to Linux.},
  comment       = {17},
  doi           = {https://doi.org/10.1016/j.jss.2011.10.050},
  keywords      = {Software evolution, Software release, Maintenance, Linux kernel},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121211002822},
}

@Article{Bettini2017,
  author        = {Lorenzo Bettini and Ferruccio Damiani},
  title         = {Xtraitj: Traits for the Java platform},
  journal       = {Journal of Systems and Software},
  year          = {2017},
  volume        = {131},
  pages         = {419 - 441},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {Traits were proposed as a mechanism for fine-grained code reuse to overcome many limitations of class-based inheritance. A trait is a set of methods that is independent from any class hierarchy and can be flexibly used to build other traits or classes by means of a suite of composition operations. In this paper we present the new version of Xtraitj, a trait-based programming language that features complete compatibility and interoperability with the Java platform. Xtraitj is implemented in Xtext and Xbase, and it provides a full Eclipse IDE that supports an incremental adoption of traits in existing Java projects. The new version of Xtraitj allows traits to be accessed from any Java project or library, even if the original Xtraitj source code is not available, since traits can be accessed in their byte-code format. This allows developers to create Xtraitj libraries that can be provided in their binary only format. We detail the technique we used to achieve such an implementation; this technique can be reused in other languages implemented in Xtext for the Java platform. We formalize our traits by means of flattening semantics and we provide some performance benchmarks that show that the runtime overhead introduced by our traits is acceptable.},
  comment       = {23},
  doi           = {https://doi.org/10.1016/j.jss.2016.07.035},
  keywords      = {Java, Trait, IDE, Implementation, Eclipse},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121216301297},
}

@Article{Ovaska2010,
  author        = {Eila Ovaska and Antti Evesti and Katja Henttonen and Marko Palviainen and Pekka Aho},
  title         = {Knowledge based quality-driven architecture design and evaluation},
  journal       = {Information and Software Technology},
  year          = {2010},
  volume        = {52},
  number        = {6},
  pages         = {577 - 601},
  issn          = {0950-5849},
  __markedentry = {[mac:]},
  abstract      = {Modelling and evaluating quality properties of software is of high importance, especially when our every day life depends on the quality of services produced by systems and devices embedded into our surroundings. This paper contributes to the body of research in quality and model driven software engineering. It does so by introducing; (1) a quality aware software architecting approach and (2) a supporting tool chain. The novel approach with supporting tools enables the systematic development of high quality software by merging benefits of knowledge modelling and management, and model driven architecture design enhanced with domain-specific quality attributes. The whole design flow of software engineering is semi-automatic; specifying quality requirements, transforming quality requirements to architecture design, representing quality properties in architectural models, predicting quality fulfilment from architectural models, and finally, measuring quality aspects from implemented source code. The semi-automatic design flow is exemplified by the ongoing development of a secure middleware for peer-to-peer embedded systems.},
  comment       = {25},
  doi           = {https://doi.org/10.1016/j.infsof.2009.11.008},
  keywords      = {Quality attribute, Model-driven development, Software architecture, Ontology, Evaluation, Tool},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584909002080},
}

@Article{Santiago2012,
  author        = {IvÃ¡n Santiago and Ãlvaro JimÃ©nez and Juan Manuel Vara and Valeria De Castro and VerÃ³nica A. Bollati and Esperanza Marcos},
  title         = {Model-Driven Engineering as a new landscape for traceability management: A systematic literature review},
  journal       = {Information and Software Technology},
  year          = {2012},
  volume        = {54},
  number        = {12},
  pages         = {1340 - 1356},
  issn          = {0950-5849},
  note          = {Special Section on Software Reliability and Security},
  __markedentry = {[mac:]},
  abstract      = {Context
Model-Driven Engineering provides a new landscape for dealing with traceability in software development.
Objective
Our goal is to analyze the current state of the art in traceability management in the context of Model-Driven Engineering.
Method
We use the systematic literature review based on the guidelines proposed by Kitchenham. We propose five research questions and six quality assessments.
Results
Of the 157 relevant studies identified, 29 have been considered primary studies. These studies have resulted in 17 proposals.
Conclusion
The evaluation shows that the most addressed operations are storage, CRUD and visualization, while the most immature operations are exchange and analysis traceability information.},
  comment       = {17},
  doi           = {https://doi.org/10.1016/j.infsof.2012.07.008},
  keywords      = {Traceability, Model-Driven Engineering, Systematic literature review},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584912001346},
}

@Article{Geppert2001,
  author        = {Birgit Geppert and Frank RÃ¶ÃŸler},
  title         = {The SDL pattern approach â€“ a reuse-driven SDL design methodology},
  journal       = {Computer Networks},
  year          = {2001},
  volume        = {35},
  number        = {6},
  pages         = {627 - 645},
  issn          = {1389-1286},
  __markedentry = {[mac:]},
  abstract      = {There are several SDL methodologies that offer full system life-cycle support. Only few of them consider software reuse, not to mention high-level reuse of architecture and design. However, software reuse is a proven software engineering paradigm leading to high quality and reduced development effort. Experience made it apparent that â€“ beyond the more traditional reuse of code â€“ especially high-level reuse of architecture and design (as in the case of design patterns or frameworks) has the potential of achieving more systematic and widespread reuse. This paper presents the SDL pattern approach, a design methodology for distributed systems which integrates SDL-based system development with the pattern paradigm. It supports reuse of design knowledge modeled as SDL patterns and concentrates on the design phase of SDL-based system development. In order to get full life-cycle support, the pattern-based design process can be integrated within existing SDL methodologies.},
  comment       = {19},
  doi           = {https://doi.org/10.1016/S1389-1286(00)00202-4},
  keywords      = {SDL, Design methodology, Software reuse, Patterns, Distributed systems, Process model},
  url           = {http://www.sciencedirect.com/science/article/pii/S1389128600002024},
}

@Article{Westman2017,
  author        = {Jonas Westman and Mattias Nyberg and Joakim Gustavsson and Dilian Gurov},
  title         = {Formal architecture modeling of sequential non-recursive C programs},
  journal       = {Science of Computer Programming},
  year          = {2017},
  volume        = {146},
  pages         = {2 - 27},
  issn          = {0167-6423},
  note          = {Special issue with extended selected papers from FACS 2015},
  __markedentry = {[mac:]},
  abstract      = {To manage the complexity of C programs, architecture models are used as high-level descriptions, allowing developers to understand, assess, and manage the C programs without having to understand the intricate complexity of the code implementations. However, for the architecture models to serve their purpose, they must be accurate representations of the C programs. In order to support creating accurate architecture models, the present paper presents a mapping from the domain of sequential non-recursive C programs to a domain of formal architecture models, each being a hierarchy of components with well-defined interfaces. The hierarchically organized components and their interfaces, which capture both data and function call dependencies, are shown to both enable high-level assessment and analysis of the C program and provide a foundation for organizing and expressing specifications for compositional verification.},
  comment       = {26},
  doi           = {https://doi.org/10.1016/j.scico.2017.03.007},
  keywords      = {C program, Modeling, Architecture, Component, Interfaces},
  url           = {http://www.sciencedirect.com/science/article/pii/S0167642317300564},
}

@Article{Diskin2016,
  author        = {Zinovy Diskin and Hamid Gholizadeh and Arif Wider and Krzysztof Czarnecki},
  title         = {A three-dimensional taxonomy for bidirectional model synchronization},
  journal       = {Journal of Systems and Software},
  year          = {2016},
  volume        = {111},
  pages         = {298 - 322},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {Early model-driven engineering (MDE) assumed simple pipeline-like scenarios specified by the Model-Driven Architecture approach: platform-independent models that describe a software system at a high-level of abstraction are transformed stepwise to platform-dependent models from which executable source code is generated. Modern applications require a shift toward networks of models related in various ways, whose synchronization often needs to be incremental and bidirectional. This new situation demands new features from transformation tools, and a solid semantic foundation to understand and classify these features. We address the problem by presenting a taxonomy of model synchronization types, organized into a 3D-space. Each point in the space refers to a specific synchronization semantics with an underlying algebraic model and the respective requirements for the change propagation operations and their properties. The taxonomy aims to help with identifying and communicating a proper specification for the synchronization problem at hand and for the available solutions offered by tools.},
  comment       = {25},
  doi           = {https://doi.org/10.1016/j.jss.2015.06.003},
  keywords      = {Model synchronization, Taxonomy, Formal semantics},
  url           = {http://www.sciencedirect.com/science/article/pii/S016412121500120X},
}

@Article{Khalil2014,
  author        = {Issa M. Khalil and Abdallah Khreishah and Faheem Ahmed and Khaled Shuaib},
  title         = {Dependable wireless sensor networks for reliable and secure humanitarian relief applications},
  journal       = {Ad Hoc Networks},
  year          = {2014},
  volume        = {13},
  pages         = {94 - 106},
  issn          = {1570-8705},
  note          = {(1)Special Issue : Wireless Technologies for Humanitarian Relief \& (2)Special Issue: Models And Algorithms For Wireless Mesh Networks},
  __markedentry = {[mac:]},
  abstract      = {Disasters such as flooding, earthquake, famine and terrorist attacks might occur any time anywhere without prior warnings. In most cases it is difficult to predict when a disaster might occur however, well-planned disaster recovery procedures will reduce the intensity of expected consequences. When a disaster occurs, infrastructure based communications are most likely to be crippled, worsening the critical situation on hand. Wireless ad hoc and sensor network (WASN) technologies are proven to be valuable in coordinating and managing rescue operations during disasters. However, the increasing reliance on WASNs make them attractive to malicious attackers, especially terrorist groups, in a bid to hamper rescue operations amplifying the damage and increasing the number of casualties. Therefore, it is necessary to ensure the fidelity of data traffic through WASN against malicious traffic disruption attacks. In this paper, we first demonstrate how WASN can be used in a well-planned disaster recovery effort. Then, we introduce and analyze one of the most severe traffic disruption attacks against WASNs, called Identity Delegation, and its countermeasures. Its severity lies in its capability to evade detection by even state-of-the-art intrusion detection techniques such as the neighbor monitoring based mechanisms. Through identity delegation, an adversary can drop packets, evade detection, and frame innocent nodes for dropping the traffic. We introduce a technique to mitigate identity delegation attack, dubbed Sadec, and compare it with the state-of-the-art mitigation technique namely Basic Local Monitoring (BLM) under a wide range of network scenarios. Our analysis which is validated by extensive ns-2 simulation scenarios show that BLM fails to efficiently mitigate packet drop through identity delegation attacks while Sadec successfully mitigates them. The results also show that Sadec achieves higher delivery ratios of data packets compared to BLM. On the other hand, the results show similar behavior in framing probabilities between Sadec and BLM. However, the desirable features of Sadec come at the expense of higher false isolation probabilities in networks with heavy traffic load and poor communication links.},
  comment       = {13},
  doi           = {https://doi.org/10.1016/j.adhoc.2012.06.002},
  keywords      = {Local monitoring, Identity delegation, Multi-hop wireless networks, Packet dropping, Security attacks},
  url           = {http://www.sciencedirect.com/science/article/pii/S1570870512001102},
}

@Article{Jansen2008,
  author        = {Anton Jansen and Jan Bosch and Paris Avgeriou},
  title         = {Documenting after the fact: Recovering architectural design decisions},
  journal       = {Journal of Systems and Software},
  year          = {2008},
  volume        = {81},
  number        = {4},
  pages         = {536 - 557},
  issn          = {0164-1212},
  note          = {Selected papers from the 10th Conference on Software Maintenance and Reengineering (CSMR 2006)},
  __markedentry = {[mac:]},
  abstract      = {Software architecture documentation helps people in understanding the software architecture of a system. In practice, software architectures are often documented after the fact, i.e. they are maintained or created after most of the design decisions have been made and implemented. To keep the architecture documentation up-to-date an architect needs to recover and describe these decisions. This paper presents ADDRA, an approach an architect can use for recovering architectural design decisions after the fact. ADDRA uses architectural deltas to provide the architect with clues about these design decisions. This allows the architect to systematically recover and document relevant architectural design decisions. The recovered architectural design decisions improve the documentation of the architecture, which increases traceability, communication, and general understanding of a system.},
  comment       = {22},
  doi           = {https://doi.org/10.1016/j.jss.2007.08.025},
  keywords      = {Architectural design decisions, Software architecture recovery},
  url           = {http://www.sciencedirect.com/science/article/pii/S016412120700194X},
}

@Article{Lee2016,
  author        = {Seonah Lee and Sungwon Kang},
  title         = {What situational information would help developers when using a graphical code recommender?},
  journal       = {Journal of Systems and Software},
  year          = {2016},
  volume        = {117},
  pages         = {199 - 217},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {Developers spend a significant amount of time trying to understand code bases. To aid developersâ€™ comprehension of code, researchers have developed software visualization tools. However, the uses of these tools in situ have rarely been investigated. To make matters worse, as studies have revealed, developers seldom use diagramming tools, making such investigations a challenge. To determine the possible uses of such tools in real practice, we conduct a diary study in which eleven developers in real-world developments use a novel visualization tool (a graphical code recommender) for one month. In the study, we ask what information and features the visualization and diagramming tools should provide to aid developersâ€™ work according to their situations. The study reveals the situations in which developers would use such visualization and diagramming tools and also the concrete requirements for such tools that would make them useful.},
  comment       = {19},
  doi           = {https://doi.org/10.1016/j.jss.2016.02.050},
  keywords      = {Diary study, Design requirement, Diagramming tool, Code navigation, Software visualization},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121216000819},
}

@Article{Oliveira2007,
  author        = {Toacy C. Oliveira and Paulo S.C. Alencar and Carlos J.P. de Lucena and Donald D. Cowan},
  title         = {RDL: A language for framework instantiation representation},
  journal       = {Journal of Systems and Software},
  year          = {2007},
  volume        = {80},
  number        = {11},
  pages         = {1902 - 1929},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {Reusing software artifacts for system development is showing increasing promise as an approach to reducing the time and effort involved in building new systems, and to improving the software development process and the quality of its outcome. However, software reuse has an associated steep learning curve, since practitioners must become familiar with a third party rationale for representing and implementing reusable assets. For this reason, enabling a systematic approach to the reuse process by making software reuse tasks explicit, allowing software frameworks to be instantiated using pre-defined primitive and complex reuse operations, and supporting the reuse process in a (semi-)automated way become crucial goals. In this paper, we present a systematic reuse approach and the Reuse Description Language (RDL), a language designed to specify object-oriented framework instantiation processes, and an RDL execution environment, which is the tool support for definition and execution of reuse processes and framework instantiations that lead to domain-specific applications. We illustrate our approach using DTFrame, a framework for creating drawing editors.},
  comment       = {28},
  doi           = {https://doi.org/10.1016/j.jss.2007.01.005},
  keywords      = {Software reuse, Object oriented framework, Model driven architecture, Product line architecture},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121207000106},
}

@Article{Groener2014,
  author        = {Gerd GrÃ¶ner and Mohsen Asadi and Bardia Mohabbati and Dragan GaÅ¡eviÄ‡ and Marko BoÅ¡koviÄ‡ and Fernando Silva Parreiras},
  title         = {Validation of user intentions in process orchestration and choreography},
  journal       = {Information Systems},
  year          = {2014},
  volume        = {43},
  pages         = {83 - 99},
  issn          = {0306-4379},
  __markedentry = {[mac:]},
  abstract      = {Goal models and business process models are complementary artifacts for capturing the requirements and their execution flow in software engineering. In this case, goal models serve as input for designing business process models. This requires mappings between both types of models in order to describe which user goals are implemented by which activities in a business process. Due to the large number of possible relationships among goals in the goal model and possible control flows of activities, developers struggle with the challenge of maintaining consistent configurations of both models and their mappings. Managing these mappings manually is error-prone. In our work, we propose an automated solution that relies on Description Logics and automated reasoners for validating mappings that describe the realization of goals by activities in business process models. The results are the identification of two inconsistency patterns â€“ orchestration inconsistency and choreography inconsistency â€“ and the development of the corresponding algorithms for detecting these inconsistencies.},
  comment       = {17},
  doi           = {https://doi.org/10.1016/j.is.2013.05.006},
  keywords      = {Requirement modeling, Goal-oriented process engineering, Inconsistency detection, Goal-oriented process design},
  url           = {http://www.sciencedirect.com/science/article/pii/S0306437913000756},
}

@Article{Giovannini2014,
  author        = {A. Giovannini and A. Aubry and H. Panetto and H. El Haouzi and L. Pierrel and M. Dassisti},
  title         = {Approach for the rationalisation of product lines variety},
  journal       = {IFAC Proceedings Volumes},
  year          = {2014},
  volume        = {47},
  number        = {3},
  pages         = {3280 - 3291},
  issn          = {1474-6670},
  note          = {19th IFAC World Congress},
  __markedentry = {[mac:]},
  abstract      = {The product variety management is a key process to deal with the flexibility requested by the mass customisation. In this paper we show that current variety-modelling methods miss a customer representation: without a proper assessment of the customers is not possible to define the product variety that has to be developed to meet the requirements of a customer segment. Here we present an innovative approach to rationalise the product variety, i.e. to link each product variant to the customer profile who needs it. The aim is to optimise the product variety avoiding excesses (variants not related to a customer), lacks (customers not related to a variant) or redundancies (two or more variants proposed to a customer). An overview of customer modelling approaches in the classic product design (non-customisable) is presented. The innovative approach is here developed using system-thinking concepts. A knowledge-based system that uses this approach is designed. Finally the approach is explained using a real industrial case of a quasi-real coil design process.},
  comment       = {12},
  doi           = {https://doi.org/10.3182/20140824-6-ZA-1003.02226},
  keywords      = {Mass customization, Product variety, Knowledge representation, Knowledge-based system},
  url           = {http://www.sciencedirect.com/science/article/pii/S1474667016421130},
}

@Article{Hallsteinsen2012,
  author        = {S. Hallsteinsen and K. Geihs and N. Paspallis and F. Eliassen and G. Horn and J. Lorenzo and A. Mamelli and G.A. Papadopoulos},
  title         = {A development framework and methodology for self-adapting applications in ubiquitous computing environments},
  journal       = {Journal of Systems and Software},
  year          = {2012},
  volume        = {85},
  number        = {12},
  pages         = {2840 - 2859},
  issn          = {0164-1212},
  note          = {Self-Adaptive Systems},
  __markedentry = {[mac:]},
  abstract      = {Today software is the main enabler of many of the appliances and devices omnipresent in our daily life and important for our well being and work satisfaction. It is expected that the software works as intended, and that the software always and everywhere provides us with the best possible utility. This paper discusses the motivation, technical approach, and innovative results of the MUSIC project. MUSIC provides a comprehensive software development framework for applications that operate in ubiquitous and dynamic computing environments and adapt to context changes. Context is understood as any information about the user needs and operating environment which vary dynamically and have an impact on design choices. MUSIC supports several adaptation mechanisms and offers a model-driven application development approach supported by a sophisticated middleware that facilitates the dynamic and automatic adaptation of applications and services based on a clear separation of business logic, context awareness and adaptation concerns. The main contribution of this paper is a holistic, coherent presentation of the motivation, design, implementation, and evaluation of the MUSIC development framework and methodology.},
  comment       = {29},
  doi           = {https://doi.org/10.1016/j.jss.2012.07.052},
  keywords      = {Adaptive software, Ubiquitous computing, Model-driven development, Middleware, Mobile computing},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121212002245},
}

@Article{Petersen2010,
  author        = {Kai Petersen and Claes Wohlin},
  title         = {Software process improvement through the Lean Measurement (SPI-LEAM) method},
  journal       = {Journal of Systems and Software},
  year          = {2010},
  volume        = {83},
  number        = {7},
  pages         = {1275 - 1287},
  issn          = {0164-1212},
  note          = {SPLC 2008},
  __markedentry = {[mac:]},
  abstract      = {Software process improvement methods help to continuously refine and adjust the software process to improve its performance (e.g., in terms of lead-time, quality of the software product, reduction of change requests, and so forth). Lean software development propagates two important principles that help process improvement, namely identification of waste in the process and considering interactions between the individual parts of the software process from an end-to-end perspective. A large shift of thinking about the own way of working is often required to adopt lean. One of the potential main sources of failure is to try to make a too large shift about the ways of working at once. Therefore, the change to lean has to be done in a continuous and incremental way. In response to this we propose a novel approach to bring together the quality improvement paradigm and lean software development practices, the approach being called Software Process Improvement through the Lean Measurement (SPI-LEAM) Method. The method allows to assess the performance of the development process and take continuous actions to arrive at a more lean software process over time. The method is under implementation in industry and an initial evaluation of the method has been performed.},
  comment       = {13},
  doi           = {https://doi.org/10.1016/j.jss.2010.02.005},
  keywords      = {Lean software development, Software process improvement, Quality improvement paradigm},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121210000403},
}

@Article{Abate2013,
  author        = {Pietro Abate and Roberto Di Cosmo and Ralf Treinen and Stefano Zacchiroli},
  title         = {A modular package manager architecture},
  journal       = {Information and Software Technology},
  year          = {2013},
  volume        = {55},
  number        = {2},
  pages         = {459 - 474},
  issn          = {0950-5849},
  note          = {Special Section: Component-Based Software Engineering (CBSE), 2011},
  __markedentry = {[mac:]},
  abstract      = {Context
The success of modern software distributions in the Free and Open Source world can be explained, among other factors, by the availability of a large collection of software packages and the possibility to easily install and remove those components using state-of-the-art package managers. However, package managers are often built using a monolithic architecture and hard-wired and ad-hoc dependency solvers implementing some customized heuristics.
Objective
We aim at laying the foundation for improving on existing package managers. Package managers should be complete, that is find a solution whenever there exists one, and allow the user to specify complex criteria that define how to pick the best solution according to the userâ€™s preferences.
Method
In this paper we propose a modular architecture relying on precise interface formalisms that allows the system administrator to choose from a variety of dependency solvers and backends.
Results
We have built a working prototypeâ€“called MPMâ€“following the design advocated in this paper, and we show how it largely outperforms a variety of current package managers.
Conclusion
We argue that a modular architecture, allowing for delegating the task of constraint solving to external solvers, is the path that leads to the next generation of package managers that will deliver better results, offer more expressive preference languages, and be easily adaptable to new platforms.},
  comment       = {16},
  doi           = {https://doi.org/10.1016/j.infsof.2012.09.002},
  keywords      = {Software dependencies, Software repositories, Software components, Package manager, Open source},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584912001851},
}

@Article{Baghdadi2016,
  author        = {Youcef Baghdadi},
  title         = {A framework for social commerce design},
  journal       = {Information Systems},
  year          = {2016},
  volume        = {60},
  pages         = {95 - 113},
  issn          = {0306-4379},
  __markedentry = {[mac:]},
  abstract      = {Interaction features of social web sites, including social networks and social media, enable a new kind of commerce referred to as social commerce (s-commerce). It refers to doing commerce in a collaborative and participative way, through a uniform and interactive enterprise interface, by extending current social web sites initially designed for social interactions of individuals, to promote new business models. On one hand, none of the major social networks or social media providers has yet figured out how to bring commercial transactions directly to their platforms. On the other hand, there is a lack of a comprehensive framework to shape social commerce from both business and IT perspectives, which would guide a design process of s-commerce platforms. Indeed, s-commerce platforms differ from e-commerce web sites in many aspects from both business and IT perspectives and has more challenges in terms of (i) business models, architectures, principles, and even theories, (ii) complex constructs in terms of participants, interaction features, communities, and content, and (iii) issues such as social, control, security, and privacy issues. Therefore, there is a need for framing the elements of s-commerce, focusing on enterprise social interactions as first class citizens, in an abstract model that guides the architecture, the requirement engineering, the design, and the implementation of a uniform and interactive enterprise interface. Only this enterprise social interaction-enabling interface would promote the emerging knowledge and intelligence that are required for value (co-) creation in s-commerce model. This work fills the gap by proposing a framework that guides a design process to develop s-commerce.},
  comment       = {19},
  doi           = {https://doi.org/10.1016/j.is.2016.03.007},
  keywords      = {Social commerce, Enterprise social interactions, Social commerce framework, Social design, Social commerce design process},
  url           = {http://www.sciencedirect.com/science/article/pii/S0306437916301144},
}

@Article{Cabanillas2015,
  author        = {Cristina Cabanillas and Manuel Resinas and Adela del-RÃ­o-Ortega and Antonio Ruiz-CortÃ©s},
  title         = {Specification and automated design-time analysis of the business process human resource perspective},
  journal       = {Information Systems},
  year          = {2015},
  volume        = {52},
  pages         = {55 - 82},
  issn          = {0306-4379},
  note          = {Special Issue on Selected Papers from SISAP 2013},
  __markedentry = {[mac:]},
  abstract      = {The human resource perspective of a business process is concerned with the relation between the activities of a process and the actors who take part in them. Unlike other process perspectives, such as control flow, for which many different types of analyses have been proposed, such as finding deadlocks, there is an important gap regarding the human resource perspective. Resource analysis in business processes has not been defined, and only a few analysis operations can be glimpsed in previous approaches. In this paper, we identify and formally define seven design-time analysis operations related to how resources are involved in process activities. Furthermore, we demonstrate that for a wide variety of resource-aware BP models, those analysis operations can be automated by leveraging Description Logic (DL) off-the-shelf reasoners. To this end, we rely on Resource Assignment Language (RAL), a domain-specific language that enables the definition of conditions to select the candidates to participate in a process activity. We provide a complete formal semantics for RAL based on DLs and extend it to address the operations, for which the control flow of the process must also be taken into consideration. A proof-of-concept implementation has been developed and integrated in a system called CRISTAL. As a result, we can give an automatic answer to different questions related to the management of resources in business processes at design time.},
  comment       = {28},
  doi           = {https://doi.org/10.1016/j.is.2015.03.002},
  keywords      = {Automated analysis, Analysis operation, Business process management, Human resource perspective, RAL, Resource assignment},
  url           = {http://www.sciencedirect.com/science/article/pii/S0306437915000460},
}

@Article{Kazman2006,
  author        = {Rick Kazman and Len Bass and Mark Klein},
  title         = {The essential components of software architecture design and analysis},
  journal       = {Journal of Systems and Software},
  year          = {2006},
  volume        = {79},
  number        = {8},
  pages         = {1207 - 1216},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {Architecture analysis and design methods such as ATAM, QAW, ADD and CBAM have enjoyed modest success and are being adopted by many companies as part of their standard software development processes. They are used in the lifecycle, as a means of understanding business goals and stakeholders concerns, mapping these onto an architectural representation, and assessing the risks associated with this mapping. These methods have evolved a set of shared component techniques. In this paper we show how these techniques can be combined in countless ways to create needs-specific methods in an agile way. We demonstrate the generality of these techniques by describing a new architecture improvement method called APTIA (Analytic Principles and Tools for the Improvement of Architectures). APTIA almost entirely reuses pre-existing techniques but in a new combination, with new goals and results. We exemplify APTIAâ€™s use in improving the architecture of a commercial information system.},
  comment       = {10},
  doi           = {https://doi.org/10.1016/j.jss.2006.05.001},
  keywords      = {Software architecture, Analysis methodologies, Design methodologies},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121206001439},
}

@Article{Uzunov2015a,
  author        = {Anton V. Uzunov and Katrina Falkner and Eduardo B. Fernandez},
  title         = {A comprehensive pattern-oriented approach to engineering security methodologies},
  journal       = {Information and Software Technology},
  year          = {2015},
  volume        = {57},
  pages         = {217 - 247},
  issn          = {0950-5849},
  __markedentry = {[mac:]},
  abstract      = {Context
Developing secure software systems is an issue of ever-growing importance. Researchers have generally come to acknowledge that to develop such systems successfully, their security features must be incorporated in the context of a systematic approach: a security methodology. There are a number of such methodologies in the literature, but no single security methodology is adequate for every situation, requiring the construction of â€œfit-to-purposeâ€ methodologies or the tailoring of existing methodologies to the project specifics at hand. While a large body of research exists addressing the same requirement for development methodologies â€“ constituting the field of Method Engineering â€“ there is nothing comparable for security methodologies as such; in fact, the topic has never been studied before in such a context.
Objective
In this paper we draw inspiration from a number of Method Engineering ideas and fill the latter gap by proposing a comprehensive approach to engineering security methodologies.
Method
Our approach is embodied in three interconnected parts: a framework of interrelated security process patterns; a security-specific meta-model; and a meta-methodology to guide engineers in using the latter artefacts in a step-wise fashion. A UML-inspired notation is used for representing all pattern-based methodology models during design and construction. The approach is illustrated and evaluated by tailoring an existing, real-life security methodology to a distributed-system-specific project situation.
Results
The paper proposes a novel pattern-oriented approach to modeling, constructing, tailoring and combining security methodologies, which is the very first and currently sole such approach in the literature. We illustrate and evaluate our approach in an academic setting, and perform a feature analysis to highlight benefits and deficiencies.
Conclusion
Using our proposal, developers, architects and researchers can analyze and engineer security methodologies in a structured, systematic fashion, taking into account all security methodology aspects.},
  comment       = {31},
  doi           = {https://doi.org/10.1016/j.infsof.2014.09.001},
  keywords      = {Secure software engineering, Security methodologies, Method engineering, Process patterns, Software security, Modeling},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584914002006},
}

@Article{Aalst2005,
  author        = {Wil M.P. van der Aalst and Mathias Weske and Dolf GrÃ¼nbauer},
  title         = {Case handling: a new paradigm for business process support},
  journal       = {Data \& Knowledge Engineering},
  year          = {2005},
  volume        = {53},
  number        = {2},
  pages         = {129 - 162},
  issn          = {0169-023X},
  __markedentry = {[mac:]},
  abstract      = {Case handling is a new paradigm for supporting flexible and knowledge intensive business processes. It is strongly based on data as the typical product of these processes. Unlike workflow management, which uses predefined process control structures to determine what should be done during a workflow process, case handling focuses on what can be done to achieve a business goal. In case handling, the knowledge worker in charge of a particular case actively decides on how the goal of that case is reached, and the role of a case handling system is assisting rather than guiding her in doing so. In this paper, case handling is introduced as a new paradigm for supporting flexible business processes. It is motivated by comparing it to workflow management as the traditional way to support business processes. The main entities of case handling systems are identified and classified in a meta model. Finally, the basic functionality and usage of a case handling system is illustrated by an example.},
  comment       = {34},
  doi           = {https://doi.org/10.1016/j.datak.2004.07.003},
  keywords      = {Case handling, Workflow management systems, Adaptive workflow, Flexibility, Business process management},
  url           = {http://www.sciencedirect.com/science/article/pii/S0169023X04001296},
}

@Article{Zilli2015,
  author        = {Massimiliano Zilli and Wolfgang Raschke and Reinhold Weiss and Johannes Loinig and Christian Steger},
  title         = {Hardware/software co-design for a high-performance Java Card interpreter in low-end embedded systems},
  journal       = {Microprocessors and Microsystems},
  year          = {2015},
  volume        = {39},
  number        = {8},
  pages         = {1076 - 1086},
  issn          = {0141-9331},
  __markedentry = {[mac:]},
  abstract      = {Java Card is a Java running environment specific for smart cards. In such low-end embedded systems, the execution time of the applications is an issue of first order. One of the components of the Java Card Virtual Machine (JCVM) playing an important role in the execution speed is the bytecode interpreter. In Java systems the main technique for speeding-up the interpreter execution is the Just-In-Time compilation (JIT), but this resource consuming technique is inapplicable in systems with as restricted resources available as in smart cards. This paper presents a hardware/software co-design solution for the performance improvement of the interpreter. In the software domain, we adopted a pseudo-threaded code interpreter that allows a better run-time performance with a small amount of additional code. In the hardware domain, we proceeded moving parts of the interpreter into hardware, giving origin to a Java Card interpreter based on an application specific instruction set processor.},
  comment       = {11},
  doi           = {https://doi.org/10.1016/j.micpro.2015.05.004},
  keywords      = {Hardware/software co-design, Smart card, Java Card, Java interpreter, Hardware-supported interpreter, Application specific instruction set processor},
  url           = {http://www.sciencedirect.com/science/article/pii/S0141933115000563},
}

@Article{Bettini2013a,
  author        = {Lorenzo Bettini and Ferruccio Damiani and Kathrin Geilmann and Jan SchÃ¤fer},
  title         = {Combining traits with boxes and ownership types in a Java-like setting},
  journal       = {Science of Computer Programming},
  year          = {2013},
  volume        = {78},
  number        = {2},
  pages         = {218 - 247},
  issn          = {0167-6423},
  note          = {Coordination 2010},
  __markedentry = {[mac:]},
  abstract      = {The box model is a lightweight component model for the object-oriented paradigm, which structures the flat object-heap into hierarchical runtime components called boxes. Boxes have clear runtime boundaries that divide the objects of a box into objects that can be used to interact with the box (the boundary objects) and objects that are encapsulated and represent the state of the box (the local objects). The distinction into local and boundary objects is statically achieved by an ownership type system for boxes that uses domain annotations to classify objects into local and boundary objects and that guarantees that local objects can never be directly accessed by the context of a box. A trait is a set of methods divorced from any class hierarchy. Traits are units of fine-grained reuse that can be composed together to form classes or other traits. This paper integrates traits into an ownership type system for boxes. This combination is fruitful in two ways: it can statically guarantee encapsulation of objects and still provide fine-grained reuse among classes that goes beyond the possibilities of standard inheritance. It also solves a specific problem of the box ownership type system: namely that box classes cannot inherit from standard classes (and vice versa), and thus code sharing between these two kinds of classes was not possible in this setting so far. We present an ownership type system and the corresponding soundness proofs that guarantee encapsulation of objects in an object-oriented language with traits.},
  comment       = {30},
  doi           = {https://doi.org/10.1016/j.scico.2011.10.006},
  keywords      = {Boxes, Featherweight Java, Ownership types, Traits},
  url           = {http://www.sciencedirect.com/science/article/pii/S0167642311001833},
}

@Article{Anaya2010,
  author        = {Victor Anaya and Giuseppe Berio and Mounira Harzallah and Patrick Heymans and Raimundas MatuleviÄius and Andreas L. Opdahl and HervÃ© Panetto and Maria Jose Verdecho},
  title         = {The Unified Enterprise Modelling Languageâ€”Overview and further work},
  journal       = {Computers in Industry},
  year          = {2010},
  volume        = {61},
  number        = {2},
  pages         = {99 - 111},
  issn          = {0166-3615},
  note          = {Integration and Information in Networked Enterprises},
  __markedentry = {[mac:]},
  abstract      = {The Unified Enterprise Modelling Language (UEML) aims at supporting integrated use of enterprise and IS models expressed using different languages. To achieve this aim, UEML offers a hub through which modelling languages can be connected, thereby paving the way for also connecting the models expressed in those languages. This paper motivates and presents the most central parts of the UEML approach: a structured path to describing enterprise and IS modelling constructs; a common ontology to interrelate construct descriptions at the semantic level; a correspondence analysis approach to estimate semantic construct similarity; a quality framework to aid selection of languages; a meta-meta model to integrate the different parts of the approach; and a set of tools to aid its use and evolution. The paper also discusses the benefits of UEML and points to paths for further work.},
  comment       = {13},
  doi           = {https://doi.org/10.1016/j.compind.2009.10.013},
  keywords      = {Enterprise modelling, Information Systems Modelling, Modelling Languages, Interoperability, Unified Enterprise Modelling Language (UEML), Ontology, OWL, Bungeâ€“Wandâ€“Weber Model},
  url           = {http://www.sciencedirect.com/science/article/pii/S0166361509002061},
}

@Article{Gurp2002,
  author        = {Jilles van Gurp and Jan Bosch},
  title         = {Design erosion: problems and causes},
  journal       = {Journal of Systems and Software},
  year          = {2002},
  volume        = {61},
  number        = {2},
  pages         = {105 - 119},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {Design erosion is a common problem in software engineering. We have found that invariably, no matter how ambitious the intentions of the designers were, software designs tend to erode over time to the point that redesigning from scratch becomes a viable alternative compared to prolonging the life of the existing design. In this paper, we illustrate how design erosion works by presenting the evolution of the design of a small software system. In our analysis of this example, we show how design decisions accumulate and become invalid because of new requirements. Also it is argued that even an optimal strategy for designing the system (i.e. no compromises with respect to e.g. cost are made) does not lead to an optimal design because of unforeseen requirement changes that invalidate design decisions that were once optimal.},
  comment       = {15},
  doi           = {https://doi.org/10.1016/S0164-1212(01)00152-2},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121201001522},
}

@Article{Tang2010,
  author        = {Antony Tang and Paris Avgeriou and Anton Jansen and Rafael Capilla and Muhammad Ali Babar},
  title         = {A comparative study of architecture knowledge management tools},
  journal       = {Journal of Systems and Software},
  year          = {2010},
  volume        = {83},
  number        = {3},
  pages         = {352 - 370},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {Recent research suggests that architectural knowledge, such as design decisions, is important and should be recorded alongside the architecture description. Different approaches have emerged to support such architectural knowledge (AK) management activities. However, there are different notions of and emphasis on what and how architectural activities should be supported. This is reflected in the design and implementation of existing AK tools. To understand the current status of software architecture knowledge engineering and future research trends, this paper compares five architectural knowledge management tools and the support they provide in the architecture life-cycle. The comparison is based on an evaluation framework defined by a set of 10 criteria. The results of the comparison provide insights into the current focus of architectural knowledge management support, their advantages, deficiencies, and conformance to the current architectural description standard. Based on the outcome of this comparison a research agenda is proposed for future work on AK tools.},
  comment       = {19},
  doi           = {https://doi.org/10.1016/j.jss.2009.08.032},
  keywords      = {Architectural knowledge management tool, Architectural design, Design rationale},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121209002295},
}

@Article{Coman2014,
  author        = {Irina D. Coman and Pierre N. Robillard and Alberto Sillitti and Giancarlo Succi},
  title         = {Cooperation, collaboration and pair-programming: Field studies on backup behavior},
  journal       = {Journal of Systems and Software},
  year          = {2014},
  volume        = {91},
  pages         = {124 - 134},
  issn          = {0164-1212},
  __markedentry = {[mac:]},
  abstract      = {Considering that pair programming has been extensively studied for more than a decade, it can seem quite surprising that there is such a lack of consensus on both its best use and its benefits. We argue that pair programming is not a replacement of usual developer interactions, but rather a formalization and enhancement of naturally occurring interactions. Consequently, we study and classify a broader range of developer interactions, evaluating them for type, purpose and patterns of occurrence, with the aim to identify situations in which pair programming is likely to be truly needed and thus most beneficial. We study the concrete pair programming practices in both academic and industrial settings. All interactions between teammates were recorded as backup behavior activities. In each of these two projects, developers were free to interact when needed. All team interactions were self-recorded by the teammates. The analysis of the interaction tokens shows two salient features: solo work is an important component of teamwork and team interactions have two main purposes, namely cooperation and collaboration. Cooperative backup behavior occurs when a developer provides help to a teammate. Collaborative backup behavior occurs when the teammates are sharing the same goal toward solving an issue. We found that collaborative backup behavior, which occurred much less often, is close to the formal definition of pair programming. This study suggests that mandatory pair programming may be less efficient in organizations where solo work could be done and when some interactions are for cooperative activities. Based on these results, we discussed the potential implications concerning the best use of pair programming in practice, a more effective evaluation of its use, its potential benefits and emerging directions of future research.},
  comment       = {11},
  doi           = {https://doi.org/10.1016/j.jss.2013.12.037},
  keywords      = {Backup-behavior, Pair-programming, Field study},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121214000107},
}

@Article{Li2014,
  author        = {Wei Li and FlÃ¡via C. Delicato and Paulo F. Pires and Young Choon Lee and Albert Y. Zomaya and Claudio Miceli and Luci Pirmez},
  title         = {Efficient allocation of resources in multiple heterogeneous Wireless Sensor Networks},
  journal       = {Journal of Parallel and Distributed Computing},
  year          = {2014},
  volume        = {74},
  number        = {1},
  pages         = {1775 - 1788},
  issn          = {0743-7315},
  __markedentry = {[mac:]},
  abstract      = {Wireless Sensor Networks (WSNs) are useful for a wide range of applications, from different domains. Recently, new features and design trends have emerged in the WSN field, making those networks appealing not only to the scientific community but also to the industry. One such trend is the running different applications on heterogeneous sensor nodes deployed in multiple WSNs in order to better exploit the expensive physical network infrastructure. Another trend deals with the capability of accessing sensor generated data from the Web, fitting WSNs in novel paradigms of Internet of Things (IoT) and Web of Things (WoT). Using well-known and broadly accepted Web standards and protocols enables the interoperation of heterogeneous WSNs and the integration of their data with other Web resources, in order to provide the final user with value-added information and applications. Such emergent scenarios where multiple networks and applications interoperate to meet high level requirements of the user will pose several changes in the design and execution of WSN systems. One of these challenges regards the fact that applications will probably compete for the resources offered by the underlying sensor nodes through the Web. Thus, it is crucial to design mechanisms that effectively and dynamically coordinate the sharing of the available resources to optimize resource utilization while meeting application requirements. However, it is likely that Quality of Service (QoS) requirements of different applications cannot be simultaneously met, while efficiently sharing the scarce networks resources, thus bringing the need of managing an inherent tradeoff. In this paper, we argue that a middleware platform is required to manage heterogeneous WSNs and efficiently share their resources while satisfying user needs in the emergent scenarios of WoT. Such middleware should provide several services to control running application as well as to distribute and coordinate nodes in the execution of submitted sensing tasks in an energy-efficient and QoS-enabled way. As part of the middleware provided services we present the Resource Allocation in Heterogeneous WSNs (SACHSEN) algorithm. SACHSEN is a new resource allocation heuristic for systems composed of heterogeneous WSNs that effectively deals with the tradeoff between possibly conflicting QoS requirements and exploits heterogeneity of multiple WSNs.},
  comment       = {14},
  doi           = {https://doi.org/10.1016/j.jpdc.2013.09.012},
  keywords      = {Wireless sensor network, Resource allocation, Task allocation},
  url           = {http://www.sciencedirect.com/science/article/pii/S0743731513002104},
}

@Article{Blake2005,
  author        = {M.Brian Blake and Hassan Gomaa},
  title         = {Agent-oriented compositional approaches to services-based cross-organizational workflow},
  journal       = {Decision Support Systems},
  year          = {2005},
  volume        = {40},
  number        = {1},
  pages         = {31 - 50},
  issn          = {0167-9236},
  note          = {Web services and process management},
  __markedentry = {[mac:]},
  abstract      = {With the sophistication and maturity of distributed component-based services and semantic web services, the idea of specification-driven service composition is becoming a reality. One such approach is workflow composition of services that span multiple, distributed web-accessible locations. Given the dynamic nature of this domain, the adaptation of software agents represents a possible solution for the composition and enactment of cross-organizational services. This paper details design aspects of an architecture that would support this evolvable service-based workflow composition. The internal coordination and control aspects of such an architecture is addressed. These agent developmental processes are aligned with industry-standard software engineering processes.},
  comment       = {20},
  doi           = {https://doi.org/10.1016/j.dss.2004.04.003},
  keywords      = {Agent architectures, Workflow modeling, Coordination, UML, Web services},
  url           = {http://www.sciencedirect.com/science/article/pii/S0167923604000624},
}

@Article{Menasce2007,
  author        = {Daniel A. MenascÃ© and Honglei Ruan and Hassan Gomaa},
  title         = {QoS management in service-oriented architectures},
  journal       = {Performance Evaluation},
  year          = {2007},
  volume        = {64},
  number        = {7},
  pages         = {646 - 663},
  issn          = {0166-5316},
  __markedentry = {[mac:]},
  abstract      = {The next generation of software systems will be highly distributed, component-based and service-oriented. They will need to operate in unattended mode and possibly in hostile environments, will be composed of a large number of â€˜replaceableâ€™ components discoverable at run-time, and will have to run on a multitude of unknown and heterogeneous hardware and network platforms. This paper focuses on QoS management in service-oriented architectures in which service providers (SP) provide a set of interrelated services to service consumers, and a QoS broker mediates QoS negotiations between SPs and consumers. The main contributions of this paper are: (i) the description of an architecture that includes a QoS broker and service provider software components, (ii) the specification of a secure protocol for QoS negotiation with the support of a QoS broker, (iii) the specification of an admission control mechanism used by SPs, (iv) a report on the implementation of the QoS broker and SPs, and (v) the experimental validation of the ideas presented in the paper.},
  comment       = {18},
  doi           = {https://doi.org/10.1016/j.peva.2006.10.001},
  keywords      = {QoS, Service oriented architectures, Performance, QoS broker},
  url           = {http://www.sciencedirect.com/science/article/pii/S0166531606000940},
}

@Article{Sillitti2004,
  author        = {Alberto Sillitti and Andrea Janes and Giancarlo Succi and Tullio Vernazza},
  title         = {Measures for mobile users: an architecture},
  journal       = {Journal of Systems Architecture},
  year          = {2004},
  volume        = {50},
  number        = {7},
  pages         = {393 - 405},
  issn          = {1383-7621},
  note          = {Adaptable System/Software Architectures},
  __markedentry = {[mac:]},
  abstract      = {Software measures are important to evaluate software properties like complexity, reusability, maintainability, effort required, etc. Collecting such data is difficult because of the lack of tools that perform acquisition automatically. It is not possible to implement a manual data collection because it is error prone and very time expensive. Moreover, developers often work in teams and sometimes in different places using laptops. These conditions require tools that collect data automatically, can work offline and merge data from different developers working in the same project. This paper presents PROM (PRO Metrics), a distributed Java based tool designed to collect automatically software measures. This tool uses a distributed architecture based on plug-ins, integrated in most popular development tools, and the SOAP communication protocol.},
  comment       = {13},
  doi           = {https://doi.org/10.1016/j.sysarc.2003.09.005},
  keywords      = {Development monitoring, Process metrics, Product metrics, Java},
  url           = {http://www.sciencedirect.com/science/article/pii/S1383762103001607},
}

@Article{Pedrycz2003,
  author        = {W. Pedrycz and M.G. Chun and G. Succi},
  title         = {N4: computing with neural receptive fields},
  journal       = {Neurocomputing},
  year          = {2003},
  volume        = {55},
  number        = {1},
  pages         = {383 - 401},
  issn          = {0925-2312},
  note          = {Support Vector Machines},
  __markedentry = {[mac:]},
  abstract      = {In this study, we introduce a new neural architecture called N4 that is based on a collection of local receptive fields realized in the form of referential neural networks. While the network exhibits some similarities to other structures of modular neural networks (such as expert networks), it comes with a number of unique features. Especially, its receptive fields exhibit high flexibility by being formed by neural networks. Subsequently, the processing therein is of referential nature. A â€skeletonâ€ (structure) of the network is completed through unsupervised learning that is aimed at â€œdiscoveringâ€ and structuring the main dependencies in data. More specifically, the design of the network consists of two phases. First, a blueprint of the network is formed and this involves the prototypes obtained through clustering of training data. This structural development of the network is followed by further refinement in a form of parametric training of the individual neural receptive fields. The study provides a detailed analysis and learning of the network and includes experimental investigations.},
  comment       = {19},
  doi           = {https://doi.org/10.1016/S0925-2312(02)00630-6},
  keywords      = {Local neural networks, RBF neural networks, Fuzzy clustering, Neural receptive fields, Nearest neighbor classification rule, Modular neural networks},
  url           = {http://www.sciencedirect.com/science/article/pii/S0925231202006306},
}

@Comment{jabref-meta: databaseType:bibtex;}
