% Encoding: UTF-8

@Article{Capilla2016191,
  author   = {Capilla, Rafael and Jansen, Anton and Tang, Antony and Avgeriou, Paris and Babar, Muhammad Ali},
  title    = {{10 years of software architecture knowledge management: Practice and future}},
  journal  = {Journal of Systems and Software},
  year     = {2016},
  volume   = {116},
  pages    = {191--205},
  issn     = {0164-1212},
  abstract = {Abstract The importance of architectural knowledge (AK) management for software development has been highlighted over the past ten years, where a significant amount of research has been done. Since the first systems using design rationale in the seventies and eighties to the more modern approaches using {\{}AK{\}} for designing software architectures, a variety of models, approaches, and research tools have leveraged the interests of researchers and practitioners in {\{}AK{\}} management (AKM). Capturing, sharing, and using {\{}AK{\}} has many benefits for software designers and maintainers, but the cost to capture this relevant knowledge hampers a widespread use by software companies. However, as the improvements made over the last decade didn't boost a wider adoption of {\{}AKM{\}} approaches, there is a need to identify the successes and shortcomings of current {\{}AK{\}} approaches and know what industry needs from AK. Therefore, as researchers and promoters of many of the {\{}AK{\}} research tools in the early stages where {\{}AK{\}} became relevant for the software architecture community, and based on our experience and observations, we provide in this research an informal retrospective analysis of what has been done and the challenges and trends for a future research agenda to promote {\{}AK{\}} use in modern software development practices. },
  doi      = {https://doi.org/10.1016/j.jss.2015.08.054},
  keywords = {Agile development, Architectural design decisions, Architectural knowledge management},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121215002034},
}

@Article{SilveiraNeto2013872,
  author   = {{da Mota Silveira Neto}, Paulo Anselmo and Gomes, Jo{\'{a}}s Sousa and de Almeida, Eduardo Santana and Leite, Jair Cavalcanti and Batista, Thais Vasconcelos and Leite, Larissa},
  title    = {{25 years of software engineering in Brazil: Beyond an insider's view}},
  journal  = {Journal of Systems and Software},
  year     = {2013},
  volume   = {86},
  number   = {4},
  pages    = {872--889},
  issn     = {0164-1212},
  abstract = {The software engineering area is facing a growing number of challenges due to the continuing increase in software size and complexity. The challenges are addressed by the very relevant and high quality publications of the Brazilian Symposium on Software Engineering (SBES), in the past 25 editions. This article summarizes the findings from two different mapping studies about these 25 {\{}SBES{\}} editions. It also reports the results of an expert opinion survey with the most important Brazilian researchers in the software engineering (SE) area. The survey reinforces the findings of the mapping studies. It also provides guidance for future research. In addition, the studies report several findings that confirmed the validity of the research methods applied. All of these findings are important input to the current Brazilian {\{}SE{\}} scenario. Our findings also suggest that greater attention should be given to the {\{}SE{\}} area, by improving researchers' interaction with industry and increasing collaboration between researchers, especially internationally. },
  annote   = {{\{}SI{\}} : Software Engineering in Brazil: Retrospective and Prospective Views},
  doi      = {https://doi.org/10.1016/j.jss.2012.10.041},
  keywords = {Expert opinion survey, Mapping study, Software engineering},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121212002981},
}

@Article{Heradio20161,
  author   = {Heradio, Ruben and Perez-Morago, Hector and Fernandez-Amoros, David and Cabrerizo, Francisco Javier and Herrera-Viedma, Enrique},
  title    = {{A bibliometric analysis of 20 years of research on software product lines}},
  journal  = {Information and Software Technology},
  year     = {2016},
  volume   = {72},
  pages    = {1--15},
  issn     = {0950-5849},
  abstract = {Abstract Context: Software product line engineering has proven to be an efficient paradigm to developing families of similar software systems at lower costs, in shorter time, and with higher quality. Objective: This paper analyzes the literature on product lines from 1995 to 2014, identifying the most influential publications, the most researched topics, and how the interest in those topics has evolved along the way. Method: Bibliographic data have been gathered from {\{}ISI{\}} Web of Science and Scopus. The data have been examined using two prominent bibliometric approaches: science mapping and performance analysis. Results: According to the study carried out, (i) software architecture was the initial motor of research in SPL; (ii) work on systematic software reuse has been essential for the development of the area; and (iii) feature modeling has been the most important topic for the last fifteen years, having the best evolution behavior in terms of number of published papers and received citations. Conclusion: Science mapping has been used to identify the main researched topics, the evolution of the interest in those topics and the relationships among topics. Performance analysis has been used to recognize the most influential papers, the journals and conferences that have published most papers, how numerous is the literature on product lines and what is its distribution over time. },
  doi      = {https://doi.org/10.1016/j.infsof.2015.11.004},
  keywords = {Bibliometrics, Performance analysis, Science mapping, Software product lines},
  url      = {http://www.sciencedirect.com/science/article/pii/S0950584915001883},
}

@Article{Roško2014145,
  author        = {Ro{\v{s}}ko, Z and Strahonja, V},
  title         = {{A case study of software product line for business applications changeability prediction}},
  journal       = {Journal of Information and Organizational Sciences},
  year          = {2014},
  volume        = {38},
  number        = {2},
  pages         = {145--160},
  abstract      = {The changeability, a sub-characteristic of maintainability, refers to the level of effort which is required to do modifications to a software product line (SPL) application component. Assuming dependencies between SPL application components and reference architecture implementation (a platform), this paper empirically investigates the relationship between 7 design metrics and changeability of 46 server components of a product line for business applications. In addition, we investigated the usefulness of Platform Responsibility (PR) metric as an indicator of product line component changeability. The results show that most of the design metrics are strongly related to the changeability of server component and also indicate statistically significant correlation between Maintainability Index (MI) and PR metric. The assessment is based on a case study of the implementation of the product line for business applications in a financial institution. The results show that PR metric can be used as good predictor of changeability in the software product line environment. {\textcopyright} 2014, University of Zagreb. All rights reserved.},
  annote        = {cited By 0},
  keywords      = {case study},
  mendeley-tags = {case study},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84918540616{\&}partnerID=40{\&}md5=5c9f9c6b3eeb9a82151b8ed9178db021},
}

@Article{Lavoie201732,
  author        = {Lavoie, Thierry and M{\'{e}}rineau, Mathieu and Merlo, Ettore and Potvin, Pascal},
  title         = {{A case study of TTCN-3 test scripts clone analysis in an industrial telecommunication setting}},
  journal       = {Information and Software Technology},
  year          = {2017},
  volume        = {87},
  pages         = {32--45},
  issn          = {0950-5849},
  abstract      = {Abstract Context: This paper presents a novel experiment focused on detecting and analyzing clones in test suites written in TTCN-3, a standard telecommunication test script language, for different industrial projects. Objective: This paper investigates frequencies, types, and similarity distributions of TTCN-3 clones in test scripts from three industrial projects in telecommunication. We also compare the distribution of clones in TTCN-3 test scripts with the distribution of clones in C/C++ and Java projects from the telecommunication domain. We then perform a statistical analysis to validate the significance of differences between these distributions. Method: Similarity is computed using CLAN, which compares metrics syntactically derived from script fragments. Metrics are computed from the Abstract Syntax Trees produced by a TTCN-3 parser called Titan developed by Ericsson as an Eclipse plugin. Finally, clone classification of similar script pairs is computed using the Longest Common Subsequence algorithm on token types and token images. Results: This paper presents figures and diagrams reporting TTCN-3 clone frequencies, types, and similarity distributions. We show that the differences between the distribution of clones in test scripts and the distribution of clones in applications are statistically significant. We also present and discuss some lessons that can be learned about the transferability of technology from this study. Conclusion: About 24{\%} of fragments in the test suites are cloned, which is a very high proportion of clones compared to what is generally found in source code. The difference in proportion of Type-1 and Type-2 clones is statistically significant and remarkably higher in TTCN-3 than in source code. Type-1 and Type-2 clones represent 82.9{\%} and 15.3{\%} of clone fragments for a total of 98.2{\%}. Within the projects this study investigated, this represents more and easier potential re-factoring opportunities for test scripts than for code.},
  doi           = {https://doi.org/10.1016/j.infsof.2017.01.008},
  keywords      = {Clone detection, Telecommunications software, Test, case study},
  mendeley-tags = {case study},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584917300605},
}

@Article{Raatikainen2004403,
  author        = {Raatikainen, M and Soininen, T and M{\"{a}}nnist{\"{o}}, T and Mattila, A},
  title         = {{A case study of two configurable software product families}},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year          = {2004},
  volume        = {3014},
  pages         = {403--421},
  abstract      = {A configurable software product family allows the deployment of individual products without customer-specific design or programming effort. Despite the fact that such software product families have recently gained research interest, there are only few empirical studies on them. This paper presents some results of a descriptive case study undertaken in two companies that develop and deploy configurable software product families. The similarities found in comparisons between characteristics of the configurable software product families were remarkable, although the companies, products, and application domains were different. The study shows that the configurable software product family approach is already applied in the industry. Furthermore, the approach seems to be a feasible and even efficient way to systematically develop a family of products and manage the variability within it. {\textcopyright} Springer-Verlag Berlin Heidelberg 2004.},
  annote        = {cited By 4},
  keywords      = {Application programs, Configurable software product family, Empirical s, Product design, case study},
  mendeley-tags = {case study},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-35048821987{\&}partnerID=40{\&}md5=c80c150668f0c1fff6ec434df422f942},
}

@Article{Kim2008364,
  author        = {Kim, K},
  title         = {{A case study on architectural maturity evaluation: Experience in the consumer electronics domain}},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year          = {2008},
  volume        = {5333},
  pages         = {364--373},
  abstract      = {This paper introduces our experience of applying an architecture evaluation model to enhance quality attributes of product line in the consumer electronics field. We proposed an evaluation model for architecture design and performed assessment on several platforms to evaluate reusability and maintainability as a core asset of product line. Former researches such as PULSETM, FEF(Families Evaluation Framework), and PLPF(Product Line Practice Framework) provide the whole spectrum of building, deploying and maintaining software product lines based on the BAPO(Business, Architecture, Process and Organization). But we focused on software architecture design itself further because we are mainly concerned with design and implementation issues thus concentrating on the architecture criteria. In this paper, we will provide the practitioners with an experience of our approach, core concept and lessons learned relies on several case studies on design practices. {\textcopyright} Springer-Verlag Berlin Heidelberg 2008.},
  annote        = {cited By 1},
  keywords      = {Architecture designs, Architecture evaluation, Case Study, Consumer electronics, De, Design, Internet, Product de, Quality control},
  mendeley-tags = {Case Study},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964550165{\&}partnerID=40{\&}md5=dd50e2f018cc9d4c89b8542d909f8779},
}

@Article{Zhang2010723,
  author   = {Zhang, Pengcheng and Muccini, Henry and Li, Bixin},
  title    = {{A classification and comparison of model checking software architecture techniques}},
  journal  = {Journal of Systems and Software},
  year     = {2010},
  volume   = {83},
  number   = {5},
  pages    = {723--744},
  issn     = {0164-1212},
  abstract = {Software architecture specifications are used for many different purposes, such as documenting architectural decisions, predicting architectural qualities before the system is implemented, and guiding the design and coding process. In these contexts, assessing the architectural model as early as possible becomes a relevant challenge. Various analysis techniques have been proposed for testing, model checking, and evaluating performance based on architectural models. Among them, model checking is an exhaustive and automatic verification technique, used to verify whether an architectural specification conforms to expected properties. While model checking is being extensively applied to software architectures, little work has been done to comprehensively enumerate and classify these different techniques. The goal of this paper is to investigate the state-of-the-art in model checking software architectures. For this purpose, we first define the main activities in a model checking software architecture process. Then, we define a classification and comparison framework and compare model checking software architecture techniques according to it. },
  doi      = {https://doi.org/10.1016/j.jss.2009.11.709},
  keywords = {Model checking, Software architecture},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121209003070},
}

@Article{Thum:2014:CSA:2620784.2580950,
  author    = {Th{\"{u}}m, Thomas and Apel, Sven and K{\"{a}}stner, Christian and Schaefer, Ina and Saake, Gunter},
  title     = {{A Classification and Survey of Analysis Strategies for Software Product Lines}},
  journal   = {ACM Comput. Surv.},
  year      = {2014},
  volume    = {47},
  number    = {1},
  pages     = {6:1----6:45},
  issn      = {0360-0300},
  address   = {New York, NY, USA},
  doi       = {10.1145/2580950},
  keywords  = {Product-line analysis, model checking, program family, software analysis, software product line, static analysis, theorem proving, type checking},
  publisher = {ACM},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/2580950},
}

@Article{Linsbauer:2017:CVC:3170492.3136054,
  author    = {Linsbauer, Lukas and Berger, Thorsten and Gr{\"{u}}nbacher, Paul},
  title     = {{A Classification of Variation Control Systems}},
  journal   = {SIGPLAN Not.},
  year      = {2017},
  volume    = {52},
  number    = {12},
  pages     = {49--62},
  issn      = {0362-1340},
  address   = {New York, NY, USA},
  doi       = {10.1145/3170492.3136054},
  keywords  = {Variability management, configuration management, software product lines, software repositories},
  publisher = {ACM},
  url       = {http://doi.acm.org/10.1145/3170492.3136054},
}

@Article{Noor200869,
  author   = {Noor, M A and Gr{\"{u}}nbacher, P and Hoyer, C},
  title    = {{A collaborative method for reuse potential assessment in reengineering-based product line adoption}},
  journal  = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year     = {2008},
  volume   = {5082 LNCS},
  pages    = {69--83},
  abstract = {Software product lines are rarely developed from scratch. Instead the development of a product line by reengineering existing systems is a more common scenario, which relies on the collaboration of diverse stakeholders to lay its foundations. The paper describes a collaborative scoping approach for organizations migrating existing products to a product line. The approach uses established practices from the field of reengineering and architectural recovery and synthesizes them in a collaborative process. The proposed approach employs best practices and tools from the area of collaboration engineering to achieve effective collaboration. The paper presents a case study as initial validation of the proposed approach. {\textcopyright} 2008 Springer-Verlag Berlin Heidelberg.},
  annote   = {cited By 3},
  doi      = {10.1007/978-3-540-85279-7_6},
  keywords = {Architectural recovery; Collaboration; Collaborat, Collaboration; Engineering techniques; Product li, Software design; Software engineering; Transients;, Technology; Software engineering},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-50949096881{\&}doi=10.1007{\%}2F978-3-540-85279-7{\_}6{\&}partnerID=40{\&}md5=a9c312b4655b961bdb0e1ec0ae66a3fc},
}

@Article{Tang2010352,
  author   = {Tang, Antony and Avgeriou, Paris and Jansen, Anton and Capilla, Rafael and Babar, Muhammad Ali},
  title    = {{A comparative study of architecture knowledge management tools}},
  journal  = {Journal of Systems and Software},
  year     = {2010},
  volume   = {83},
  number   = {3},
  pages    = {352--370},
  issn     = {0164-1212},
  abstract = {Recent research suggests that architectural knowledge, such as design decisions, is important and should be recorded alongside the architecture description. Different approaches have emerged to support such architectural knowledge (AK) management activities. However, there are different notions of and emphasis on what and how architectural activities should be supported. This is reflected in the design and implementation of existing {\{}AK{\}} tools. To understand the current status of software architecture knowledge engineering and future research trends, this paper compares five architectural knowledge management tools and the support they provide in the architecture life-cycle. The comparison is based on an evaluation framework defined by a set of 10 criteria. The results of the comparison provide insights into the current focus of architectural knowledge management support, their advantages, deficiencies, and conformance to the current architectural description standard. Based on the outcome of this comparison a research agenda is proposed for future work on {\{}AK{\}} tools. },
  doi      = {https://doi.org/10.1016/j.jss.2009.08.032},
  keywords = {Architectural design, Architectural knowledge management tool, Design rationale},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121209002295},
}

@Article{Stol20111319,
  author   = {Stol, Klaas-Jan and Babar, Muhammad Ali and Avgeriou, Paris and Fitzgerald, Brian},
  title    = {{A comparative study of challenges in integrating Open Source Software and Inner Source Software}},
  journal  = {Information and Software Technology},
  year     = {2011},
  volume   = {53},
  number   = {12},
  pages    = {1319--1336},
  issn     = {0950-5849},
  abstract = {Context Several large software-developing organizations have adopted Open Source Software development (OSSD) practices to develop in-house components that are subsequently integrated into products. This phenomenon is also known as “Inner Source”. While there have been several reports of successful cases of this phenomenon, little is known about the challenges that practitioners face when integrating software that is developed in such a setting. Objective The objective of this study was to shed light on challenges related to building products with components that have been developed within an Inner Source development environment. Method Following an initial systematic literature review to generate seed category data constructs, we performed an in-depth exploratory case study in an organization that has a significant track record in the implementation of Inner Source. Data was gathered through semi-structured interviews with participants from a range of divisions across the organization. Interviews were transcribed and analyzed using qualitative data analysis techniques. Results We have identified a number of challenges and approaches to address them, and compared the findings to challenges related to development with {\{}OSS{\}} products reported in the literature. We found that many challenges identified in the case study could be mapped to challenges related to integration of OSS. Conclusion The results provide important insights into common challenges of developing with {\{}OSS{\}} and Inner Source and may help organizations to understand how to improve their software development practices by adopting certain {\{}OSSD{\}} practices. The findings also identify the areas that need further research. },
  doi      = {https://doi.org/10.1016/j.infsof.2011.06.007},
  keywords = {Case study, Challenges, Empirical studies, Inner Source, Open Source Software, Software development},
  url      = {http://www.sciencedirect.com/science/article/pii/S095058491100142X},
}

@Article{RABISER2017309,
  author   = {Rabiser, Rick and Guinea, Sam and Vierhauser, Michael and Baresi, Luciano and Gr{\"{u}}nbacher, Paul},
  title    = {{A comparison framework for runtime monitoring approaches}},
  journal  = {Journal of Systems and Software},
  year     = {2017},
  volume   = {125},
  pages    = {309--321},
  issn     = {0164-1212},
  abstract = {The full behavior of complex software systems often only emerges during operation. They thus need to be monitored at run time to check that they adhere to their requirements. Diverse runtime monitoring approaches have been developed in various domains and for different purposes. Their sheer number and heterogeneity, however, make it hard to find the right approach for a specific application or purpose. The aim of our research therefore was to develop a comparison framework for runtime monitoring approaches. Our framework is based on an analysis of the literature and existing taxonomies for monitoring languages and patterns. We use examples from existing monitoring approaches to explain the framework. We demonstrate its usefulness by applying it to 32 existing approaches and by comparing 3 selected approaches in the light of different monitoring scenarios. We also discuss perspectives for researchers.},
  doi      = {https://doi.org/10.1016/j.jss.2016.12.034},
  keywords = {Comparison framework, Literature review, Runtime monitoring},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121216302618},
}

@Article{PETERSEN20091479,
  author   = {Petersen, Kai and Wohlin, Claes},
  title    = {{A comparison of issues and advantages in agile and incremental development between state of the art and an industrial case}},
  journal  = {Journal of Systems and Software},
  year     = {2009},
  volume   = {82},
  number   = {9},
  pages    = {1479--1490},
  issn     = {0164-1212},
  abstract = {Recent empirical studies have been conducted identifying a number of issues and advantages of incremental and agile methods. However, the majority of studies focused on one model (Extreme Programming) and small projects. To draw more general conclusions we conduct a case study in large-scale development identifying issues and advantages, and compare the results with previous empirical studies on the topic. The principle results are that (1) the case study and literature agree on the benefits while new issues arise when using agile in large-scale and (2) an empirical research framework is needed to make agile studies comparable.},
  annote   = {SI: QSIC 2007},
  doi      = {https://doi.org/10.1016/j.jss.2009.03.036},
  keywords = {Agile, Case study, Incremental, State of the art},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121209000855},
}

@Article{Kolesnikov:2013:CPF:2637365.2517213,
  author    = {Kolesnikov, Sergiy and von Rhein, Alexander and Hunsen, Claus and Apel, Sven},
  title     = {{A Comparison of Product-based, Feature-based, and Family-based Type Checking}},
  journal   = {SIGPLAN Not.},
  year      = {2013},
  volume    = {49},
  number    = {3},
  pages     = {115--124},
  issn      = {0362-1340},
  address   = {New York, NY, USA},
  doi       = {10.1145/2637365.2517213},
  keywords  = {feature-oriented programming, fuji, product-line analysis, type checking},
  publisher = {ACM},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/2637365.2517213},
}

@Article{Kim2014677,
  author   = {Kim, J and Kang, S and Lee, J},
  title    = {{A comparison of software product line traceability approaches from end-to-end traceability perspectives}},
  journal  = {International Journal of Software Engineering and Knowledge Engineering},
  year     = {2014},
  volume   = {24},
  number   = {4},
  pages    = {677--714},
  abstract = {Software traceability is the ability to provide trace information on requirements, design, and implementation of a system. It helps stakeholders understand the many associations of software artifacts created during a software development project. End-to-end traceability refers to linkage of all artifacts in the entire lifecycle of a software development project. Its goal is to provide stakeholders of the software development with trace information in order to analyze impacts due to changes in a software system. Compared to that of a single product, the end-to-end traceability of software product line is more complicated because Software Product Line Development (SPLD) requires two separate but intimately related phases of domain engineering and application engineering. Various SPLD traceability approaches have been proposed in the past. However, thus far no research work on SPLD traceability has focused on SPLD end-to-end traceability. This paper defines SPLD end-to-end traceability and evaluates the existing SPLD traceability approaches from SPLD end-to-end traceability perspectives. We surveyed studies on SPLD traceability methods, traceability mechanisms used in major SPLD approaches, and software traceability survey papers. We compared the existing SPLD traceability approaches based on Systematic Literature Review (SLR). Through the survey, we found that none of the SPLD traceability studies fully supports SPLD end-to-end traceability, and there are unexplored research areas of SPLD end-to-end traceability in the existing SPLD traceability studies. The contribution of this paper is that it presents future research directions that give research guidelines for each unexplored research area in SPLD end-to-end traceability. Finally, based on the research directions, this paper suggests future research opportunities for SPLD end-to-end traceability. {\textcopyright} 2014 World Scientific Publishing Company.},
  annote   = {cited By 4},
  doi      = {10.1142/S0218194014500260},
  keywords = {Application engineering; Future research directio, Application programs; Computer software; Engineeri, Software design},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929319373{\&}doi=10.1142{\%}2FS0218194014500260{\&}partnerID=40{\&}md5=1954a03cd457a16de3963c2bd0040fe0},
}

@Article{Li201539,
  author   = {Li, Zonghua and Zhou, Xiaofeng and Gu, Aihua and Li, Qinfeng},
  title    = {{A complete approach for {\{}CIM{\}} modelling and model formalising}},
  journal  = {Information and Software Technology},
  year     = {2015},
  volume   = {65},
  pages    = {39--55},
  issn     = {0950-5849},
  abstract = {AbstractContext Computation Independent Model (CIM) as a business model describes the requirements and environment of a business system and instructs the designing and development; it is a key to influencing software success. Although many studies currently focus on model driven development (MDD); those researches, to a large extent, study the PIM-level and PSM-level model, and few have dealt with CIM-level modelling for case in which the requirements are unclear or incomplete. Objective This paper proposes a CIM-level modelling approach, which applies a stepwise refinement approach to modelling the CIM-level model starting from a high-level goal model to a lower-level business process model. A key advantage of our approach is the combination of the requirement model with the business model, which helps software engineers to define business models exactly for cases in which the requirements are unclear or incomplete. Method This paper, based on the model driven approach, proposes a set of models at the CIM-level and model transformations to connect these models. Accordingly, the formalisation approach of this paper involves formalising the goal model using the category theory and the scenario model and business process model using Petri nets. Results We have defined a set of metamodels and transformation rules making it possible to obtain automatically a scenario model from the goal model and a business process model from the scenario model. At the same time, we have defined a mapping rule to formalise these models. Our proposed {\{}CIM{\}} modelling approach and formalisation approach are implemented with an {\{}MDA{\}} tool, and it has been empirically validated by a travel agency case study. Conclusion This study shows how a {\{}CIM{\}} modelling approach helps to build a complete and consistent model at the {\{}CIM{\}} level for cases in which the requirements are unclear or incomplete in advance. },
  doi      = {https://doi.org/10.1016/j.infsof.2015.04.003},
  keywords = {CIM, Model consistency verification, Model formalisation, Model transformations, Petri nets},
  url      = {http://www.sciencedirect.com/science/article/pii/S0950584915000786},
}

@Article{Bazi201787,
  author   = {reza Bazi, Hamid and Hassanzadeh, Alireza and Moeini, Ali},
  title    = {{A comprehensive framework for cloud computing migration using Meta-synthesis approach}},
  journal  = {Journal of Systems and Software},
  year     = {2017},
  volume   = {128},
  pages    = {87--105},
  issn     = {0164-1212},
  abstract = {Abstract Migration to the cloud computing environment is a strategic organizational decision. Using a reliable framework for migration ensures managers to mitigate risks in the cloud computing technology. Therefore, organizations always search for cloud migration frameworks with dynamic nature as well as integrity beside their simplicity. In previous studies, these important features have received less attention and have not been achieved in an integrated and comprehensive way. The aim of this study is to use a meta-synthesis method for the first time for analysis and synthesis of previous published studies and suggests a comprehensive cloud migration framework. We review more than 657 papers from relevant journals and conference proceedings. The concepts which are extracted from these papers are classified to related sub-categories and categories. Then, our proposed framework based on these concepts and categories is developed. It includes seven main phases (categories) and fifteen sub-categories. To improve the migration process a maturity model called “ClM3” is introduced. Finally, proposed framework and maturity model is evaluated by forming different focus group meetings and taking advantages of the cloud experts' opinion. The results of this research can help managers have a safe and effective migration to cloud computing environment. },
  doi      = {https://doi.org/10.1016/j.jss.2017.02.049},
  keywords = {Cloud computing, Meta-synthesis, Migration framework, Process maturity model},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121217300456},
}

@Article{Uzunov2015217,
  author   = {Uzunov, Anton V and Falkner, Katrina and Fernandez, Eduardo B},
  title    = {{A comprehensive pattern-oriented approach to engineering security methodologies}},
  journal  = {Information and Software Technology},
  year     = {2015},
  volume   = {57},
  pages    = {217--247},
  issn     = {0950-5849},
  abstract = {AbstractContext Developing secure software systems is an issue of ever-growing importance. Researchers have generally come to acknowledge that to develop such systems successfully, their security features must be incorporated in the context of a systematic approach: a security methodology. There are a number of such methodologies in the literature, but no single security methodology is adequate for every situation, requiring the construction of “fit-to-purpose” methodologies or the tailoring of existing methodologies to the project specifics at hand. While a large body of research exists addressing the same requirement for development methodologies – constituting the field of Method Engineering – there is nothing comparable for security methodologies as such; in fact, the topic has never been studied before in such a context. Objective In this paper we draw inspiration from a number of Method Engineering ideas and fill the latter gap by proposing a comprehensive approach to engineering security methodologies. Method Our approach is embodied in three interconnected parts: a framework of interrelated security process patterns; a security-specific meta-model; and a meta-methodology to guide engineers in using the latter artefacts in a step-wise fashion. A UML-inspired notation is used for representing all pattern-based methodology models during design and construction. The approach is illustrated and evaluated by tailoring an existing, real-life security methodology to a distributed-system-specific project situation. Results The paper proposes a novel pattern-oriented approach to modeling, constructing, tailoring and combining security methodologies, which is the very first and currently sole such approach in the literature. We illustrate and evaluate our approach in an academic setting, and perform a feature analysis to highlight benefits and deficiencies. Conclusion Using our proposal, developers, architects and researchers can analyze and engineer security methodologies in a structured, systematic fashion, taking into account all security methodology aspects. },
  doi      = {https://doi.org/10.1016/j.infsof.2014.09.001},
  keywords = {Method engineering, Modeling, Process patterns, Secure software engineering, Security methodologies, Software security},
  url      = {http://www.sciencedirect.com/science/article/pii/S0950584914002006},
}

@Article{Rosemann20071,
  author   = {Rosemann, M and van der Aalst, W M P},
  title    = {{A configurable reference modelling language}},
  journal  = {Information Systems},
  year     = {2007},
  volume   = {32},
  number   = {1},
  pages    = {1--23},
  issn     = {0306-4379},
  abstract = {Enterprise Systems (ES) are comprehensive off-the-shelf packages that have to be configured to suit the requirements of an organization. Most {\{}ES{\}} solutions provide reference models that describe the functionality and structure of the system. However, these models do not capture the potential configuration alternatives. This paper discusses the shortcomings of current reference modelling languages using Event-Driven Process Chains (EPCs) as an example. We propose Configurable {\{}EPCs{\}} (C-EPCs) as an extended reference modelling language which allows capturing the core configuration patterns. A formalization of this language as well as examples for typical configurations are provided. A program of further research including the identification of a comprehensive list of configuration patterns, deriving possible notations for reference model configurations and testing the quality of these proposed extensions in experiments and focus groups is presented. },
  doi      = {https://doi.org/10.1016/j.is.2005.05.003},
  keywords = {Configuration, Enterprise systems, Event-Driven Process Chains, Reference model},
  url      = {http://www.sciencedirect.com/science/article/pii/S0306437905000487},
}

@Article{Jörges2012511,
  author   = {J{\"{o}}rges, S and Lamprecht, A.-L. and Margaria, T and Schaefer, I and Steffen, B},
  title    = {{A constraint-based variability modeling framework}},
  journal  = {International Journal on Software Tools for Technology Transfer},
  year     = {2012},
  volume   = {14},
  number   = {5},
  pages    = {511--530},
  abstract = {Constraint-based variability modeling is a flexible, declarative approach to managing solution-space variability. Product variants are defined in a top-down manner by successively restricting the admissible combinations of product artifacts until a specific product variant is determined. In this paper, we illustrate the range of constraint-based variability modeling by discussing two of its extreme flavors: constraint-guarded variability modeling and constraint-driven variability modeling. The former applies model checking to establish the global consistency of product variants which are built by manual specification of variations points, whereas the latter uses synthesis technology to fully automatically generate product variants that satisfy all given constraints. Each flavor is illustrated by means of a concrete case study. {\textcopyright} 2012 Springer-Verlag.},
  annote   = {cited By 21},
  doi      = {10.1007/s10009-012-0254-x},
  keywords = {Constraint-based; Global consistency; Product vari, Information systems; Software engineering, Model checking},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866284416{\&}doi=10.1007{\%}2Fs10009-012-0254-x{\&}partnerID=40{\&}md5=63ea47588f2d468bea6a411704f58378},
}

@Article{Narwane2014212,
  author   = {Narwane, G K and Krishna, S N and Bhattacharjee, A K},
  title    = {{A cost effective approach for analyzing software product lines}},
  journal  = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year     = {2014},
  volume   = {8337 LNCS},
  pages    = {212--223},
  abstract = {In the area of Software Product Lines(SPL), most of the research work focuses on automated analysis of SPLs and the traceability relation between the problem domain and solution domain. An SPL with few features can generate billions of products; to analyze such a large product space, we need efficient analysis operations. For a given specification, we can get many possible implementations; choosing one implementation from this is a non-trivial task. In this paper, we extend the work on analyzing software product lines to propose a cost effective approach that fetches products from a given SPL based on various factors. When there are multiple implementations for a given specification, then it is the cost factors which determine the product selection. To this end, we propose a revised formal framework for SPLs with cost factors. This approach has been implemented in a tool SPLANE-CF (SPL Analysis Engine with Cost Factors). We illustrate the efficiency of SPLANE-CF on a fairly large size case study. {\textcopyright} 2014 Springer International Publishing Switzerland.},
  annote   = {cited By 0},
  doi      = {10.1007/978-3-319-04483-5_22},
  keywords = {Automated analysis; Cost-effective approach; Effi, Computer software; Cost effectiveness; Costs; Inte, Cost benefit analysis},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958534666{\&}doi=10.1007{\%}2F978-3-319-04483-5{\_}22{\&}partnerID=40{\&}md5=5f3ce1b31fba5eef3c2ba304c81841a1},
}

@Article{Fasquel2011847,
  author   = {Fasquel, Jean-Baptiste and Moreau, Johan},
  title    = {{A design pattern coupling role and component concepts: Application to medical software}},
  journal  = {Journal of Systems and Software},
  year     = {2011},
  volume   = {84},
  number   = {5},
  pages    = {847--863},
  issn     = {0164-1212},
  abstract = {One of the challenges in software development regards the appropriate coupling of separated code elements in order to correctly build initially expected high-level software functionalities. In this context, we address issues related to the dynamic composition of such code elements (i.e. how they are dynamically plugged together) as well as their collaboration (i.e. how they work together). We also consider the limitation of build-level dependencies, to avoid the entire re-compilation and re-deployment of a software when modifying it or integrating new functionalities. To solve these issues, we propose a new design pattern coupling role and component concepts and illustrate its relevance for medical software. Compared to most related works focusing on few role concepts while ignoring others, the proposed pattern integrates many role concepts as first-class entities, including in particular a refinement of the notion of collaboration. Another significant contribution of our proposal concerns the coupling of role and component concepts. Roles are related to the functional aspects of a target software program (composition and collaboration of functional units). Components correspond to the physical distribution of code elements with limited build-level dependencies. As illustrated in this paper, such a coupling enables to instantiate a software program using a generic main program together with a description file focusing on software functionalities only. Related code elements are transparently retrieved and composed at run-time before appropriately collaborating, regardless the specificity of their distribution over components. },
  doi      = {https://doi.org/10.1016/j.jss.2011.01.026},
  keywords = {Collaboration,Component,Design pattern,Dynamic composition,Medical software,Role},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121211000185},
}

@Article{CostaNeto20132333,
  author   = {Neto, Alberto Costa and Bonif{\'{a}}cio, Rodrigo and Ribeiro, M{\'{a}}rcio and Pontual, Carlos Eduardo and Borba, Paulo and Castor, Fernando},
  title    = {{A design rule language for aspect-oriented programming}},
  journal  = {Journal of Systems and Software},
  year     = {2013},
  volume   = {86},
  number   = {9},
  pages    = {2333--2356},
  issn     = {0164-1212},
  abstract = {Abstract Aspect-oriented programming is known as a technique for modularizing crosscutting concerns. However, constructs aimed to support crosscutting modularity might actually break class modularity. As a consequence, class developers face changeability, parallel development and comprehensibility problems, because they must be aware of aspects whenever they develop or maintain a class. At the same time, aspects are vulnerable to changes in classes, since there is no contract specifying the points of interaction amongst these elements. These problems can be mitigated by using adequate design rules between classes and aspects. We present a design rule specification language and explore its benefits since the initial phases of the development process, specially with the aim of supporting modular development of classes and aspects. We discuss how our language improves crosscutting modularity without breaking class modularity. We evaluate it using a real case study and compare it with other approaches. },
  doi      = {https://doi.org/10.1016/j.jss.2013.03.104},
  keywords = {Aspect-oriented programming,Design rules,Modularity},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121213000861},
}

@Article{Hallsteinsen20122840,
  author   = {Hallsteinsen, S and Geihs, K and Paspallis, N and Eliassen, F and Horn, G and Lorenzo, J and Mamelli, A and Papadopoulos, G A},
  title    = {{A development framework and methodology for self-adapting applications in ubiquitous computing environments}},
  journal  = {Journal of Systems and Software},
  year     = {2012},
  volume   = {85},
  number   = {12},
  pages    = {2840--2859},
  issn     = {0164-1212},
  abstract = {Today software is the main enabler of many of the appliances and devices omnipresent in our daily life and important for our well being and work satisfaction. It is expected that the software works as intended, and that the software always and everywhere provides us with the best possible utility. This paper discusses the motivation, technical approach, and innovative results of the {\{}MUSIC{\}} project. {\{}MUSIC{\}} provides a comprehensive software development framework for applications that operate in ubiquitous and dynamic computing environments and adapt to context changes. Context is understood as any information about the user needs and operating environment which vary dynamically and have an impact on design choices. {\{}MUSIC{\}} supports several adaptation mechanisms and offers a model-driven application development approach supported by a sophisticated middleware that facilitates the dynamic and automatic adaptation of applications and services based on a clear separation of business logic, context awareness and adaptation concerns. The main contribution of this paper is a holistic, coherent presentation of the motivation, design, implementation, and evaluation of the {\{}MUSIC{\}} development framework and methodology. },
  annote   = {Self-Adaptive Systems},
  doi      = {https://doi.org/10.1016/j.jss.2012.07.052},
  keywords = {Adaptive software,Middleware,Mobile computing,Model-driven development,Ubiquitous computing},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121212002245},
}

@Article{Aleem201655,
  author   = {Aleem, Saiqa and Capretz, Luiz Fernando and Ahmed, Faheem},
  title    = {{A Digital Game Maturity Model (DGMM)}},
  journal  = {Entertainment Computing},
  year     = {2016},
  volume   = {17},
  pages    = {55--73},
  issn     = {1875-9521},
  abstract = {Abstract Game development is an interdisciplinary concept that embraces artistic, software engineering, management, and business disciplines. This research facilitates a better understanding of important dimensions of digital game development methodology. Game development is considered as one of the most complex tasks in software engineering. The increased popularity of digital games, the challenges faced by game development organizations in developing quality games, and high competition in the digital game industry demand a game development maturity assessment. Consequently, this study presents a Digital Game Maturity Model to evaluate the current development methodology in an organization. The framework of this model consists of assessment questionnaires, a performance scale, and a rating method. The main goal of the questionnaires is to collect information about current processes and practices. In general, this research contributes towards formulating a comprehensive and unified strategy for game development maturity evaluation. Two case studies were conducted and their assessment results reported. These demonstrate the level of maturity of current development practices in two organizations. },
  doi      = {https://doi.org/10.1016/j.entcom.2016.08.004},
  keywords = {Game development methodology,Game performance,Online game,Process assessment,Software game,Software process improvement,Video game},
  url      = {http://www.sciencedirect.com/science/article/pii/S1875952116300246},
}

@Article{Smith20091155,
  author   = {Smith, S and Yu, W},
  title    = {{A document driven methodology for developing a high quality Parallel Mesh Generation Toolbox}},
  journal  = {Advances in Engineering Software},
  year     = {2009},
  volume   = {40},
  number   = {11},
  pages    = {1155--1167},
  issn     = {0965-9978},
  abstract = {This paper motivates the value of using a document driven methodology to improve the quality of scientific computing applications by illustrating the design and documentation of a Parallel Mesh Generation Toolbox (PMGT). Formal mathematical specification is promoted for writing unambiguous requirements, which can be used to judge the correctness and thus the reliability of PMGT. Mathematics is also shown to improve understandability, reusability and maintainability through modelling software modules as finite state machines. The proposed methodology includes explicit traceability between requirements, design, implementation and test cases. Traceability improves the verification of completeness and consistency and it allows for proper change management. To improve the reliability of PMGT, given the challenge that the correct solution is unknown a priori, an automated testing approach is adopted to verify the known properties of a correct solution, such as conformality and counterclockwise vertex numbering. },
  doi      = {https://doi.org/10.1016/j.advengsoft.2009.05.003},
  keywords = {Mesh generation,Scientific computing,Software engineering,Software quality},
  url      = {http://www.sciencedirect.com/science/article/pii/S0965997809001306},
}

@Article{Nunes2009716,
  author   = {Nunes, I and Kulesza, U and Nunes, C and {De Lucena}, C J P and Cirilo, E},
  title    = {{A domain analysis approach for multi-agent systems product lines}},
  journal  = {Lecture Notes in Business Information Processing},
  year     = {2009},
  volume   = {24 LNBIP},
  pages    = {716--727},
  abstract = {In this paper, we propose an approach for documenting and modeling Multi-agent System Product Lines (MAS-PLs) in the domain analysis stage. MAS-PLs are the integration between two promising techniques, software product lines and agent-oriented software engineering, aiming at incorporating their respective benefits and helping the industrial exploitation of agent technology. Our approach explores the scenario of including agency features to existing web applications and is based on PASSI, an agent-oriented methodology, to which we added some extensions to address agency variability. A case study, OLIS (OnLine Intelligent Services), illustrates our approach. {\textcopyright} 2009 Springer Berlin Heidelberg.},
  annote   = {cited By 1},
  doi      = {10.1007/978-3-642-01347-8_59},
  keywords = {Agent Oriented Software Engineering; Agent techno,Information systems; Software agents; Software eng,Multi agent systems},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-65949110866{\&}doi=10.1007{\%}2F978-3-642-01347-8{\_}59{\&}partnerID=40{\&}md5=f7d7f126f58f0f6b380dd9c52a08ec54},
}

@Article{GómezAbajo2017152,
  author   = {G{\'{o}}mez-Abajo, Pablo and Guerra, Esther and de Lara, Juan},
  title    = {{A domain-specific language for model mutation and its application to the automated generation of exercises}},
  journal  = {Computer Languages, Systems {\&} Structures},
  year     = {2017},
  volume   = {49},
  pages    = {152--173},
  issn     = {1477-8424},
  abstract = {Abstract Model-Driven Engineering (MDE) is a software engineering paradigm that uses models as main assets in all development phases. While many languages for model manipulation exist (e.g., for model transformation or code generation), there is a lack of frameworks to define and apply model mutations. A model mutant is a variation of an original model, created by the application of specific model mutation operations. Model mutation has many applications, for instance, in the areas of model transformation testing, model-based testing or education. In this paper, we present a domain-specific language called Wodel for the specification and generation of model mutants. Wodel is domain-independent, as it can be used to generate mutants of models conformant to arbitrary meta-models. Its development environment is extensible, permitting the incorporation of post-processors for different applications. In particular, we describe Wodel-Edu, a post-processing extension directed to the automated generation of exercises for particular domains and their automated correction. We show the application of Wodel-Edu to the generation of exercises for deterministic automata, and report on an evaluation of the quality of the generated exercises, obtaining overall good results. },
  doi      = {https://doi.org/10.1016/j.cl.2016.11.001},
  keywords = {Automatic exercise generation and correction,Domain-Specific Languages,Education,Model mutation,Model-Driven Engineering},
  url      = {http://www.sciencedirect.com/science/article/pii/S147784241630094X},
}

@Article{Tanhaei2016951,
  author   = {Tanhaei, M and Habibi, J and Mirian-Hosseinabadi, S.-H.},
  title    = {{A Feature Model Based Framework for Refactoring Software Product Line Architecture}},
  journal  = {Journal of Computer Science and Technology},
  year     = {2016},
  volume   = {31},
  number   = {5},
  pages    = {951--986},
  abstract = {Software product line (SPL) is an approach used to develop a range of software products with a high degree of similarity. In this approach, a feature model is usually used to keep track of similarities and differences. Over time, as modifications are made to the SPL, inconsistencies with the feature model could arise. The first approach to dealing with these inconsistencies is refactoring. Refactoring consists of small steps which, when accumulated, may lead to large-scale changes in the SPL, resulting in features being added to or eliminated from the SPL. In this paper, we propose a framework for refactoring SPLs, which helps keep SPLs consistent with the feature model. After some introductory remarks, we describe a formal model for representing the feature model. We express various refactoring patterns applicable to the feature model and the SPL formally, and then introduce an algorithm for finding them in the SPL. In the end, we use a real-world case study of an SPL to illustrate the applicability of the framework introduced in the paper. {\textcopyright} 2016, Springer Science+Business Media New York.},
  annote   = {cited By 2},
  doi      = {10.1007/s11390-016-1674-y},
  keywords = {Computer software,Degree of similarity; Feature modeling; Keep trac,Software architecture},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84984908557{\&}doi=10.1007{\%}2Fs11390-016-1674-y{\&}partnerID=40{\&}md5=bb87ce0a2218e447cdac76a5b15fa444},
}

@Article{Jordan2015120,
  author   = {Jordan, Howell and Botterweck, Goetz and Noll, John and Butterfield, Andrew and Collier, Rem},
  title    = {{A feature model of actor, agent, functional, object, and procedural programming languages}},
  journal  = {Science of Computer Programming},
  year     = {2015},
  volume   = {98, Part 2},
  pages    = {120--139},
  issn     = {0167-6423},
  abstract = {Abstract The number of programming languages is large and steadily increasing. However, little structured information and empirical evidence is available to help software engineers assess the suitability of a language for a particular development project or software architecture. We argue that these shortages are partly due to a lack of high-level, objective programming language feature assessment criteria: existing advice to practitioners is often based on ill-defined notions of ‘paradigms' [3, p. xiii] and ‘orientation', while researchers lack a shared common basis for generalisation and synthesis of empirical results. This paper presents a feature model constructed from the programmer's perspective, which can be used to precisely compare general-purpose programming languages in the actor-oriented, agent-oriented, functional, object-oriented, and procedural categories. The feature model is derived from the existing literature on general concepts of programming, and validated with concrete mappings of well-known languages in each of these categories. The model is intended to act as a tool for both practitioners and researchers, to facilitate both further high-level comparative studies of programming languages, and detailed investigations of feature usage and efficacy in specific development contexts. },
  annote   = {Special Issue on Programming Based on Actors, Agents and Decentralized Control},
  doi      = {https://doi.org/10.1016/j.scico.2014.02.009},
  keywords = {Agent-oriented programming,Functional programming,Object-oriented programming,Programming language constructs,Programming languages},
  url      = {http://www.sciencedirect.com/science/article/pii/S0167642314000501},
}

@Article{Kim20112035,
  author   = {Kim, Sangsig and Kim, Dae-Kyoo and Lu, Lunjin and Kim, Suntae and Park, Sooyong},
  title    = {{A feature-based approach for modeling role-based access control systems}},
  journal  = {Journal of Systems and Software},
  year     = {2011},
  volume   = {84},
  number   = {12},
  pages    = {2035--2052},
  issn     = {0164-1212},
  abstract = {Role-based access control (RBAC) is a popular access control model for enterprise systems due to its flexibility and scalability. There are many {\{}RBAC{\}} features available, each providing a different function. Not all features are needed for an {\{}RBAC{\}} system. Depending on the requirements, one should be able to configure features on a need basis, which reduces development complexity and thus fosters development. However, there have not been suitable methods that enable systematic configuration of {\{}RBAC{\}} features for system development. This paper presents an approach for configuring {\{}RBAC{\}} features using a combination of feature modeling and {\{}UML{\}} modeling. Feature modeling is used for capturing the structure of features and configuration rules, and {\{}UML{\}} modeling is used for defining the semantics of features. {\{}RBAC{\}} features are defined based on design principles of partial inheritance and compatibility, which facilitates feature composition and verification. We demonstrate the approach using a banking application and present tool support developed for the approach. },
  doi      = {https://doi.org/10.1016/j.jss.2011.03.084},
  keywords = {Feature modeling,Role-based access control,UML},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121211000926},
}

@Article{Colanzi2016126,
  author   = {Colanzi, Thelma Elita and Vergilio, Silvia Regina},
  title    = {{A feature-driven crossover operator for multi-objective and evolutionary optimization of product line architectures}},
  journal  = {Journal of Systems and Software},
  year     = {2016},
  volume   = {121},
  pages    = {126--143},
  issn     = {0164-1212},
  abstract = {Abstract The optimization of a Product Line Architecture (PLA) design can be modeled as a multi-objective problem, influenced by many factors, such as feature modularization, extensibility and other design principles. Due to this it has been properly solved in the Search Based Software Engineering (SBSE) field. However, previous empirical studies optimized {\{}PLA{\}} design using the multi-objective and evolutionary algorithm NSGA-II, without applying one of the most important genetic operators: the crossover. To overcome this limitation, this paper presents a feature-driven crossover operator that aims at improving feature modularization in {\{}PLA{\}} design. The proposed operator was applied in two empirical studies using NSGA-II in comparison with another version of NSGA-II that uses only mutation operators. The results show the usefulness and applicability of the proposed operator. The NSGA-II version that applies the feature-driven crossover found a greater diversity of solutions (potential {\{}PLA{\}} designs), with higher feature-based cohesion, and less feature scattering and tangling. },
  doi      = {https://doi.org/10.1016/j.jss.2016.02.026},
  keywords = {Crossover operator,Empirical study,Multi-objective genetic algorithm,Product line architecture design},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121216000583},
}

@Article{Lee20101123,
  author   = {Lee, Jaejoon and Muthig, Dirk and Naab, Matthias},
  title    = {{A feature-oriented approach for developing reusable product line assets of service-based systems}},
  journal  = {Journal of Systems and Software},
  year     = {2010},
  volume   = {83},
  number   = {7},
  pages    = {1123--1136},
  issn     = {0164-1212},
  abstract = {Service orientation (SO) is a relevant promising candidate for accommodating rapidly changing user needs and expectations. One of the goals of adopting {\{}SO{\}} is the improvement of reusability, however, the development of service-based system in practice has uncovered several challenging issues, such as how to identify reusable services, how to determine configurations of services that are relevant to users' current product configuration and context, and how to maintain service validity after configuration changes. In this paper, we propose a method that addresses these issues by adapting a feature-oriented product line engineering approach. The method is notable in that it guides developers to identify reusable services at the right level of granularity and to map users' context to relevant service configuration, and it also provides a means to check the validity of services at runtime in terms of invariants and pre/post-conditions of services. Moreover, we propose a heterogeneous style based architecture model for developing such systems. },
  annote   = {{\{}SPLC{\}} 2008},
  doi      = {https://doi.org/10.1016/j.jss.2010.01.048},
  keywords = {Feature-oriented,Service-based systems,Software architecture,Software architecture styles,Software product line engineering},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121210000324},
}

@Article{Andrés20131925,
  author   = {Andr{\'{e}}s, C{\'{e}}sar and Camacho, Carlos and Llana, Luis},
  title    = {{A formal framework for software product lines}},
  journal  = {Information and Software Technology},
  year     = {2013},
  volume   = {55},
  number   = {11},
  pages    = {1925--1947},
  issn     = {0950-5849},
  abstract = {AbstractContext A Software Product Line is a set of software systems that are built from a common set of features. These systems are developed in a prescribed way and they can be adapted to fit the needs of customers. Feature models specify the properties of the systems that are meaningful to customers. A semantics that models the feature level has the potential to support the automatic analysis of entire software product lines. Objective The objective of this paper is to define a formal framework for Software Product Lines. This framework needs to be general enough to provide a formal semantics for existing frameworks like {\{}FODA{\}} (Feature Oriented Domain Analysis), but also to be easily adaptable to new problems. Method We define an algebraic language, called SPLA, to describe Software Product Lines. We provide the semantics for the algebra in three different ways. The approach followed to give the semantics is inspired by the semantics of process algebras. First we define an operational semantics, next a denotational semantics, and finally an axiomatic semantics. We also have defined a representation of the algebra into propositional logic. Results We prove that the three semantics are equivalent. We also show how {\{}FODA{\}} diagrams can be automatically translated into SPLA. Furthermore, we have developed our tool, called AT, that implements the formal framework presented in this paper. This tool uses a SAT-solver to check the satisfiability of an SPL. Conclusion This paper defines a general formal framework for software product lines. We have defined three different semantics that are equivalent; this means that depending on the context we can choose the most convenient approach: operational, denotational or axiomatic. The framework is flexible enough because it is closely related to process algebras. Process algebras are a well-known paradigm for which many extensions have been defined. },
  doi      = {https://doi.org/10.1016/j.infsof.2013.05.005},
  keywords = {Feature models,Formal methods,Software product lines},
  url      = {http://www.sciencedirect.com/science/article/pii/S0950584913001262},
}

@Article{Angelov2012417,
  author   = {Angelov, Samuil and Grefen, Paul and Greefhorst, Danny},
  title    = {{A framework for analysis and design of software reference architectures}},
  journal  = {Information and Software Technology},
  year     = {2012},
  volume   = {54},
  number   = {4},
  pages    = {417--431},
  issn     = {0950-5849},
  abstract = {Context A software reference architecture is a generic architecture for a class of systems that is used as a foundation for the design of concrete architectures from this class. The generic nature of reference architectures leads to a less defined architecture design and application contexts, which makes the architecture goal definition and architecture design non-trivial steps, rooted in uncertainty. Objective The paper presents a structured and comprehensive study on the congruence between context, goals, and design of software reference architectures. It proposes a tool for the design of congruent reference architectures and for the analysis of the level of congruence of existing reference architectures. Method We define a framework for congruent reference architectures. The framework is based on state of the art results from literature and practice. We validate our framework and its quality as analytical tool by applying it for the analysis of 24 reference architectures. The conclusions from our analysis are compared to the opinions of experts on these reference architectures documented in literature and dedicated communication. Results Our framework consists of a multi-dimensional classification space and of five types of reference architectures that are formed by combining specific values from the multi-dimensional classification space. Reference architectures that can be classified in one of these types have better chances to become a success. The validation of our framework confirms its quality as a tool for the analysis of the congruence of software reference architectures. Conclusion This paper facilitates software architects and scientists in the inception, design, and application of congruent software reference architectures. The application of the tool improves the chance for success of a reference architecture. },
  doi      = {https://doi.org/10.1016/j.infsof.2011.11.009},
  keywords = {Software architecture design,Software domain architecture,Software product line architecture,Software reference architecture},
  url      = {http://www.sciencedirect.com/science/article/pii/S0950584911002333},
}

@Article{Mizouni20147549,
  author   = {Mizouni, Rabeb and Matar, Mohammad Abu and Mahmoud, Zaid Al and Alzahmi, Salwa and Salah, Aziz},
  title    = {{A framework for context-aware self-adaptive mobile applications {\{}SPL{\}}}},
  journal  = {Expert Systems with Applications},
  year     = {2014},
  volume   = {41},
  number   = {16},
  pages    = {7549--7564},
  issn     = {0957-4174},
  abstract = {Abstract Mobile Applications are rapidly emerging as a convenient medium for using a variety of services. Over time and with the high penetration of smartphones in society, self-adaptation has become an essential capability required by mobile application users. In an ideal scenario, an application is required to adjust its behavior according to the current context of its use. This raises the challenge in mobile computing towards the design and development of applications that sense and react to contextual changes to provide a value-added user experience. In its general sense, context information can relate to the environment, the user, or the device status. In this paper, we propose a novel framework for building context aware and adaptive mobile applications. Based on feature modeling and Software Product Lines (SPL) concepts, this framework guides the modeling of adaptability at design time and supports context awareness and adaptability at runtime. In the core of the approach, is a feature meta-model that incorporates, in addition to {\{}SPL{\}} concepts, application feature priorities to drive the adaptability. A tool, based on that feature model, is presented to model the mobile application features and to derive the {\{}SPL{\}} members. A mobile framework, built on top of {\{}OSGI{\}} framework to dynamically adapt the application at runtime is also described. },
  doi      = {https://doi.org/10.1016/j.eswa.2014.05.049},
  keywords = {Feature priority,Mobile devices,Multi-view variability model,Runtime adaptability,SPL},
  url      = {http://www.sciencedirect.com/science/article/pii/S0957417414003364},
}

@Article{Her2007740,
  author   = {Her, Jin Sun and Kim, Ji Hyeok and Oh, Sang Hun and Rhew, Sung Yul and Kim, Soo Dong},
  title    = {{A framework for evaluating reusability of core asset in product line engineering}},
  journal  = {Information and Software Technology},
  year     = {2007},
  volume   = {49},
  number   = {7},
  pages    = {740--760},
  issn     = {0950-5849},
  abstract = {Product line engineering (PLE) is a new effective approach to software reuse, where applications are generated by instantiating a core asset which is a large-grained reuse unit. Hence, a core asset is a key element of PLE, and therefore the reusability of the core asset largely determines the success of {\{}PLE{\}} projects. However, current quality models to evaluate reusability do not adequately address the unique characteristics of core assets in PLE. This paper proposes a comprehensive framework for evaluating the reusability of core assets. We first identify the key characteristics of core assets, and derive a set of quality attributes that characterizes the reusability of core assets. Then, we define metrics for each quality attribute and finally present practical guidelines for applying the evaluation framework in {\{}PLE{\}} projects. Using the proposed framework, the reusability of core assets can be more effectively and precisely evaluated. },
  doi      = {https://doi.org/10.1016/j.infsof.2006.08.008},
  keywords = {Core asset,Metric,Product line engineering,Quality model,Reusability},
  url      = {http://www.sciencedirect.com/science/article/pii/S095058490600111X},
}

@Article{Meyers20111223,
  author   = {Meyers, Bart and Vangheluwe, Hans},
  title    = {{A framework for evolution of modelling languages}},
  journal  = {Science of Computer Programming},
  year     = {2011},
  volume   = {76},
  number   = {12},
  pages    = {1223--1246},
  issn     = {0167-6423},
  abstract = {In model-driven engineering, evolution is inevitable over the course of the complete life cycle of complex software-intensive systems and more importantly of entire product families. Not only instance models, but also entire modelling languages are subject to change. This is in particular true for domain-specific languages, whose language constructs are tightly coupled to an application domain. The most popular approach to evolution in the modelling domain is a manual process, with tedious and error-prone migration of artefacts such as instance models as a result. This paper provides a taxonomy for evolution of modelling languages and discusses the different evolution scenarios for various kinds of modelling artefacts, such as instance models, meta-models, and transformation models. Subsequently, the consequences of evolution and the required remedial actions are decomposed into primitive scenarios such that all possible evolutions can be covered exhaustively. These primitives are then used in a high-level framework for the evolution of modelling languages. We suggest that our structured approach enables the design of (semi-)automatic modelling language evolution solutions. },
  annote   = {Special Issue on Software Evolution, Adaptability and Variability},
  doi      = {https://doi.org/10.1016/j.scico.2011.01.002},
  keywords = {Evolution,Language engineering,Model transformation,Model-driven engineering,Modelling languages},
  url      = {http://www.sciencedirect.com/science/article/pii/S0167642311000141},
}

@Article{Baghdadi201695,
  author   = {Baghdadi, Youcef},
  title    = {{A framework for social commerce design}},
  journal  = {Information Systems},
  year     = {2016},
  volume   = {60},
  pages    = {95--113},
  issn     = {0306-4379},
  abstract = {Abstract Interaction features of social web sites, including social networks and social media, enable a new kind of commerce referred to as social commerce (s-commerce). It refers to doing commerce in a collaborative and participative way, through a uniform and interactive enterprise interface, by extending current social web sites initially designed for social interactions of individuals, to promote new business models. On one hand, none of the major social networks or social media providers has yet figured out how to bring commercial transactions directly to their platforms. On the other hand, there is a lack of a comprehensive framework to shape social commerce from both business and {\{}IT{\}} perspectives, which would guide a design process of s-commerce platforms. Indeed, s-commerce platforms differ from e-commerce web sites in many aspects from both business and {\{}IT{\}} perspectives and has more challenges in terms of (i) business models, architectures, principles, and even theories, (ii) complex constructs in terms of participants, interaction features, communities, and content, and (iii) issues such as social, control, security, and privacy issues. Therefore, there is a need for framing the elements of s-commerce, focusing on enterprise social interactions as first class citizens, in an abstract model that guides the architecture, the requirement engineering, the design, and the implementation of a uniform and interactive enterprise interface. Only this enterprise social interaction-enabling interface would promote the emerging knowledge and intelligence that are required for value (co-) creation in s-commerce model. This work fills the gap by proposing a framework that guides a design process to develop s-commerce. },
  doi      = {https://doi.org/10.1016/j.is.2016.03.007},
  keywords = {Enterprise social interactions,Social commerce,Social commerce design process,Social commerce framework,Social design},
  url      = {http://www.sciencedirect.com/science/article/pii/S0306437916301144},
}

@Article{Gómez20141101,
  author   = {G{\'{o}}mez, Abel and Penad{\'{e}}s, M Carmen and Can{\'{o}}s, Jos{\'{e}} H and Borges, Marcos R S and Llavador, Manuel},
  title    = {{A framework for variable content document generation with multiple actors}},
  journal  = {Information and Software Technology},
  year     = {2014},
  volume   = {56},
  number   = {9},
  pages    = {1101--1121},
  issn     = {0950-5849},
  abstract = {AbstractContext Advances in customization have highlighted the need for tools supporting variable content document management and generation in many domains. Current tools allow the generation of highly customized documents that are variable in both content and layout. However, most frameworks are technology-oriented, and their use requires advanced skills in implementation-related tools, which means their use by end users (i.e. document designers) is severely limited. Objective Starting from past and current trends for customized document authoring, our goal is to provide a document generation alternative in which variants are specified at a high level of abstraction and content reuse can be maximized in high variability scenarios. Method Based on our experience in Document Engineering, we identified areas in the variable content document management and generation field open to further improvement. We first classified the primary sources of variability in document composition processes and then developed a methodology, which we called {\{}DPL{\}} – based on Software Product Lines principles – to support document generation in high variability scenarios. Results In order to validate the applicability of our methodology we implemented a tool – {\{}DPLfw{\}} – to carry out {\{}DPL{\}} processes. After using this in different scenarios, we compared our proposal with other state-of-the-art tools for variable content document management and generation. Conclusion The {\{}DPLfw{\}} showed a good capacity for the automatic generation of variable content documents equal to or in some cases surpassing other currently available approaches. To the best of our knowledge, {\{}DPLfw{\}} is the only framework that combines variable content and document workflow facilities, easing the generation of variable content documents in which multiple actors play different roles. },
  annote   = {Special Sections from “Asia-Pacific Software Engineering Conference (APSEC), 2012” and “ Software Product Line conference (SPLC), 2012”},
  doi      = {https://doi.org/10.1016/j.infsof.2013.12.006},
  keywords = {Document generation,Document product line,Document workflow,Feature modeling,Model driven engineering,Variable data printing},
  url      = {http://www.sciencedirect.com/science/article/pii/S0950584913002358},
}

@Article{Hofmeister2007106,
  author   = {Hofmeister, Christine and Kruchten, Philippe and Nord, Robert L and Obbink, Henk and Ran, Alexander and America, Pierre},
  title    = {{A general model of software architecture design derived from five industrial approaches}},
  journal  = {Journal of Systems and Software},
  year     = {2007},
  volume   = {80},
  number   = {1},
  pages    = {106--126},
  issn     = {0164-1212},
  abstract = {We compare five industrial software architecture design methods and we extract from their commonalities a general software architecture design approach. Using this general approach, we compare across the five methods the artifacts and activities they use or recommend, and we pinpoint similarities and differences. Once we get beyond the great variance in terminology and description, we find that the five approaches have a lot in common and match more or less the “ideal” pattern we introduced. From the ideal pattern we derive an evaluation grid that can be used for further method comparisons. },
  doi      = {https://doi.org/10.1016/j.jss.2006.05.024},
  keywords = {Architectural method,Software architecture,Software architecture analysis,Software architecture design},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121206001634},
}

@Article{Guo20112208,
  author   = {Guo, J and White, J and Wang, G and Li, J and Wang, Y},
  title    = {{A genetic algorithm for optimized feature selection with resource constraints in software product lines}},
  journal  = {Journal of Systems and Software},
  year     = {2011},
  volume   = {84},
  number   = {12},
  pages    = {2208--2221},
  abstract = {Software product line (SPL) engineering is a software engineering approach to building configurable software systems. SPLs commonly use a feature model to capture and document the commonalities and variabilities of the underlying software system. A key challenge when using a feature model to derive a new SPL configuration is determining how to find an optimized feature selection that minimizes or maximizes an objective function, such as total cost, subject to resource constraints. To help address the challenges of optimizing feature selection in the face of resource constraints, this paper presents an approach that uses G enetic A lgorithms for optimized FE ature S election (GAFES) in SPLs. Our empirical results show that GAFES can produce solutions with 86-97{\%} of the optimality of other automated feature selection algorithms and in 45-99{\%} less time than existing exact and heuristic feature selection techniques. {\textcopyright} 2011 Elsevier Inc. All rights reserved.},
  annote   = {cited By 69},
  doi      = {10.1016/j.jss.2011.06.026},
  keywords = {Automated features; Configurable; Configuration; E,Computer software selection and evaluation; Flow,Feature extraction},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053576823{\&}doi=10.1016{\%}2Fj.jss.2011.06.026{\&}partnerID=40{\&}md5=30f35b5b93f00b26fa9f0a96ec2e89a2},
}

@Article{SCHWAGERL201551,
  author   = {Schw{\"{a}}gerl, Felix and Uhrig, Sabrina and Westfechtel, Bernhard},
  title    = {{A graph-based algorithm for three-way merging of ordered collections in EMF models}},
  journal  = {Science of Computer Programming},
  year     = {2015},
  volume   = {113},
  pages    = {51--81},
  issn     = {0167-6423},
  abstract = {In EMF models, ordered collections appear as the values of multi-valued structural features. Traditional, text-based version control systems do not sufficiently support three-way merging of ordered collections inside EMF models since they cannot guarantee a consistent result. The operation three-way merging is defined as follows: based on a common base version b, two alternative versions a1 and a2 were developed by copying and modifying the base version. To reconcile these changes, a merged version m is to be created as a common successor of a1 and a2. In this paper, we present a graph algorithm to solve the problem of three-way merging of ordered collections in EMF models. Each version of a collection can be represented by means of a linearly ordered graph. To create the merged version, these graphs are combined to a merged collection graph using set formula. To create the merged collection, a generalized topological sort is performed on the merged collection graph. Conflicts occur in case the order of elements cannot be deduced automatically; these conflicts are resolved either interactively or by default rules. We have implemented the merge algorithm in our tool BTMerge, which performs a consistency-preserving three-way merge of versions of EMF models being instances of arbitrary Ecore models. Our implementation relies on an alternative form of representing multiple versions of a collection, namely a versioned collection graph which forms a superimposition of collection versions. The algorithm presented here is purely state-based. Matching and merging of collections are clearly separated sub-problems. Insertions and deletions performed on the elements of the collection are propagated into the merged version in a consistent way. Our algorithm makes only minimal assumptions with regard to the underlying product model and thus may be applied to ordered collections inside plain text or XML files. By taking arbitrary move operations into account, the algorithm considerably goes beyond the functionality of contemporary merge tools which cannot adequately handle move operations.},
  annote   = {Model Driven Development (Selected {\&} extended papers from MODELSWARD 2014)},
  doi      = {https://doi.org/10.1016/j.scico.2015.02.008},
  keywords = {EMF models,Graph algorithm,Model-driven software engineering,Three-way merging,Version control},
  url      = {http://www.sciencedirect.com/science/article/pii/S0167642315000532},
}

@Article{Küçük2015513,
  author   = {K{\"{u}}{\c{c}}{\"{u}}k, Dilek},
  title    = {{A high-level electrical energy ontology with weighted attributes}},
  journal  = {Advanced Engineering Informatics},
  year     = {2015},
  volume   = {29},
  number   = {3},
  pages    = {513--522},
  issn     = {1474-0346},
  abstract = {Abstract One of the significant application areas of domain ontologies is known to be text analysis applications like information extraction and text classification systems, and semantic portals. In this paper, we present a high-level ontology for the electrical energy domain. This domain ontology has weighted attributes to cover the inherent fuzziness in the textual representations of its concepts. Additionally, we have included in the ontology the necessary attributes to align the ontology concepts to on-line collaborative knowledge bases like Wikipedia and linked open data sources like DBpedia, other attributes to facilitate its use in multilingual applications, and concepts to hold the named entities in the domain. The ultimate ontology is aligned with the previously proposed ontologies for the energy-related subdomains after extending the latter ones with weighted attributes. We make the ultimate form of the electrical energy ontology, as well as the extended versions of the domain ontologies for the subdomains, available for research purposes. Also included in the paper are sample text analysis applications which mainly exploit the weighted attributes within the ontology. },
  doi      = {https://doi.org/10.1016/j.aei.2015.04.002},
  keywords = {Domain ontology,Electrical energy,Ontology learning,Text analysis applications,Weighted attributes,Wikipedia},
  url      = {http://www.sciencedirect.com/science/article/pii/S1474034615000385},
}

@Article{Chen20092051,
  author   = {Chen, Chung-Yang and Chen, Pei-Chi},
  title    = {{A holistic approach to managing software change impact}},
  journal  = {Journal of Systems and Software},
  year     = {2009},
  volume   = {82},
  number   = {12},
  pages    = {2051--2067},
  issn     = {0164-1212},
  abstract = {Change is inevitable in the software product lifecycle. When a software change occurs, all of the stakeholders and related artifacts should be considered in determining the success of the change action in a collaborative development environment such as {\{}JAD{\}} (joint application development). In this regard, current implementation-based or homogeneous impact analyses are insufficient; therefore, this paper presents a holistic approach to change impact analysis in handling not only software contents but also other items such as requirements, documents and data. This approach characterizes product contents and relates heterogeneous items by using attributes and linkages. It also uses an object-oriented propagation mechanism to handle dynamic looping in determining the impact of changes. A prototype, EPIC, was built to realize this approach and these concepts. A walkthrough example is provided in order to verify the work of the proposed approach. An empirical study is presented to discuss the benefits of the proposed approach and the application of {\{}EPIC{\}} in a software company. Lessons learned from the case study and improvement issues of the proposed approach and the tool are also discussed. },
  doi      = {https://doi.org/10.1016/j.jss.2009.06.052},
  keywords = {Collaborative development,Holistic approach,Object technology,Software change management},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121209001654},
}

@Article{Olumofin2007309,
  author   = {Olumofin, F G and Mi{\v{s}}i{\'{c}}, V B},
  title    = {{A holistic architecture assessment method for software product lines}},
  journal  = {Information and Software Technology},
  year     = {2007},
  volume   = {49},
  number   = {4},
  pages    = {309--323},
  abstract = {The success of architecture-centric development of software product lines is critically dependent upon the availability of suitable architecture assessment methods. While a number of architecture assessment methods are available and some of them have been widely used in the process of evaluating single product architectures, none of them is equipped to deal with the main challenges of product line development. In this paper we present an adaptation of the Architecture Tradeoff Analysis Method (ATAM) for the task of assessing product line architectures. The new method, labeled Holistic Product Line Architecture Assessment (HoPLAA), uses a holistic approach that focuses on risks and quality attribute tradeoffs - not only for the common product line architecture, but for the individual product architectures as well. In addition, it prescribes a qualitative analytical treatment of variation points using scenarios. The use of the new method is illustrated through a case study. {\textcopyright} 2006 Elsevier B.V. All rights reserved.},
  annote   = {cited By 15},
  doi      = {10.1016/j.infsof.2006.05.003},
  file     = {:Users/mac/Library/Application Support/Mendeley Desktop/Downloaded/Olumofin, Mi{\v{s}}i{\'{c}} - 2007 - A holistic architecture assessment method for software product lines.pdf:pdf},
  keywords = {Architecture Tradeoff Analysis Method (ATAM); Sof,Computer architecture; Information technology; Ris,Computer software},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-33846550580{\&}doi=10.1016{\%}2Fj.infsof.2006.05.003{\&}partnerID=40{\&}md5=3881718a2b055a96a89780dbdce15ba3},
}

@Article{Autili2013987,
  author   = {Autili, Marco and Benedetto, Paolo Di and Inverardi, Paola},
  title    = {{A hybrid approach for resource-based comparison of adaptable Java applications}},
  journal  = {Science of Computer Programming},
  year     = {2013},
  volume   = {78},
  number   = {8},
  pages    = {987--1009},
  issn     = {0167-6423},
  abstract = {During the last decade, context-awareness and adaptation have been receiving significant attention in many research areas. For application developers, the heterogeneity of resource-constrained mobile terminals creates serious problems for the development of mobile applications able to run properly on a large number of different devices. Thus, resource awareness plays a crucial role when developing such applications. It identifies the capability of being aware of the resources offered by an execution environment, in order to decide whether that environment is suited to receive and execute the application. Within this line of research, we propose Chameleon, a framework that provides both an integrated development environment and a proper context-aware support to adaptable Java applications for limited devices. In this paper we present the novel hybrid (from static to dynamic) analysis approach that Chameleon uses for inspecting (adaptable) Java programs with respect to their resource consumption in a given execution environment. This analysis permits to quantitatively compare alternative versions of the same program. The analysis is based on a resource model for specifying resource provisions and consumptions, and a parametric transition system that performs the actual analysis. },
  annote   = {Special section on software evolution, adaptability, and maintenance {\&} Special section on the Brazilian Symposium on Programming Languages},
  doi      = {https://doi.org/10.1016/j.scico.2012.01.005},
  keywords = {Adaptable applications,Analysis,Tool support},
  url      = {http://www.sciencedirect.com/science/article/pii/S0167642312000147},
}

@Article{dosSantosNeto20161243,
  author   = {{de Alc{\^{a}}ntara dos Santos Neto}, Pedro and Britto, Ricardo and {de Andrade Lira Rab{\^{e}}lo}, Ricardo and {de Almeida Cruz}, Jonathas Jivago and Lira, Werney Ayala Luz},
  title    = {{A hybrid approach to suggest software product line portfolios}},
  journal  = {Applied Soft Computing},
  year     = {2016},
  volume   = {49},
  pages    = {1243--1255},
  issn     = {1568-4946},
  abstract = {Abstract Software product line (SPL) development is a new approach to software engineering which aims at the development of a whole range of products. However, as long as {\{}SPL{\}} can be useful, there are many challenges regarding the use of that approach. One of the main problems which hinders the adoption of software product line (SPL) is the complexity regarding product management. In that context, we can remark the scoping problem. One of the existent ways to deal with scoping is the product portfolio scoping (PPS). {\{}PPS{\}} aims to define the products that should be developed as well as their key features. In general, that approach is driven by marketing aspects, like cost of the product and customer satisfaction. Defining a product portfolio by using the many different available aspects is a NP-hard problem. This work presents an improved hybrid approach to solve the feature model selection problem, aiming at supporting product portfolio scoping. The proposal is based in a hybrid approach not dependent on any particular algorithm/technology. We have evaluated the usefulness and scalability of our approach using one real {\{}SPL{\}} (ArgoUML-SPL) and synthetic SPLs. As per the evaluation results, our approach is both useful from a practitioner's perspective and scalable. },
  doi      = {https://doi.org/10.1016/j.asoc.2016.08.024},
  keywords = {Feature model selection problem,Fuzzy inference systems,NSGA-II,Product portfolio scoping,Search based feature model selection,Search based software engineering,Software product lines},
  url      = {http://www.sciencedirect.com/science/article/pii/S1568494616304185},
}

@Article{Siek2011423,
  author   = {Siek, Jeremy G and Lumsdaine, Andrew},
  title    = {{A language for generic programming in the large}},
  journal  = {Science of Computer Programming},
  year     = {2011},
  volume   = {76},
  number   = {5},
  pages    = {423--465},
  issn     = {0167-6423},
  abstract = {Generic programming is an effective methodology for developing reusable software libraries. Many programming languages provide generics and have features for describing interfaces, but none completely support the idioms used in generic programming. To address this need we developed the language G . The central feature of G is the concept, a mechanism for organizing constraints on generics that is inspired by the needs of modern C++ libraries. G provides modular type checking and separate compilation (even of generics). These characteristics support modular software development, especially the smooth integration of independently developed components. In this article we present the rationale for the design of G and demonstrate the expressiveness of G with two case studies: porting the Standard Template Library and the Boost Graph Library from C++ to G . The design of G shares much in common with the concept extension proposed for the next C++ Standard (the authors participated in its design) but there are important differences described in this article. },
  annote   = {Special Issue on Generative Programming and Component Engineering (Selected Papers from {\{}GPCE{\}} 2004/2005)},
  doi      = {https://doi.org/10.1016/j.scico.2008.09.009},
  keywords = {Associated types,Concepts,Functors,Generic programming,Generics,Modules,Polymorphism,Programming language design,Signatures,Software reuse,Type classes,Virtual types},
  url      = {http://www.sciencedirect.com/science/article/pii/S0167642308001123},
}

@Article{SADATMOHTASHAM20081196,
  author   = {Sadat-Mohtasham, S Hossein and Ghorbani, Ali A},
  title    = {{A language for high-level description of adaptive web systems}},
  journal  = {Journal of Systems and Software},
  year     = {2008},
  volume   = {81},
  number   = {7},
  pages    = {1196--1217},
  issn     = {0164-1212},
  abstract = {Adaptive Web systems (AWS) are Web-based systems that can adapt their features such as, presentation, content, and structure, based on users' behaviour and preferences, device capabilities, and environment attributes. A framework was developed in our research group to provide the necessary components and protocols for the development of adaptive Web systems; however, there were several issues and shortcomings (e.g. low productivity, lack of verification mechanisms, etc.) in using the framework that inspired the development of a domain-specific language for the framework. This paper focuses on the proposal, design, and implementation of AWL, the Adaptive Web Language, which is used to develop adaptive Web systems within our framework. Not only does AWL address the existing issues in the framework, but it also offers mechanisms to increase software quality attributes, especially, reusability. An example application named PENS (a personalized e-News system) is explained and implemented in AWL. AWL has been designed based on the analysis of the adaptive Web domain, having taken into account the principles of reuse-based software engineering (product-lines), domain-specific languages, and aspect-oriented programming. Specially, a novel design decision, inspired by aspect-oriented programming paradigm, allows separate specification of presentation features in an application from its adaptation features. The AWL's design decisions and their benefits are explained.},
  doi      = {https://doi.org/10.1016/j.jss.2007.08.033},
  keywords = {Adaptive web system,Aspect-oriented programming,Domain-specific programming language},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121207002233},
}

@Article{RodríguezCovili20132009,
  author   = {Rodr{\'{i}}guez-Covili, Juan and Ochoa, Sergio F},
  title    = {{A lightweight and distributed middleware to provide presence awareness in mobile ubiquitous systems}},
  journal  = {Science of Computer Programming},
  year     = {2013},
  volume   = {78},
  number   = {10},
  pages    = {2009--2025},
  issn     = {0167-6423},
  abstract = {Abstract Several researchers have identified the need to count on presence awareness in ubiquitous systems that support mobile activities, particularly when these systems are used to perform loosely-coupled mobile work. In such a work style, mobile users conduct face-to-face on-demand interactions, therefore counting on awareness information about the position and availability of potential collaborators becomes mandatory for these applications. Most proposed solutions that provide user presence awareness involve centralized components, have reusability limitations, or simply address a part of that service. This article presents a lightweight and fully distributed middleware named Moware, which allows developers to embed presence awareness services in mobile ubiquitous systems in a simple way. The article also describes the Moware architecture, its main components and strategies used to deal with several aspects of the presence awareness support. These design strategies can be reused by software designers to provide presence awareness capabilities into middleware and specific software applications. Moware services were embedded in a mobile ubiquitous system that supports inspectors during the construction inspection process. The preliminary results indicate that the middleware was easy to use for developers, and its services were useful for the end-users. },
  annote   = {Special section on Language Descriptions Tools and Applications (LDTA'08 {\&} '09) {\&} Special section on Software Engineering Aspects of Ubiquitous Computing and Ambient Intelligence (UCAmI 2011)},
  doi      = {https://doi.org/10.1016/j.scico.2013.02.003},
  keywords = {Loosely-coupled mobile work,Middleware,Mobile ubiquitous computing,Presence awareness},
  url      = {http://www.sciencedirect.com/science/article/pii/S0167642313000282},
}

@Article{Hanssen20121455,
  author        = {Hanssen, G K},
  title         = {{A longitudinal case study of an emerging software ecosystem: Implications for practice and theory}},
  journal       = {Journal of Systems and Software},
  year          = {2012},
  volume        = {85},
  number        = {7},
  pages         = {1455--1466},
  abstract      = {Software ecosystems is an emerging trend within the software industry, implying a shift from closed organizations and processes towards open structures, where actors external to the software development organization are becoming increasingly involved in development. This forms an ecosystem of organizations that are related through the shared interest in a software product, leading to new opportunities and new challenges to the industry and its organizational environment. To understand why and how this change occurs, we have followed the development of a software product line organization for a period of approximately five years. We have studied their change from a waterfall-like approach, via agile software product line engineering, towards an emerging software ecosystem. We discuss implications for practice, and propose a nascent theory on software ecosystems. We conclude that the observed change has led to an increase in collaboration across (previously closed) organizational borders, and to the development of a shared value consisting of two components: the technology (the product line, as an extensible platform), and the business domain it supports. Opening up both the technical interface of the product and the organizational interfaces are key enablers of such a change. {\textcopyright} 2012 Elsevier Inc. All rights reserved.},
  annote        = {cited By 39 listo},
  doi           = {10.1016/j.jss.2011.04.020},
  keywords      = {Agile software development,Agile softwares,Busin,Ecosystems,Software design,case study},
  mendeley-tags = {case study},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-79958283622{\&}doi=10.1016{\%}2Fj.jss.2011.04.020{\&}partnerID=40{\&}md5=8fb17817169ad361e9012b12f779eaba},
}

@Article{Catala20131930,
  author   = {Catala, Alejandro and Pons, Patricia and Jaen, Javier and Mocholi, Jose A and Navarro, Elena},
  title    = {{A meta-model for dataflow-based rules in smart environments: Evaluating user comprehension and performance}},
  journal  = {Science of Computer Programming},
  year     = {2013},
  volume   = {78},
  number   = {10},
  pages    = {1930--1950},
  issn     = {0167-6423},
  abstract = {A considerable part of the behavior in smart environments relies on event-driven and rule specification. Rules are the mechanism most often used to enable user customization of the environment. However, the expressiveness of the rules available to users in editing and other tools is usually either limited or the available rule editing interfaces are not designed for end-users with low skills in programming. This means we have to look for interaction techniques and new ways to define user customization rules. This paper describes a generic and flexible meta-model to support expressive rules enhanced with data flow expressions that will graphically support the definition of rules without writing code. An empirical study was conducted on the ease of understanding of the visual data flow expressions, which are the key elements in our rule proposal. The visual dataflow language was compared to its corresponding textual version in terms of comprehension and ease of learning by teenagers in exercises involving calculations, modifications, writing and detecting equivalences in expressions in both languages. Although the subjects had some previous experience in editing mathematical expressions on spreadsheets, the study found their performance with visual dataflows to be significantly better in calculation and modification exercises. This makes our dataflow approach a promising mechanism for expressing user-customized reactive behavior in Ambient Intelligence (AmI) environments. The performance of the rule matching processor was validated by means of two stress tests to ensure that the meta-model approach adopted would be able to scale up with the number of types and instances in the space. },
  annote   = {Special section on Language Descriptions Tools and Applications (LDTA'08 {\&} '09) {\&} Special section on Software Engineering Aspects of Ubiquitous Computing and Ambient Intelligence (UCAmI 2011)},
  doi      = {https://doi.org/10.1016/j.scico.2012.06.010},
  keywords = {Ambient intelligence,Customization,Dataflow,Event based,Non-expert programmer,Rule,Smart home,Visual language},
  url      = {http://www.sciencedirect.com/science/article/pii/S0167642312001232},
}

@Article{LARRUCEA2016247,
  author   = {Larrucea, Xabier and Nanclares, Felix and Santamaria, Izaskun},
  title    = {{A method for defining a regional software ecosystem strategy: Colombia as a case study}},
  journal  = {Technological Forecasting and Social Change},
  year     = {2016},
  volume   = {104},
  pages    = {247--258},
  issn     = {0040-1625},
  abstract = {Software ecosystems (SECO) have been related to products or to a community of developers around a product. The SECO concept can also be applied to describe regional software ecosystems in which different software companies collaborate in a specific market based on a set of concrete technologies and using a set of capabilities. This paper details a regional SECO concept and a method based on regional endogenous capabilities and country needs to define a SECO strategy. Traditional strategy definition approaches are top-down, whereas this approach is a blended approach that merges bottom-up based on current regional capabilities and top-down based on market and technology trends. This paper presents a large case study performed in 6 regions of Colombia. We conducted 49 interviews and 16 workshops in which 654 attendees participated, and we developed the Colombian ICT national strategic plan based on this approach.},
  doi      = {https://doi.org/10.1016/j.techfore.2016.01.008},
  keywords = {Regional software ecosystems,Strategy,TRM},
  url      = {http://www.sciencedirect.com/science/article/pii/S0040162516000093},
}

@Article{Alsawalqah201479,
  author   = {Alsawalqah, H I and Kang, S and Lee, J},
  title    = {{A method to optimize the scope of a software product platform based on end-user features}},
  journal  = {Journal of Systems and Software},
  year     = {2014},
  volume   = {98},
  pages    = {79--106},
  abstract = {Context: Due to increased competition and the advent of mass customization, many software firms are utilizing product families - groups of related products derived from a product platform - to provide product variety in a cost-effective manner. The key to designing a successful software product family is the product platform, so it is important to determine the most appropriate product platform scope related to business objectives, for product line development.
Aim: This paper proposes a novel method to find the optimized scope of a software product platform based on end-user features.
Method: The proposed method, PPSMS (Product Platform Scoping Method for Software Product Lines), mathematically formulates the product platform scope selection as an optimization problem. The problem formulation targets identification of an optimized product platform scope that will maximize life cycle cost savings and the amount of commonality, while meeting the goals and needs of the envisioned customers' segments. A simulated annealing based algorithm that can solve problems heuristically is then used to help the decision maker in selecting a scope for the product platform, by performing tradeoff analysis of the commonality and cost savings objectives.
Results In a case study, PPSMS helped in identifying 5 non-dominated solutions considered to be of highest preference for decision making, taking into account both cost savings and commonality objectives. A quantitative and qualitative analysis indicated that human experts perceived value in adopting the method in practice, and that it was effective in identifying appropriate product platform scope. {\textcopyright} 2014 Elsevier Inc. All rights reserved.},
  annote   = {cited By 5},
  doi      = {10.1016/j.jss.2014.08.034},
  keywords = {End users; Software product line engineerings; Sof},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908291258{\&}doi=10.1016{\%}2Fj.jss.2014.08.034{\&}partnerID=40{\&}md5=b78a56f37674d45f046b08e09e784143},
}

@Article{Thurimella20131831,
  author   = {Thurimella, A K and Br{\"{u}}gge, B},
  title    = {{A mixed-method approach for the empirical evaluation of the issue-based variability modeling}},
  journal  = {Journal of Systems and Software},
  year     = {2013},
  volume   = {86},
  number   = {7},
  pages    = {1831--1849},
  abstract = {Background: Variability management is the fundamental part of software product line engineering, which deals with customization and reuse of artifacts for developing a family of systems. Rationale approaches structure decision-making by managing the tacit-knowledge behind decisions. This paper reports a quasi-experiment for evaluating a rationale enriched collaborative variability management methodology called issue-based variability modeling. Objective: We studied the interaction of stakeholders with issue-based modeling to evaluate its applicability in requirements engineering teams. Furthermore, we evaluated the reuse of rationale while instantiating and changing variability. Approach: We enriched a quasi-experimental design with a variety of methods found in case study research. A sample of 258 students was employed with data collection and analysis based on a mix of qualitative and quantitative methods. Our study was performed in two phases: the first phase focused on variability identification and instantiation, while the second phase included tasks on variability evolution. Results: We obtained strong empirical evidence on reuse patterns for rationale during instantiation and evolution of variability. The tabular representations used by rationale modeling are learnable and usable in teams of diverse backgrounds. {\textcopyright} 2013 Elsevier Inc. All rights reserved.},
  annote   = {cited By 7},
  doi      = {10.1016/j.jss.2013.01.038},
  keywords = {Computer software,Empirical evaluations; Empirical Software Engineer,Requirements engineering; Software design},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879943433{\&}doi=10.1016{\%}2Fj.jss.2013.01.038{\&}partnerID=40{\&}md5=2fc1982c04931e26e6518b14ddf38d75},
}

@Article{Diaz2015,
  author   = {D{\'{i}}az, Jessica and P{\'{e}}rez, Jennifer and Garbajosa, Juan},
  title    = {{A model for tracing variability from features to product-line architectures: A case study in smart grids}},
  journal  = {Requirements Engineering},
  year     = {2015},
  volume   = {20},
  number   = {3},
  pages    = {323--343},
  issn     = {1432010X},
  abstract = {In current software systems with highly volatile requirements, traceability plays a key role to maintain the consistency between requirements and code. Traceability between artifacts involved in the development of software product line (SPL) is still more critical because it is nec- essary to guarantee that the selection of variants that realize the different SPL products meet the requirements. Current SPL traceability mechanisms trace from variability in features to variations in the configuration of product-line architecture (PLA) in terms of adding and removing components. However, it is not always possible to mate- rialize the variable features of a SPL through adding or removing components, since sometimes they are materi- alized inside components, i.e., in part of their functionality: a class, a service, and/or an interface. Additionally, varia- tions that happen inside components may crosscut several components of architecture. These kinds of variations are still challenging and their traceability is not currently well supported. Therefore, it is not possible to guarantee that those SPL products with these kinds of variations meet the requirements. This paper presents a solution for tracing variability from features to PLA by taking these kinds of variations into account. This solution is based on models and traceability between models in order to automate SPL configuration by selecting the variants and realizing the product application. The FPLA modeling framework sup- ports this solution which has been deployed in a software factory. Validation has consisted in putting the solution into practice to develop a product line of power metering management applications for smart grids.},
  doi      = {10.1007/s00766-014-0203-1},
  file     = {:Users/mac/ownCloud/Anita/SLR/REVISION DE JOURNALS/PAPERS{\_}D/SELECT DAVID/checked/analizados/casos de estudios analizados/7 A model for tracing variability from features to product-line architectures- A case study in smart grids.pdf:pdf},
  isbn     = {0076601402031},
  keywords = {Product-line architecture,Software product line engineering,Traceability modeling,Variability},
}

@Article{Pedrycz2011739,
  author   = {Pedrycz, Witold and Russo, Barbara and Succi, Giancarlo},
  title    = {{A model of job satisfaction for collaborative development processes}},
  journal  = {Journal of Systems and Software},
  year     = {2011},
  volume   = {84},
  number   = {5},
  pages    = {739--752},
  issn     = {0164-1212},
  abstract = {Modern software development relies on collaborative work as a means for sharing knowledge, distributing tasks and responsibilities, reducing risk of failures, and increasing the overall quality of the software product. Such objectives are achieved with a continuous share of the programmers' daily working life that inevitably influences the programmers' job satisfaction. One of the major challenges in process management is to determine the causes of this satisfaction. Traditional research models job satisfaction with social aspects of collaborative work like communication, work sustainability, and work environment. This study reflects on existing models of job satisfaction in collaborative environments, creates one for modern software development processes, and validates it with a retrospective comparative survey run on a sample of 108 respondents. In addition, the work investigates the impact on job satisfaction and its model of the agile practice of Pair Programming that pushes job sharing to the extreme. With this intent, the questionnaire also collected feedback from pair programmers whose responses were used for a comparative analysis. The results demonstrate that Pair Programming has actually a strong positive effect on satisfaction, work sustainability, and communication. },
  doi      = {https://doi.org/10.1016/j.jss.2010.12.018},
  keywords = {Job satisfaction,Log linear model,Pair programming},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121210003407},
}

@Article{Kastner:2009:MRP:1837852.1621632,
  author    = {K{\"{a}}stner, Christian and Apel, Sven and Kuhlemann, Martin},
  title     = {{A Model of Refactoring Physically and Virtually Separated Features}},
  journal   = {SIGPLAN Not.},
  year      = {2009},
  volume    = {45},
  number    = {2},
  pages     = {157--166},
  issn      = {0362-1340},
  address   = {New York, NY, USA},
  doi       = {10.1145/1837852.1621632},
  keywords  = {AHEAD,CIDE,FeatureHouse,preprocessor,refinements,separation of concerns,software product lines},
  publisher = {ACM},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/1837852.1621632},
}

@Article{Paz201667,
  author   = {Paz, Andr{\'{e}}s and Arboleda, Hugo},
  title    = {{A Model to Guide Dynamic Adaptation Planning in Self-Adaptive Systems}},
  journal  = {Electronic Notes in Theoretical Computer Science},
  year     = {2016},
  volume   = {321},
  pages    = {67--88},
  issn     = {1571-0661},
  abstract = {Abstract Self-adaptive enterprise applications have the ability to continuously reconfigure themselves according to changes in their execution contexts or user requirements. The infrastructure managing such systems is based on IBM's MAPE-K reference model: a Monitor and an Analyzer to sense and interpret context data, a Planner and an Executor to create and apply structural adaptation plans, and a Knowledge manager to share relevant information. In this paper we present a formal model, built on the principles of constraint satisfaction, to address dynamic adaptation planning for self-adaptive enterprise applications. We formalize, modify and extend the approach presented in [H. Arboleda, J. F. D{\'{i}}az, V. Vargas, and J.-C. Royer, “Automated reasoning for derivation of modeldriven spls,” in SPLC'10 MAPLE'10, 2010, pp. 181–188] for working with self-adaptation infrastructures in order to provide automated reasoning on the dynamic creation of structural adaptation plans. We use a running example to demonstrate the applicability of such model, even in situations where complex interactions arise between context elements and the target self-adaptive enterprise application. },
  annote   = {{\{}CLEI{\}} 2015, the {\{}XLI{\}} Latin American Computing Conference},
  doi      = {https://doi.org/10.1016/j.entcs.2016.02.005},
  keywords = {Automated Reasoning,Dynamic Adaptation Planning,Self-Adaptive Enterprise Applications},
  url      = {http://www.sciencedirect.com/science/article/pii/S1571066116300056},
}

@Article{FuentesFernández2012247,
  author   = {Fuentes-Fern{\'{a}}ndez, Rub{\'{e}}n and Pav{\'{o}}n, Juan and Garijo, Francisco},
  title    = {{A model-driven process for the modernization of component-based systems}},
  journal  = {Science of Computer Programming},
  year     = {2012},
  volume   = {77},
  number   = {3},
  pages    = {247--269},
  issn     = {0167-6423},
  abstract = {Software modernization is critical for organizations that need cost-effective solutions to deal with the rapid obsolescence of software and the increasing demand for new functionality. This paper presents the {\{}XIRUP{\}} modernization methodology, which proposes a highly iterative process, structured into four phases: preliminary evaluation, understanding, building and migration. This modernization process is feature-driven, component-based, focused on the early elicitation of key information, and relies on a model-driven approach with extensive use of experience from the previous projects. {\{}XIRUP{\}} has been defined in the European {\{}IST{\}} project MOMOCS, which has also built a suite of support tools. This paper introduces the process using a case study that illustrates its activities, related tools and results. The discussion highlights the specific characteristics of modernization projects and how a customized methodology can take advantage of them. },
  annote   = {Feature-Oriented Software Development (FOSD 2009)},
  doi      = {https://doi.org/10.1016/j.scico.2011.04.003},
  keywords = {Agile process,Component,Model-driven engineering,Modernization of software systems,Software engineering,Software methodology},
  url      = {http://www.sciencedirect.com/science/article/pii/S0167642311001110},
}

@Article{Abate2013459,
  author   = {Abate, Pietro and Cosmo, Roberto Di and Treinen, Ralf and Zacchiroli, Stefano},
  title    = {{A modular package manager architecture}},
  journal  = {Information and Software Technology},
  year     = {2013},
  volume   = {55},
  number   = {2},
  pages    = {459--474},
  issn     = {0950-5849},
  abstract = {Context The success of modern software distributions in the Free and Open Source world can be explained, among other factors, by the availability of a large collection of software packages and the possibility to easily install and remove those components using state-of-the-art package managers. However, package managers are often built using a monolithic architecture and hard-wired and ad-hoc dependency solvers implementing some customized heuristics. Objective We aim at laying the foundation for improving on existing package managers. Package managers should be complete, that is find a solution whenever there exists one, and allow the user to specify complex criteria that define how to pick the best solution according to the user's preferences. Method In this paper we propose a modular architecture relying on precise interface formalisms that allows the system administrator to choose from a variety of dependency solvers and backends. Results We have built a working prototype–called MPM–following the design advocated in this paper, and we show how it largely outperforms a variety of current package managers. Conclusion We argue that a modular architecture, allowing for delegating the task of constraint solving to external solvers, is the path that leads to the next generation of package managers that will deliver better results, offer more expressive preference languages, and be easily adaptable to new platforms. },
  annote   = {Special Section: Component-Based Software Engineering (CBSE), 2011},
  doi      = {https://doi.org/10.1016/j.infsof.2012.09.002},
  keywords = {Open source,Package manager,Software components,Software dependencies,Software repositories},
  url      = {http://www.sciencedirect.com/science/article/pii/S0950584912001851},
}

@Article{Bjarnason201661,
  author        = {Bjarnason, Elizabeth and Unterkalmsteiner, Michael and Borg, Markus and Engstr{\"{o}}m, Emelie},
  title         = {{A multi-case study of agile requirements engineering and the use of test cases as requirements}},
  journal       = {Information and Software Technology},
  year          = {2016},
  volume        = {77},
  pages         = {61--79},
  issn          = {0950-5849},
  abstract      = {AbstractContext It is an enigma that agile projects can succeed ‘without requirements' when weak requirements engineering is a known cause for project failures. While agile development projects often manage well without extensive requirements test cases are commonly viewed as requirements and detailed requirements are documented as test cases. Objective We have investigated this agile practice of using test cases as requirements to understand how test cases can support the main requirements activities, and how this practice varies. Method We performed an iterative case study at three companies and collected data through 14 interviews and two focus groups. Results The use of test cases as requirements poses both benefits and challenges when eliciting, validating, verifying, and managing requirements, and when used as a documented agreement. We have identified five variants of the test-cases-as-requirements practice, namely de facto, behaviour-driven, story-test driven, stand-alone strict and stand-alone manual for which the application of the practice varies concerning the time frame of requirements documentation, the requirements format, the extent to which the test cases are a machine executable specification and the use of tools which provide specific support for the practice of using test cases as requirements. Conclusions The findings provide empirical insight into how agile development projects manage and communicate requirements. The identified variants of the practice of using test cases as requirements can be used to perform in-depth investigations into agile requirements engineering. Practitioners can use the provided recommendations as a guide in designing and improving their agile requirements practices based on project characteristics such as number of stakeholders and rate of change.},
  doi           = {https://doi.org/10.1016/j.infsof.2016.03.008},
  keywords      = {Acceptance test,Agile development,Behaviour-driven development,Case study,Empirical software engineering,Requirements,Test-driven development,Test-first development,Testing,case study},
  mendeley-tags = {case study},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584916300544},
}

@Article{Albert201591,
  author   = {Albert, Elvira and Correas, Jes{\'{u}}s and Puebla, Germ{\'{a}}n and Rom{\'{a}}n-D{\'{i}}ez, Guillermo},
  title    = {{A multi-domain incremental analysis engine and its application to incremental resource analysis}},
  journal  = {Theoretical Computer Science},
  year     = {2015},
  volume   = {585},
  pages    = {91--114},
  issn     = {0304-3975},
  abstract = {Abstract The aim of incremental analysis is, given a program, its analysis results, and a series of changes to the program, to obtain the new analysis results as efficiently as possible and, ideally, without having to (re-)analyze fragments of code which are not affected by the changes. Incremental analysis can significantly reduce both the time and the memory requirements of analysis. The first contribution of this article is a multi-domain incremental fixed-point algorithm for a sequential Java-like language. The algorithm is multi-domain in the sense that it interleaves the (re-)analysis for multiple domains by taking into account dependencies among them. Importantly, this allows the incremental analyzer to invalidate only those analysis results previously inferred by certain dependent domains. The second contribution is an incremental resource usage analysis which, in its first phase, uses the multi-domain incremental fixed-point algorithm to carry out all global pre-analyses required to infer cost in an interleaved way. Such resource analysis is parametric on the cost metrics one wants to measure (e.g., number of executed instructions, number of objects created, etc.). Besides, we present a novel form of cost summaries which allows us to incrementally reconstruct only those components of cost functions affected by the changes. Experimental results in the costa system show that the proposed incremental analysis provides significant performance gains, ranging from a speedup of 1.48 up to 5.13 times faster than non-incremental analysis. },
  annote   = {Developments in Implicit Complexity},
  doi      = {https://doi.org/10.1016/j.tcs.2015.03.002},
  keywords = {Cost analysis,Incremental analysis,Resource usage analysis,Static analysis},
  url      = {http://www.sciencedirect.com/science/article/pii/S0304397515001954},
}

@Article{Atkinson2015289,
  author   = {Atkinson, Colin and Gerbig, Ralph and Fritzsche, Mathias},
  title    = {{A multi-level approach to modeling language extension in the Enterprise Systems Domain}},
  journal  = {Information Systems},
  year     = {2015},
  volume   = {54},
  pages    = {289--307},
  issn     = {0306-4379},
  abstract = {Abstract As the number and diversity of technologies involved in building enterprise systems continues to grow so does the importance of modeling tools that are able to present customized views of enterprise systems to different stakeholders according to their needs and skills. Moreover, since the range of required view types is continuously evolving, it must be possible to extend and enhance the languages and services offered by such tools on an ongoing basis. However, this can be difficult with today׳s modeling tools because the meta-models that define the languages, views and services they support are usually hardwired and thus not amenable to extensions. In practice, therefore, various workarounds have to be used to extend a tool׳s underlying meta-model. Some of these are built into the implemented modeling standards (e.g. {\{}UML{\}} 2, {\{}BPMN{\}} 2.0 and ArchiMate 2.0) while others have to be applied by complementary, external tools (e.g. annotation models). These techniques not only increase accidental complexity, they also reduce the ability of the modeling tool to ensure adherence to enterprise rules and constraints. In this paper we discuss the strengths and weaknesses of the various approaches for language extension and propose a modeling framework best able to support the main extension scenarios currently found in practice today. },
  doi      = {https://doi.org/10.1016/j.is.2015.01.003},
  keywords = {Linguistic classification,Model language extension,Multi-level modeling,Ontological classification,Orthogonal classification architecture},
  url      = {http://www.sciencedirect.com/science/article/pii/S0306437915000137},
}

@Article{Guizzo2017331,
  author   = {Guizzo, Giovani and Vergilio, Silvia R and Pozo, Aurora T R and Fritsche, Gian M},
  title    = {{A multi-objective and evolutionary hyper-heuristic applied to the Integration and Test Order Problem}},
  journal  = {Applied Soft Computing},
  year     = {2017},
  volume   = {56},
  pages    = {331--344},
  issn     = {1568-4946},
  abstract = {Abstract The field of Search-Based Software Engineering (SBSE) has widely utilized Multi-Objective Evolutionary Algorithms (MOEAs) to solve complex software engineering problems. However, the use of such algorithms can be a hard task for the software engineer, mainly due to the significant range of parameter and algorithm choices. To help in this task, the use of Hyper-heuristics is recommended. Hyper-heuristics can select or generate low-level heuristics while optimization algorithms are executed, and thus can be generically applied. Despite their benefits, we find only a few works using hyper-heuristics in the {\{}SBSE{\}} field. Considering this fact, we describe HITO, a Hyper-heuristic for the Integration and Test Order Problem, to adaptively select search operators while {\{}MOEAs{\}} are executed using one of the selection methods: Choice Function and Multi-Armed Bandit. The experimental results show that {\{}HITO{\}} can outperform the traditional {\{}MOEAs{\}} NSGA-II and MOEA/DD. {\{}HITO{\}} is also a generic algorithm, since the user does not need to select crossover and mutation operators, nor adjust their parameters. },
  doi      = {https://doi.org/10.1016/j.asoc.2017.03.012},
  keywords = {Hyper-heuristic,Metaheuristic,Multi-objective algorithm,Search-Based Software Engineering,Software testing},
  url      = {http://www.sciencedirect.com/science/article/pii/S1568494617301357},
}

@Article{Assunção2014119,
  author   = {Assun{\c{c}}{\~{a}}o, Wesley Klewerton Guez and Colanzi, Thelma Elita and Vergilio, Silvia Regina and Pozo, Aurora},
  title    = {{A multi-objective optimization approach for the integration and test order problem}},
  journal  = {Information Sciences},
  year     = {2014},
  volume   = {267},
  pages    = {119--139},
  issn     = {0020-0255},
  abstract = {Abstract A common problem found during the integration testing is to determine an order to integrate and test the units. Important factors related to stubbing costs and constraints regarding to the software development context must be considered. To solve this problem, the most promising results were obtained with multi-objective algorithms, however few algorithms and contexts have been addressed by existing works. Considering such fact, this paper aims at introducing a generic approach based on multi-objective optimization to be applied in different development contexts and with distinct multi-objective algorithms. The approach is instantiated in the object and aspect-oriented contexts, and evaluated with real systems and three algorithms: NSGA-II, {\{}SPEA2{\}} and PAES. The algorithms are compared by using different number of objectives and four quality indicators. Results point out that the characteristics of the systems, the instantiation context and the number of objectives influence on the behavior of the algorithms. Although for more complex systems, {\{}PAES{\}} reaches better results, NSGA-II is more suitable to solve the referred problem in general cases, considering all systems and indicators. },
  doi      = {https://doi.org/10.1016/j.ins.2013.12.040},
  keywords = {Integration testing,Multi-objective optimization,Search-based algorithm},
  url      = {http://www.sciencedirect.com/science/article/pii/S0020025513008967},
}

@Article{Martini20164,
  author        = {Martini, A and Pareto, L and Bosch, J},
  title         = {{A multiple case study on the inter-group interaction speed in large, embedded software companies employing agile}},
  journal       = {Journal of Software: Evolution and Process},
  year          = {2016},
  volume        = {28},
  number        = {1},
  pages         = {4--26},
  abstract      = {The adoption of Agile Software Development in large companies is a recent phenomenon of great interest both for researchers and practitioners. Although intra-team interaction is well supported by established agile practices, the critical interaction between the agile team and other parts of the organization is still unexplored in literature. Such interactions slow down the development, hindering the achievement of business goals based on speed: short time to market, quick replication of products of a product-line, and reaction time for product evolution. We have employed a two-year long multiple-case case-study, collecting data through interviews and a survey in three large companies developing embedded software. Through a combination of qualitative and quantitative analysis, we have found strong evidence that interaction challenges between the development team and other groups in the organization hinder speed and are widespread in the organizations. This paper also identifies current practices in use at the studied companies and provides detailed guidelines for novel solutions in the investigated domain. Such practices are called boundary-spanning activities in information system research and coordination theory. We present a comparison between large embedded software companies employing agile and developing a line of products based on reused assets and agile companies developing pure software. We highlight specific contextual factors and areas where novel spanning activities are needed for mitigating the interaction challenges hindering speed. Copyright {\textcopyright} 2015 John Wiley {\&} Sons, Ltd.},
  annote        = {cited By 0},
  doi           = {10.1002/smr.1757},
  keywords      = {Agile software development,Boundary spanning,Co,Embedded software,Face recognition,Software design,Software engi,case study},
  mendeley-tags = {case study},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84956572746{\&}doi=10.1002{\%}2Fsmr.1757{\&}partnerID=40{\&}md5=47ef8f873f968c785386cd31a24d639f},
}

@Article{Arcaini201752,
  author   = {Arcaini, Paolo and Gargantini, Angelo and Riccobene, Elvinia and Vavassori, Paolo},
  title    = {{A novel use of equivalent mutants for static anomaly detection in software artifacts}},
  journal  = {Information and Software Technology},
  year     = {2017},
  volume   = {81},
  pages    = {52--64},
  issn     = {0950-5849},
  abstract = {Abstract Context: In mutation analysis, a mutant of a software artifact, either a program or a model, is said equivalent if it leaves the artifact meaning unchanged. Equivalent mutants are usually seen as an inconvenience and they reduce the applicability of mutation analysis. Objective: Instead, we here claim that equivalent mutants can be useful to define, detect, and remove static anomalies, i.e., deficiencies of given qualities: If an equivalent mutant has a better quality value than the original artifact, then an anomaly has been found and removed. Method: We present a process for detecting static anomalies based on mutation, equivalence checking, and quality measurement. Results: Our proposal and the originating technique are applicable to different kinds of software artifacts. We present anomalies and conduct several experiments in different contexts, at specification, design, and implementation level. Conclusion: We claim that in mutation analysis a new research direction should be followed, in which equivalent mutants and operators generating them are welcome. },
  doi      = {https://doi.org/10.1016/j.infsof.2016.01.019},
  keywords = {Equivalent mutant,Quality measure,Static anomaly},
  url      = {http://www.sciencedirect.com/science/article/pii/S0950584916300180},
}

@Article{Shi20181588,
  author   = {Shi, K and Yu, H and Guo, J and Fan, G and Yang, X},
  title    = {{A parallel portfolio approach to configuration optimization for large software product lines}},
  journal  = {Software - Practice and Experience},
  year     = {2018},
  volume   = {48},
  number   = {9},
  pages    = {1588--1606},
  abstract = {Software product line (SPL) engineering demands for optimal or near-optimal products that balance multiple often competing and conflicting objectives. A major challenge for large SPLs is to efficiently explore a huge space of various products and satisfy a large number of predefined constraints simultaneously. To improve the optimality and convergence speed, we propose a parallel portfolio approach, called IBEAPORT, which designs three algorithm variants by incorporating constraint solving into the indicator-based evolutionary algorithm in different ways and performs these variants by utilizing parallelization techniques. Our approach utilizes the exploration capabilities of different algorithms and improves optimality as far as possible within a limited time budget. We evaluate our approach on five large-scale real-world SPLs. Empirical results demonstrate that our approach significantly outperforms the state of the art for all five SPLs on a quality indicator and a diversity indicator. Moreover, IBEAPORT quickly converges to a relatively stable hypervolume value even for the largest SPL with 6888 features. {\textcopyright} 2018 John Wiley {\&} Sons, Ltd.},
  annote   = {cited By 0},
  doi      = {10.1002/spe.2594},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051181003{\&}doi=10.1002{\%}2Fspe.2594{\&}partnerID=40{\&}md5=ee488ce5f83aa22f026d84e38547ee7b},
}

@Article{Zhao20081272,
  author   = {Zhao, Liping and Macaulay, Linda and Adams, Jonathan and Verschueren, Paul},
  title    = {{A pattern language for designing e-business architecture}},
  journal  = {Journal of Systems and Software},
  year     = {2008},
  volume   = {81},
  number   = {8},
  pages    = {1272--1287},
  issn     = {0164-1212},
  abstract = {The pattern language for e-business provides a holistic support for developing software architectures for the e-business domain. The pattern language contains four related pattern categories: Business Patterns, Integration Patterns, Application Patterns, and Runtime Patterns. These pattern categories organise an e-business architecture into three layers—business interaction, application infrastructure and middleware infrastructure—and provide reusable design solutions to these layers in a top–down decomposition fashion. Business and Integration Patterns partition the business interaction layer into a set of subsystems; Application Patterns provide a high-level application infrastructure for these subsystems and separate business abstractions from their software solutions; Runtime Patterns then define a middleware infrastructure for the subsystems and shield design solutions from their implementations. The paper describes, demonstrates and evaluates this pattern language. },
  doi      = {https://doi.org/10.1016/j.jss.2007.11.717},
  keywords = {Architectural design,Pattern,Pattern languages,Software architecture,e-Business architecture},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121207003123},
}

@Article{Guizzo201477,
  author   = {Guizzo, G and Colanzi, T E and Vergilio, S R},
  title    = {{A pattern-driven mutation operator for search-based product line architecture design}},
  journal  = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year     = {2014},
  volume   = {8636 LNCS},
  pages    = {77--91},
  abstract = {The application of design patterns through mutation operators in search-based design may improve the quality of the architectures produced in the evolution process. However, we did not find, in the literature, works applying such patterns in the optimization of Product Line Architecture (PLA). Existing works offer manual approaches, which are not search-based, and only apply specific patterns in particular domains. Considering this fact, this paper introduces a meta-model and a mutation operator to allow the design patterns application in the search-based PLA design. The model represents suitable scopes, that is, set of architectural elements that are suitable to receive a pattern. The mutation operator is used with a multi-objective and evolutionary approach to obtain PLA alternatives. Quantitative and qualitative analysis of empirical results show an improvement in the quality of the obtained solutions. {\textcopyright} 2014 Springer International Publishing Switzerland.},
  annote   = {cited By 4},
  doi      = {10.1007/978-3-319-09940-8_6},
  keywords = {Architectural element; Design Patterns; Evolution,Architecture,Design; Software engineering},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958553590{\&}doi=10.1007{\%}2F978-3-319-09940-8{\_}6{\&}partnerID=40{\&}md5=2d80cf1f16b4ae50ca0b8c782e82e48e},
}

@Article{Wang201341,
  author   = {Wang, Y and Kobsa, A},
  title    = {{A PLA-based privacy-enhancing user modeling framework and its evaluation}},
  journal  = {User Modeling and User-Adapted Interaction},
  year     = {2013},
  volume   = {23},
  number   = {1},
  pages    = {41--82},
  abstract = {Reconciling personalization with privacy has been a continuing interest in user modeling research. This aim has computational, legal and behavioral/attitudinal ramifications. We present a dynamic privacy-enhancing user modeling framework that supports compliance with users' individual privacy preferences and with the privacy laws and regulations that apply to each user. The framework is based on a software product line architecture. It dynamically selects personalization methods during runtime that meet the current privacy constraints. Since dynamic architectural reconfiguration is typically resource-intensive, we conducted a performance evaluation with four implementations of our system that vary two factors. The results demonstrate that at least one implementation of our approach is technically feasible with comparatively modest additional resources, even for websites with the highest traffic today. To gauge user reactions to privacy controls that our framework enables, we also conducted a controlled experiment that allowed one group of users to specify privacy preferences and view the resulting effects on employed personalization methods. We found that users in this treatment group utilized this feature, deemed it useful, and had fewer privacy concerns as measured by higher disclosure of their personal data. {\textcopyright} 2012 Springer Science+Business Media B.V.},
  annote   = {cited By 6},
  doi      = {10.1007/s11257-011-9114-8},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872381970{\&}doi=10.1007{\%}2Fs11257-011-9114-8{\&}partnerID=40{\&}md5=1a3a96d04f816a8a746cd93b932acbf8},
}

@Article{Alvim201731,
  author   = {Alvim, L F M and Machado, I C and de Almeida, E S},
  title    = {{A preliminary assessment of variability implementation mechanisms in service-oriented computing}},
  journal  = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year     = {2017},
  volume   = {10221 LNCS},
  pages    = {31--47},
  abstract = {Service-Oriented Computing and Software Product Lines are software development strategies capable to provide a systematic means to reuse existing software assets, rather than repeatedly developing them from scratch, for every new software system. The inherent characteristics of both strategies has led the research community to combine them, in what is commonly referred to as Service-Oriented Product Lines (SOPL) strategies. Despite the perceived potential of such a combination, there are many challenges to confront in order to provide a practical generalizable solution. In particular, there is a lack of empirical evidence on the actual support of variability implementation mechanisms, typical in SPL engineering, and their suitability for SOPL. In line with such a challenge, this paper presents a preliminary assessment aimed to identify variability implementation mechanisms which may improve measures of complexity, instability and modularity, quality attributes particularly important for modular and reusable software systems, as is the case of SOPL. Based on the results of these evaluations, an initial decision model is developed to provide software engineers with an adequate support for the selection of variability mechanisms. {\textcopyright} Springer International Publishing AG 2017.},
  annote   = {cited By 0},
  doi      = {10.1007/978-3-319-56856-0_3},
  keywords = {Computer software reusability,Computer software; Distributed computer systems; S,Decision modeling; Implementation mechanisms; Inh},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019257326{\&}doi=10.1007{\%}2F978-3-319-56856-0{\_}3{\&}partnerID=40{\&}md5=bc005d44a2bb58ab1d2c387c316dbcdc},
}

@Article{Deelstra2004473,
  author   = {Deelstra, S and Sinnema, M and Bosch, J},
  title    = {{A product derivation framework for software product families}},
  journal  = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year     = {2004},
  volume   = {3014},
  pages    = {473--484},
  abstract = {From our experience with several organizations that employ software product families, we have learned that deriving individual products from shared software artifacts is a time-consuming and expensive activity. In the research community, product derivation methodologies are rather scarce, however. By studying product derivation, we believe we will be better able to provide and validate industrially practicable solutions for application engineering. In this paper, we present a framework of terminology and concepts regarding product derivation that serves as basis for further discussion. We exemplify this framework with two industrial case studies, i.e. Thales Nederland B.V. and Robert Bosch GmbH. {\textcopyright} Springer-Verlag Berlin Heidelberg 2004.},
  annote   = {cited By 1},
  keywords = {Application engineering; Industrial case study; P,Artificial intelligence,Computer science; Computers},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-35048844224{\&}partnerID=40{\&}md5=ce618c240de4f8fd9b6e4420479cd5dd},
}

@Article{Coallier200573,
  author   = {Coallier, Fran{\c{c}}ois and Champagne, Roger},
  title    = {{A Product Line engineering practices model}},
  journal  = {Science of Computer Programming},
  year     = {2005},
  volume   = {57},
  number   = {1},
  pages    = {73--87},
  issn     = {0167-6423},
  abstract = {This paper describes work in progress towards the elaboration of a Product Line practices model that combines concepts proposed by various authors. The strengths of existing Product Line frameworks and models are summarized and a new model is proposed in the form of 31 Product Line practice areas, grouped in five categories. An important objective of this Product Line practices model is that it should be easily incorporated into existing development methodologies, while remaining aligned with existing systems engineering standards. },
  annote   = {System and Software Architectures3rd International Workshop on System/Software Architectures},
  doi      = {https://doi.org/10.1016/j.scico.2004.10.006},
  keywords = {Modeling,Product Lines,Software engineering,System analysis and design},
  url      = {http://www.sciencedirect.com/science/article/pii/S0167642304001923},
}

@Article{KARAM2008855,
  author   = {Karam, Marcel and Dascalu, Sergiu and Safa, Haidar and Santina, Rami and Koteich, Zeina},
  title    = {{A product-line architecture for web service-based visual composition of web applications}},
  journal  = {Journal of Systems and Software},
  year     = {2008},
  volume   = {81},
  number   = {6},
  pages    = {855--867},
  issn     = {0164-1212},
  abstract = {A web service-based web application (WSbWA) is a collection of web services or reusable proven software parts that can be discovered and invoked using standard Internet protocols. The use of these web services in the development process of WSbWAs can help overcome many problems of software use, deployment and evolution. Although the cost-effective software engineering of WSbWAs is potentially a very rewarding area, not much work has been done to accomplish short time to market conditions by viewing and dealing with WSbWAs as software products that can be derived from a common infrastructure and assets with a captured specific abstraction in the domain. Both Product Line Engineering (PLE) and Agile Methods (AMs), albeit with different philosophies, are software engineering approaches that can significantly shorten the time to market and increase the quality of products. Using the PLE approach we built, at the domain engineering level, a WSbWA-specific lightweight product-line architecture and combined it, at the application engineering level, with an Agile Method that uses a domain-specific visual language with direct manipulation and extraction capabilities of web services to perform customization and calibration of a product or WSBWA for a specific customer. To assess the effectiveness of our approach we designed and implemented a tool that we used to investigate the return on investment of the activities related to PLE and AMs. Details of our proposed approach, the related tool developed, and the experimental study performed are presented in this article together with a discussion of planned directions of future work.},
  annote   = {Agile Product Line Engineering},
  doi      = {https://doi.org/10.1016/j.jss.2007.10.031},
  keywords = {Agile methods,Product line architecture,Product line engineering,Visual languages,Web services},
  url      = {http://www.sciencedirect.com/science/article/pii/S016412120700252X},
}

@Article{Usman2017,
  author    = {Usman, Muhammad and Iqbal, Muhammad Zohaib and Khan, Muhammad Uzair},
  title     = {{A product-line model-driven engineering approach for generating feature-based mobile applications}},
  journal   = {Journal of Systems and Software},
  year      = {2017},
  volume    = {123},
  number    = {October 2013},
  pages     = {1--32},
  issn      = {01641212},
  abstract  = {A significant challenge faced by the mobile application industry is developing and maintaining multiple native variants of mobile applications to support different mobile operating systems, devices and varying application functional requirements. The current industrial practice is to develop and maintain these variants separately. Any potential change has to be applied across variants manually, which is neither efficient nor scalable. We consider the problem of supporting multiple platforms as a ‘software product-line engineering' problem. The paper proposes a novel application of product-line model-driven engineering to mobile application development and addresses the key challenges of feature-based native mobile application variants for multiple platforms. Specifically, we deal with three types of variations in mobile applications: variation due to operation systems and their versions, software and hardware capabilities of mobile devices, and functionalities offered by the mobile application. We develop a tool MOPPET that automates the proposed approach. Finally, the results of applying the approach on two industrial case studies show that the proposed approach is applicable to industrial mobile applications and have potential to significantly reduce the development effort and time.},
  doi       = {10.1016/j.jss.2016.09.049},
  file      = {:Users/mac/ownCloud/Anita/SLR/REVISION DE JOURNALS/PAPERS{\_}A/selected/checked/casos de estudios analizados/24 A product-line model-driven engineering approach for generating feature-based mobile applications.pdf:pdf},
  isbn      = {0769523420 | 9780769523422},
  keywords  = {Feature model,Mobile applications,Software product-line engineering},
  publisher = {Elsevier Inc.},
}

@Article{Grbac2016967,
  author   = {Grbac, T G and Runeson, P and Huljeni{\'{c}}, D},
  title    = {{A quantitative analysis of the unit verification perspective on fault distributions in complex software systems: an operational replication}},
  journal  = {Software Quality Journal},
  year     = {2016},
  volume   = {24},
  number   = {4},
  pages    = {967--995},
  abstract = {Unit verification, including software inspections and unit tests, is usually the first code verification phase in the software development process. However, principles of unit verification are weakly explored, mostly due to the lack of data, since unit verification data are rarely systematically collected and only a few studies have been published with such data from industry. Therefore, we explore the theory of fault distributions, originating in the quantitative analysis by Fenton and Ohlsson, in the weakly explored context of unit verification in large-scale software development. We conduct a quantitative case study on a sequence of four development projects on consecutive releases of the same complex software product line system for telecommunication exchanges. We replicate the operationalization from earlier studies, analyzed hypotheses related to the Pareto principle of fault distribution, persistence of faults, effects of module size, and quality in terms of fault densities, however, now from the perspective of unit verification. The patterns in unit verification results resemble those of later verification phases, e.g., regarding the Pareto principle, and may thus be used for prediction and planning purposes. Using unit verification results as predictors may improve the quality and efficiency of software verification. {\textcopyright} 2015, Springer Science+Business Media New York.},
  annote   = {cited By 0},
  doi      = {10.1007/s11219-015-9273-7},
  keywords = {Computer software; Computer software selection and,Empirical research; Replication; Software fault;,Verification},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84925610743{\&}doi=10.1007{\%}2Fs11219-015-9273-7{\&}partnerID=40{\&}md5=06a3c6fc1f779d20074176c121d7df1a},
}

@Article{Gaia2014230,
  author   = {Gaia, Felipe Nunes and Ferreira, Gabriel Coutinho Sousa and Figueiredo, Eduardo and {de Almeida Maia}, Marcelo},
  title    = {{A quantitative and qualitative assessment of aspectual feature modules for evolving software product lines}},
  journal  = {Science of Computer Programming},
  year     = {2014},
  volume   = {96, Part 2},
  pages    = {230--253},
  issn     = {0167-6423},
  abstract = {Abstract Feature-Oriented Programming (FOP) and Aspect-Oriented Programming (AOP) are programming techniques based on composition mechanisms, called refinements and aspects, respectively. These techniques are assumed to be good variability mechanisms for implementing Software Product Lines (SPLs). Aspectual Feature Modules (AFM) is an approach that combines advantages of feature modules and aspects to increase concern modularity. Some guidelines on how to integrate these techniques have been established in some studies, but these studies do not focus the analysis on how effectively {\{}AFM{\}} can preserve the modularity and stability facilitating {\{}SPL{\}} evolution. The main purpose of this paper is to investigate whether the simultaneous use of aspects and features through the {\{}AFM{\}} approach facilitates the evolution of SPLs. The quantitative data were collected from two {\{}SPLs{\}} developed using four different variability mechanisms: (1) feature modules, aspects and aspects refinements of AFM, (2) aspects of aspect-oriented programming (AOP), (3) feature modules of feature-oriented programming (FOP), and (4) conditional compilation (CC) with object-oriented programming. Metrics for change propagation and modularity were calculated and the results support the benefits of the {\{}AFM{\}} option in a context where the product line has been evolved with addition or modification of crosscutting concerns. However a drawback of this approach is that refactoring components' design requires a higher degree of modifications to the {\{}SPL{\}} structure. },
  annote   = {Selected and extended papers of the Brazilian Symposium on Programming Languages 2012 (SBLP 2012)},
  doi      = {https://doi.org/10.1016/j.scico.2014.03.006},
  keywords = {Aspect-oriented programming,Aspectual feature modules,Feature-oriented programming,Software product lines,Variability mechanisms},
  url      = {http://www.sciencedirect.com/science/article/pii/S0167642314001336},
}

@Article{Guerra20131239,
  author   = {Guerra, Eduardo and Alves, Felipe and Kulesza, Uir{\'{a}} and Fernandes, Clovis},
  title    = {{A reference architecture for organizing the internal structure of metadata-based frameworks}},
  journal  = {Journal of Systems and Software},
  year     = {2013},
  volume   = {86},
  number   = {5},
  pages    = {1239--1256},
  issn     = {0164-1212},
  abstract = {Metadata-based frameworks enable behavior adaptation through the configuration of custom metadata in application classes. Most of the current frameworks used in the industry for building enterprise applications adopt this approach. However, there is a lack of proven techniques for building such kind of framework, allowing for a better organization of its internal structure. In this paper we propose a pattern language and a reference architecture for better organizing the internal structure of metadata-based frameworks, which were defined as a result of a pattern mining process applied to a set of existing open source frameworks. To evaluate the resulting structure generated by the reference architecture application, a case study examined three frameworks developed according to the proposed reference architecture, each one referring to a distinct application domain. The assessment was conducted by using a metrics suite, metrics thresholds derived from a large set of open source metadata-based frameworks, a process for automatic detection of design disharmonies and manual source code analysis. As a result of this study, framework developers can understand and use the proposed reference architecture to develop new frameworks and refactor existing ones. The assessment revealed that the organization provided by the reference architecture is suitable for metadata-based frameworks, helping in the division of responsibility and functionality among their classes. },
  doi      = {https://doi.org/10.1016/j.jss.2012.12.024},
  keywords = {Framework,Metadata,Metadata-based framework,Pattern language,Reference architecture,Software architecture},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121212003366},
}

@Article{Brugali2012361,
  author   = {Brugali, D and Gherardi, L and Biziak, A and Luzzana, A and Zakharov, A},
  title    = {{A reuse-oriented development process for component-based robotic systems}},
  journal  = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year     = {2012},
  volume   = {7628 LNAI},
  pages    = {361--374},
  abstract = {State of the art in robot software development mostly relies on class library reuse and only to a limited extent to component-based design. In the BRICS project we have defined a software development process that is based on the two most recent and promising approaches to software reuse, i.e. Software Product Line (SPL) and Model-Driven Engineering (MDE). The aim of this paper is to illustrate the whole software development process that we have defined for developing flexible and reusable component-based robotics libraries, to exemplify it with the case study of robust navigation functionality, and to present the software tools that we have developed for supporting the proposed process. {\textcopyright} 2012 Springer-Verlag Berlin Heidelberg.},
  annote   = {cited By 9},
  doi      = {10.1007/978-3-642-34327-8_33},
  keywords = {Class libraries; Component based; Component based,Computer software reusability; Robots,Software design},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868026835{\&}doi=10.1007{\%}2F978-3-642-34327-8{\_}33{\&}partnerID=40{\&}md5=a0a0f5546d6072db1d495974364b898b},
}

@Article{Costa2015239,
  author   = {Costa, G C B and Braga, R and David, J M N and Campos, F},
  title    = {{A Scientific Software Product Line for the Bioinformatics domain}},
  journal  = {Journal of Biomedical Informatics},
  year     = {2015},
  volume   = {56},
  pages    = {239--264},
  abstract = {Context: Most specialized users (scientists) that use bioinformatics applications do not have suitable training on software development. Software Product Line (SPL) employs the concept of reuse considering that it is defined as a set of systems that are developed from a common set of base artifacts. In some contexts, such as in bioinformatics applications, it is advantageous to develop a collection of related software products, using SPL approach. If software products are similar enough, there is the possibility of predicting their commonalities, differences and then reuse these common features to support the development of new applications in the bioinformatics area. Objectives: This paper presents the PL-Science approach which considers the context of SPL and ontology in order to assist scientists to define a scientific experiment, and to specify a workflow that encompasses bioinformatics applications of a given experiment. This paper also focuses on the use of ontologies to enable the use of Software Product Line in biological domains. Method: In the context of this paper, Scientific Software Product Line (SSPL) differs from the Software Product Line due to the fact that SSPL uses an abstract scientific workflow model. This workflow is defined according to a scientific domain and using this abstract workflow model the products (scientific applications/algorithms) are instantiated. Results: Through the use of ontology as a knowledge representation model, we can provide domain restrictions as well as add semantic aspects in order to facilitate the selection and organization of bioinformatics workflows in a Scientific Software Product Line. The use of ontologies enables not only the expression of formal restrictions but also the inferences on these restrictions, considering that a scientific domain needs a formal specification. Conclusions: This paper presents the development of the PL-Science approach, encompassing a methodology and an infrastructure, and also presents an approach evaluation. This evaluation presents case studies in bioinformatics, which were conducted in two renowned research institutions in Brazil. {\textcopyright} 2015 Elsevier Inc.},
  annote   = {cited By 3},
  doi      = {10.1016/j.jbi.2015.05.014},
  file     = {:Users/mac/Library/Application Support/Mendeley Desktop/Downloaded/Costa et al. - 2015 - A Scientific Software Product Line for the Bioinformatics domain.pdf:pdf},
  keywords = {Algorithms; Brazil; Cloud Computing; Cluster Anal,Application programs; Bioinformatics; Computer sof,Bioinformatics applications; Bioinformatics workf,DNA; Software,Factual; Internet; Observer Variation; Programmin,Software design,algorithm; Article; bioinformatics; computer prog},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84938568062{\&}doi=10.1016{\%}2Fj.jbi.2015.05.014{\&}partnerID=40{\&}md5=e2192bcdee6013c6539bb89f6a19253a},
}

@Article{Lago2009168,
  author   = {Lago, Patricia and Muccini, Henry and van Vliet, Hans},
  title    = {{A scoped approach to traceability management}},
  journal  = {Journal of Systems and Software},
  year     = {2009},
  volume   = {82},
  number   = {1},
  pages    = {168--182},
  issn     = {0164-1212},
  abstract = {Traceability is the ability to describe and follow the life of a software artifact and a means for modeling the relations between software artifacts in an explicit way. Traceability has been successfully applied in many software engineering communities and has recently been adopted to document the transition among requirements, architecture and implementation. We present an approach to customize traceability to the situation at hand. Instead of automating tracing, or representing all possible traces, we scope the traces to be maintained to the activities stakeholders must carry out. We define core traceability paths, consisting of essential traceability links required to support the activities. We illustrate the approach through two examples: product derivation in software product lines, and release planning in software process management. By using a running software product line example, we explain why the core traceability paths identified are needed when navigating from feature to structural models and from family to product level and backward between models used in software product derivation. A feasibility study in release planning carried out in an industrial setting further illustrates the use of core traceability paths during production and measures the increase in performance of the development processes supported by our approach. These examples show that our approach can be successfully used to support both product and process traceability in a pragmatic yet efficient way. },
  annote   = {Special Issue: Software Performance - Modeling and Analysis},
  doi      = {https://doi.org/10.1016/j.jss.2008.08.026},
  keywords = {Software process management,Software product line,Traceability issues,Traceability paths},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121208002033},
}

@Article{Guo2016204,
  author   = {Guo, Hai-Feng},
  title    = {{A semantic approach for automated test oracle generation}},
  journal  = {Computer Languages, Systems {\&} Structures},
  year     = {2016},
  volume   = {45},
  pages    = {204--219},
  issn     = {1477-8424},
  abstract = {Abstract This paper presents the design, implementation, and applications of a software testing tool, TAO, which allows users to specify and generate test cases and oracles in a declarative way. Extended from its previous grammar-based test generation tool, {\{}TAO{\}} provides a declarative notation for defining denotational semantics on each productive grammar rule, such that when a test case is generated, its expected semantics will be evaluated automatically as well, serving as its test oracle. {\{}TAO{\}} further provides a simple tagging mechanism to embed oracles into test cases for bridging the automation between test case generation and software testing. Two practical case studies are used to illustrate how automated oracle generation can be effectively integrated with grammar-based test generation in different testing scenarios: locating fault-inducing input patterns on Java applications; and Selenium-based automated web testing. },
  doi      = {https://doi.org/10.1016/j.cl.2016.01.006},
  keywords = {Denotational semantics,Software testing,Test case generation,Test oracle},
  url      = {http://www.sciencedirect.com/science/article/pii/S147784241530021X},
}

@Article{Yongsiriwit2016168,
  author   = {Yongsiriwit, Karn and Assy, Nour and Gaaloul, Walid},
  title    = {{A semantic framework for configurable business process as a service in the cloud}},
  journal  = {Journal of Network and Computer Applications},
  year     = {2016},
  volume   = {59},
  pages    = {168--184},
  issn     = {1084-8045},
  abstract = {Abstract With the advent of Cloud Computing, new opportunities for Business Process Outsourcing services have emerged. Business Process as a Service (BPaaS), a new cloud service model, has recently gained a great importance for outsourcing cloud-based business processes constructed for multi-tenancy. In such a multi-tenant environment, using configurable business process models enables the sharing of a reference process among different tenants that can be customized according to specific needs. With a large choice of configurable process modeling languages, different providers may deliver configurable processes with common functionalities but different representations which makes the process discovery and configuration a manual tedious task. This in turn creates cloud silos and vendors lock-in with non-reusable configurable {\{}BPaaS{\}} models. Therefore, with the aim of enabling the interoperability between multiple {\{}BPaaS{\}} providers, we propose in this paper a semantic framework for {\{}BPaaS{\}} configurable models. Taking advantage of Semantic Web technologies and data mining techniques, our framework allows for (1) an ontology-based high level abstract representation of {\{}BPaaS{\}} configurable models enriched with configuration guidelines and (2) an automated approach for extracting the configuration guidelines from existing process repositories. To show the feasibility and effectiveness of our approach, we extend Signavio with our semantic framework and conduct experiments on a dataset from {\{}SAP{\}} reference model. },
  doi      = {https://doi.org/10.1016/j.jnca.2015.07.007},
  keywords = {BPaaS,Business Process as a Service,Cloud Computing,Configurable process model,Green {\{}IT{\}},Semantic technology},
  url      = {http://www.sciencedirect.com/science/article/pii/S1084804515001708},
}

@Article{Bassiliades2017203,
  author   = {Bassiliades, Nick and Symeonidis, Moisis and Meditskos, Georgios and Kontopoulos, Efstratios and Gouvas, Panagiotis and Vlahavas, Ioannis},
  title    = {{A semantic recommendation algorithm for the PaaSport platform-as-a-service marketplace}},
  journal  = {Expert Systems with Applications},
  year     = {2017},
  volume   = {67},
  pages    = {203--227},
  issn     = {0957-4174},
  abstract = {Abstract Platform as a service (PaaS) is one of the Cloud computing services that provide a computing platform in the Cloud, allowing customers to develop, run, and manage web applications without the complexity of building and maintaining the infrastructure. The primary disadvantage for an {\{}SME{\}} to enter the emerging PaaS market is the possibility of being locked into a certain platform, mostly provided by the market's giants. The PaaSport project focuses on facilitating {\{}SMEs{\}} to deploy business applications on the best-matching Cloud PaaS offering and to seamlessly migrate these applications on demand, via a thin, non-intrusive Cloud-broker, in the form of a Cloud PaaS Marketplace. PaaSport enables PaaS provider {\{}SMEs{\}} to roll out semantically interoperable PaaS offerings, by annotating them using a unified PaaS semantic model that has been defined as an {\{}OWL{\}} ontology. In this paper we focus on the recommendation algorithm that has been developed on top of the ontology, for providing the application developer with recommendations about the best-matching Cloud PaaS offering. The algorithm consists of: a) a matchmaking part, where the functional parameters of the application are taken into account to rule out inconsistent offerings, and b) a ranking part, where the non-functional parameters of the application are considered to score and rank offerings. Τhe algorithm is extensively evaluated showing linear scalability to the number of offerings and application requirements. Furthermore, it is extensible upon future semantic model extensions, because it is agnostic to domain specific concepts and parameters, using {\{}SPARQL{\}} template queries. },
  doi      = {https://doi.org/10.1016/j.eswa.2016.09.032},
  keywords = {Cloud application,Cloud computing,Platform offering,Platform-as-a-service,Ranking,Recommendation,Semantic interoperability,Semantic matchmaking},
  url      = {http://www.sciencedirect.com/science/article/pii/S0957417416305164},
}

@Article{Palviainen201417,
  author   = {Palviainen, Marko and Kuusij{\"{a}}rvi, Jarkko and Ovaska, Eila},
  title    = {{A semi-automatic end-user programming approach for smart space application development}},
  journal  = {Pervasive and Mobile Computing},
  year     = {2014},
  volume   = {12},
  pages    = {17--36},
  issn     = {1574-1192},
  abstract = {Abstract This article describes a semi-automatic end-user programming approach that: (i) assists in the creation of easy-to-apply Semantic End-User Application Programming Interfaces(S-APIs) for the {\{}APIs{\}} of legacy software components; and (ii) enables the usage of S-APIs in command-oriented and goal-oriented end-user application programming. Furthermore, a reference implementation is presented for the approach that provides visual programming tools and an agent-based execution environment for smart space applications. The use of the approach is exemplified and tested in a case study in which S-APIs are created for a home automation system and for a personal assistant application, and then utilized in end-user programming performed in desktop and mobile environments. },
  doi      = {https://doi.org/10.1016/j.pmcj.2013.04.002},
  keywords = {Command-oriented end-user programming,Goal-oriented end-user programming,Ontology,Smart Modeler,Smart space application},
  url      = {http://www.sciencedirect.com/science/article/pii/S157411921300062X},
}

@Article{Guillén20132294,
  author   = {Guill{\'{e}}n, Joaqu{\'{i}}n and Miranda, Javier and Murillo, Juan Manuel and Canal, Carlos},
  title    = {{A service-oriented framework for developing cross cloud migratable software}},
  journal  = {Journal of Systems and Software},
  year     = {2013},
  volume   = {86},
  number   = {9},
  pages    = {2294--2308},
  issn     = {0164-1212},
  abstract = {Whilst cloud computing has burst into the current scene as a technology that allows companies to access high computing rates at limited costs, cloud vendors have rushed to provide tools that allow developers to build software for their cloud platforms. The software developed with these tools is often tightly coupled to their services and restrictions. Consequently vendor lock in becomes a common problem which multiple cloud users have to tackle in order to exploit the full potential of cloud computing. A scenario where component-based applications are developed for being deployed across several clouds, and each component can independently be deployed in one cloud or another, remains fictitious due to the complexity and the cost of their development. This paper presents a cloud development framework for developing cloud agnostic applications that may be deployed indifferently across multiple cloud platforms. Information about cloud deployment and cloud integration is separated from the source code and managed by the framework. Interoperability between interdependent components deployed in different clouds is achieved by automatically generating services and service clients. This allows software developers to segment their applications into different modules that can easily be deployed and redistributed across heterogeneous cloud platforms. },
  doi      = {https://doi.org/10.1016/j.jss.2012.12.033},
  keywords = {Adaptation,Cloud framework,Cross-cloud applications},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121212003421},
}

@Article{NEEDHAM20071530,
  author   = {Needham, D M and Jones, S A},
  title    = {{A software fault tree key node metric}},
  journal  = {Journal of Systems and Software},
  year     = {2007},
  volume   = {80},
  number   = {9},
  pages    = {1530--1540},
  issn     = {0164-1212},
  abstract = {Analysis of software fault trees exposes failure events that can impact safety within safety-critical software product lines. This paper presents a software fault tree key node safety metric for measuring software safety within product lines. Fault tree structures impacting the metric's composition are provided, and the mathematical basis for the metric is defined. The metric is applied to an embedded control system as well as to a series of experiments expected to either improve or degrade system safety. The effectiveness of the metric is analyzed, and lessons learned during the application of the metric are discussed.},
  annote   = {Evaluation and Assessment in Software Engineering},
  doi      = {https://doi.org/10.1016/j.jss.2007.01.042},
  keywords = {Safety-critical software,Software fault trees,Software metrics,Software product lines},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121207000064},
}

@Article{April200973,
  author   = {April, Alain and Abran, Alain},
  title    = {{A Software Maintenance Maturity Model (S3M): Measurement Practices at Maturity Levels 3 and 4}},
  journal  = {Electronic Notes in Theoretical Computer Science},
  year     = {2009},
  volume   = {233},
  pages    = {73--87},
  issn     = {1571-0661},
  abstract = {Evaluation and continuous improvement of software maintenance are key contributors to improving software quality. The software maintenance function suffers from a scarcity of the management models that would facilitate these functions. This paper presents an overview of the measurement practices that are being introduced for level 3 and higher to the Software Maintenance Maturity Model (S3M). },
  annote   = {Proceedings of the International Workshop on Software Quality and Maintainability (SQM 2008)},
  doi      = {https://doi.org/10.1016/j.entcs.2009.02.062},
  keywords = {Maturity Model,Process Assessment,Process Improvement,Product Assessment,Software Maintenance},
  url      = {http://www.sciencedirect.com/science/article/pii/S157106610900067X},
}

@Article{Ognjanović20131094,
  author   = {Ognjanovi{\'{c}}, Ivana and Ga{\v{s}}evi{\'{c}}, Dragan and Bagheri, Ebrahim},
  title    = {{A stratified framework for handling conditional preferences: An extension of the analytic hierarchy process}},
  journal  = {Expert Systems with Applications},
  year     = {2013},
  volume   = {40},
  number   = {4},
  pages    = {1094--1115},
  issn     = {0957-4174},
  abstract = {Representing and reasoning over different forms of preferences is of crucial importance to many different fields, especially where numerical comparisons need to be made between critical options. Focusing on the well-known Analytical Hierarchical Process (AHP) method, we propose a two-layered framework for addressing different kinds of conditional preferences which include partial information over preferences and preferences of a lexicographic kind. The proposed formal two-layered framework, called CS-AHP, provides the means for representing and reasoning over conditional preferences. The framework can also effectively order decision outcomes based on conditional preferences in a way that is consistent with well-formed preferences. Finally, the framework provides an estimation of the potential number of violations and inconsistencies within the preferences. We provide and report extensive performance analysis for the proposed framework from three different perspectives, namely time-complexity, simulated decision making scenarios, and handling cyclic and partially defined preferences. },
  doi      = {https://doi.org/10.1016/j.eswa.2012.08.026},
  keywords = {AHP method,Comparative preferences,Conditional preferences,Lexicographic order,S-AHP method,Well-formed preferences},
  url      = {http://www.sciencedirect.com/science/article/pii/S0957417412009876},
}

@Article{Berger2013,
  author  = {Berger, Thorsten and She, Steven and Lotufo, Rafael and Wasowski, Andrzej and Czarnecki, Krzysztof},
  title   = {{A Study of Variability Models and Languages in the Systems Software Domain}},
  journal = {IEEE Transactions on Software Engineering},
  year    = {2013},
  volume  = {39},
  number  = {12},
  pages   = {1611--1640},
  month   = {dec},
  issn    = {0098-5589},
  doi     = {10.1109/TSE.2013.34},
  file    = {:Users/mac/Library/Application Support/Mendeley Desktop/Downloaded/Berger et al. - 2013 - A study of variability models and languages in the systems software domain(2).pdf:pdf},
  url     = {http://ieeexplore.ieee.org/document/6572787/},
}

@Article{Robinson:2010:SCS:1842713.1842717,
  author    = {Robinson, William N and Ding, Yi},
  title     = {{A Survey of Customization Support in Agent-based Business Process Simulation Tools}},
  journal   = {ACM Trans. Model. Comput. Simul.},
  year      = {2010},
  volume    = {20},
  number    = {3},
  pages     = {14:1----14:29},
  issn      = {1049-3301},
  address   = {New York, NY, USA},
  doi       = {10.1145/1842713.1842717},
  keywords  = {Agent-based modeling,application frameworks,encapsulation,event-driven simulation,modularity,software product line engineering},
  publisher = {ACM},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/1842713.1842717},
}

@Article{Babur20151088,
  author   = {Babur, {\"{O}}nder and Smilauer, Vit and Verhoeff, Tom and van den Brand, Mark},
  title    = {{A Survey of Open Source Multiphysics Frameworks in Engineering}},
  journal  = {Procedia Computer Science},
  year     = {2015},
  volume   = {51},
  pages    = {1088--1097},
  issn     = {1877-0509},
  abstract = {Abstract This paper presents a systematic survey of open source multiphysics frameworks in the en- gineering domains. These domains share many commonalities despite the diverse application areas. A thorough search for the available frameworks with both academic and industrial ori- gins has revealed numerous candidates. Considering key characteristics such as project size, maturity and visibility, we selected Elmer, OpenFOAM and Salome for a detailed analysis. All the public documentation for these tools has been manually collected and inspected. Based on the analysis, we built a feature model for multiphysics in engineering, which captures the commonalities and variability in the domain. We in turn validated the resulting model via two other tools; Kratos by manual inspection, and {\{}OOFEM{\}} by means of expert validation by domain experts. },
  annote   = {International Conference On Computational Science, {\{}ICCS{\}} 2015Computational Science at the Gates of Nature},
  doi      = {https://doi.org/10.1016/j.procs.2015.05.273},
  keywords = {Domain Analysis,Feature Model,Modelling and Simulation,Multiphysics,Multiscale},
  url      = {http://www.sciencedirect.com/science/article/pii/S1877050915010819},
}

@Article{Krupitzer2015184,
  author   = {Krupitzer, Christian and Roth, Felix Maximilian and VanSyckel, Sebastian and Schiele, Gregor and Becker, Christian},
  title    = {{A survey on engineering approaches for self-adaptive systems}},
  journal  = {Pervasive and Mobile Computing},
  year     = {2015},
  volume   = {17, Part B},
  pages    = {184--206},
  issn     = {1574-1192},
  abstract = {Abstract The complexity of information systems is increasing in recent years, leading to increased effort for maintenance and configuration. Self-adaptive systems (SASs) address this issue. Due to new computing trends, such as pervasive computing, miniaturization of {\{}IT{\}} leads to mobile devices with the emerging need for context adaptation. Therefore, it is beneficial that devices are able to adapt context. Hence, we propose to extend the definition of {\{}SASs{\}} and include context adaptation. This paper presents a taxonomy of self-adaptation and a survey on engineering SASs. Based on the taxonomy and the survey, we motivate a new perspective on {\{}SAS{\}} including context adaptation. },
  annote   = {10 years of Pervasive Computing' In Honor of Chatschik Bisdikian},
  doi      = {https://doi.org/10.1016/j.pmcj.2014.09.009},
  keywords = {Context adaptation,Self-adaptation,Self-adaptive systems,Survey,Taxonomy},
  url      = {http://www.sciencedirect.com/science/article/pii/S157411921400162X},
}

@Article{KHAN2016963,
  author   = {Khan, Saif Ur Rehman and Lee, Sai Peck and Ahmad, Raja Wasim and Akhunzada, Adnan and Chang, Victor},
  title    = {{A survey on Test Suite Reduction frameworks and tools}},
  journal  = {International Journal of Information Management},
  year     = {2016},
  volume   = {36},
  number   = {6, Part A},
  pages    = {963--975},
  issn     = {0268-4012},
  abstract = {Software testing is a widely accepted practice that ensures the quality of a System under Test (SUT). However, the gradual increase of the test suite size demands high portion of testing budget and time. Test Suite Reduction (TSR) is considered a potential approach to deal with the test suite size problem. Moreover, a complete automation support is highly recommended for software testing to adequately meet the challenges of a resource constrained testing environment. Several TSR frameworks and tools have been proposed to efficiently address the test-suite size problem. The main objective of the paper is to comprehensively review the state-of-the-art TSR frameworks to highlights their strengths and weaknesses. Furthermore, the paper focuses on devising a detailed thematic taxonomy to classify existing literature that helps in understanding the underlying issues and proof of concept. Moreover, the paper investigates critical aspects and related features of TSR frameworks and tools based on a set of defined parameters. We also rigorously elaborated various testing domains and approaches followed by the extant TSR frameworks. The results reveal that majority of TSR frameworks focused on randomized unit testing, and a considerable number of frameworks lacks in supporting multi-objective optimization problems. Moreover, there is no generalized framework, effective for testing applications developed in any programming domain. Conversely, Integer Linear Programming (ILP) based TSR frameworks provide an optimal solution for multi-objective optimization problems and improve execution time by running multiple ILP in parallel. The study concludes with new insights and provides an unbiased view of the state-of-the-art TSR frameworks. Finally, we present potential research issues for further investigation to anticipate efficient TSR frameworks.},
  doi      = {https://doi.org/10.1016/j.ijinfomgt.2016.05.025},
  keywords = {Fault localization,Frameworks,Regression testing,Test Suite Reduction,Test suite optimization},
  url      = {http://www.sciencedirect.com/science/article/pii/S0268401216303437},
}

@Article{MAHDAVIHEZAVEHI20171,
  author   = {Mahdavi-Hezavehi, Sara and Durelli, Vinicius H S and Weyns, Danny and Avgeriou, Paris},
  title    = {{A systematic literature review on methods that handle multiple quality attributes in architecture-based self-adaptive systems}},
  journal  = {Information and Software Technology},
  year     = {2017},
  volume   = {90},
  pages    = {1--26},
  issn     = {0950-5849},
  abstract = {Context
Handling multiple quality attributes (QAs) in the domain of self-adaptive systems is an understudied research area. One well-known approach to engineer adaptive software systems and fulfill QAs of the system is architecture-based self-adaptation. In order to develop models that capture the required knowledge of the QAs of interest, and to investigate how these models can be employed at runtime to handle multiple quality attributes, we need to first examine current architecture-based self-adaptive methods.
Objective
In this paper we review the state-of-the-art of architecture-based methods for handling multiple QAs in self-adaptive systems. We also provide a descriptive analysis of the collected data from the literature.
Method
We conducted a systematic literature review by performing an automatic search on 28 selected venues and books in the domain of self-adaptive systems. As a result, we selected 54 primary studies which we used for data extraction and analysis.
Results
Performance and cost are the most frequently addressed set of QAs. Current self-adaptive systems dealing with multiple QAs mostly belong to the domain of robotics and web-based systems paradigm. The most widely used mechanisms/models to measure and quantify QAs sets are QA data variables. After QA data variables, utility functions and Markov chain models are the most common models which are also used for decision making process and selection of the best solution in presence of many alternatives. The most widely used tools to deal with multiple QAs are PRISM and IBM's autonomic computing toolkit. KLAPER is the only language that has been specifically developed to deal with quality properties analysis.
Conclusions
Our results help researchers to understand the current state of research regarding architecture-based methods for handling multiple QAs in self-adaptive systems, and to identity areas for improvement in the future. To summarize, further research is required to improve existing methods performing tradeoff analysis and preemption, and in particular, new methods may be proposed to make use of models to handle multiple QAs and to enhance and facilitate the tradeoffs analysis and decision making mechanism at runtime.},
  doi      = {https://doi.org/10.1016/j.infsof.2017.03.013},
  url      = {http://www.sciencedirect.com/science/article/pii/S0950584917302860},
}

@Article{DaMotaSilveiraNeto2011407,
  author    = {{Da Mota Silveira Neto}, Paulo Anselmo and {Carmo MacHado}, Ivan Do and McGregor, John D. and {De Almeida}, Eduardo Santana and {De Lemos Meira}, Silvio Romero},
  title     = {{A systematic mapping study of software product lines testing}},
  journal   = {Information and Software Technology},
  year      = {2011},
  volume    = {53},
  number    = {5},
  pages     = {407--423},
  issn      = {09505849},
  abstract  = {Context: In software development, Testing is an important mechanism both to identify defects and assure that completed products work as specified. This is a common practice in single-system development, and continues to hold in Software Product Lines (SPL). Even though extensive research has been done in the SPL Testing field, it is necessary to assess the current state of research and practice, in order to provide practitioners with evidence that enable fostering its further development. Objective: This paper focuses on Testing in SPL and has the following goals: investigate state-of-the-art testing practices, synthesize available evidence, and identify gaps between required techniques and existing approaches, available in the literature. Method: A systematic mapping study was conducted with a set of nine research questions, in which 120 studies, dated from 1993 to 2009, were evaluated. Results: Although several aspects regarding testing have been covered by single-system development approaches, many cannot be directly applied in the SPL context due to specific issues. In addition, particular aspects regarding SPL are not covered by the existing SPL approaches, and when the aspects are covered, the literature just gives brief overviews. This scenario indicates that additional investigation, empirical and practical, should be performed. Conclusion: The results can help to understand the needs in SPL Testing, by identifying points that still require additional investigation, since important aspects regarding particular points of software product lines have not been addressed yet. {\textcopyright} 2010 Elsevier B.V. All rights reserved.},
  annote    = {From Duplicate 1 (A systematic mapping study of software product lines testing - Da Mota Silveira Neto, P A; Carmo MacHado, I D; McGregor, J D; De Almeida, E S; De Lemos Meira, S R) cited By 87},
  doi       = {10.1016/j.infsof.2010.12.003},
  file      = {:Users/mac/Library/Application Support/Mendeley Desktop/Downloaded/Da Mota Silveira Neto et al. - 2011 - A systematic mapping study of software product lines testing.pdf:pdf},
  isbn      = {0950-5849},
  keywords  = {Computer software selection and evaluation,Further development,Mappi,Mapping studies,Mapping study,Research que,Software design,Software product lines,Software testing},
  publisher = {Elsevier B.V.},
  url       = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952451558{\&}doi=10.1016{\%}2Fj.infsof.2010.12.003{\&}partnerID=40{\&}md5=48146e279174abef9ae7749f41cec642 http://dx.doi.org/10.1016/j.infsof.2010.12.003},
}

@Article{Laguna20131010,
  author   = {Laguna, M A and Crespo, Y},
  title    = {{A systematic mapping study on software product line evolution: From legacy system reengineering to product line refactoring}},
  journal  = {Science of Computer Programming},
  year     = {2013},
  volume   = {78},
  number   = {8},
  pages    = {1010--1034},
  abstract = {Software product lines (SPLs) are used in industry to develop families of similar software systems. Legacy systems, either highly configurable or with a story of versions and local variations, are potential candidates for reconfiguration as SPLs using reengineering techniques. Existing SPLs can also be restructured using specific refactorings to improve their internal quality. Although many contributions (including industrial experiences) can be found in the literature, we lack a global vision covering the whole life cycle of an evolving product line. This study aims to survey existing research on the reengineering of legacy systems into SPLs and the refactoring of existing SPLs in order to identify proven approaches and pending challenges for future research in both subfields. We launched a systematic mapping study to find as much literature as possible, covering the diverse terms involved in the search string (restructuring, refactoring, reengineering, etc. always connected with SPLs) and filtering the papers using relevance criteria. The 74 papers selected were classified with respect to several dimensions: main focus, research and contribution type, academic or industrial validation if included, etc. We classified the research approaches and analyzed their feasibility for use in industry. The results of the study indicate that the initial works focused on the adaptation of generic reengineering processes to SPL extraction. Starting from that foundation, several trends have been detected in recent research: the integrated or guided reengineering of (typically object-oriented) legacy code and requirements; specific aspect-oriented or feature-oriented refactoring into SPLs, and more recently, refactoring for the evolution of existing product lines. A majority of papers include academic or industrial case studies, though only a few are based on quantitative data. The degree of maturity of both subfields is different: Industry examples for the reengineering of the legacy system subfield are abundant, although more evaluation research is needed to provide better evidence for adoption in industry. Product line evolution through refactoring is an emerging topic with some pending challenges. Although it has recently received some attention, the theoretical foundation is rather limited in this subfield and should be addressed in the near future. To sum up, the main contributions of this work are the classification of research approaches as well as the analysis of remaining challenges, open issues, and research opportunities. {\textcopyright} 2012 Elsevier B.V. All rights reserved.},
  annote   = {cited By 24},
  doi      = {10.1016/j.scico.2012.05.003},
  keywords = {Computer software; Industry; Legacy systems; Rese,Evolution; Legacy system reengineering; Product li,Reengineering},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878225914{\&}doi=10.1016{\%}2Fj.scico.2012.05.003{\&}partnerID=40{\&}md5=b8fba00f0623f75c93648c14304019ce},
}

@Article{Holl2012828,
  author   = {Holl, Gerald and Gr{\"{u}}nbacher, Paul and Rabiser, Rick},
  title    = {{A systematic review and an expert survey on capabilities supporting multi product lines}},
  journal  = {Information and Software Technology},
  year     = {2012},
  volume   = {54},
  number   = {8},
  pages    = {828--852},
  issn     = {0950-5849},
  abstract = {Context Complex software-intensive systems comprise many subsystems that are often based on heterogeneous technological platforms and managed by different organizational units. Multi product lines (MPLs) are an emerging area of research addressing variability management for such large-scale or ultra-large-scale systems. Despite the increasing number of publications addressing {\{}MPLs{\}} the research area is still quite fragmented. Objective The aims of this paper are thus to identify, describe, and classify existing approaches supporting {\{}MPLs{\}} and to increase the understanding of the underlying research issues. Furthermore, the paper aims at defining success-critical capabilities of infrastructures supporting MPLs. Method Using a systematic literature review we identify and analyze existing approaches and research issues regarding MPLs. Approaches described in the literature support capabilities needed to define and operate MPLs. We derive capabilities supporting {\{}MPLs{\}} from the results of the systematic literature review. We validate and refine these capabilities based on a survey among experts from academia and industry. Results The paper discusses key research issues in {\{}MPLs{\}} and presents basic and advanced capabilities supporting MPLs. We also show examples from research approaches that demonstrate how these capabilities can be realized. Conclusions We conclude that approaches supporting {\{}MPLs{\}} need to consider both technical aspects like structuring large models and defining dependencies between product lines as well as organizational aspects such as distributed modeling and product derivation by multiple stakeholders. The identified capabilities can help to build, enhance, and evaluate {\{}MPL{\}} approaches. },
  annote   = {Special Issue: Voice of the Editorial BoardSpecial Issue: Voice of the Editorial Board},
  doi      = {https://doi.org/10.1016/j.infsof.2012.02.002},
  keywords = {Large-scale systems,Multi product lines,Product line engineering,Systematic literature review},
  url      = {http://www.sciencedirect.com/science/article/pii/S095058491200033X},
}

@Article{Khurum20091982,
  author   = {Khurum, M and Gorschek, T},
  title    = {{A systematic review of domain analysis solutions for product lines}},
  journal  = {Journal of Systems and Software},
  year     = {2009},
  volume   = {82},
  number   = {12},
  pages    = {1982--2003},
  abstract = {Domain analysis is crucial and central to software product line engineering (SPLE) as it is one of the main instruments to decide what to include in a product and how it should fit in to the overall software product line. For this reason many domain analysis solutions have been proposed both by researchers and industry practitioners. Domain analysis comprises various modeling and scoping activities. This paper presents a systematic review of all the domain analysis solutions presented until 2007. The goal of the review is to analyze the level of industrial application and/or empirical validation of the proposed solutions with the purpose of mapping maturity in terms of industrial application, as well as to what extent proposed solutions might have been evaluated in terms of usability and usefulness. The finding of this review indicates that, although many new domain analysis solutions for software product lines have been proposed over the years, the absence of qualitative and quantitative results from empirical application and/or validation makes it hard to evaluate the potential of proposed solutions with respect to their usability and/or usefulness for industry adoption. The detailed results of the systematic review can be used by individual researchers to see large gaps in research that give opportunities for future work, and from a general research perspective lessons can be learned from the absence of validation as well as from good examples presented. From an industry practitioner view, the results can be used to gauge as to what extent solutions have been applied and/or validated and in what manner, both valuable as input prior to industry adoption of a domain analysis solution. {\textcopyright} 2009 Elsevier Inc. All rights reserved.},
  annote   = {cited By 33},
  doi      = {10.1016/j.jss.2009.06.048},
  file     = {:Users/mac/Library/Application Support/Mendeley Desktop/Downloaded/Khurum, Gorschek - 2009 - A systematic review of domain analysis solutions for product lines.pdf:pdf},
  keywords = {Computer software; Industrial applications; Indus,Domain analysis; Domain modeling; Empirical eviden,Research},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-71749101571{\&}doi=10.1016{\%}2Fj.jss.2009.06.048{\&}partnerID=40{\&}md5=918b8e050edb9174e0efeaeb1f2290ee},
}

@Article{Chen2011,
  author    = {Chen, Lianping and {Ali Babar}, Muhammad},
  title     = {{A systematic review of evaluation of variability management approaches in software product lines}},
  journal   = {Information and Software Technology},
  year      = {2011},
  volume    = {53},
  number    = {4},
  pages     = {344--362},
  month     = {apr},
  issn      = {09505849},
  doi       = {10.1016/j.infsof.2010.12.006},
  file      = {:Users/mac/Library/Application Support/Mendeley Desktop/Downloaded/Chen, Ali Babar - 2011 - A systematic review of evaluation of variability management approaches in software product lines.pdf:pdf},
  keywords  = {software product line,systematic literature reviews,variability management},
  publisher = {Elsevier B.V.},
  url       = {http://linkinghub.elsevier.com/retrieve/pii/S0950584910002223},
}

@Article{Montagud2012425,
  author   = {Montagud, S and Abrah{\~{a}}o, S and Insfran, E},
  title    = {{A systematic review of quality attributes and measures for software product lines}},
  journal  = {Software Quality Journal},
  year     = {2012},
  volume   = {20},
  number   = {3-4},
  pages    = {425--486},
  abstract = {It is widely accepted that software measures provide an appropriate mechanism for understanding, monitoring, controlling, and predicting the quality of software development projects. In software product lines (SPL), quality is even more important than in a single software product since, owing to systematic reuse, a fault or an inadequate design decision could be propagated to several products in the family. Over the last few years, a great number of quality attributes and measures for assessing the quality of SPL have been reported in literature. However, no studies summarizing the current knowledge about them exist. This paper presents a systematic literature review with the objective of identifying and interpreting all the available studies from 1996 to 2010 that present quality attributes and/or measures for SPL. These attributes and measures have been classified using a set of criteria that includes the life cycle phase in which the measures are applied; the corresponding quality characteristics; their support for specific SPL characteristics (e. g., variability, compositionality); the procedure used to validate the measures, etc. We found 165 measures related to 97 different quality attributes. The results of the review indicated that 92{\%} of the measures evaluate attributes that are related to maintainability. In addition, 67{\%} of the measures are used during the design phase of Domain Engineering, and 56{\%} are applied to evaluate the product line architecture. However, only 25{\%} of them have been empirically validated. In conclusion, the results provide a global vision of the state of the research within this area in order to help researchers in detecting weaknesses, directing research efforts, and identifying new research lines. In particular, there is a need for new measures with which to evaluate both the quality of the artifacts produced during the entire SPL life cycle and other quality characteristics. There is also a need for more validation (both theoretical and empirical) of existing measures. In addition, our results may be useful as a reference guide for practitioners to assist them in the selection or the adaptation of existing measures for evaluating their software product lines. {\textcopyright} 2011 Springer Science+Business Media, LLC.},
  annote   = {cited By 24},
  doi      = {10.1007/s11219-011-9146-7},
  keywords = {Computer software reusability; Image quality; Life,Measures; Product line architecture; Quality attr,Quality control},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865626740{\&}doi=10.1007{\%}2Fs11219-011-9146-7{\&}partnerID=40{\&}md5=33b25cd0a25fc09685e06cbc9c988f14},
}

@Article{Silva201719,
  author   = {Silva, Rodolfo Adamshuk and {do Rocio Senger de Souza}, Simone and de Souza, Paulo S{\'{e}}rgio Lopes},
  title    = {{A systematic review on search based mutation testing}},
  journal  = {Information and Software Technology},
  year     = {2017},
  volume   = {81},
  pages    = {19--35},
  issn     = {0950-5849},
  abstract = {AbstractContext Search Based Software Testing refers to the use of meta-heuristics for the optimization of a task in the context of software testing. Meta-heuristics can solve complex problems in which an optimum solution must be found among a large amount of possibilities. The use of meta-heuristics in testing activities is promising because of the high number of inputs that should be tested. Previous studies on search based software testing have focused on the application of meta-heuristics for the optimization of structural and functional criteria. Recently, some researchers have proposed the use of {\{}SBST{\}} for mutation testing and explored solutions for the cost of application of this testing criterion. Objective The objective is to identify how {\{}SBST{\}} has been explored in the context of mutation testing, how fitness functions are defined and the challenges and research opportunities in the application of meta-heuristic search techniques. Method A systematic review involving 263 papers published between 1996 and 2014 examined the studies on the use of meta-heuristic search techniques for the optimization of mutation testing. Results The results show meta-heuristic search techniques have been applied for the optimization of test data generation, mutant generation and selection of effective mutation operators. Five meta-heuristic techniques, namely Genetic Algorithm, Ant Colony, Bacteriological Algorithm, Hill Climbing and Simulated Annealing have been used in search based mutation testing. The review addressed different fitness functions used to guide the search. Conclusion Search based mutation testing is a field of interest, however, some issues remain unexplored. For instance, the use of meta-heuristics for the selection of effective mutation operators was identified in only one study. The results have pointed a range of possibilities for new studies to be developed, i.e., identification of equivalent mutants, experimental studies and application to different domains, such as concurrent programs. },
  doi      = {https://doi.org/10.1016/j.infsof.2016.01.017},
  keywords = {Meta-heuristic,Mutation testing,Search based software testing},
  url      = {http://www.sciencedirect.com/science/article/pii/S0950584916300167},
}

@Article{SánchezGuinea2016251,
  author   = {Guinea, Alejandro S{\'{a}}nchez and Nain, Gr{\'{e}}gory and Traon, Yves Le},
  title    = {{A systematic review on the engineering of software for ubiquitous systems}},
  journal  = {Journal of Systems and Software},
  year     = {2016},
  volume   = {118},
  pages    = {251--276},
  issn     = {0164-1212},
  abstract = {Abstract Context: Software engineering for ubiquitous systems has experienced an important and rapid growth, however the vast research corpus makes it difficult to obtain valuable information from it. Objective: To identify, evaluate, and synthesize research about the most relevant approaches addressing the different phases of the software development life cycle for ubiquitous systems. Method: We conducted a systematic literature review of papers presenting and evaluating approaches for the different phases of the software development life cycle for ubiquitous systems. Approaches were classified according to the phase of the development cycle they addressed, identifying their main concerns and limitations. Results: We identified 128 papers reporting 132 approaches addressing issues related to different phases of the software development cycle for ubiquitous systems. Most approaches have been aimed at addressing the implementation, evolution/maintenance, and feedback phases, while others phases such as testing need more attention from researchers. Conclusion: We recommend to follow existing guidelines when conducting case studies to make the studies more reproducible and closer to real life cases. While some phases of the development cycle have been extensively explored, there is still room for research in other phases, toward a more agile and integrated cycle, from requirements to testing and feedback. },
  doi      = {https://doi.org/10.1016/j.jss.2016.05.024},
  keywords = {Development methods,Empirical software engineering,Evidence-based software engineering,Pervasive systems,Research synthesis,Software development cycle,Systematic review,Ubiquitous systems},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121216300553},
}

@Article{Tahir20132877,
  author   = {Tahir, Abbas and Tosi, Davide and Morasca, Sandro},
  title    = {{A systematic review on the functional testing of semantic web services}},
  journal  = {Journal of Systems and Software},
  year     = {2013},
  volume   = {86},
  number   = {11},
  pages    = {2877--2889},
  issn     = {0164-1212},
  abstract = {Abstract Semantic web services are gaining more attention as an important element of the emerging semantic web. Therefore, testing semantic web services is becoming a key concern as an essential quality assurance measure. The objective of this systematic literature review is to summarize the current state of the art of functional testing of semantic web services by providing answers to a set of research questions. The review follows a predefined procedure that involves automatically searching 5 well-known digital libraries. After applying the selection criteria to the results, a total of 34 studies were identified as relevant. Required information was extracted from the studies and summarized. Our systematic literature review identified some approaches available for deriving test cases from the specifications of semantic web services. However, many of the approaches are either not validated or the validation done lacks credibility. We believe that a substantial amount of work remains to be done to improve the current state of research in the area of testing semantic web services. },
  doi      = {https://doi.org/10.1016/j.jss.2013.06.064},
  keywords = {Functional testing,Semantic web services,Systematic literature review,Testing approach},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121213001659},
}

@Article{Niu2014,
  author  = {Niu, Nan and Savolainen, Juha and Niu, Zhendong and Jin, Mingzhou and Cheng, Jing-Ru C.},
  title   = {{A Systems Approach to Product Line Requirements Reuse}},
  journal = {IEEE Systems Journal},
  year    = {2014},
  volume  = {8},
  number  = {3},
  pages   = {827--836},
  month   = {sep},
  issn    = {1932-8184},
  doi     = {10.1109/JSYST.2013.2260092},
  url     = {http://ieeexplore.ieee.org/document/6545326/},
}

@Article{Zamli201657,
  author   = {Zamli, Kamal Z and Alkazemi, Basem Y and Kendall, Graham},
  title    = {{A Tabu Search hyper-heuristic strategy for t-way test suite generation}},
  journal  = {Applied Soft Computing},
  year     = {2016},
  volume   = {44},
  pages    = {57--74},
  issn     = {1568-4946},
  abstract = {Abstract This paper proposes a novel hybrid t-way test generation strategy (where t indicates interaction strength), called High Level Hyper-Heuristic (HHH). {\{}HHH{\}} adopts Tabu Search as its high level meta-heuristic and leverages on the strength of four low level meta-heuristics, comprising of Teaching Learning based Optimization, Global Neighborhood Algorithm, Particle Swarm Optimization, and Cuckoo Search Algorithm. {\{}HHH{\}} is able to capitalize on the strengths and limit the deficiencies of each individual algorithm in a collective and synergistic manner. Unlike existing hyper-heuristics, {\{}HHH{\}} relies on three defined operators, based on improvement, intensification and diversification, to adaptively select the most suitable meta-heuristic at any particular time. Our results are promising as {\{}HHH{\}} manages to outperform existing t-way strategies on many of the benchmarks. },
  doi      = {https://doi.org/10.1016/j.asoc.2016.03.021},
  keywords = {Cuckoo Search Algorithm,Global Neighborhood Algorithm,Hyper-heuristic,Particle Swarm Optimization,Software testing,Teaching Learning based Optimization,t-way Testing},
  url      = {http://www.sciencedirect.com/science/article/pii/S1568494616301302},
}

@Article{Classen20111130,
  author   = {Classen, Andreas and Boucher, Quentin and Heymans, Patrick},
  title    = {{A text-based approach to feature modelling: Syntax and semantics of {\{}TVL{\}}}},
  journal  = {Science of Computer Programming},
  year     = {2011},
  volume   = {76},
  number   = {12},
  pages    = {1130--1143},
  issn     = {0167-6423},
  abstract = {In the scientific community, feature models are the de-facto standard for representing variability in software product line engineering. This is different from industrial settings where they appear to be used much less frequently. We and other authors found that in a number of cases, they lack concision, naturalness and expressiveness. This is confirmed by industrial experience. When modelling variability, an efficient tool for making models intuitive and concise are feature attributes. Yet, the semantics of feature models with attributes is not well understood and most existing notations do not support them at all. Furthermore, the graphical nature of feature models' syntax also appears to be a barrier to industrial adoption, both psychological and rational. Existing tool support for graphical feature models is lacking or inadequate, and inferior in many regards to tool support for text-based formats. To overcome these shortcomings, we designed TVL, a text-based feature modelling language. In terms of expressiveness, {\{}TVL{\}} subsumes most existing dialects. The main goal of designing {\{}TVL{\}} was to provide engineers with a human-readable language with a rich syntax to make modelling easy and models natural, but also with a formal semantics to avoid ambiguity and allow powerful automation. },
  annote   = {Special Issue on Software Evolution, Adaptability and Variability},
  doi      = {https://doi.org/10.1016/j.scico.2010.10.005},
  file     = {:Users/mac/ownCloud/Anita/SLR/REVISION DE JOURNALS/PAPERS{\_}D/SELECT DAVID/checked/analizados/casos de estudios analizados/19 A text-based approach to feature modelling- Syntax and semantics of TVL.pdf:pdf},
  keywords = {Code,Feature models,Language,Modelling,Semantics,Software product lines,Syntax},
  url      = {http://www.sciencedirect.com/science/article/pii/S0167642310001899},
}

@Article{Borba20122,
  author   = {Borba, Paulo and Teixeira, Leopoldo and Gheyi, Rohit},
  title    = {{A theory of software product line refinement}},
  journal  = {Theoretical Computer Science},
  year     = {2012},
  volume   = {455},
  pages    = {2--30},
  issn     = {0304-3975},
  abstract = {To safely evolve a software product line, it is important to have a notion of product line refinement that assures behavior preservation of the original product line products. So in this article we present a language independent theory of product line refinement, establishing refinement properties that justify stepwise and compositional product line evolution. Moreover, we instantiate our theory with the formalization of specific languages for typical product lines artifacts, and then introduce and prove soundness of a number of associated product line refinement transformation templates. These templates can be used to reason about specific product lines and as a basis to derive comprehensive product line refinement catalogues. },
  annote   = {International Colloquium on Theoretical Aspects of Computing 2010},
  doi      = {https://doi.org/10.1016/j.tcs.2012.01.031},
  keywords = {Refactoring,Refinement,Software evolution,Software product lines},
  url      = {http://www.sciencedirect.com/science/article/pii/S0304397512000679},
}

@Article{DISKIN2016298,
  author   = {Diskin, Zinovy and Gholizadeh, Hamid and Wider, Arif and Czarnecki, Krzysztof},
  title    = {{A three-dimensional taxonomy for bidirectional model synchronization}},
  journal  = {Journal of Systems and Software},
  year     = {2016},
  volume   = {111},
  pages    = {298--322},
  issn     = {0164-1212},
  abstract = {Early model-driven engineering (MDE) assumed simple pipeline-like scenarios specified by the Model-Driven Architecture approach: platform-independent models that describe a software system at a high-level of abstraction are transformed stepwise to platform-dependent models from which executable source code is generated. Modern applications require a shift toward networks of models related in various ways, whose synchronization often needs to be incremental and bidirectional. This new situation demands new features from transformation tools, and a solid semantic foundation to understand and classify these features. We address the problem by presenting a taxonomy of model synchronization types, organized into a 3D-space. Each point in the space refers to a specific synchronization semantics with an underlying algebraic model and the respective requirements for the change propagation operations and their properties. The taxonomy aims to help with identifying and communicating a proper specification for the synchronization problem at hand and for the available solutions offered by tools.},
  doi      = {https://doi.org/10.1016/j.jss.2015.06.003},
  keywords = {Formal semantics,Model synchronization,Taxonomy},
  url      = {http://www.sciencedirect.com/science/article/pii/S016412121500120X},
}

@Article{Sboui20177066,
  author   = {Sboui, T and {Ben Ayed}, M and Alimi, A M},
  title    = {{A UI-DSPL Approach for the Development of Context-Adaptable User Interfaces}},
  journal  = {IEEE Access},
  year     = {2017},
  volume   = {6},
  pages    = {7066--7081},
  abstract = {Unlike adaptive interfaces which use sensors to adapt themselves, adaptable interfaces need the intervention of end users to adapt their different aspects according to user requirements. These requirements are commonly expressed according to the context of use. This latter was defined by the triplet {\textless}platform, environment, user{\textgreater} where the platform refers to the physical device and the device software, the environment refers to the physical environment in which the application is used and the user element refers to the user preferences and user profile. In this paper, we define a dynamic software product line (DSPL) approach for the development of a family of context-adaptable user interfaces. The DSPL paradigm exploits the knowledge acquired in software product line engineering to develop systems that can be context-aware, or runtime adaptable. Our approach satisfies a set of contributions which will be validated by implementing and evaluating them according to an illustrative case study. {\textcopyright} 2013 IEEE.},
  annote   = {cited By 0},
  doi      = {10.1109/ACCESS.2017.2782880},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038856795{\&}doi=10.1109{\%}2FACCESS.2017.2782880{\&}partnerID=40{\&}md5=8f91eea93d07df30a585882acfb2ccd6},
}

@Article{Burgareli20081,
  author   = {Burgareli, L A and Melnikoff, S S S and Ferreira, M G V},
  title    = {{A variability management strategy for software product lines of Brazilian satellite luncher vehicles}},
  journal  = {Studies in Computational Intelligence},
  year     = {2008},
  volume   = {150},
  pages    = {1--14},
  abstract = {The Product Line approach offers to the software development benefits such as savings, large-scale productivity and increased product quality. The management of variability is a key and challenging issue in the development of the software product line and product derivation. This work presents a strategy for the variability management for software product line of Brazilian Satellite Launcher Vehicles. After modeling the variability, extracting them from use case diagrams and features, the proposed strategy uses a variation mechanism based on a set of Adaptive Design Patterns as support in the creation of variants. The proposed strategy uses as case study the software system of an existing specific vehicle, the Brazilian Satellite Launcher (BSL). {\textcopyright} Springer-Verlag Berlin Heidelberg 2008.},
  annote   = {cited By 1},
  doi      = {10.1007/978-3-540-70561-1_1},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-59549088931{\&}doi=10.1007{\%}2F978-3-540-70561-1{\_}1{\&}partnerID=40{\&}md5=48b5f80ab238e0fb937c5c2af0267953},
}

@Article{Nguyen2013321,
  author   = {Nguyen, T and Colman, A and Han, J},
  title    = {{A web services variability description language (WSVL) for business users oriented service customization}},
  journal  = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year     = {2013},
  volume   = {7652 LNCS},
  pages    = {321--334},
  abstract = {To better facilitate business users in customizing Web services, customization options need to be described at a high level of abstraction. In contrast to related efforts that describe customization options at the technical level of service description, we propose a Web Services Variability description Language (WSVL) that facilitates the representation of such options at business level. The language has several advantages. Firstly, it does not require people, who perform customization, to have knowledge of Web service technologies. Thus, the language enables business users-friendly service customization. Secondly, the language captures not only what can be customized, but also how and where customization operations should happen in a service-oriented way. This self-described property removes the need for a separate procedure for governing service customization. Consequently, this property eases the adoption of the language. We elaborate the design of the language using a case study and describe its usages from both consumers and providers' viewpoints. {\textcopyright} 2013 Springer-Verlag.},
  annote   = {cited By 0},
  doi      = {10.1007/978-3-642-38333-5_34},
  keywords = {Feature modeling; Service customization; Service d,Quality of service; Systems engineering; Web serv,Websites},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891308404{\&}doi=10.1007{\%}2F978-3-642-38333-5{\_}34{\&}partnerID=40{\&}md5=9f61afa271af6d2fde8e81f80bbf48c3},
}

@Article{GonzalezPerez20081288,
  author   = {Gonzalez-Perez, Cesar and Henderson-Sellers, Brian},
  title    = {{A work product pool approach to methodology specification and enactment}},
  journal  = {Journal of Systems and Software},
  year     = {2008},
  volume   = {81},
  number   = {8},
  pages    = {1288--1305},
  issn     = {0164-1212},
  abstract = {Software development methodologies advocated and used today, whether traditional and plan-based or contemporary and agile, usually focus on process steps i.e. they start with requirements and iteratively describe what steps are necessary to move to the next stage or phase, until the software application is delivered to the end user. Such a process-oriented view of methodologies, based on the metaphor that human organizations are “machines” that “execute” processes, often results in methodologies that are too rigid and hard to follow, and most often than not end up being ignored or bypassed. Our proposal here is that, since the ultimate aim of software development is to provide a software product, software development methodologies should be described in terms of the intermediate products that are necessary to reach such a final product, plus the needed micro-processes that, as necessary evils, will be required to produce the appropriate work products from other, previously created ones. Using this product-oriented approach, software development methodologies can be specified that are, at least, as flexible as lightweight, agile approaches and, at the same time, as powerful and scalable as plan-oriented ones. },
  doi      = {https://doi.org/10.1016/j.jss.2007.10.001},
  keywords = {Enactment,ISO/IEC 24744,Metamodelling,Software development methodologies},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121207002439},
}

@Article{Apel2012174,
  author   = {Apel, Sven and Kolesnikov, Sergiy and Liebig, J{\"{o}}rg and K{\"{a}}stner, Christian and Kuhlemann, Martin and Leich, Thomas},
  title    = {{Access control in feature-oriented programming}},
  journal  = {Science of Computer Programming},
  year     = {2012},
  volume   = {77},
  number   = {3},
  pages    = {174--187},
  issn     = {0167-6423},
  abstract = {In feature-oriented programming (FOP) a programmer decomposes a program in terms of features. Ideally, features are implemented modularly so that they can be developed in isolation. Access control mechanisms in the form of access or visibility modifiers are an important ingredient to attain feature modularity as they allow programmers to hide and expose internal details of a module's implementation. But developers of contemporary feature-oriented languages have not considered access control mechanisms so far. The absence of a well-defined access control model for {\{}FOP{\}} breaks encapsulation of feature code and leads to unexpected program behaviors and inadvertent type errors. We raise awareness of this problem, propose three feature-oriented access modifiers, and present a corresponding access modifier model. We offer an implementation of the model on the basis of a fully-fledged feature-oriented compiler. Finally, by analyzing ten feature-oriented programs, we explore the potential of feature-oriented modifiers in FOP. },
  annote   = {Feature-Oriented Software Development (FOSD 2009)},
  doi      = {https://doi.org/10.1016/j.scico.2010.07.005},
  keywords = {Access control,Access modifier model,Feature modularity,Feature-oriented programming,Fuji},
  url      = {http://www.sciencedirect.com/science/article/pii/S0167642310001528},
}

@Article{Strecker:2012:ADC:2211616.2211620,
  author    = {Strecker, Jaymie and Memon, Atif M},
  title     = {{Accounting for Defect Characteristics in Evaluations of Testing Techniques}},
  journal   = {ACM Trans. Softw. Eng. Methodol.},
  year      = {2012},
  volume    = {21},
  number    = {3},
  pages     = {17:1----17:43},
  issn      = {1049-331X},
  address   = {New York, NY, USA},
  doi       = {10.1145/2211616.2211620},
  keywords  = {Defects,GUI testing,faults},
  publisher = {ACM},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/2211616.2211620},
}

@Article{ALFEREZ2017332,
  author   = {Alf{\'{e}}rez, Germ{\'{a}}n H and Pelechano, Vicente},
  title    = {{Achieving autonomic Web service compositions with models at runtime}},
  journal  = {Computers {\&} Electrical Engineering},
  year     = {2017},
  volume   = {63},
  pages    = {332--352},
  issn     = {0045-7906},
  abstract = {Several exceptional situations may arise in the complex, heterogeneous, and changing contexts where Web service operations run. For instance, a Web service operation may have greatly increased its execution time or may have become unavailable. The contribution of this article is to provide a tool-supported framework to guide autonomic adjustments of context-aware service compositions using models at runtime. During execution, when problematic events arise in the context, models are used by an autonomic architecture to guide changes of the service composition. Under the closed-world assumption, the possible context events are fully known at design time. Nevertheless, it is difficult to foresee all the possible situations arising in uncertain contexts where service compositions run. Therefore, the proposed framework also covers the dynamic evolution of service compositions to deal with unexpected events in the open world. An evaluation demonstrates that our framework is efficient during dynamic adjustments.},
  doi      = {https://doi.org/10.1016/j.compeleceng.2017.08.004},
  keywords = {Autonomic computing,Dynamic adaptation,Dynamic evolution,Dynamic software product lines,Models at runtime,Web service compositions},
  url      = {http://www.sciencedirect.com/science/article/pii/S0045790617324965},
}

@Article{Amoui20122720,
  author   = {Amoui, Mehdi and Derakhshanmanesh, Mahdi and Ebert, J{\"{u}}rgen and Tahvildari, Ladan},
  title    = {{Achieving dynamic adaptation via management and interpretation of runtime models}},
  journal  = {Journal of Systems and Software},
  year     = {2012},
  volume   = {85},
  number   = {12},
  pages    = {2720--2737},
  issn     = {0164-1212},
  abstract = {In this article, we present a generic model-centric approach for realizing fine-grained dynamic adaptation in software systems by managing and interpreting graph-based models of software at runtime. We implemented this approach as the Graph-based Runtime Adaptation Framework (GRAF), which is particularly tailored to facilitate and simplify the process of evolving and adapting current software towards runtime adaptivity. As a proof of concept, we present case study results that show how to achieve runtime adaptivity with {\{}GRAF{\}} and sketch the framework's capabilities for facilitating the evolution of real-world applications towards self-adaptive software. The case studies also provide some details of the {\{}GRAF{\}} implementation and examine the usability and performance of the approach. },
  annote   = {Self-Adaptive Systems},
  doi      = {https://doi.org/10.1016/j.jss.2012.05.033},
  keywords = {Adaptation framework,Model transformation,Models at runtime,Runtime adaptivity,Self-adaptive software},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121212001458},
}

@Article{Deng2006247,
  author   = {Deng, G and Lenz, G and Schmidt, D C},
  title    = {{Addressing domain evolution challenges in software product lines}},
  journal  = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year     = {2006},
  volume   = {3844 LNCS},
  pages    = {247--261},
  abstract = {It is hard to develop and evolve software product-line architectures (PLAs) for large-scale distributed real-time and embedded (DRE) systems. Although certain challenges of PLAs can be addressed by combining model-driven development (MDD) techniques with component frameworks, domain evolution problems remain largely unresolved. In particular, extending or refactoring existing software product-lines to handle unanticipated requirements or better satisfy current requirements requires significant effort. This paper describes techniques for minimizing such impacts on MDD-based PLAs for DRE systems through a case study that shows how a layered architecture and model-to-model transformation tool support can reduce the effort of PLA evolution. {\textcopyright} Springer-Verlag Berlin Heidelberg 2006.},
  annote   = {cited By 8},
  keywords = {Computer architecture; Computer software; Distrib,Domains; Model Transformation; Model-driven develo,Software engineering},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745644065{\&}partnerID=40{\&}md5=f9b4f3ee8eaac0b2b74ead6f121a4e5e},
}

@Article{Blake200531,
  author   = {Blake, M.Brian and Gomaa, Hassan},
  title    = {{Agent-oriented compositional approaches to services-based cross-organizational workflow}},
  journal  = {Decision Support Systems},
  year     = {2005},
  volume   = {40},
  number   = {1},
  pages    = {31--50},
  issn     = {0167-9236},
  abstract = {With the sophistication and maturity of distributed component-based services and semantic web services, the idea of specification-driven service composition is becoming a reality. One such approach is workflow composition of services that span multiple, distributed web-accessible locations. Given the dynamic nature of this domain, the adaptation of software agents represents a possible solution for the composition and enactment of cross-organizational services. This paper details design aspects of an architecture that would support this evolvable service-based workflow composition. The internal coordination and control aspects of such an architecture is addressed. These agent developmental processes are aligned with industry-standard software engineering processes. },
  annote   = {Web services and process management},
  doi      = {https://doi.org/10.1016/j.dss.2004.04.003},
  keywords = {Agent architectures,Coordination,UML,Web services,Workflow modeling},
  url      = {http://www.sciencedirect.com/science/article/pii/S0167923604000624},
}

@Article{Kakarontzas2011999,
  author   = {Kakarontzas, G and Savvas, I K and Stamelos, I},
  title    = {{Agents, clusters and components: A synergistic approach to the {\{}GSP{\}}}},
  journal  = {Future Generation Computer Systems},
  year     = {2011},
  volume   = {27},
  number   = {8},
  pages    = {999--1010},
  issn     = {0167-739X},
  abstract = {Grids provide access to a vast amount of computational resources for the execution of demanding computations. These resources are geographically distributed, owned by different organizations and are vastly heterogeneous. The aforementioned factors introduce uncertainty in all phases of a Grid Scheduling Process (GSP). This work describes a synergistic multidisciplinary approach which aims at addressing this uncertainty. It proposes a network of resource representatives (RRs), which maintain the more or less static characteristics of available workers they represent. Clustering techniques are used for the efficient searching in the network of {\{}RRs{\}} by client agents. After the discovery of possibly suitable resources, client agents and resource agents negotiate directly for the selection of the best available resource set. Finally, according to the characteristics of the selected resource set and its current state, we propose a component-based application configuration approach based on component variants, that adjusts the application for the forthcoming execution phase in the selected resource set. We evaluate our approach using simulation and we show that it outperforms centralized index approaches for large computational grids. },
  doi      = {https://doi.org/10.1016/j.future.2011.05.002},
  keywords = {Clustering,Grid scheduling process,Software agents,Software components},
  url      = {http://www.sciencedirect.com/science/article/pii/S0167739X11000835},
}

@Article{STETTINA2015140,
  author   = {Stettina, Christoph Johann and H{\"{o}}rz, Jeannette},
  title    = {{Agile portfolio management: An empirical perspective on the practice in use}},
  journal  = {International Journal of Project Management},
  year     = {2015},
  volume   = {33},
  number   = {1},
  pages    = {140--152},
  issn     = {0263-7863},
  abstract = {Agile project management methods revolutionized the way how software projects are executed and organized. The question, however, on how to enable agility outside of individual projects and help larger organizations to compete with small entrepreneurial companies requires further attention. As a possible perspective, project portfolio management provides a global view on resources and their distribution across individual projects according to strategic choices. Based on 30 interviews conducted in 14 large European organizations this study contributes to the understanding of agile project management methods applied in IT project portfolios. First, we empirically identify the domains of practice. Then, guided by literature and our data we discuss the characteristics and implications of the agile portfolio management practice in our case organizations.},
  doi      = {https://doi.org/10.1016/j.ijproman.2014.03.008},
  keywords = {Agile project management,Empirical study,Organizational routines,Project portfolio management,Software project management},
  url      = {http://www.sciencedirect.com/science/article/pii/S0263786314000489},
}

@Article{Noor2008868,
  author        = {Noor, Muhammad A and Rabiser, Rick and Gr{\"{u}}nbacher, Paul},
  title         = {{Agile product line planning: A collaborative approach and a case study}},
  journal       = {Journal of Systems and Software},
  year          = {2008},
  volume        = {81},
  number        = {6},
  pages         = {868--882},
  issn          = {0164-1212},
  abstract      = {Agile methods and product line engineering (PLE) have both proven successful in increasing customer satisfaction and decreasing time to market under certain conditions. Key characteristics of agile methods are lean and highly iterative development with a strong emphasis on stakeholder involvement. {\{}PLE{\}} leverages reuse through systematic approaches such as variability modeling or product derivation. Integrating agile approaches with product line engineering is an interesting proposition which – not surprisingly – entails several challenges: Product lines (PL) rely on complex plans and models to ensure their long-term evolution while agile methods emphasize simplicity and short-term value-creation for customers. When incorporating agility in product line engineering, it is thus essential to define carefully how agile principles can support particular {\{}PLE{\}} processes. For instance, the processes of defining and setting up a product line (domain engineering) and deriving products (application engineering) differ significantly in practices and focus with implications on the suitability of agile principles. This paper presents practical experiences of adopting agile principles in product line planning (a domain engineering activity). ThinkLets, i.e., collaborative practices from the area of collaboration engineering, are the building blocks of the presented approach as they codify agile principles such as stakeholder involvement, rapid feedback, or value-based prioritization. We discuss how our approach balances agility and the intrinsic needs of product line planning. A case study carried out with an industrial partner indicates that the approach is practicable, usable, and useful.},
  annote        = {Agile Product Line Engineering},
  doi           = {https://doi.org/10.1016/j.jss.2007.10.028},
  keywords      = {Agile methods,Collaboration engineering,Product line engineering,Product line planning,case study},
  mendeley-tags = {case study},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121207002531},
}

@Article{Díaz2014727,
  author        = {D{\'{i}}az, J and P{\'{e}}rez, J and Garbajosa, J},
  title         = {{Agile product-line architecting in practice: A case study in smart grids}},
  journal       = {Information and Software Technology},
  year          = {2014},
  volume        = {56},
  number        = {7},
  pages         = {727--748},
  abstract      = {Context Software Product Line Engineering implies the upfront design of a Product-Line Architecture (PLA) from which individual product applications can be engineered. The big upfront design associated with PLAs is in conflict with the current need of "being open to change". To make the development of product-lines more flexible and adaptable to changes, several companies are adopting Agile Product Line Engineering. However, to put Agile Product Line Engineering into practice it is still necessary to make mechanisms available to assist and guide the agile construction and evolution of PLAs. Objective This paper presents the validation of a process for "the agile construction and evolution of product-line architectures", called Agile Product-Line Architecting (APLA). The contribution of the APLA process is the integration of a set of models for describing, documenting, and tracing PLAs, as well as an algorithm for guiding the change decision-making process of PLAs. The APLA process is assessed to prove that assists Agile Product Line Engineering practitioners in the construction and evolution of PLAs. Method Validation is performed through a case study by using both quantitative and qualitative analysis. Quantitative analysis was performed using statistics, whereas qualitative analysis was performed through interviews using constant comparison, triangulation, and supporting tools. This case study was conducted according to the guidelines of Runeson and H{\"{o}}st in a software factory where three projects in the domain of Smart Grids were involved. Results APLA is deployed through the Flexible-PLA modeling framework. This framework supported the successful development and evolution of the PLA of a family of power metering management applications for Smart Grids. Conclusions APLA is a well-supported solution for the agile construction and evolution of PLAs. This case study illustrates that the proposed solution for the agile construction of PLAs is viable in an industry project on Smart Grids. {\textcopyright} 2014 Elsevier B.V. All rights reserved.},
  annote        = {cited By 9},
  doi           = {10.1016/j.infsof.2014.01.014},
  file          = {:Users/mac/ownCloud/Anita/SLR/REVISION DE JOURNALS/PAPERS{\_}D/SELECT DAVID/checked/analizados/casos de estudios analizados/4 Agile product-line architecting in practice- A case study in smart grids.pdf:pdf},
  keywords      = {Agile product-line architecting,Architectural kn,Product design,Production engineering,Research,Smart power grid,case study},
  mendeley-tags = {case study},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84898810060{\&}doi=10.1016{\%}2Fj.infsof.2014.01.014{\&}partnerID=40{\&}md5=79cd1ad172a29071b1a41d63ef7c884c},
}

@Article{Hanssen2011883,
  author   = {Hanssen, G K},
  title    = {{Agile software product line engineering: Enabling factors}},
  journal  = {Software - Practice and Experience},
  year     = {2011},
  volume   = {41},
  number   = {8},
  pages    = {883--897},
  abstract = {This paper reports on a study of a software product line organization that has adopted agile software development to address process rigidity and slowing performance. Experience has showed that despite some impediments, this has become a valuable change to both the organization and its development process. The aim of this study is to identify and understand enabling factors of a combined process, and to understand their subsequent effects. Qualitative data are summarized and analyzed, giving insight into the actions taken, their effects that have emerged over time, and the enabling and contextual factors. The study concludes that a combined process is feasible, that the simplified approach makes the organization more flexible and thus capable of serving a volatile market with fast-changing technologies. This has also enabled the organization to collaborate better with external actors. Copyright {\textcopyright} 2011 John Wiley {\&} Sons, Ltd.},
  annote   = {cited By 5},
  doi      = {10.1002/spe.1064},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-79958281206{\&}doi=10.1002{\%}2Fspe.1064{\&}partnerID=40{\&}md5=c50cda5a08c3f2e8eb05f37a2cdc5aa3},
}

@Article{TorrecillaSalinas201692,
  author   = {Torrecilla-Salinas, C J and Sede{\~{n}}o, J and Escalona, M J and Mej{\'{i}}as, M},
  title    = {{Agile, Web Engineering and Capability Maturity Model Integration: A systematic literature review.}},
  journal  = {Information and Software Technology},
  year     = {2016},
  volume   = {71},
  pages    = {92--107},
  issn     = {0950-5849},
  abstract = {AbstractContext Agile approaches are an alternative for organizations developing software, particularly for those who develop Web applications. Besides, {\{}CMMI{\}} (Capability Maturity Model Integration) models are well-established approaches focused on assessing the maturity of an organization that develops software. Web Engineering is the field of Software Engineering responsible for analyzing and studying the specific characteristics of the Web. The suitability of an Agile approach to help organizations reach a certain {\{}CMMI{\}} maturity level in Web environments will be very interesting, as they will be able to keep the ability to quickly react and adapt to changes as long as their development processes get mature. Objective This paper responds to whether it is feasible or not, for an organization developing Web systems, to achieve a certain maturity level of the CMMI-DEV model using Agile methods. Method The proposal is analyzed by means of a systematic literature review of the relevant approaches in the field, defining a characterization schema in order to compare them to introduce the current state-of-the-art. Results The results achieved after the systematic literature review are presented, analyzed and compared against the defined schema, extracting relevant conclusions for the different dimensions of the problem: compatibility, compliance, experience, maturity and Web. Conclusion It is concluded that although the definition of an Agile approach to meet the different {\{}CMMI{\}} maturity levels goals could be possible for an organization developing Web systems, there is still a lack of detailed studies and analysis on the field. },
  doi      = {https://doi.org/10.1016/j.infsof.2015.11.002},
  keywords = {Agile,CMMI,Scrum,Software Engineering,Web Engineering},
  url      = {http://www.sciencedirect.com/science/article/pii/S095058491500186X},
}

@Article{Postmus2008753,
  author   = {Postmus, Douwe and Meijler, Theo Dirk},
  title    = {{Aligning the economic modeling of software reuse with reuse practices}},
  journal  = {Information and Software Technology},
  year     = {2008},
  volume   = {50},
  number   = {7–8},
  pages    = {753--762},
  issn     = {0950-5849},
  abstract = {In contrast to current practices where software reuse is applied recursively and reusable assets are tailored trough parameterization or specialization, existing reuse economic models assume that (i) the cost of reusing a software asset depends on its size and (ii) reusable assets are developed from scratch. The contribution of this paper is that it provides modeling elements and an economic model that is better aligned with current practices. The functioning of the model is illustrated in an example. The example also shows how the model can support practitioners in deciding whether it is economically feasible to apply software reuse recursively. },
  doi      = {https://doi.org/10.1016/j.infsof.2007.07.009},
  keywords = {Composition,Reuse economic model,Software reuse,Variation},
  url      = {http://www.sciencedirect.com/science/article/pii/S095058490700081X},
}

@Article{Börger2012939,
  author   = {B{\"{o}}rger, Egon and Cisternino, Antonio and Gervasi, Vincenzo},
  title    = {{Ambient Abstract State Machines with applications}},
  journal  = {Journal of Computer and System Sciences},
  year     = {2012},
  volume   = {78},
  number   = {3},
  pages    = {939--959},
  issn     = {0022-0000},
  abstract = {We define a flexible abstract ambient concept which turned out to support current programming practice, in fact can be instantiated to apparently any environment paradigm in use in frameworks for distributed computing with heterogeneous components. For the sake of generality and to also support rigorous high-level system design practice we give the definition in terms of Abstract State Machines. We show the definition to uniformly capture the common static and dynamic disciplines for isolating states or concurrent behavior (e.g. handling of multiple threads for Java) as well as for sharing memory, patterns of object-oriented programming (e.g. for delegation, incremental refinement, encapsulation, views) and agent mobility. },
  annote   = {In Commemoration of Amir Pnueli},
  doi      = {https://doi.org/10.1016/j.jcss.2011.08.004},
  keywords = {Abstract State Machines,Ambient concept,Memory sharing disciplines,Mobile agents,Naming disciplines,Object-oriented design patterns},
  url      = {http://www.sciencedirect.com/science/article/pii/S0022000011000833},
}

@Article{Huang20121332,
  author   = {Huang, Ge and Zomaya, Albert Y and Delicato, Fl{\'{a}}via C and Pires, Paulo F},
  title    = {{An accurate on-demand time synchronization protocol for wireless sensor networks}},
  journal  = {Journal of Parallel and Distributed Computing},
  year     = {2012},
  volume   = {72},
  number   = {10},
  pages    = {1332--1346},
  issn     = {0743-7315},
  abstract = {Time synchronization is a critical component in any wireless sensor network (WSN). In terms of energy consumption, on-demand time synchronization is better than continuous synchronization. However, currently existing on-demand time synchronization protocols have a very low accuracy and very strong spatial accumulative effect. These features are not suitable for several types of {\{}WSN{\}} applications, such as applications with stringent temporal requirements, or applications that have a large spatial region of interest. In this paper, we propose an on-demand time synchronization protocol, named {\{}AOTSP{\}} (Accurate On-demand Time Synchronization Protocol), which differs from other protocols of the same category by having the following advantages, as shown in our theoretical analysis and simulation results: (1) weak spatial accumulative effect; (2) fairly low communication cost; (3) low computational complexity; (4) high accuracy; (5) high scalability. Such features make {\{}AOTSP{\}} a suitable time synchronization protocol for a broad range of {\{}WSN{\}} applications. },
  doi      = {https://doi.org/10.1016/j.jpdc.2012.06.003},
  keywords = {On-demand synchronization,Taylor expansion,Wireless sensor networks},
  url      = {http://www.sciencedirect.com/science/article/pii/S0743731512001426},
}

@Article{Ganesan20132360,
  author   = {Ganesan, Dharmalingam and Lindvall, Mikael and McComas, David and Bartholomew, Maureen and Slegel, Steve and Medina, Barbara and Krikhaar, Rene and Verhoef, Chris and Montgomery, Lisa P},
  title    = {{An analysis of unit tests of a flight software product line}},
  journal  = {Science of Computer Programming},
  year     = {2013},
  volume   = {78},
  number   = {12},
  pages    = {2360--2380},
  issn     = {0167-6423},
  abstract = {This paper presents an analysis of the unit testing approach developed and used by the Core Flight Software System (CFS) product line team at the {\{}NASA{\}} Goddard Space Flight Center (GSFC). The goal of the analysis is to understand, review, and recommend strategies for improving the CFS' existing unit testing infrastructure as well as to capture lessons learned and best practices that can be used by other software product line (SPL) teams for their unit testing. The results of the analysis show that the core and application modules of the {\{}CFS{\}} are unit tested in isolation using a stub framework developed by the {\{}CFS{\}} team. The application developers can unit test their code without waiting for the core modules to be completed, and vice versa. The analysis found that this unit testing approach incorporates many practical and useful solutions such as allowing for unit testing without requiring hardware and special {\{}OS{\}} features in-the-loop by defining stub implementations of dependent modules. These solutions are worth considering when deciding how to design the testing architecture for a SPL. },
  annote   = {Special Section on International Software Product Line Conference 2010 and Fundamentals of Software Engineering (selected papers of {\{}FSEN{\}} 2011)},
  doi      = {https://doi.org/10.1016/j.scico.2012.02.006},
  keywords = {Flight software,Metrics,Self-testable components,Software architecture,Stub,Unit testing},
  url      = {http://www.sciencedirect.com/science/article/pii/S0167642312000317},
}

@Article{Horcas201620,
  author   = {Horcas, Jose-Miguel and Pinto, M{\'{o}}nica and Fuentes, Lidia and Mallouli, Wissam and de Oca, Edgardo Montes},
  title    = {{An approach for deploying and monitoring dynamic security policies}},
  journal  = {Computers {\&} Security},
  year     = {2016},
  volume   = {58},
  pages    = {20--38},
  issn     = {0167-4048},
  abstract = {Abstract Security policies are enforced through the deployment of certain security functionalities within the applications. When the security policies dynamically change, the associated security functionalities currently deployed within the applications must be adapted at runtime in order to enforce the new security policies. INTER-TRUST is a framework for the specification, negotiation, deployment and dynamic adaptation of interoperable security policies, in the context of pervasive systems where devices are constantly exchanging critical information through the network. The dynamic adaptation of the security policies at runtime is addressed using Aspect-Oriented Programming (AOP) that allows enforcing security requirements by dynamically weaving security aspects into the applications. However, a mechanism to guarantee the correct adaptation of the functionality that enforces the changing security policies is needed. In this paper, we present an approach based on the combination of monitoring and detection techniques in order to maintain the correlation between the security policies and the associated functionality deployed using AOP, allowing the INTER-TRUST framework to automatically react when needed. },
  doi      = {https://doi.org/10.1016/j.cose.2015.11.007},
  keywords = {Aspect-oriented programming,Dynamic deployment,Monitoring,Security framework,Security policies},
  url      = {http://www.sciencedirect.com/science/article/pii/S0167404815001832},
}

@Article{Moon2005551,
  author   = {Moon, M and Yeom, K and Chae, H S},
  title    = {{An approach to developing domain requirements as a core asset based on commonality and variability analysis in a product line}},
  journal  = {IEEE Transactions on Software Engineering},
  year     = {2005},
  volume   = {31},
  number   = {7},
  pages    = {551--569},
  abstract = {The methodologies of product line engineering emphasize proactive reuse to construct high-quality products more quickly that are less costly. Requirements engineering for software product families differs significantly from requirements engineering for single software products. The requirements for a product line are written for the group of systems as a whole, with requirements for individual systems specified by a delta or an increment to the generic set. Therefore, it is necessary to identify and explicitly denote the regions of commonality and points of variation at the requirements level. In this paper, we suggest a method of producing requirements that will be a core asset in the product line. We describe a process for developing domain requirements where commonality and variability in a domain are explicitly considered. A CASE environment, named DREAM, for managing commonality and valiability analysis of domain requirements is also described. We also describe a case study for an e-Travel System domain where we found that our approach to developing domain requirements based on commonality and variability analysis helped to produce domain requirements as a core asset for product lines. {\textcopyright} 2005 IEEE.},
  annote   = {cited By 78},
  doi      = {10.1109/TSE.2005.76},
  keywords = {Commonality analysis; Core assets; Domain analysi,Computer aided software engineering; Computer arch,Computer software reusability},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-25844465686{\&}doi=10.1109{\%}2FTSE.2005.76{\&}partnerID=40{\&}md5=51373b68aac3e74d68e205d6d1f5da7b},
}

@Article{Lee2006127,
  author   = {Lee, Y and Yang, C and Zhu, C and Zhao, W},
  title    = {{An approach to managing feature dependencies for product releasing in software product lines}},
  journal  = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year     = {2006},
  volume   = {4039 LNCS},
  pages    = {127--141},
  abstract = {Product line software engineering is a systematic approach to realize large scale software reuse. Software product lines deal with reusable assets across a domain by exploring requirements commonality and variability. Requirements dependencies have very strong influence on all development phases of member products in a product line. There are many feature oriented approaches on requirement dependencies. However, most of them are limited to the problem domain. Among those few focusing on the solution domain, they are limited to modeling requirement dependencies. This paper presents a feature oriented approach to managing domain requirements dependencies. Not only is a requirement dependencies model presented, but a directed graph-based approach is also developed to analyze domain requirement dependencies for effective release of member products in a product line. This approach returns a simple directed graph, and uses an effective algorithm to get a set of requirements to be released in a member product. A case study for spot and futures transaction domain is described to illustrate the approach. {\textcopyright} Springer-Verlag Berlin Heidelberg 2006.},
  annote   = {cited By 9},
  keywords = {Computer software reusability; Database systems; G,Feature dependencies; Graph-based approach; Softw,Software engineering},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-33746191953{\&}partnerID=40{\&}md5=cf0236cda43ad4ae7c11c7b928c64e65},
}

@Article{Boudaa201717,
  author   = {Boudaa, Boudjemaa and Hammoudi, Slimane and Mebarki, Leila Amel and Bouguessa, Abdelkader and Chikh, Mohammed Amine},
  title    = {{An aspect-oriented model-driven approach for building adaptable context-aware service-based applications}},
  journal  = {Science of Computer Programming},
  year     = {2017},
  volume   = {136},
  pages    = {17--42},
  issn     = {0167-6423},
  abstract = {AbstractContext Context-aware service-based applications development has been considered among the most studied research fields in the last decade. The objective was to accompany the rapid technology evolution of mobile computing devices by providing customized services able to interact with different contextual situations of a pervasive environment. For this purpose, many research works have advocated Model-Driven Development (MDD) for building context-aware service-based applications. However, the proposed approaches have presented specific methodologies without using development standards, which may be followed by developers. In addition, most of them have ignored the dynamic adaptation aspect at runtime that should characterize such kind of applications and no adaptation strategy was considered in their proposals. Objective The current paper aims to propose a generic model-driven approach for context-aware service-based applications engineering with a software development methodology including a reconfiguration loop to achieve the dynamic adaptation of these applications. Method This approach focuses on the combination of {\{}MDD{\}} and Aspect Oriented Modelling (AOM) to take advantage of their benefits. {\{}AOM{\}} encapsulates different context-awareness logics separately in aspect models called ContextAspect that can be easily woven into the service's business logic according to the changing context over time. The proposed development methodology includes four phases (modelling, composition, transformation and adaptation) which act in conformance with the {\{}MDA{\}} technology. Results The main results gained by using the present approach are the possibility to combine the {\{}MDA{\}} technology with the aspect-oriented paradigm in a generic development methodology for context-aware service-based applications, and the handling of their dynamic adaptation at execution time according to the changes in the context. Conclusion The development of context-aware applications is a complex, cumbersome, and time-consuming task. However, the experience reached by implementing the proposed methodology leads us to believe that the involvement of {\{}MDD{\}} and {\{}AOM{\}} is significantly beneficial to overcome some recognised shortcomings of several existing approaches and to make this task simpler, easier and faster. },
  doi      = {https://doi.org/10.1016/j.scico.2016.08.009},
  keywords = {Aspect weaving,Context-aware service-based application,ContextAspect,Dynamic adaptation,Model-driven methodology},
  url      = {http://www.sciencedirect.com/science/article/pii/S0167642316301253},
}

@Article{Nakagawa20111670,
  author   = {Nakagawa, Elisa Y and Ferrari, Fabiano C and Sasaki, Mariela M F and Maldonado, Jos{\'{e}} C},
  title    = {{An aspect-oriented reference architecture for Software Engineering Environments}},
  journal  = {Journal of Systems and Software},
  year     = {2011},
  volume   = {84},
  number   = {10},
  pages    = {1670--1684},
  issn     = {0164-1212},
  abstract = {Reusable and evolvable Software Engineering Environments (SEEs) are essential to software production and have increasingly become a need. In another perspective, software architectures and reference architectures have played a significant role in determining the success of software systems. In this paper we present a reference architecture for SEEs, named RefASSET, which is based on concepts coming from the aspect-oriented approach. This architecture is specialized to the software testing domain and the development of tools for that domain is discussed. This and other case studies have pointed out that the use of aspects in RefASSET provides a better Separation of Concerns, resulting in reusable and evolvable SEEs. },
  doi      = {https://doi.org/10.1016/j.jss.2011.04.052},
  keywords = {Aspect orientation,Reference architecture,Software Engineering Environment,Software architecture,Software testing},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121211001038},
}

@Article{LopezHerrejon2015353,
  author   = {Lopez-Herrejon, Roberto E and Linsbauer, Lukas and Galindo, Jos{\'{e}} A and Parejo, Jos{\'{e}} A and Benavides, David and Segura, Sergio and Egyed, Alexander},
  title    = {{An assessment of search-based techniques for reverse engineering feature models}},
  journal  = {Journal of Systems and Software},
  year     = {2015},
  volume   = {103},
  pages    = {353--369},
  issn     = {0164-1212},
  abstract = {Abstract Successful software evolves from a single system by adding and changing functionality to keep up with users' demands and to cater to their similar and different requirements. Nowadays it is a common practice to offer a system in many variants such as community, professional, or academic editions. Each variant provides different functionality described in terms of features. Software Product Line Engineering (SPLE) is an effective software development paradigm for this scenario. At the core of {\{}SPLE{\}} is variability modelling whose goal is to represent the combinations of features that distinguish the system variants using feature models, the de facto standard for such task. As {\{}SPLE{\}} practices are becoming more pervasive, reverse engineering feature models from the feature descriptions of each individual variant has become an active research subject. In this paper we evaluated, for this reverse engineering task, three standard search based techniques (evolutionary algorithms, hill climbing, and random search) with two objective functions on 74 SPLs. We compared their performance using precision and recall, and found a clear trade-off between these two metrics which we further reified into a third objective function based on F$\beta$, an information retrieval measure, that showed a clear performance improvement. We believe that this work sheds light on the great potential of search-based techniques for {\{}SPLE{\}} tasks. },
  doi      = {https://doi.org/10.1016/j.jss.2014.10.037},
  keywords = {Feature model,Reverse engineering,Search Based Software Engineering},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121214002349},
}

@Article{Casare2018180,
  author   = {Casare, S and Ziadi, T and Brand{\~{a}}o, A A F and Guessoum, Z},
  title    = {{An Automated Approach to Manage MAS-Product Line Methods}},
  journal  = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year     = {2018},
  volume   = {10738 LNAI},
  pages    = {180--197},
  abstract = {Multiagent systems (MAS) can vary in several ways: by involving different agents, distinct interaction patterns, various forms of agent organizations and environments. One promising approach to consider this variability in MAS is the use of the concept of Multiagent System-Product Line (MAS-PL). The idea is to implement a family of MAS that belong to the same domain, instead of a single MAS. However, there is still a lack of methodological support to develop MAS-PL. This paper tackles this problem with a rigorous respect of software product line principals. We propose an automated approach, the Meduse for MAS-PL, to generate families of MAS-PL methods that offer software product line best practices integrated with existing MAS development approaches to support MAS-PL development. To illustrate, we present a case study involving a family of MAS-PL methods that extends Gaia and Tropos. {\textcopyright} 2018, Springer International Publishing AG, part of Springer Nature.},
  annote   = {cited By 0; Conference of 5th International Workshop on Engineering Multi-Agent Systems, EMAS 2017 ; Conference Date: 8 May 2017 Through 9 May 2017; Conference Code:213539},
  doi      = {10.1007/978-3-319-91899-0_11},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047157561{\&}doi=10.1007{\%}2F978-3-319-91899-0{\_}11{\&}partnerID=40{\&}md5=f32316cb523e162cfde22ba31d545f1b},
}

@Article{Wang201846900,
  author   = {Wang, L and Li, S and Wei, O and Huang, M and Hu, J},
  title    = {{An Automated Fault Tree Generation Approach with Fault Configuration Based on Model Checking}},
  journal  = {IEEE Access},
  year     = {2018},
  volume   = {6},
  pages    = {46900--46914},
  abstract = {Fault tree generation technology is a key issue for safety analysis of large complex systems. Traditional safety analysis methods usually describe the origin, propagation, or concrete behavior of the fault and do not portray the constraints between faults. However, these constraints are the system's characteristics, and a lack of expression of these constraints will make the fault model defective, thereby resulting in a fault tree that will reduce the accuracy of the safety analysis. To improve the efficiency and accuracy of safety analysis, this paper proposes a fault tree generation method that is based on fault configuration and introduces the variability management of software product lines to model system faults and perform the formal analysis. First, the fault feature diagram is defined to describe the constraint relationships between system faults, and the fault-labeled transition system is defined based on the Kripke structure to describe the system behavior. Then, based on the model semantics, the procedure for generating fault trees by model checking is established. Finally, using temporal logic to describe the system safety attributes, we adopt the model checking tool SNIP to verify the safety attributes and generate the fault tree automatically. The fault modeling method that is proposed in this paper includes the inherent constraints between faults, which makes the system fault model more realistic and accurate. A case study demonstrates the effectiveness of the proposed method. {\textcopyright} 2013 IEEE.},
  annote   = {cited By 0},
  doi      = {10.1109/ACCESS.2018.2863696},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051402145{\&}doi=10.1109{\%}2FACCESS.2018.2863696{\&}partnerID=40{\&}md5=f477d533948f5e123c9bf96cdae679f7},
}

@Article{Horcas201678,
  author   = {Horcas, Jose-Miguel and Pinto, M{\'{o}}nica and Fuentes, Lidia},
  title    = {{An automatic process for weaving functional quality attributes using a software product line approach}},
  journal  = {Journal of Systems and Software},
  year     = {2016},
  volume   = {112},
  pages    = {78--95},
  issn     = {0164-1212},
  abstract = {Abstract Some quality attributes can be modelled using software components, and are normally known as Functional Quality Attributes (FQAs). Applications may require different FQAs, and each {\{}FQA{\}} (e.g., security) can be composed of many concerns (e.g., access control or authentication). They normally have dependencies between them and crosscut the system architecture. The goal of the work presented here is to provide the means for software architects to focus only on application functionality, without having to worry about FQAs. The idea is to model {\{}FQAs{\}} separately from application functionality following a Software Product Line (SPL) approach. By combining {\{}SPL{\}} and aspect-oriented mechanisms, we will define a generic process to model and automatically inject {\{}FQAs{\}} into the application without breaking the base architecture. We will provide and compare two implementations of our generic approach using different variability and architecture description languages: (i) feature models and an aspect-oriented architecture description language; and (ii) the Common Variability Language (CVL) and a MOF-compliant language (e.g., UML). We also discuss the benefits and limitations of our approach. Modelling {\{}FQAs{\}} separately from the base application has many advantages (e.g., reusability, less coupled components, high cohesive architectures). },
  doi      = {https://doi.org/10.1016/j.jss.2015.11.005},
  keywords = {Quality attributes,Software product lines,Weaving},
  url      = {http://www.sciencedirect.com/science/article/pii/S016412121500240X},
}

@Article{Knodel200517,
  author   = {Knodel, Jens and Anastasopolous, Michalis and Forster, Thomas and Muthig, Dirk},
  title    = {{An Efficient Migration to Model-driven Development (MDD)}},
  journal  = {Electronic Notes in Theoretical Computer Science},
  year     = {2005},
  volume   = {137},
  number   = {3},
  pages    = {17--27},
  issn     = {1571-0661},
  abstract = {Model-driven development envisions raising the abstraction level of software development. To fully realize this vision, technology-specific aspects must be completely hidden from developers. They produce only platform-independent models (PIM), which are automatically transformed into executable systems. To enable an efficient migration to MDD, we recommend taking advantage of concepts from software architectures, product line engineering and reverse engineering. },
  annote   = {Proceedings of the 2nd International Workshop on Metamodels, Schemas, and Grammars for Reverse Engineering (ateM 2004)Metamodels, Schemas, and Grammars for Reverse Engineering 2004},
  doi      = {https://doi.org/10.1016/j.entcs.2005.07.002},
  keywords = {PuLSE,architecture-driven migration,model-driven development,product lines,reverse engineering},
  url      = {http://www.sciencedirect.com/science/article/pii/S1571066105051029},
}

@Article{Mohagheghi:2008:EIS:1363102.1363104,
  author    = {Mohagheghi, Parastoo and Conradi, Reidar},
  title     = {{An Empirical Investigation of Software Reuse Benefits in a Large Telecom Product}},
  journal   = {ACM Trans. Softw. Eng. Methodol.},
  year      = {2008},
  volume    = {17},
  number    = {3},
  pages     = {13:1----13:31},
  issn      = {1049-331X},
  address   = {New York, NY, USA},
  doi       = {10.1145/1363102.1363104},
  keywords  = {Software reuse,fault density,product family,risks,standardization},
  publisher = {ACM},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/1363102.1363104},
}

@Article{Ampatzoglou20112265,
  author   = {Ampatzoglou, Apostolos and Kritikos, Apostolos and Kakarontzas, George and Stamelos, Ioannis},
  title    = {{An empirical investigation on the reusability of design patterns and software packages}},
  journal  = {Journal of Systems and Software},
  year     = {2011},
  volume   = {84},
  number   = {12},
  pages    = {2265--2283},
  issn     = {0164-1212},
  abstract = {Nowadays open-source software communities are thriving. Successful open-source projects are competitive and the amount of source code that is freely available offers great reuse opportunities to software developers. Thus, it is expected that several requirements can be implemented based on open source software reuse. Additionally, design patterns, i.e. well-known solution to common design problems, are introduced as elements of reuse. This study attempts to empirically investigate the reusability of design patterns, classes and software packages. Thus, the results can help developers to identify the most beneficial starting points for white box reuse, which is quite popular among open source communities. In order to achieve this goal we conducted a case study on one hundred (100) open source projects. More specifically, we identified 27,461 classes that participate in design patterns and compared the reusability of each of these classes with the reusability of the pattern and the package that this class belongs to. In more than 40{\%} of the cases investigated, design pattern based class selection, offers the most reusable starting point for white-box reuse. However there are several cases when package based selection might be preferable. The results suggest that each pattern has different level of reusability. },
  doi      = {https://doi.org/10.1016/j.jss.2011.06.047},
  keywords = {Design,Design patterns,Empirical approach,Quality,Reusability},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121211001592},
}

@Article{Laitenberger20005,
  author   = {Laitenberger, Oliver and DeBaud, Jean-Marc},
  title    = {{An encompassing life cycle centric survey of software inspection}},
  journal  = {Journal of Systems and Software},
  year     = {2000},
  volume   = {50},
  number   = {1},
  pages    = {5--31},
  issn     = {0164-1212},
  abstract = {This paper contributes an integrated survey of the work in the area of software inspection. It consists of two main sections. The first one introduces a detailed description of the core concepts and relationships that together define the field of software inspection. The second one elaborates a taxonomy that uses a generic development life-cycle to contextualize software inspection in detail. After Fagan's seminal work presented in 1976, the body of work in software inspection has greatly increased and reached measured maturity. Yet, there is still no encompassing and systematic view of this research body driven from a life-cycle perspective. This perspective is important since inspection methods and refinements are most often aligned to particular life-cycle artifacts. It also provides practitioners with a roadmap available in their terms. To provide a systematic and encompassing view of the research and practice body in software inspection, the contribution of this survey is, in a first step, to introduce in detail the core concepts and relationships that together embody the field of software inspection. This lays out the field key ideas and benefits and elicits a common vocabulary. There, we make a strong effort to unify the relevant vocabulary used in available literature sources. In a second step, we use this vocabulary to build a contextual map of the field in the form of a taxonomy indexed by the different development stages of a generic process. This contextual map can guide practitioners and focus their attention on the inspection work most relevant to the introduction or development of inspections at the level of their particular development stage; or to help motivate the use of software inspection earlier in their development cycle. Our work provides three distinct, practical benefits: First, the index taxonomy can help practitioners identify inspection experience directly related to a particular life-cycle stage. Second, our work allows structuring of the large amount of published inspection work. Third, such taxonomy can help researchers compare and assess existing inspection methods and refinements to identify fruitful areas of future work. },
  doi      = {https://doi.org/10.1016/S0164-1212(99)00073-4},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121299000734},
}

@Article{Anastasopoulos2004141,
  author   = {Anastasopoulos, M and Muthig, D},
  title    = {{An evaluation of aspect-oriented programming as a product line implementation technology}},
  journal  = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year     = {2004},
  volume   = {3107},
  pages    = {141--156},
  abstract = {A systematic approach for implementing software product lines is more than just a selection of techniques. Its selection should be based on a systematic analysis of technical requirements and constraints, as well as of the types of variabilities, which occur in a particular application domain and are relevant for the planned product line (PL). In addition, each technique should provide a set of guidelines and criteria that support developers in applying the techniques in a systematic and unified way. This paper presents a case study that was performed to evaluate aspect-oriented programming (AOP) as a PL implementation technology. The systematical evaluation is organized along a general evaluation schema for PL implementation technologies. {\textcopyright} Springer-Verlag 2004.},
  annote   = {cited By 19},
  keywords = {Aspect oriented programming,Aspect-Oriented Programming (AOP); Product-lines;,Computer software reusability},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-33947708260{\&}partnerID=40{\&}md5=b158dc68c45534fe5851c245e48ff223},
}

@Article{Yagüe2016184,
  author   = {Yag{\"{u}}e, Agustin and Garbajosa, Juan and D{\'{i}}az, Jessica and Gonz{\'{a}}lez, Eloy},
  title    = {{An exploratory study in communication in Agile Global Software Development}},
  journal  = {Computer Standards {\&} Interfaces},
  year     = {2016},
  volume   = {48},
  pages    = {184--197},
  issn     = {0920-5489},
  abstract = {Abstract Global software development (GSD) is gaining ever more relevance. Although communication is key in the exchange of information between team members, multi-site software development has introduced additional obstacles (different time-zones and cultures, {\{}IT{\}} infrastructure, etc.) and delays into the act of communication, which is already problematic. Communication is even more critical in the case of Agile Global Software Development (AGSD) in which communication plays a primary role. This paper reports an exploratory study of the effects of tools supporting communication in AGSD. More precisely, this paper analyses the perception of team members about communication infrastructures in AGSD. The research question to which this study responds concerns how development teams perceive the communication infrastructure while developing products using agile methodologies. Most previous studies have dealt with communication support from a highly technological media tool perspective. In this research work, instead, observations were obtained from three perspectives: communication among team members, communication of the status of the development process, and communication of the status of the progress of the product under development. It has been possible to show that team members perceive advantages to using media tools that make them feel in practice that teams are co-located, such as smartboards supported by efficient video-tools, and combining media tools with centralized repository tools, with information from the process development and product characteristics, that allow distributed teams to effectively share information about the status of the project/process/product during the development process in order to overcome some of the still existing problems in communication in AGSD. },
  annote   = {Special Issue on Information System in Distributed Environment},
  doi      = {https://doi.org/10.1016/j.csi.2016.06.002},
  keywords = {Agile,Exploratory research,Global Distributed Software Development,Infrastructure,Tools and technologies},
  url      = {http://www.sciencedirect.com/science/article/pii/S0920548916300381},
}

@Article{Damiani2017111,
  author   = {Damiani, F and Lienhardt, M and Muschevici, R and Schaefer, I},
  title    = {{An extension of the ABS toolchain with a mechanism for type checking SPLs}},
  journal  = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year     = {2017},
  volume   = {10510 LNCS},
  pages    = {111--126},
  abstract = {A Software Product Line (SPL) is a set of similar programs, called variants, with a common code base and well documented variability. Because the number of variants in an SPL can be large, checking them efficiently (e.g., to ensure that they are all well-typed) is a challenging problem. Delta-Oriented Programming (DOP) is a flexible approach to implement SPLs. The Abstract Behavioral Specification (ABS) modeling language and toolchain supports delta-oriented SPLs. In this paper we present an extension of the ABS toolchain with a mechanism for checking that all the variants of an SPL can be generated and are well-typed ABS programs. Currently we have implemented only part of this mechanism: our implementation (integrated in version 1.4.2 of the ABS toolchain and released in April 2017) checks whether all variants can be generated, however it does not check, in particular, whether the bodies of the methods are well-typed. Empirical evaluation shows that the current implementation allows for efficient partial type checking of existing ABS SPLs. {\textcopyright} Springer International Publishing AG 2017.},
  annote   = {cited By 1; Conference of 13th International Conference on Integrated Formal Methods, IFM 2017 ; Conference Date: 20 September 2017 Through 22 September 2017; Conference Code:198629},
  doi      = {10.1007/978-3-319-66845-1_8},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030149220{\&}doi=10.1007{\%}2F978-3-319-66845-1{\_}8{\&}partnerID=40{\&}md5=bda134f0880a96dacc8f65a515a60891},
}

@Article{Galster201516,
  author        = {Galster, Matthias and Avgeriou, Paris},
  title         = {{An industrial case study on variability handling in large enterprise software systems}},
  journal       = {Information and Software Technology},
  year          = {2015},
  volume        = {60},
  pages         = {16--31},
  issn          = {0950-5849},
  abstract      = {AbstractContext Enterprise software systems (e.g., enterprise resource planning software) are often deployed in different contexts (e.g., different organizations or different business units or branches of one organization). However, even though organizations, business units or branches have the same or similar business goals, they may differ in how they achieve these goals. Thus, many enterprise software systems are subject to variability and adapted depending on the context in which they are used. Objective Our goal is to provide a snapshot of variability in large scale enterprise software systems. We aim at understanding the types of variability that occur in large industrial enterprise software systems. Furthermore, we aim at identifying how variability is handled in such systems. Method We performed an exploratory case study in two large software organizations, involving two large enterprise software systems. Data were collected through interviews and document analysis. Data were analyzed following a grounded theory approach. Results We identified seven types of variability (e.g., functionality, infrastructure) and eight mechanisms to handle variability (e.g., add-ons, code switches). Conclusions We provide generic types for classifying variability in enterprise software systems, and reusable mechanisms for handling such variability. Some variability types and handling mechanisms for enterprise software systems found in the real world extend existing concepts and theories. Others confirm findings from previous research literature on variability in software in general and are therefore not specific to enterprise software systems. Our findings also offer a theoretical foundation for describing variability handling in practice. Future work needs to provide more evaluations of the theoretical foundations, and refine variability handling mechanisms into more detailed practices.},
  doi           = {https://doi.org/10.1016/j.infsof.2014.12.003},
  file          = {:Users/mac/ownCloud/Anita/SLR/REVISION DE JOURNALS/PAPERS{\_}D/SELECT DAVID/checked/analizados/casos de estudios analizados/8 An industrial case study on variability handling in large enterprise software systems.pdf:pdf},
  keywords      = {Case study,Enterprise software systems,Grounded theory,Variability,case study},
  mendeley-tags = {case study},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584914002572},
}

@Article{MERNIK20132451,
  author   = {Mernik, Marjan},
  title    = {{An object-oriented approach to language compositions for software language engineering}},
  journal  = {Journal of Systems and Software},
  year     = {2013},
  volume   = {86},
  number   = {9},
  pages    = {2451--2464},
  issn     = {0164-1212},
  abstract = {In this paper, it is shown that inheritance, a core concept from object-oriented programming, is a possible solution for realizing composition of computer languages. Language composability is a property of language descriptions, which can be further classified into informal (language syntax and semantics are hard-coded in compiler/interpreter) and formal language descriptions (syntax and semantics are formally specified with one of several formal methods for language definition). However, language composition is much easier to achieve with declarative formal language descriptions into which the notion of inheritance is introduced. Multiple attribute grammar inheritance, as implemented in the language implementation system LISA, can assist in realizing all of the different types of language compositions identified in Erdweg et al. (2012). Different examples are given throughout the paper using an easy to understand domain-specific language that describes simple robot movement.},
  doi      = {https://doi.org/10.1016/j.jss.2013.04.087},
  keywords = {Domain-specific languages,Language composition,Software language engineering},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121213001271},
}

@Article{Rincón2014111,
  author   = {Rinc{\'{o}}n, L F and Giraldo, G L and Mazo, R and Salinesi, C},
  title    = {{An ontological rule-based approach for analyzing dead and false optional features in feature models}},
  journal  = {Electronic Notes in Theoretical Computer Science},
  year     = {2014},
  volume   = {302},
  pages    = {111--132},
  abstract = {Feature models are a common way to represent variability requirements of software product lines by expressing the set of feature combinations that software products can have. Assuring quality of feature models is thus of paramount importance for assuring quality in software product line engineering. However, feature models can have several types of defects that disminish benefits of software product line engineering.Two of such defects are dead features and false optional features. Several state-of-the-art techniques identify these defects, but only few of them tackle the problem of identifying their causes. Besides, the explanations they provide are cumbersome and hard to understand by humans. In this paper, we propose an ontological rule-based approach to: (a) identify dead and false optional features; (b)identify certain causes of these defects; and (c) explain these causes in natural language helping modelers to correct found defects. We represent our approach with a feature model taken from literature. A preliminary empirical evaluation of our approach over 31 FMs shows that our proposal is effective, accurate and scalable to 150 features. {\textcopyright} 2014 Elsevier B.V.},
  annote   = {cited By 13},
  doi      = {10.1016/j.entcs.2014.01.023},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894169420{\&}doi=10.1016{\%}2Fj.entcs.2014.01.023{\&}partnerID=40{\&}md5=c8eb87067b63d1f08d18a4f24d963b13},
}

@Article{Wu2011811,
  author   = {Wu, Z and Tang, J and Kwong, C K and Chan, C Y},
  title    = {{An optimization model for reuse scenario selection considering reliability and cost in software product line development}},
  journal  = {International Journal of Information Technology and Decision Making},
  year     = {2011},
  volume   = {10},
  number   = {5},
  pages    = {811--841},
  abstract = {In this paper, a model that assists developers to evaluate and compare alternative reuse scenarios in software product line (SPL) development systematically in proposed. The model can identify basic activities (abstracted as operations) and precisely relate cost and reliability with each basic operation. A typical reuse mode is described from the perspectives of application and domain engineering. According to this scheme, six reuse modes are identified, and alternative industry reuse scenarios can be derived from these modes. A bi-objective 0-1 integer programming model is developed to help decision makers select reuse scenarios when they develop a SPL to minimize cost and maximize reliability while satisfying system requirements to a certain degree. This model is called the cost and reliability optimization under constraint satisfaction (CROS). To design the model efficiently, a three-phase algorithm for finding all efficient solutions is developed, where the first two phases can obtain an efficient solution, and the last phase can generate a nonsupported efficient solution. Two practical methods are presented to facilitate decision making on selecting from the entire range of efficient solutions in light of the decision-maker's preference for mancomputer interaction. An application of the CROS model in a mail server system development is presented as a case study. {\textcopyright} 2011 World Scientific Publishing Company.},
  annote   = {cited By 5},
  doi      = {10.1142/S0219622011004580},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052562807{\&}doi=10.1142{\%}2FS0219622011004580{\&}partnerID=40{\&}md5=a0262f562a9e74e50aa90712dde54c2d},
}

@Article{Anand20131978,
  author   = {Anand, Saswat and Burke, Edmund K and Chen, Tsong Yueh and Clark, John and Cohen, Myra B and Grieskamp, Wolfgang and Harman, Mark and Harrold, Mary Jean and McMinn, Phil and Bertolino, Antonia and Li, J Jenny and Zhu, Hong},
  title    = {{An orchestrated survey of methodologies for automated software test case generation}},
  journal  = {Journal of Systems and Software},
  year     = {2013},
  volume   = {86},
  number   = {8},
  pages    = {1978--2001},
  issn     = {0164-1212},
  abstract = {Abstract Test case generation is among the most labour-intensive tasks in software testing. It also has a strong impact on the effectiveness and efficiency of software testing. For these reasons, it has been one of the most active research topics in software testing for several decades, resulting in many different approaches and tools. This paper presents an orchestrated survey of the most prominent techniques for automatic generation of software test cases, reviewed in self-standing sections. The techniques presented include: (a) structural testing using symbolic execution, (b) model-based testing, (c) combinatorial testing, (d) random testing and its variant of adaptive random testing, and (e) search-based testing. Each section is contributed by world-renowned active researchers on the technique, and briefly covers the basic ideas underlying the method, the current state of the art, a discussion of the open research problems, and a perspective of the future development of the approach. As a whole, the paper aims at giving an introductory, up-to-date and (relatively) short overview of research in automatic test case generation, while ensuring a comprehensive and authoritative treatment. },
  doi      = {https://doi.org/10.1016/j.jss.2013.02.061},
  keywords = {Adaptive random testing,Combinatorial testing,Model-based testing,Orchestrated survey,Search-based software testing,Software testing,Symbolic execution,Test automation,Test case generation},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121213000563},
}

@Article{Capilla2014,
  author    = {Capilla, Rafael and Bosch, Jan and Trinidad, Pablo and Ruiz-Cort??s, Antonio and Hinchey, Mike},
  title     = {{An overview of Dynamic Software Product Line architectures and techniques: Observations from research and industry}},
  journal   = {Journal of Systems and Software},
  year      = {2014},
  volume    = {91},
  number    = {1},
  pages     = {3--23},
  issn      = {01641212},
  abstract  = {Over the last two decades, software product lines have been used successfully in industry for building families of systems of related products, maximizing reuse, and exploiting their variable and configurable options. In a changing world, modern software demands more and more adaptive features, many of them performed dynamically, and the requirements on the software architecture to support adaptation capabilities of systems are increasing in importance. Today, many embedded system families and application domains such as ecosystems, service-based applications, and self-adaptive systems demand runtime capabilities for flexible adaptation, reconfiguration, and post-deployment activities. However, as traditional software product line architectures fail to provide mechanisms for runtime adaptation and behavior of products, there is a shift toward designing more dynamic software architectures and building more adaptable software able to handle autonomous decision-making, according to varying conditions. Recent development approaches such as Dynamic Software Product Lines (DSPLs) attempt to face the challenges of the dynamic conditions of such systems but the state of these solution architectures is still immature. In order to provide a more comprehensive treatment of DSPL models and their solution architectures, in this research work we provide an overview of the state of the art and current techniques that, partially, attempt to face the many challenges of runtime variability mechanisms in the context of Dynamic Software Product Lines. We also provide an integrated view of the challenges and solutions that are necessary to support runtime variability mechanisms in DSPL models and software architectures. ?? 2014 Elsevier Inc.},
  doi       = {10.1016/j.jss.2013.12.038},
  file      = {:Users/mac/Library/Application Support/Mendeley Desktop/Downloaded/Capilla et al. - 2014 - An overview of Dynamic Software Product Line architectures and techniques Observations from research and industr.pdf:pdf},
  isbn      = {0164-1212},
  keywords  = {Dynamic Software Product Lines,Dynamic variability,Feature models,Software architecture},
  publisher = {Elsevier Inc.},
  url       = {http://dx.doi.org/10.1016/j.jss.2013.12.038},
}

@Article{ESCALONA20111379,
  author   = {Escalona, M J and Gutierrez, J J and Mej{\'{i}}as, M and Arag{\'{o}}n, G and Ramos, I and Torres, J and Dom{\'{i}}nguez, F J},
  title    = {{An overview on test generation from functional requirements}},
  journal  = {Journal of Systems and Software},
  year     = {2011},
  volume   = {84},
  number   = {8},
  pages    = {1379--1393},
  issn     = {0164-1212},
  abstract = {Despite the fact that the test phase is described in the literature as one of the most relevant for quality assurance in software projects, this test phase is not usually developed, among others, with enough resources, time or suitable techniques. To offer solutions which supply the test phase, with appropriate tools for the automation of tests generation, or even, for their self-execution, could become a suitable way to improve this phase and reduce the cost constraints in real projects. This paper focuses on answering a concrete research question: is it possible to generate test cases from functional requirements described in an informal way? For this aim, it presents an overview of a set of relevant approaches that works in this field and offers a set of comparative analysis to determine which the state of the art is.},
  doi      = {https://doi.org/10.1016/j.jss.2011.03.051},
  keywords = {Early testing,Functional test generation,Testing},
  url      = {http://www.sciencedirect.com/science/article/pii/S016412121100080X},
}

@Article{Cuesta2005177,
  author   = {Cuesta, Carlos E and de la Fuente, Pablo and Barrio-Sol{\'{o}}rzano, Manuel and Beato, M Encarnaci{\'{o}}n},
  title    = {{An “abstract process” approach to algebraic dynamic architecture description}},
  journal  = {The Journal of Logic and Algebraic Programming},
  year     = {2005},
  volume   = {63},
  number   = {2},
  pages    = {177--214},
  issn     = {1567-8326},
  abstract = {Current software development methodologies recognize the critical importance of the architectural concerns during the design phase. Software Architecture promises to be the solution for a number of recurring problems; but to do so, the first task is to be able to obtain a precise description of a system architecture. In late years, a number of specific architecture description languages (Adls) have been proposed in order to achieve the required precision. Most of them have solid formal foundations; among them, several process-algebraic Adls stand out for their popularity and expressive power. The algebraic approach to architecture description is probably the most successful in the field. There is a natural intuition relating the concepts of algebraic process and architectural component; anyway, none of the existing approaches seems to have found the right balance between them. This article explains what is the problem with them, and defines the informal concept of abstract process, trying to provide a reference for the right level of abstraction. After presenting the concept, the article presents a dynamic, reflective Adl named P i L ar, which has been designed using this notion. The syntax and semantics of this language are briefly summarized and explained. Finally, the classic example of the Gas Station is described in terms of P i L ar, and then compared to previous presentations in other Adls. },
  annote   = {Special Issue on Process Algebra and System Architecture},
  doi      = {https://doi.org/10.1016/j.jlap.2004.05.003},
  keywords = {Abstract process,Architecture description language,Dynamic software architecture,Process algebra,Reflection,$\pi$-Calculus},
  url      = {http://www.sciencedirect.com/science/article/pii/S1567832604000360},
}

@Article{El-Sharkawy:2015:AKS:2936314.2814222,
  author    = {El-Sharkawy, Sascha and Krafczyk, Adam and Schmid, Klaus},
  title     = {{Analysing the Kconfig Semantics and Its Analysis Tools}},
  journal   = {SIGPLAN Not.},
  year      = {2015},
  volume    = {51},
  number    = {3},
  pages     = {45--54},
  issn      = {0362-1340},
  address   = {New York, NY, USA},
  doi       = {10.1145/2936314.2814222},
  keywords  = {Kconfig,Kconfig Reader,LVAT,Linux Kernel Analysis,Linux Variability Analysis Tools,Software Product Lines,Undertaker},
  publisher = {ACM},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/2936314.2814222},
}

@Article{Lutz2003253,
  author   = {Lutz, Robyn R and Gannod, Gerald C},
  title    = {{Analysis of a software product line architecture: an experience report}},
  journal  = {Journal of Systems and Software},
  year     = {2003},
  volume   = {66},
  number   = {3},
  pages    = {253--267},
  issn     = {0164-1212},
  abstract = {This paper describes experiences with the architectural specification and tool-assisted architectural analysis of a mission-critical, high-performance software product line. The approach used defines a “good” product line architecture in terms of those quality attributes required by the particular product line under development. Architectures are analyzed against several criteria by both manual and tool-supported methods. The approach described in this paper provides a structured analysis of an existing product line architecture using (1) architecture recovery and specification, (2) architecture evaluation, and (3) model checking of behavior to determine the level of robustness and fault tolerance at the architectural level that are required for all systems in the product line. Results of an application to a software product line of spaceborne telescopes are used to explain the approach and describe lessons learned. },
  annote   = {Software architecture -- Engineering quality attributes},
  doi      = {https://doi.org/10.1016/S0164-1212(02)00081-X},
  url      = {http://www.sciencedirect.com/science/article/pii/S016412120200081X},
}

@Article{Peng2011707,
  author   = {Peng, X and Yu, Y and Zhao, W},
  title    = {{Analyzing evolution of variability in a software product line: From contexts and requirements to features}},
  journal  = {Information and Software Technology},
  year     = {2011},
  volume   = {53},
  number   = {7},
  pages    = {707--721},
  abstract = {Context: In the long run, features of a software product line (SPL) evolve with respect to changes in stakeholder requirements and system contexts. Neither domain engineering nor requirements engineering handles such co-evolution of requirements and contexts explicitly, making it especially hard to reason about the impact of co-changes in complex scenarios. Objective: In this paper, we propose a problem-oriented and value-based analysis method for variability evolution analysis. The method takes into account both kinds of changes (requirements and contexts) during the life of an evolving software product line. Method: The proposed method extends the core requirements engineering ontology with the notions to represent variability-intensive problem decomposition and evolution. On the basis of problemorientation, the analysis method identifies candidate changes, detects influenced features, and evaluates their contributions to the value of the SPL. Results and Conclusion: The process of applying the analysis method is illustrated using a concrete case study of an evolving enterprise software system, which has confirmed that tracing back to requirements and contextual changes is an effective way to understand the evolution of variability in the software product line. {\textcopyright} 2011 Elsevier B.V. All rights reserved.},
  annote   = {cited By 10},
  doi      = {10.1016/j.infsof.2011.01.001},
  keywords = {Context; Evolution; Feature; Requirements; Softwar,Ontology,Requirements engineering},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-79955070030{\&}doi=10.1016{\%}2Fj.infsof.2011.01.001{\&}partnerID=40{\&}md5=dba8bf360fcd61e03ea82701cae2869b},
}

@Article{Tüzün2015136,
  author   = {T{\"{u}}z{\"{u}}n, Eray and Tekinerdogan, Bedir},
  title    = {{Analyzing impact of experience curve on {\{}ROI{\}} in the software product line adoption process}},
  journal  = {Information and Software Technology},
  year     = {2015},
  volume   = {59},
  pages    = {136--148},
  issn     = {0950-5849},
  abstract = {AbstractContext Experience curve is a well-known concept in management and education science, which explains the phenomenon of increased worker efficiency with repetitive production of a good or service. Objective We aim to analyze the impact of the experience curve effect on the Return on Investment (ROI) in the software product line engineering (SPLE) process. Method We first present the results of a systematic literature review (SLR) to explicitly depict the studies that have considered the impact of experience curve effect on software development in general. Subsequently, based on the results of the SLR, the experience curve effect models in the literature, and the {\{}SPLE{\}} cost models, we define an approach for extending the cost models with the experience curve effect. Finally, we discuss the application of the refined cost models in a real industrial context. Results The {\{}SLR{\}} resulted in 15 primary studies which confirm the impact of experience curve effect on software development in general but the experience curve effect in the adoption of {\{}SPLE{\}} got less attention. The analytical discussion of the cost models and the application of the refined {\{}SPLE{\}} cost models in the industrial context showed a clear impact of the experience curve effect on the time-to-market, cost of development and {\{}ROI{\}} in the {\{}SPLE{\}} adoption process. Conclusions The proposed analysis with the newly defined cost models for {\{}SPLE{\}} adoption provides a more precise analysis tool for the management, and as such helps to support a better decision making. },
  doi      = {https://doi.org/10.1016/j.infsof.2014.09.008},
  keywords = {Cost models,Experience curve,Learning curve,Productivity,Software product line engineering,Software reuse},
  url      = {http://www.sciencedirect.com/science/article/pii/S0950584914002079},
}

@Article{Benlarabi2015550,
  author   = {Benlarabi, A and Khtira, A and {El Asri}, B},
  title    = {{Analyzing trends in software product lines evolution using a cladistics based approach}},
  journal  = {Information (Switzerland)},
  year     = {2015},
  volume   = {6},
  number   = {3},
  pages    = {550--563},
  abstract = {Abstract: A software product line is a complex system the aim of which is to provide a platform dedicated to large reuse. It necessitates a great investment. Thus, its ability to cope with customers' ever-changing requirements is among its key success factors. Great effort has been made to deal with the software product line evolution. In our previous works, we carried out a classification of these works to provide an overview of the used techniques. We also identified the following key challenges of software product lines evolution: the ability to predict future changes, the ability to define the impact of a change easily and the improvement in understanding the change. We have already tackled the second and the third challenges. The objective of this paper is to deal with the first challenge. We use the cladistics classification which was used in biology to understand the evolution of organisms sharing the same ancestor and their process of descent at the aim of predicting their future changes. By analogy, we consider a population of applications for media management on mobile devices derived from the same platform and we use cladistics to construct their evolutionary tree. We conducted an analysis to show how to identify the evolution trends of the case study products and to predict future changes. {\textcopyright} 2015 by the authors.},
  annote   = {cited By 1},
  doi      = {10.3390/info6030550},
  keywords = {Biology,Cladistics; Evolution; Evolution trend; Evolution,Computer software; Computer software reusability;},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84943561816{\&}doi=10.3390{\%}2Finfo6030550{\&}partnerID=40{\&}md5=892c17009c76fbae807d486836450566},
}

@Article{Hoffman2003143,
  author   = {Hoffman, Daniel and Strooper, Paul},
  title    = {{{\{}API{\}} documentation with executable examples}},
  journal  = {Journal of Systems and Software},
  year     = {2003},
  volume   = {66},
  number   = {2},
  pages    = {143--156},
  issn     = {0164-1212},
  abstract = {The rise of component-based software development has created an urgent need for effective application program interface (API) documentation. Experience has shown that it is hard to create precise and readable documentation. Prose documentation can provide a good overview but lacks precision. Formal methods offer precision but the resulting documentation is expensive to develop. Worse, few developers have the skill or inclination to read formal documentation. We present a pragmatic solution to the problem of {\{}API{\}} documentation. We augment the prose documentation with executable test cases, including expected outputs, and use the prose plus the test cases as the documentation. With appropriate tool support, the test cases are easy to develop and read. Such test cases constitute a completely formal, albeit partial, specification of input/output behavior. Equally important, consistency between code and documentation is demonstrated by running the test cases. This approach provides an attractive bridge between formal and informal documentation. We also present a tool that supports compact and readable test cases, and generation of test drivers and documentation, and illustrate the approach with detailed case studies. },
  doi      = {https://doi.org/10.1016/S0164-1212(02)00055-9},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121202000559},
}

@Article{Vázquez-Ingelmo2018518,
  author   = {V{\'{a}}zquez-Ingelmo, A and Garc{\'{i}}a-Pe{\~{n}}alvo, F J and Ther{\'{o}}n, R},
  title    = {{Application of Domain Engineering to Generate Customized Information Dashboards}},
  journal  = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year     = {2018},
  volume   = {10925 LNCS},
  pages    = {518--529},
  abstract = {Information dashboards play a key role in analyzing and visualizing data about a specific topic or domain. In essence, these dashboards display information and enable users to reach insights and make informed decisions. However, end users can have several necessities that are different from each other, including the displayed information itself or other design features. For these reasons, a domain engineering approach is proposed in order to produce customized dashboards adapted to singular requirements of every involved user (or group of users) by the parameterization of features, presentation components and data sources, obtaining a software product line of information dashboards. The creation of a product line would increase productivity, maintainability and traceability regarding the evolution of the dashboards' requirements. To validate this approach, a case study of its application in the context of the Spanish Observatory of Employability and University Employment ecosystem is described, where users (Spanish universities and administrators) will control their own dashboards to reach insights about the employability of their graduates. {\textcopyright} 2018, Springer International Publishing AG, part of Springer Nature.},
  annote   = {cited By 0; Conference of 5th International Conference on Learning and Collaboration Technologies, LCT 2018 Held as Part of HCI International 2018 ; Conference Date: 15 July 2018 Through 20 July 2018; Conference Code:216129},
  doi      = {10.1007/978-3-319-91152-6_40},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050491915{\&}doi=10.1007{\%}2F978-3-319-91152-6{\_}40{\&}partnerID=40{\&}md5=764dd55ab95eee86ce46a905774b8c73},
}

@Article{vanderSpek20111261,
  author   = {van der Spek, Pieter and Klusener, Steven},
  title    = {{Applying a dynamic threshold to improve cluster detection of {\{}LSI{\}}}},
  journal  = {Science of Computer Programming},
  year     = {2011},
  volume   = {76},
  number   = {12},
  pages    = {1261--1274},
  issn     = {0167-6423},
  abstract = {Latent Semantic Indexing (LSI) is a standard approach for extracting and representing the meaning of words in a large set of documents. Recently it has been shown that it is also useful for identifying concerns in source code. The tree cutting strategy plays an important role in obtaining the clusters, which identify the concerns. In this contribution the authors compare two tree cutting strategies: the Dynamic Hybrid cut and the commonly used fixed height threshold. Two case studies have been performed on the source code of Philips Healthcare to compare the results using both approaches. While some of the settings are particular to the Philips-case, the results show that applying a dynamic threshold, implemented by the Dynamic Hybrid cut, is an improvement over the fixed height threshold in the detection of clusters representing relevant concerns. This makes the approach as a whole more usable in practice. },
  annote   = {Special Issue on Software Evolution, Adaptability and Variability},
  doi      = {https://doi.org/10.1016/j.scico.2010.12.004},
  keywords = {Clustering,Feature extraction,Latent Semantic Indexing,Reverse engineering,Software architecture},
  url      = {http://www.sciencedirect.com/science/article/pii/S0167642310002297},
}

@Article{Figueiredo2012227,
  author   = {Figueiredo, Eduardo and Sant'Anna, Claudio and Garcia, Alessandro and Lucena, Carlos},
  title    = {{Applying and evaluating concern-sensitive design heuristics}},
  journal  = {Journal of Systems and Software},
  year     = {2012},
  volume   = {85},
  number   = {2},
  pages    = {227--243},
  issn     = {0164-1212},
  abstract = {Manifestation of crosscutting concerns in software systems is often an indicative of design modularity flaws and further design instabilities as those systems evolve. Without proper design evaluation mechanisms, the identification of harmful crosscutting concerns can become counter-productive and impractical. Nowadays, metrics and heuristics are the basic mechanisms to support their identification and classification either in object-oriented or aspect-oriented programs. However, conventional mechanisms have a number of limitations to support an effective identification and classification of crosscutting concerns in a software system. In this paper, we claim that those limitations are mostly caused by the fact that existing metrics and heuristics are not sensitive to primitive concern properties, such as either their degree of tangling and scattering or their specific structural shapes. This means that modularity assessment is rooted only at conventional attributes of modules, such as module cohesion, coupling and size. This paper proposes a representative suite of concern-sensitive heuristic rules. The proposed heuristics are supported by a prototype tool. The paper also reports an exploratory study to evaluate the accuracy of the proposed heuristics by applying them to seven systems. The results of this exploratory analysis give evidences that the heuristics offer support for: (i) addressing the shortcomings of conventional metrics-based assessments, (ii) reducing the manifestation of false positives and false negatives in modularity assessment, (iii) detecting sources of design instability, and (iv) finding the presence of design modularity flaws in both object-oriented and aspect-oriented programs. Although our results are limited to a number of decisions we made in this study, they indicate a promising research direction. Further analyses are required to confirm or refute our preliminary findings and, so, this study should be seen as a stepping stone on understanding how concerns can be useful assessment abstractions. We conclude this paper by discussing the limitations of this exploratory study focusing on some situations which hinder the accuracy of concern-sensitive heuristics. },
  annote   = {Special issue with selected papers from the 23rd Brazilian Symposium on Software Engineering},
  doi      = {https://doi.org/10.1016/j.jss.2011.09.060},
  keywords = {Aspect-oriented software development,Crosscutting concerns,Design heuristics,Modularity,Software metrics},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121211002585},
}

@Article{Pascual2015392,
  author   = {Pascual, Gustavo G and Lopez-Herrejon, Roberto E and Pinto, M{\'{o}}nica and Fuentes, Lidia and Egyed, Alexander},
  title    = {{Applying multiobjective evolutionary algorithms to dynamic software product lines for reconfiguring mobile applications}},
  journal  = {Journal of Systems and Software},
  year     = {2015},
  volume   = {103},
  pages    = {392--411},
  issn     = {0164-1212},
  abstract = {Abstract Mobile applications require dynamic reconfiguration services (DRS) to self-adapt their behavior to the context changes (e.g., scarcity of resources). Dynamic Software Product Lines (DSPL) are a well-accepted approach to manage runtime variability, by means of late binding the variation points at runtime. During the system's execution, the {\{}DRS{\}} deploys different configurations to satisfy the changing requirements according to a multiobjective criterion (e.g., insufficient battery level, requested quality of service). Search-based software engineering and, in particular, multiobjective evolutionary algorithms (MOEAs), can generate valid configurations of a {\{}DSPL{\}} at runtime. Several approaches use {\{}MOEAs{\}} to generate optimum configurations of a Software Product Line, but none of them consider {\{}DSPLs{\}} for mobile devices. In this paper, we explore the use of {\{}MOEAs{\}} to generate at runtime optimum configurations of the {\{}DSPL{\}} according to different criteria. The optimization problem is formalized in terms of a Feature Model (FM), a variability model. We evaluate six existing {\{}MOEAs{\}} by applying them to 12 different FMs, optimizing three different objectives (usability, battery consumption and memory footprint). The results are discussed according to the particular requirements of a {\{}DRS{\}} for mobile applications, showing that {\{}PAES{\}} and NSGA-II are the most suitable algorithms for mobile environments. },
  doi      = {https://doi.org/10.1016/j.jss.2014.12.041},
  keywords = {DSPL,Dynamic reconfiguration,Evolutionary algorithms},
  url      = {http://www.sciencedirect.com/science/article/pii/S016412121400291X},
}

@Article{ROMERO20171075,
  author   = {Romero, L F and Arce, A},
  title    = {{Applying Value Stream Mapping in Manufacturing: A Systematic Literature Review}},
  journal  = {IFAC-PapersOnLine},
  year     = {2017},
  volume   = {50},
  number   = {1},
  pages    = {1075--1086},
  issn     = {2405-8963},
  abstract = {Value Stream Mapping is a critical tool when it comes to implement the lean approach and it has spanned to many sectors in industry. Although previous studies justify its use in manufacturing sector by identifying previous cases within the literature, none to the best of our knowledge has used our approach to explore the aspects covered it this review, yet the potential exists. Based on a systematic approach, we analyzed available literature published in refereed journals, providing academics and researchers with valuable findings related to the evolution, application and performance of the Value Stream Mapping in context of the manufacturing sector.},
  annote   = {20th IFAC World Congress},
  doi      = {https://doi.org/10.1016/j.ifacol.2017.08.385},
  keywords = {Value stream mapping,business process management systems,lean manufacturing,logistics in manufacturing,methodologies,tools for analysis of productive systems},
  url      = {http://www.sciencedirect.com/science/article/pii/S2405896317307292},
}

@Article{GIOVANNINI20143280,
  author   = {Giovannini, A and Aubry, A and Panetto, H and Haouzi, H El and Pierrel, L and Dassisti, M},
  title    = {{Approach for the rationalisation of product lines variety}},
  journal  = {IFAC Proceedings Volumes},
  year     = {2014},
  volume   = {47},
  number   = {3},
  pages    = {3280--3291},
  issn     = {1474-6670},
  abstract = {The product variety management is a key process to deal with the flexibility requested by the mass customisation. In this paper we show that current variety-modelling methods miss a customer representation: without a proper assessment of the customers is not possible to define the product variety that has to be developed to meet the requirements of a customer segment. Here we present an innovative approach to rationalise the product variety, i.e. to link each product variant to the customer profile who needs it. The aim is to optimise the product variety avoiding excesses (variants not related to a customer), lacks (customers not related to a variant) or redundancies (two or more variants proposed to a customer). An overview of customer modelling approaches in the classic product design (non-customisable) is presented. The innovative approach is here developed using system-thinking concepts. A knowledge-based system that uses this approach is designed. Finally the approach is explained using a real industrial case of a quasi-real coil design process.},
  annote   = {19th IFAC World Congress},
  doi      = {https://doi.org/10.3182/20140824-6-ZA-1003.02226},
  keywords = {Knowledge representation,Knowledge-based system,Mass customization,Product variety},
  url      = {http://www.sciencedirect.com/science/article/pii/S1474667016421130},
}

@Article{Eklund20132347,
  author   = {Eklund, Ulrik and Gustavsson, H{\aa}kan},
  title    = {{Architecting automotive product lines: Industrial practice}},
  journal  = {Science of Computer Programming},
  year     = {2013},
  volume   = {78},
  number   = {12},
  pages    = {2347--2359},
  issn     = {0167-6423},
  abstract = {This paper presents an in-depth view of how architects work with maintaining product line architectures at two internationally well-known automotive companies. The case study shows several interesting results. The process of managing architectural changes as well as the information the architects maintain and update is surprisingly similar between the two companies, despite that one has a strong line organisation and the other a strong project organisation. The architecting process found does not differ from what can be seen in other business domains. What does differ is that the architects studied see themselves interacting much more with other stakeholders than architects in general. The actual architectures are based on similar technology, e.g. CAN, but the network topology, S/W deployment and interfaces are totally different. The results indicate how the company's different core values influence the architects when defining and maintaining the architectures over time. One company maintains four similar architectures in parallel, each at a different stage in their respective life-cycle, while the other has a single architecture for all products since 2002. The organisational belonging of the architects in the former company has been turbulent in contrast to the latter and there is some speculation if this is correlated. },
  annote   = {Special Section on International Software Product Line Conference 2010 and Fundamentals of Software Engineering (selected papers of {\{}FSEN{\}} 2011)},
  doi      = {https://doi.org/10.1016/j.scico.2012.06.008},
  file     = {:Users/mac/ownCloud/Anita/SLR/REVISION DE JOURNALS/PAPERS{\_}D/SELECT DAVID/checked/analizados/casos de estudios analizados/18 Architecting automotive product lines- Industrial practice.pdf:pdf},
  keywords = {Architecting,Automotive industry,Case study,Process},
  url      = {http://www.sciencedirect.com/science/article/pii/S0167642312001190},
}

@Article{Folmer200461,
  author   = {Folmer, Eelke and Bosch, Jan},
  title    = {{Architecting for usability: a survey}},
  journal  = {Journal of Systems and Software},
  year     = {2004},
  volume   = {70},
  number   = {1–2},
  pages    = {61--78},
  issn     = {0164-1212},
  abstract = {Over the years the software engineering community has increasingly realized the important role software architecture plays in fulfilling the quality requirements of a system. The quality attributes of a software system are, to a large extent determined by the system's software architecture. In recent years, the software engineering community has developed various tools and techniques that allow for design for quality attributes, such as performance or maintainability, at the software architecture level. We believe this design approach can be applied not only to “traditional” quality attributes such as performance or maintainability but also to usability. This survey explores the feasibility of such a design approach. Current practice is surveyed from the perspective of a software architect. Are there any design methods that allow for design for usability at the architectural level? Are there any evaluation tools that allow assessment of architectures for their support of usability? What is usability? A framework is presented which visualizes these three research questions. Usability should drive design at all stages, but current usability engineering practice fails to fully achieve this goal. Our survey shows that there are no design techniques or assessment tools that allow for design for usability at the architectural level. },
  doi      = {https://doi.org/10.1016/S0164-1212(02)00159-0},
  keywords = {Design for quality attributes,Software architecture,Usability},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121202001590},
}

@Article{Gamez2013563,
  author   = {Gamez, Nadia and Fuentes, Lidia},
  title    = {{Architectural evolution of FamiWare using cardinality-based feature models}},
  journal  = {Information and Software Technology},
  year     = {2013},
  volume   = {55},
  number   = {3},
  pages    = {563--580},
  issn     = {0950-5849},
  abstract = {Context Ambient Intelligence systems domain is an outstanding example of modern systems that are in permanent evolution, as new devices, technologies or facilities are continuously appearing. This means it would be desirable to have a mechanism that helps with the propagation of evolution changes in deployed systems. Objective We present a software product line engineering process to manage the evolution of FamiWare, a family of middleware for ambient intelligence environments. This process drives the evolution of FamiWare middleware configurations using cardinality-based feature models, which are especially well suited to express the structural variability of ambient intelligence systems. Method FamiWare uses cardinality-based feature models and clonable features to model the structural variability present in ambient intelligence systems, composed of a large variety of heterogeneous devices. Since the management evolution of configurations with clonable features is manually untreatable due to the high number of features, our process automates it and propagates changes made at feature level to the architectural components of the FamiWare middleware. This is a model driven development process as the evolution management, the propagation of evolution changes and the code generation are performed using some kind of model mappings and transformations. Concretely we present a variability modelling language to map the selection of features to the corresponding FamiWare middleware architectural components. Results Our process is able to manage the evolution of cardinality-based feature models with thousands of features, something which is not possible to tackle manually. Thanks to the use of the variability language and the automatic code generation it is possible to propagate and maintain a correspondence between the FamiWare architectural model and the code. The process is then able to calculate the architectural differences between the evolved configuration and the previous one. Checking these differences, our process helps to calculate the effort needed to perform the evolution changes in the customized products. To perform those tasks we have defined two operators, one to calculate the differences between two feature model configurations and another to create a new configuration from a previous one. Conclusion Our process automatically propagates the evolution changes of the middleware family into the existing configurations where the middleware is already deployed and also helps us to calculate the effort in performing the changes in every configuration. Finally, we validated our approach, demonstrating the functioning of the defined operators and showing that by using our tool we can generate evolved configurations for FamiWare with thousands of cloned features, for several case studies. },
  annote   = {Special Issue on Software Reuse and Product LinesSpecial Issue on Software Reuse and Product Lines},
  doi      = {https://doi.org/10.1016/j.infsof.2012.06.012},
  keywords = {Evolution,Feature Models,Middleware family,Software Product Lines},
  url      = {http://www.sciencedirect.com/science/article/pii/S0950584912001152},
}

@Article{Eklund2014128,
  author   = {Eklund, Ulrik and Bosch, Jan},
  title    = {{Architecture for embedded open software ecosystems}},
  journal  = {Journal of Systems and Software},
  year     = {2014},
  volume   = {92},
  pages    = {128--142},
  issn     = {0164-1212},
  abstract = {Abstract Software is prevalent in embedded products and may be critical for the success of the products, but manufacturers may view software as a necessary evil rather than as a key strategic opportunity and business differentiator. One of the reasons for this can be extensive supplier and subcontractor relationships and the cost, effort or unpredictability of the deliverables from the subcontractors are experienced as a major problem. The paper proposes open software ecosystem as an alternative approach to develop software for embedded systems, and elaborates on the necessary quality attributes of an embedded platform underlying such an ecosystem. The paper then defines a reference architecture consisting of 17 key decisions together with four architectural patterns, and provides the rationale why they are essential for an open software ecosystem platform for embedded systems in general and automotive systems in particular. The reference architecture is validated through a prototypical platform implementation in an industrial setting, providing a deeper understanding of how the architecture could be realised in the automotive domain. Four potential existing platforms, all targeted at the embedded domain (Android, OKL4, {\{}AUTOSAR{\}} and Robocop), are evaluated against the identified quality attributes to see how they could serve as a basis for an open software ecosystem platform with the conclusion that while none of them is a perfect fit they all have fundamental mechanisms necessary for an open software ecosystem approach. },
  doi      = {https://doi.org/10.1016/j.jss.2014.01.009},
  keywords = {Embedded software,Software architecture,Software ecosystem},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121214000211},
}

@Article{Bjarnason20121107,
  author        = {Bjarnason, Elizabeth and Wnuk, Krzysztof and Regnell, Bj{\"{o}}rn},
  title         = {{Are you biting off more than you can chew? A case study on causes and effects of overscoping in large-scale software engineering}},
  journal       = {Information and Software Technology},
  year          = {2012},
  volume        = {54},
  number        = {10},
  pages         = {1107--1124},
  issn          = {0950-5849},
  abstract      = {Context Scope management is a core part of software release management and often a key factor in releasing successful software products to the market. In a market-driven case, when only a few requirements are known a priori, the risk of overscoping may increase. Objective This paper reports on findings from a case study aimed at understanding overscoping in large-scale, market-driven software development projects, and how agile requirements engineering practices may affect this situation. Method Based on a hypothesis of which factors that may be involved in an overscoping situation, semi-structured interviews were performed with nine practitioners at a large, market-driven software company. The results from the interviews were validated by six (other) practitioners at the case company via a questionnaire. Results The results provide a detailed picture of overscoping as a phenomenon including a number of causes, root causes and effects, and indicate that overscoping is mainly caused by operating in a fast-moving market-driven domain and how this ever-changing inflow of requirements is managed. Weak awareness of overall goals, in combination with low development involvement in early phases, may contribute to ‘biting off' more than a project can ‘chew'. Furthermore, overscoping may lead to a number of potentially serious and expensive consequences, including quality issues, delays and failure to meet customer expectations. Finally, the study indicates that overscoping occurs also when applying agile requirements engineering practices, though the overload is more manageable and perceived to result in less wasted effort when applying a continuous scope prioritization, in combination with gradual requirements detailing and a close cooperation within cross-functional teams. Conclusion The results provide an increased understanding of scoping as a complex and continuous activity, including an analysis of the causes, effects, and a discussion on possible impact of agile requirements engineering practices to the issue of overscoping. The results presented in this paper can be used to identify potential factors to address in order to achieve a more realistic project scope.},
  doi           = {https://doi.org/10.1016/j.infsof.2012.04.006},
  keywords      = {Agile requirements engineering,Case study,Empirical study,Requirements scoping,Software release planning,case study},
  mendeley-tags = {case study},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584912000778},
}

@Article{Bass20161,
  author   = {Bass, Julian M},
  title    = {{Artefacts and agile method tailoring in large-scale offshore software development programmes}},
  journal  = {Information and Software Technology},
  year     = {2016},
  volume   = {75},
  pages    = {1--16},
  issn     = {0950-5849},
  abstract = {Abstract Context: Large-scale offshore software development programmes are complex, with challenging deadlines and a high risk of failure. Agile methods are being adopted, despite the challenges of coordinating multiple development teams. Agile processes are tailored to support team coordination. Artefacts are tangible products of the software development process, intended to ensure consistency in the approach of teams on the same development programme. Objective: This study aims to increase understanding of how development processes are tailored to meet the needs of large-scale offshore software development programmes, by focusing on artefact inventories used in the development process. Method: A grounded theory approach using 46 practitioner interviews, supplemented with documentary sources and observations, in nine international companies was adopted. The grounded theory concepts of open coding, memoing, constant comparison and saturation were used in data analysis. Results: The study has identified 25 artefacts, organised into five categories: feature, sprint, release, product and corporate governance. It was discovered that conventional agile artefacts are enriched with artefacts associated with plan-based methods in order to provide governance. The empirical evidence collected in the study has been used to identify a primary owner of each artefact and map each artefact to specific activities within each of the agile roles. Conclusion: The development programmes in this study create agile and plan-based artefacts to improve compliance with enterprise quality standards and technology strategies, whilst also mitigating risk of failure. Management of these additional artefacts is currently improvised because agile development processes lack corresponding ceremonies. },
  doi      = {https://doi.org/10.1016/j.infsof.2016.03.001},
  keywords = {Agile software development,Enterprise,Grounded theory,Large-scale,Offshore,Outsourced,Process tailoring,Scrum,Software development artefacts},
  url      = {http://www.sciencedirect.com/science/article/pii/S0950584916300350},
}

@Article{Uzunov2015112,
  author   = {Uzunov, Anton V and Fernandez, Eduardo B and Falkner, Katrina},
  title    = {{ASE: A comprehensive pattern-driven security methodology for distributed systems}},
  journal  = {Computer Standards {\&} Interfaces},
  year     = {2015},
  volume   = {41},
  pages    = {112--137},
  issn     = {0920-5489},
  abstract = {Abstract Incorporating security features is one of the most important and challenging tasks in designing distributed systems. Over the last decade, researchers and practitioners have come to recognize that the incorporation of security features should proceed by means of a structured, systematic approach, combining principles from both software and security engineering. Such systematic approaches, particularly those implying some sort of process aligned with the development life-cycle, are termed security methodologies. There are a number of security methodologies in the literature, of which the most flexible and, according to a recent survey, most satisfactory from an industry-adoption viewpoint are methodologies that encapsulate their security solutions in some fashion, especially via the use of security patterns. While the literature does present several mature pattern-driven security methodologies with either a general or a highly specific system applicability, there are currently no (pattern-driven) security methodologies specifically designed for general distributed systems. Going further, there are also currently no methodologies with mixed specific applicability, e.g. for both general and peer-to-peer distributed systems. In this paper we aim to fill these gaps by presenting a comprehensive pattern-driven security methodology – arrived at by applying a previously devised approach to engineering security methodologies – specifically designed for general distributed systems, which is also capable of taking into account the specifics of peer-to-peer systems as needed. Our methodology takes the principle of encapsulation several steps further, by employing patterns not only for the incorporation of security features (via security solution frames), but also for the modeling of threats, and even as part of its process. We illustrate and evaluate the presented methodology in detail via a realistic example – the development of a distributed system for file sharing and collaborative editing. In both the presentation of the methodology and example our focus is on the early life-cycle phases (analysis and design). },
  doi      = {https://doi.org/10.1016/j.csi.2015.02.011},
  keywords = {Distributed systems security,Secure software engineering,Security methodologies,Security patterns,Security solution frames},
  url      = {http://www.sciencedirect.com/science/article/pii/S0920548915000276},
}

@Article{Fabry2016528,
  author   = {Fabry, Johan and Roover, Coen De and Noguera, Carlos and Zschaler, Steffen and Rashid, Awais and Jonckers, Viviane},
  title    = {{AspectJ code analysis and verification with {\{}GASR{\}}}},
  journal  = {Journal of Systems and Software},
  year     = {2016},
  volume   = {117},
  pages    = {528--544},
  issn     = {0164-1212},
  abstract = {Abstract Aspect-oriented programming languages extend existing languages with new features for supporting modularization of crosscutting concerns. These features however make existing source code analysis tools unable to reason over this code. Consequently, all code analysis efforts of aspect-oriented code that we are aware of have either built limited analysis tools or were performed manually. Given the significant complexity of building them or manual analysis, a lot of duplication of effort could have been avoided by using a general-purpose tool. To address this, in this paper we present Gasr: a source code analysis tool that reasons over AspectJ source code, which may contain metadata in the form of annotations. {\{}GASR{\}} provides multiple kinds of analyses that are general enough such that they are reusable, tailorable and can reason over annotations. We demonstrate the use of {\{}GASR{\}} in two ways: we first automate the recognition of previously identified aspectual source code assumptions. Second, we turn implicit assumptions into explicit assumptions through annotations and automate their verification. In both uses {\{}GASR{\}} performs detection and verification of aspect assumptions on two well-known case studies that were manually investigated in earlier work. {\{}GASR{\}} finds already known aspect assumptions and adds instances that had been previously overlooked. },
  doi      = {https://doi.org/10.1016/j.jss.2016.04.014},
  keywords = {Aspect oriented programming,Logic program querying,Source code analysis},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121216300279},
}

@Article{Groher2009111,
  author   = {Groher, I and Voelter, M},
  title    = {{Aspect-oriented model-driven software product line engineering}},
  journal  = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year     = {2009},
  volume   = {5560 LNCS},
  pages    = {111--152},
  abstract = {Software product line engineering aims to reduce development time, effort, cost, and complexity by taking advantage of the commonality within a portfolio of similar products. The effectiveness of a software product line approach directly depends on how well feature variability within the portfolio is implemented and managed throughout the development lifecycle, from early analysis through maintenance and evolution. This article presents an approach that facilitates variability implementation, management, and tracing by integrating model-driven and aspect-oriented software development. Features are separated in models and composed of aspect-oriented composition techniques on model level. Model transformations support the transition from problem to solution space models. Aspect-oriented techniques enable the explicit expression and modularization of variability on model, template, and code level. The presented concepts are illustrated with a case study of a home automation system. {\textcopyright} 2009 Springer-Verlag Berlin Heidelberg.},
  annote   = {cited By 15},
  doi      = {10.1007/978-3-642-03764-1_4},
  keywords = {Aspect oriented software development; Aspect-orien,Computer software maintenance,Computer software; Computer systems programming;},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-71549123364{\&}doi=10.1007{\%}2F978-3-642-03764-1{\_}4{\&}partnerID=40{\&}md5=c78bc11a8131a3e7e439b007880e9f1f},
}

@Article{Apel2008,
  author  = {Apel, S. and Leich, T. and Saake, G.},
  title   = {{Aspectual Feature Modules}},
  journal = {IEEE Transactions on Software Engineering},
  year    = {2008},
  volume  = {34},
  number  = {2},
  pages   = {162--180},
  month   = {mar},
  issn    = {0098-5589},
  doi     = {10.1109/TSE.2007.70770},
  file    = {:Users/mac/Library/Application Support/Mendeley Desktop/Downloaded/Apel, Leich, Saake - 2008 - Aspectual Feature Modules.pdf:pdf},
  url     = {http://ieeexplore.ieee.org/document/4407729/},
}

@Article{Koziolek2016411,
  author        = {Koziolek, H and Goldschmidt, T and de Gooijer, T and Domis, D and Sehestedt, S and Gamer, T and Aleksy, M},
  title         = {{Assessing software product line potential: an exploratory industrial case study}},
  journal       = {Empirical Software Engineering},
  year          = {2016},
  volume        = {21},
  number        = {2},
  pages         = {411--448},
  abstract      = {Corporate organizations sometimes offer similar software products in certain domains due to former company mergers or due to the complexity of the organization. The functional overlap of such products is an opportunity for future systematic reuse to reduce software development and maintenance costs. Therefore, we have tailored existing domain analysis methods to our organization to identify commonalities and variabilities among such products and to assess the potential for software product line (SPL) approaches. As an exploratory case study, we report on our experiences and lessons learned from conducting the domain analysis in four application cases with large-scale software products. We learned that the outcome of a domain analysis was often a smaller integration scenario instead of an SPL and that business case calculations were less relevant for the stakeholders and managers from the business units during this phase. We also learned that architecture reconstruction using a simple block diagram notation aids domain analysis and that large parts of our approach were reusable across application cases. {\textcopyright} 2015, Springer Science+Business Media New York.},
  annote        = {cited By 0},
  doi           = {10.1007/s10664-014-9358-0},
  file          = {:Users/mac/ownCloud/Anita/SLR/REVISION DE JOURNALS/JOSE/SELECT/checked/estudios casos analizados/11 Assessing software product line potential- an exploratory industrial case study.pdf:pdf},
  keywords      = {Application programs,Architecture reconstruction,Business case,Computer,Computer software,Domai,Software design,case study},
  mendeley-tags = {case study},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84922424792{\&}doi=10.1007{\%}2Fs10664-014-9358-0{\&}partnerID=40{\&}md5=d52527a44c67d5bcbcaab488aa82cc99},
}

@Article{Korhonen2004383,
  author   = {Korhonen, Mika and Mikkonen, Tommi},
  title    = {{Assessing systems adaptability to a product family}},
  journal  = {Journal of Systems Architecture},
  year     = {2004},
  volume   = {50},
  number   = {7},
  pages    = {383--392},
  issn     = {1383-7621},
  abstract = {In many cases, product families are established on top of a successful pilot product. While this approach provides an option to measure many concrete attributes like performance and memory footprint, adequateness and adaptability of the architecture of the pilot cannot be fully verified. Yet, these properties are crucial business enablers for the whole product family. In this paper, we discuss an architectural assessment of one such seminal system, intended for monitoring electronic subsystems of a mobile machine, which is to be extended to support a wide range of different types of products. This paper shows how well the assessment reveals possible problems and existing flexibilities in assessed system, and this way helps different stakeholders in their further decisions. },
  annote   = {Adaptable System/Software Architectures},
  doi      = {https://doi.org/10.1016/j.sysarc.2003.08.011},
  keywords = {Adaptability,Assessment,Product line architecture,Software product lines},
  url      = {http://www.sciencedirect.com/science/article/pii/S1383762103001619},
}

@Article{Bagheri2011579,
  author   = {Bagheri, E and Gasevic, D},
  title    = {{Assessing the maintainability of software product line feature models using structural metrics}},
  journal  = {Software Quality Journal},
  year     = {2011},
  volume   = {19},
  number   = {3},
  pages    = {579--612},
  abstract = {A software product line is a unified representation of a set of conceptually similar software systems that share many common features and satisfy the requirements of a particular domain. Within the context of software product lines, feature models are tree-like structures that are widely used for modeling and representing the inherent commonality and variability of software product lines. Given the fact that many different software systems can be spawned from a single software product line, it can be anticipated that a low-quality design can ripple through to many spawned software systems. Therefore, the need for early indicators of external quality attributes is recognized in order to avoid the implications of defective and low-quality design during the late stages of production. In this paper, we propose a set of structural metrics for software product line feature models and theoretically validate them using valid measurement-theoretic principles. Further, we investigate through controlled experimentation whether these structural metrics can be good predictors (early indicators) of the three main subcharacteristics of maintainability: analyzability, changeability, and understandability. More specifically, a four-step analysis is conducted: (1) investigating whether feature model structural metrics are correlated with feature model maintainability through the employment of classical statistical correlation techniques; (2) understanding how well each of the structural metrics can serve as discriminatory references for maintainability; (3) identifying the sufficient set of structural metrics for evaluating each of the subcharacteristics of maintainability; and (4) evaluating how well different prediction models based on the proposed structural metrics can perform in indicating the maintainability of a feature model. Results obtained from the controlled experiment support the idea that useful prediction models can be built for the purpose of evaluating feature model maintainability using early structural metrics. Some of the structural metrics show significant correlation with the subjective perception of the subjects about the maintainability of the feature models. {\textcopyright} 2010 Springer Science+Business Media, LLC.},
  annote   = {cited By 73},
  doi      = {10.1007/s11219-010-9127-2},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-79958248566{\&}doi=10.1007{\%}2Fs11219-010-9127-2{\&}partnerID=40{\&}md5=e0e2572c8e9a517f7df77ecfd32da223},
}

@Article{Nunes20092254,
  author   = {Nunes, C and Kulesza, U and Sant'Anna, C and Nunes, I and Garcia, A and Lucena, C},
  title    = {{Assessment of the design modularity and stability of multi-agent system product lines}},
  journal  = {Journal of Universal Computer Science},
  year     = {2009},
  volume   = {15},
  number   = {11},
  pages    = {2254--2283},
  abstract = {A multi-agent system product line (MAS-PL) defines an architecture whose design and implementation is accomplished using software agents to address its common and variable features. MAS-PL promotes the large-scale reuse of common and variable agency features across multiple MAS applications. The development of MAS-PLs can be achieved through MAS-specific platforms and implementation techniques, such as conditional compilation and aspect-oriented programming (AOP). However, there is not much evidence on how these techniques provide better modularity, allowing the conception of stable MAS-PL designs. This paper presents a quantitative study on the design modularity and stability of an evolving MAS-PL. The MAS-PL was built following the reactive product line adoption approach. The product line was developed and evolved based on several versions of a conference management web-based system, named Expert Committee (EC). Our evaluation is made through a series of change scenarios related to new agency features, which are agent characteristics that enhance the system with autonomous behavior. The quantitative study consists of a systematic comparison between two different versions of the EC MAS-PL based on a MAS-specific platform, called JADE. One version was implemented with object-oriented and conditional compilation techniques. The other one relied on AOP. Our analysis was driven by well-known modularity and change impact metrics.},
  annote   = {cited By 4},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-70350176494{\&}partnerID=40{\&}md5=4c31d9416f2203717f92a4ce9001eca2},
}

@Article{ZHOU2017306,
  author   = {Zhou, Feng and Jiao, Jianxin Roger and Yang, Xi Jessie and Lei, Baiying},
  title    = {{Augmenting feature model through customer preference mining by hybrid sentiment analysis}},
  journal  = {Expert Systems with Applications},
  year     = {2017},
  volume   = {89},
  pages    = {306--317},
  issn     = {0957-4174},
  abstract = {A feature model is an essential tool to identify variability and commonality within a product line of an enterprise, assisting stakeholders to configure product lines and to discover opportunities for reuse. However, the number of product variants needed to satisfy individual customer needs is still an open question, as feature models do not incorporate any direct customer preference information. In this paper, we propose to incorporate customer preference information into feature models using sentiment analysis of user-generated online product reviews. The proposed sentiment analysis method is a hybrid combination of affective lexicons and a rough-set technique. It is able to predict sentence sentiments for individual product features with acceptable accuracy, and thus augment a feature model by integrating positive and negative opinions of the customers. Such opinionated customer preference information is regarded as one attribute of the features, which helps to decide the number of variants needed within a product line. Finally, we demonstrate the feasibility and potential of the proposed method via an application case of Kindle Fire HD tablets.},
  doi      = {https://doi.org/10.1016/j.eswa.2017.07.021},
  keywords = {Customer preference mining,Feature model,Product line planning,Sentiment analysis},
  url      = {http://www.sciencedirect.com/science/article/pii/S0957417417304980},
}

@Article{Heradio20161066,
  author   = {Heradio, Ruben and Perez-Morago, Hector and Alf{\'{e}}rez, Mauricio and Fernandez-Amoros, David and Alf{\'{e}}rez, Germ{\'{a}}n H},
  title    = {{Augmenting measure sensitivity to detect essential, dispensable and highly incompatible features in mass customization}},
  journal  = {European Journal of Operational Research},
  year     = {2016},
  volume   = {248},
  number   = {3},
  pages    = {1066--1077},
  issn     = {0377-2217},
  abstract = {Abstract Mass customization is the new frontier in business competition for both manufacturing and service industries. To improve customer satisfaction, reduce lead-times and shorten costs, families of similar products are built jointly by combining reusable parts that implement the features demanded by the customers. To guarantee the validity of the products derived from mass customization processes, feature dependencies and incompatibilities are usually specified with a variability model. As market demand grows and evolves, variability models become increasingly complex. In such entangled models it is hard to identify which features are essential, dispensable, highly required by other features, or highly incompatible with the remaining features. This paper exposes the limitations of existing approaches to gather such knowledge and provides efficient algorithms to retrieve that information from variability models. },
  doi      = {https://doi.org/10.1016/j.ejor.2015.08.005},
  keywords = {Binary decision diagram,Mass customization,Product platform,Variability modeling},
  url      = {http://www.sciencedirect.com/science/article/pii/S0377221715007225},
}

@Article{Magdalenić20132845,
  author   = {Magdaleni{\'{c}}, Ivan and Rado{\v{s}}evi{\'{c}}, Danijel and Orehova{\v{c}}ki, Tihomir},
  title    = {{Autogenerator: Generation and execution of programming code on demand}},
  journal  = {Expert Systems with Applications},
  year     = {2013},
  volume   = {40},
  number   = {8},
  pages    = {2845--2857},
  issn     = {0957-4174},
  abstract = {While generating program files that can be executed afterwards is widely established in Automatic programming, the generation of programming code and its execution on demand without creating program files is still a challenge. In the approach presented in this paper a generator entitled Autogenerator uses the ability of scripting languages to evaluate programming code from a variable. The main benefits of this approach lie in facilitating the application change during its execution on the one hand and in dependencies update on the other. Autogenerator can be useful in the development of a common Generative programming application for previewing the application before the generation of the final release. With the aim of examining specific facets of the autogeneration process, we also conducted performance tests. Finally, the presented model of Autogenerator is verified through the development of an application for the creation and handling of Universal Business Language documents. },
  doi      = {https://doi.org/10.1016/j.eswa.2012.12.003},
  keywords = {Autogenerator,Dynamic frames,Generation on demand},
  url      = {http://www.sciencedirect.com/science/article/pii/S0957417412012444},
}

@Article{Benavides2010,
  author   = {Benavides, David and Segura, Sergio and Ruiz-Cort??s, Antonio},
  title    = {{Automated analysis of feature models 20 years later: A literature review}},
  journal  = {Information Systems},
  year     = {2010},
  volume   = {35},
  number   = {6},
  pages    = {615--636},
  issn     = {03064379},
  abstract = {Software product line engineering is about producing a set of related products that share more commonalities than variabilities. Feature models are widely used for variability and commonality management in software product lines. Feature models are information models where a set of products are represented as a set of features in a single model. The automated analysis of feature models deals with the computer-aided extraction of information from feature models. The literature on this topic has contributed with a set of operations, techniques, tools and empirical results which have not been surveyed until now. This paper provides a comprehensive literature review on the automated analysis of feature models 20 years after of their invention. This paper contributes by bringing together previously disparate streams of work to help shed light on this thriving area. We also present a conceptual framework to understand the different proposals as well as categorise future contributions. We finally discuss the different studies and propose some challenges to be faced in the future. ?? 2010 Elsevier B.V. All rights reserved.},
  doi      = {10.1016/j.is.2010.01.001},
  file     = {:Users/mac/Library/Application Support/Mendeley Desktop/Downloaded/Benavides, Segura, Ruiz-Corts - 2010 - Automated analysis of feature models 20 years later A literature review.pdf:pdf},
  isbn     = {0306-4379},
  keywords = {Automated analyses,Feature models,Literature review,Software product lines},
}

@Article{GarcíaGalán2016200,
  author   = {Garc{\'{i}}a-Gal{\'{a}}n, Jes{\'{u}}s and Trinidad, Pablo and Rana, Omer F and Ruiz-Cort{\'{e}}s, Antonio},
  title    = {{Automated configuration support for infrastructure migration to the cloud}},
  journal  = {Future Generation Computer Systems},
  year     = {2016},
  volume   = {55},
  pages    = {200--212},
  issn     = {0167-739X},
  abstract = {Abstract With an increasing number of cloud computing offerings in the market, migrating an existing computational infrastructure to the cloud requires comparison of different offers in order to find the most suitable configuration. Cloud providers offer many configuration options, such as location, purchasing mode, redundancy, and extra storage. Often, the information about such options is not well organised. This leads to large and unstructured configuration spaces, and turns the comparison into a tedious, error-prone search problem for the customers. In this work we focus on supporting customer decision making for selecting the most suitable cloud configuration—in terms of infrastructural requirements and cost. We achieve this by means of variability modelling and analysis techniques. Firstly, we structure the configuration space of an IaaS using feature models, usually employed for the modelling of variability-intensive systems, and present the case study of the Amazon EC2. Secondly, we assist the configuration search process. Feature models enable the use of different analysis operations that, among others, automate the search of optimal configurations. Results of our analysis show how our approach, with a negligible analysis time, outperforms commercial approaches in terms of expressiveness and accuracy. },
  doi      = {https://doi.org/10.1016/j.future.2015.03.006},
  keywords = {Automated analysis,Cloud migration,EC2,Feature model,IaaS},
  url      = {http://www.sciencedirect.com/science/article/pii/S0167739X15000618},
}

@Article{Basso2016612,
  author   = {Basso, F{\'{a}}bio Paulo and Pillat, Raquel Mainardi and Oliveira, Toacy Cavalcante and Roos-Frantz, Fabricia and Frantz, Rafael Z},
  title    = {{Automated design of multi-layered web information systems}},
  journal  = {Journal of Systems and Software},
  year     = {2016},
  volume   = {117},
  pages    = {612--637},
  issn     = {0164-1212},
  abstract = {Abstract In the development of web information systems, design tasks are commonly used in approaches for Model-Driven Web Engineering (MDWE) to represent models. To generate fully implemented prototypes, these models require a rich representation of the semantics for actions (e.g., database persistence operations). In the development of some use case scenarios for the multi-layered development of web information systems, these design tasks may consume weeks of work even for experienced designers. The literature pointed out that the impossibility for executing a software project with short iterations hampers the adoption of some approaches for design in some contexts, such as start-up companies. A possible solution to introduce design tasks in short iterations is the use of automated design techniques, which assist the production of models by means of transformation tasks and refinements. This paper details our methodology for MDWE, which is supported by automated design techniques strictly associated with use case patterns of type CRUD. The novelty relies on iterations that are possible for execution with short time-scales. This is a benefit from automated design techniques not observed in {\{}MDWE{\}} approaches based on manual design tasks. We also report on previous experiences and address open questions relevant for the theory and practice of MDWE. },
  doi      = {https://doi.org/10.1016/j.jss.2016.04.060},
  keywords = {Automated design,Domain-specific language,Experience report,Mockup,Model-driven web engineering,Prototyping,Rapid application prototype},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121216300358},
}

@Article{White20101094,
  author   = {White, J and Benavides, D and Schmidt, D C and Trinidad, P and Dougherty, B and Ruiz-Cortes, A},
  title    = {{Automated diagnosis of feature model configurations}},
  journal  = {Journal of Systems and Software},
  year     = {2010},
  volume   = {83},
  number   = {7},
  pages    = {1094--1107},
  issn     = {0164-1212},
  abstract = {Software product-lines (SPLs) are software platforms that can be readily reconfigured for different project requirements. A key part of an {\{}SPL{\}} is a model that captures the rules for reconfiguring the software. {\{}SPLs{\}} commonly use feature models to capture {\{}SPL{\}} configuration rules. Each {\{}SPL{\}} configuration is represented as a selection of features from the feature model. Invalid {\{}SPL{\}} configurations can be created due to feature conflicts introduced via staged or parallel configuration or changes to the constraints in a feature model. When invalid configurations are created, a method is needed to automate the diagnosis of the errors and repair the feature selections. This paper provides two contributions to research on automated configuration of SPLs. First, it shows how configurations and feature models can be transformed into constraint satisfaction problems to automatically diagnose errors and repair invalid feature selections. Second, it presents empirical results from diagnosing configuration errors in feature models ranging in size from 100 to 5,000 features. The results of our experiments show that our CSP-based diagnostic technique can scale up to models with thousands of features. },
  annote   = {{\{}SPLC{\}} 2008},
  doi      = {https://doi.org/10.1016/j.jss.2010.02.017},
  keywords = {Configuration,Constraint satisfaction,Diagnosis,Optimization,Software product-lines},
  url      = {http://www.sciencedirect.com/science/article/pii/S016412121000049X},
}

@Article{Trinidad2008883,
  author   = {Trinidad, P and Benavides, D and Dur{\'{a}}n, A and Ruiz-Cort{\'{e}}s, A and Toro, M},
  title    = {{Automated error analysis for the agilization of feature modeling}},
  journal  = {Journal of Systems and Software},
  year     = {2008},
  volume   = {81},
  number   = {6},
  pages    = {883--896},
  abstract = {Software Product Lines (SPL) and agile methods share the common goal of rapidly developing high-quality software. Although they follow different approaches to achieve it, some synergies can be found between them by (i) applying agile techniques to SPL activities so SPL development becomes more agile; and (ii) tailoring agile methodologies to support the development of SPL. Both options require an intensive use of feature models, which are usually strongly affected by changes on requirements. Changing large-scale feature models as a consequence of changes on requirements is a well-known error-prone activity. Since one of the objectives of agile methods is a rapid response to changes in requirements, it is essential an automated error analysis support in order to make SPL development more agile and to produce error-free feature models. As a contribution to find the intended synergies, this article sets the basis to provide an automated support to feature model error analysis by means of a framework which is organized in three levels: a feature model level, where the problem of error treatment is described; a diagnosis level, where an abstract solution that relies on Reiter's theory of diagnosis is proposed; and an implementation level, where the abstract solution is implemented by using Constraint Satisfaction Problems (CSP). To show an application of our proposal, a real case study is presented where the Feature-Driven Development (FDD) methodology is adapted to develop an SPL. Current proposals on error analysis are also studied and a comparison among them and our proposal is provided. Lastly, the support of new kinds of errors and different implementation levels for the proposed framework are proposed as the focus of our future work. {\textcopyright} 2007 Elsevier Inc. All rights reserved.},
  annote   = {cited By 62},
  doi      = {10.1016/j.jss.2007.10.030},
  keywords = {Agile manufacturing systems,Agile methods; Constraint programming; Feature mo,Automation; Constraint theory; Error analysis; Lar},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-42049110531{\&}doi=10.1016{\%}2Fj.jss.2007.10.030{\&}partnerID=40{\&}md5=37e7cc86948271c403360a1535f92809},
}

@Article{Nasr201782,
  author   = {Nasr, S B and B{\'{e}}can, G and Acher, M and {Ferreira Filho}, J B and Sannier, N and Baudry, B and Davril, J.-M.},
  title    = {{Automated extraction of product comparison matrices from informal product descriptions}},
  journal  = {Journal of Systems and Software},
  year     = {2017},
  volume   = {124},
  pages    = {82--103},
  abstract = {Domain analysts, product managers, or customers aim to capture the important features and differences among a set of related products. A case-by-case reviewing of each product description is a laborious and time-consuming task that fails to deliver a condense view of a family of product. In this article, we investigate the use of automated techniques for synthesizing a product comparison matrix (PCM) from a set of product descriptions written in natural language. We describe a tool-supported process, based on term recognition, information extraction, clustering, and similarities, capable of identifying and organizing features and values in a PCM – despite the informality and absence of structure in the textual descriptions of products. We evaluate our proposal against numerous categories of products mined from BestBuy. Our empirical results show that the synthesized PCMs exhibit numerous quantitative, comparable information that can potentially complement or even refine technical descriptions of products. The user study shows that our automatic approach is capable of extracting a significant portion of correct features and correct values. This approach has been implemented in MatrixMiner a web environment with an interactive support for automatically synthesizing PCMs from informal product descriptions. MatrixMiner also maintains traceability with the original descriptions and the technical specifications for further refinement or maintenance by users. {\textcopyright} 2016 Elsevier Inc.},
  annote   = {cited By 0},
  doi      = {10.1016/j.jss.2016.11.018},
  keywords = {Automated extraction; Automated techniques; Autom,Hardware; Software engineering,Reverse engineering},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84996593342{\&}doi=10.1016{\%}2Fj.jss.2016.11.018{\&}partnerID=40{\&}md5=6c44aaadfbc3fb4cbc5292ddb3503d98},
}

@Article{Segura20143975,
  author   = {Segura, Sergio and Parejo, Jos{\'{e}} A and Hierons, Robert M and Benavides, David and Ruiz-Cort{\'{e}}s, Antonio},
  title    = {{Automated generation of computationally hard feature models using evolutionary algorithms}},
  journal  = {Expert Systems with Applications},
  year     = {2014},
  volume   = {41},
  number   = {8},
  pages    = {3975--3992},
  issn     = {0957-4174},
  abstract = {Abstract A feature model is a compact representation of the products of a software product line. The automated extraction of information from feature models is a thriving topic involving numerous analysis operations, techniques and tools. Performance evaluations in this domain mainly rely on the use of random feature models. However, these only provide a rough idea of the behaviour of the tools with average problems and are not sufficient to reveal their real strengths and weaknesses. In this article, we propose to model the problem of finding computationally hard feature models as an optimization problem and we solve it using a novel evolutionary algorithm for optimized feature models (ETHOM). Given a tool and an analysis operation, {\{}ETHOM{\}} generates input models of a predefined size maximizing aspects such as the execution time or the memory consumption of the tool when performing the operation over the model. This allows users and developers to know the performance of tools in pessimistic cases providing a better idea of their real power and revealing performance bugs. Experiments using {\{}ETHOM{\}} on a number of analyses and tools have successfully identified models producing much longer executions times and higher memory consumption than those obtained with random models of identical or even larger size. },
  doi      = {https://doi.org/10.1016/j.eswa.2013.12.028},
  keywords = {Automated analysis,Evolutionary algorithms,Feature models,Performance testing,Search-based testing,Software product lines},
  url      = {http://www.sciencedirect.com/science/article/pii/S0957417413010038},
}

@Article{Veerman2006287,
  author   = {Veerman, Niels},
  title    = {{Automated mass maintenance of a software portfolio}},
  journal  = {Science of Computer Programming},
  year     = {2006},
  volume   = {62},
  number   = {3},
  pages    = {287--317},
  issn     = {0167-6423},
  abstract = {This is an experience report on automated mass maintenance of a large Cobol software portfolio. A company in the financial services and insurance industry upgraded their database system to a new version, affecting their entire software portfolio. The database system was accessed by the portfolio of 45 systems, totalling nearly 3000 programs and covering over 4 million lines of Cobol code. We upgraded the programs to the new database version using several automatic tools, and we performed an automated analysis supporting further manual modifications by the system experts. The automatic tools were built using a combination of lexical and syntactic technology, and they were deployed in a mass update factory to allow large-scale application to the software portfolio. The updated portfolio has been accepted and taken into production by the company, serving over 600 employees with the new database version. In this paper, we discuss the automated upgrade from problem statement to project costs. },
  annote   = {Special issue on Source code analysis and manipulation (SCAM 2005)},
  doi      = {https://doi.org/10.1016/j.scico.2006.04.006},
  keywords = {Automated maintenance,Automatic transformations,Cobol,Mass maintenance,Mass modification,Mass update,Reengineering,Software maintenance,Software portfolio,Tool-supported maintenance},
  url      = {http://www.sciencedirect.com/science/article/pii/S0167642306000827},
}

@Article{Segura2011245,
  author   = {Segura, Sergio and Hierons, Robert M and Benavides, David and Ruiz-Cort{\'{e}}s, Antonio},
  title    = {{Automated metamorphic testing on the analyses of feature models}},
  journal  = {Information and Software Technology},
  year     = {2011},
  volume   = {53},
  number   = {3},
  pages    = {245--258},
  issn     = {0950-5849},
  abstract = {Context A feature model (FM) represents the valid combinations of features in a domain. The automated extraction of information from {\{}FMs{\}} is a complex task that involves numerous analysis operations, techniques and tools. Current testing methods in this context are manual and rely on the ability of the tester to decide whether the output of an analysis is correct. However, this is acknowledged to be time-consuming, error-prone and in most cases infeasible due to the combinatorial complexity of the analyses, this is known as the oracle problem. Objective In this paper, we propose using metamorphic testing to automate the generation of test data for feature model analysis tools overcoming the oracle problem. An automated test data generator is presented and evaluated to show the feasibility of our approach. Method We present a set of relations (so-called metamorphic relations) between input {\{}FMs{\}} and the set of products they represent. Based on these relations and given a {\{}FM{\}} and its known set of products, a set of neighbouring {\{}FMs{\}} together with their corresponding set of products are automatically generated and used for testing multiple analyses. Complex {\{}FMs{\}} representing millions of products can be efficiently created by applying this process iteratively. Results Our evaluation results using mutation testing and real faults reveal that most faults can be automatically detected within a few seconds. Two defects were found in FaMa and another two in SPLOT, two real tools for the automated analysis of feature models. Also, we show how our generator outperforms a related manual suite for the automated analysis of feature models and how this suite can be used to guide the automated generation of test cases obtaining important gains in efficiency. Conclusion Our results show that the application of metamorphic testing in the domain of automated analysis of feature models is efficient and effective in detecting most faults in a few seconds without the need for a human oracle. },
  doi      = {https://doi.org/10.1016/j.infsof.2010.11.002},
  keywords = {Automated analysis,Feature models,Metamorphic testing,Mutation testing,Product lines,Test data generation},
  url      = {http://www.sciencedirect.com/science/article/pii/S0950584910001904},
}

@Article{MeimandiParizi2015463,
  author   = {Parizi, Reza Meimandi and Ghani, Abdul Azim Abdul and Lee, Sai Peck},
  title    = {{Automated test generation technique for aspectual features in AspectJ}},
  journal  = {Information and Software Technology},
  year     = {2015},
  volume   = {57},
  pages    = {463--493},
  issn     = {0950-5849},
  abstract = {AbstractContext Aspect-oriented programming (AOP) has been promoted as a means for handling the modularization of software systems by raising the abstraction level and reducing the scattering and tangling of crosscutting concerns. Studies from literature have shown the usefulness and application of {\{}AOP{\}} across various fields of research and domains. Despite this, research shows that {\{}AOP{\}} is currently used in a cautious way due to its natural impact on testability and maintainability. Objective To realize the benefits of {\{}AOP{\}} and to increase its adoption, aspects developed using {\{}AOP{\}} should be subjected to automated testing. Automated testing, as one of the most pressing needs of the software industry to reduce both effort and costs in assuring correctness, is a delicate issue in testing aspect-oriented programs that still requires advancement and has a way to go before maturity. Method Previous attempts and studies in automated test generation process for aspect-oriented programs have been very limited. This paper proposes a rigorous automated test generation technique, called RAMBUTANS, with its tool support based on guided random testing for the AspectJ programs. Results The paper reports the results of a thorough empirical study of 9 AspectJ benchmark programs, including non-trivial and larger software, by means of mutation analysis to compare {\{}RAMBUTANS{\}} and the four existing automated {\{}AOP{\}} testing approaches for testing aspects in terms of fault detection effectiveness and test effort efficiency. The results of the experiment and statistical tests supplemented by effect size measures presented evidence of the effectiveness and efficiency of the proposed technique at 99{\%} confidence level (i.e. p {\textless} 0.01). Conclusion The study showed that the resulting randomized tests were reasonably good for {\{}AOP{\}} testing, thus the proposed technique could be worth using as an effective and efficient AOP-specific automated test generation technique. },
  doi      = {https://doi.org/10.1016/j.infsof.2014.05.020},
  keywords = {AspectJ,Automated test generation,Empirical study,Software testing,Testing tool},
  url      = {http://www.sciencedirect.com/science/article/pii/S0950584914001372},
}

@Article{Alonso2012170,
  author   = {Alonso, D and Pastor, J A and S{\'{a}}nchez, P and {\'{A}}lvarez, B and Vicente-Chicote, C},
  title    = {{Automatic code generation for real-time systems: A development approach based on components, models, and frameworks [Generaci{\'{o}}n autom{\'{a}}tica de software para sistemas de tiempo real: Un enfoque basado en componentes, modelos y frameworks]}},
  journal  = {RIAI - Revista Iberoamericana de Automatica e Informatica Industrial},
  year     = {2012},
  volume   = {9},
  number   = {2},
  pages    = {170--181},
  abstract = {Real-Time Systems have some characteristics that make them particularly sensitive to architectural decisions. The use of Frameworks and Components has proven effective in improving productivity and software quality, especially when combined with Software Product Line approaches. However, the results in terms of software reuse and standardization make the lack of portability of both the design and componentbased implementations clear. This article, based on the Model- Driven Software Development paradigm, presents an approach that separates the component-based description of real-time applications from their possible implementations on different platforms. This separation is supported by the automatic integration of the code obtained from the input models into object-oriented frameworks. The article also details the architectural decisions taken in the implementation of one of such frameworks, which is used as a case study to illustrate the proposed approach. Finally, a comparison with other alternative approaches is made in terms of development cost. {\textcopyright} 2012 CEA. Publicado por Elsevier Espa{\~{n}}a, S.L.},
  annote   = {cited By 4},
  doi      = {10.1016/j.riai.2012.02.010},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863736214{\&}doi=10.1016{\%}2Fj.riai.2012.02.010{\&}partnerID=40{\&}md5=fd2181decbe551cbf2c5afe5f1455f04},
}

@Article{Ryssel201283,
  author   = {Ryssel, Uwe and Ploennigs, Joern and Kabitzsch, Klaus},
  title    = {{Automatic library migration for the generation of hardware-in-the-loop models}},
  journal  = {Science of Computer Programming},
  year     = {2012},
  volume   = {77},
  number   = {2},
  pages    = {83--95},
  issn     = {0167-6423},
  abstract = {Embedded systems are widely used in several applications nowadays. As they integrate hard- and software elements, their functionality and reliability are often tested by hardware-in-the-loop methods, in which the system under test runs in a simulated environment. Due to the rising complexity of the embedded functions, performance limitations and practicability reasons, the simulations are often specialized to test specific aspects of the embedded system and develop a high diversity by themselves. This diversity is difficult to manage for a user and results in erroneously selected test components and compatibility problems in the test configuration. This paper presents a generative programming approach that handles the diversity of test libraries. Compatibility issues are explicitly evaluated by a new interface concept. Furthermore, a novel model analyzer facilitates the efficient application in practice by migrating existing libraries. The approach is evaluated for an example from the automotive domain using MATLAB/Simulink. },
  annote   = {Special Issue on Automatic Program Generation for Embedded Systems},
  doi      = {https://doi.org/10.1016/j.scico.2010.06.005},
  keywords = {Function-block-based design,Generative programming,Library migration,Structural comparison},
  url      = {http://www.sciencedirect.com/science/article/pii/S0167642310001115},
}

@Article{Hess:2014:ALI:2775053.2658772,
  author    = {Hess, Benjamin and Gross, Thomas R and P{\"{u}}schel, Markus},
  title     = {{Automatic Locality-friendly Interface Extension of Numerical Functions}},
  journal   = {SIGPLAN Not.},
  year      = {2014},
  volume    = {50},
  number    = {3},
  pages     = {83--92},
  issn      = {0362-1340},
  address   = {New York, NY, USA},
  doi       = {10.1145/2775053.2658772},
  keywords  = {Libraries,components,interface extension,performance,preprocessors,programming language features interaction,software product lines},
  publisher = {ACM},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/2775053.2658772},
}

@Article{Walker20132467,
  author   = {Walker, Martin and Reiser, Mark-Oliver and Tucci-Piergiovanni, Sara and Papadopoulos, Yiannis and L{\"{o}}nn, Henrik and Mraidha, Chokri and Parker, David and Chen, DeJiu and Servat, David},
  title    = {{Automatic optimisation of system architectures using EAST-ADL}},
  journal  = {Journal of Systems and Software},
  year     = {2013},
  volume   = {86},
  number   = {10},
  pages    = {2467--2487},
  issn     = {0164-1212},
  abstract = {Abstract There are many challenges which face designers of complex system architectures, particularly safety–critical or real-time systems. The introduction of Architecture Description Languages (ADLs) has helped to meet these challenges by consolidating information about a system and providing a platform for modelling and analysis capabilities. However, managing this wealth of information can still be problematic, and evaluation of potential design decisions is still often performed manually. Automatic architectural optimisation can be used to assist this decision process, enabling designers to rapidly explore many different options and evaluate them according to specific criteria. In this paper, we present a multi-objective optimisation approach based on EAST-ADL, an {\{}ADL{\}} in the automotive domain, with the goal of combining the advantages of {\{}ADLs{\}} and architectural optimisation. The approach is designed to be extensible and leverages the capabilities of EAST-ADL to provide support for evaluation according to different factors, including dependability, timing/performance, and cost. The technique is applied to an illustrative example system featuring both hardware and software perspectives, demonstrating the potential benefits of this concept to the design of embedded system architectures. },
  doi      = {https://doi.org/10.1016/j.jss.2013.04.001},
  keywords = {Architectural description languages,Dependability Analysis,Multi-objective optimisation,Timing Analysis},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121213000885},
}

@Article{Ajoudanian2015640,
  author   = {Ajoudanian, Shohreh and Hosseinabadi, Seyed-Hassan Mirian},
  title    = {{Automatic promotional specialization, generalization and analysis of extended feature models with cardinalities in Alloy}},
  journal  = {Journal of Logical and Algebraic Methods in Programming},
  year     = {2015},
  volume   = {84},
  number   = {5},
  pages    = {640--667},
  issn     = {2352-2208},
  abstract = {Abstract Software product line engineering is a method of producing a set of related products that share more commonalities than variability in a cost-effective approach. Software product lines provide systematic reuse within a product family. Extended feature models with cardinalities are widely used for managing variability and commonality in the software product line domains. In this paper, we use promotion technique in Alloy to formalize constraint based extended feature models with cardinalities and their specialization and generalization. This technique has a significant influence on applying analysis operations on feature models. To show the benefits of the promotion technique, we calculate the reuse ratio of a feature in a large scale software product line. In the presented method, in addition to feature and group cardinalities, we consider different combinations of cardinalities with each other as well as feature cloning. },
  doi      = {https://doi.org/10.1016/j.jlamp.2014.11.005},
  keywords = {Extended feature model with cardinality,Multiple multi-level promotions in Alloy,Specialization and generalization of SPLS},
  url      = {http://www.sciencedirect.com/science/article/pii/S2352220814000959},
}

@Article{Tanhaei2016138,
  author   = {Tanhaei, Mohammad and Habibi, Jafar and Mirian-Hosseinabadi, Seyed-Hassan},
  title    = {{Automating feature model refactoring: A Model transformation approach}},
  journal  = {Information and Software Technology},
  year     = {2016},
  volume   = {80},
  pages    = {138--157},
  issn     = {0950-5849},
  abstract = {Abstract Context: Feature model is an appropriate and indispensable tool for modeling similarities and differences among products of the Software Product Line (SPL). It not only exposes the validity of the products' configurations in an {\{}SPL{\}} but also changes in the course of time to support new requirements of the SPL. Modifications made on the feature model in the course of time raise a number of issues. Useless enlargements of the feature model, the existence of dead features, and violated constraints in the feature model are some of the key problems that make its maintenance difficult. Objective: The initial approach to dealing with the above-mentioned problems and improving maintainability of the feature model is refactoring. Refactoring modifies software artifacts in a way that their externally visible behavior does not change. Method: We introduce a method for defining refactoring rules and executing them on the feature model. We use the {\{}ATL{\}} model transformation language to define the refactoring rules. Moreover, we provide an Alloy model to check the feature model and the safety of the refactorings that are performed on it. Results: In this research, we propose a safe framework for refactoring a feature model. This framework enables users to perform automatic and semi-automatic refactoring on the feature model. Conclusions: Automated tool support for refactoring is a key issue for adopting approaches such as utilizing feature models and integrating them into the software development process of companies. In this work, we define some of the important refactoring rules on the feature model and provide tools that enable users to add new rules using the {\{}ATL{\}} {\{}M2M{\}} language. Our framework assesses the correctness of the refactorings using the Alloy language. },
  doi      = {https://doi.org/10.1016/j.infsof.2016.08.011},
  keywords = {Feature model refactoring,Model transformation {\&} refactoring},
  url      = {http://www.sciencedirect.com/science/article/pii/S0950584916301422},
}

@Article{SANTOS20101078,
  author   = {Santos, Andr{\'{e}} L and Koskimies, Kai and Lopes, Ant{\'{o}}nia},
  title    = {{Automating the construction of domain-specific modeling languages for object-oriented frameworks}},
  journal  = {Journal of Systems and Software},
  year     = {2010},
  volume   = {83},
  number   = {7},
  pages    = {1078--1093},
  issn     = {0164-1212},
  abstract = {The extension of frameworks with domain-specific modeling languages (DSML) has proved to be an effective way of improving the productivity in software product-line engineering. However, developing and evolving a DSML is typically a difficult and time-consuming task because it requires to develop and maintain a code generator, which transforms application models into framework-based code. In this paper, we propose a new approach for extending object-oriented frameworks that aims to alleviate this problem. The approach is based on developing an additional aspect-oriented layer that encodes a DSML for building framework-based applications, eliminating the need of implementing a code generator. We further show how a language workbench is capable of automating the construction of DSMLs using the proposed layer.},
  annote   = {SPLC 2008},
  doi      = {https://doi.org/10.1016/j.jss.2010.01.047},
  keywords = {Aspect-oriented programming,Domain-specific modeling,Object-oriented frameworks,Software product-lines},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121210000312},
}

@Article{Cirilo2012258,
  author   = {Cirilo, Elder and Nunes, Ingrid and Kulesza, Uir{\'{a}} and Lucena, Carlos},
  title    = {{Automating the product derivation process of multi-agent systems product lines}},
  journal  = {Journal of Systems and Software},
  year     = {2012},
  volume   = {85},
  number   = {2},
  pages    = {258--276},
  issn     = {0164-1212},
  abstract = {Agent-oriented software engineering and software product lines are two promising software engineering techniques. Recent research work has been exploring their integration, namely multi-agent systems product lines (MAS-PLs), to promote reuse and variability management in the context of complex software systems. However, current product derivation approaches do not provide specific mechanisms to deal with MAS-PLs. This is essential because they typically encompass several concerns (e.g., trust, coordination, transaction, state persistence) that are constructed on the basis of heterogeneous technologies (e.g., object-oriented frameworks and platforms). In this paper, we propose the use of multi-level models to support the configuration knowledge specification and automatic product derivation of MAS-PLs. Our approach provides an agent-specific architecture model that uses abstractions and instantiation rules that are relevant to this application domain. In order to evaluate the feasibility and effectiveness of the proposed approach, we have implemented it as an extension of an existing product derivation tool, called GenArch. The approach has also been evaluated through the automatic instantiation of two MAS-PLs, demonstrating its potential and benefits to product derivation and configuration knowledge specification. },
  annote   = {Special issue with selected papers from the 23rd Brazilian Symposium on Software Engineering},
  doi      = {https://doi.org/10.1016/j.jss.2011.04.066},
  keywords = {Application engineering,Model-driven development,Multi-agent systems,Product derivation tool,Software product lines},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121211001075},
}

@Article{Haghighatkhah201725,
  author   = {Haghighatkhah, Alireza and Banijamali, Ahmad and Pakanen, Olli-Pekka and Oivo, Markku and Kuvaja, Pasi},
  title    = {{Automotive software engineering: A systematic mapping study}},
  journal  = {Journal of Systems and Software},
  year     = {2017},
  volume   = {128},
  pages    = {25--55},
  issn     = {0164-1212},
  abstract = {Abstract The automotive industry is going through a fundamental change by moving from a mechanical to a software-intensive industry in which most innovation and competition rely on software engineering competence. Over the last few decades, the importance of software engineering in the automotive industry has increased significantly and has attracted much attention from both scholars and practitioners. A large body-of-knowledge on automotive software engineering has accumulated in several scientific publications, yet there is no systematic analysis of that knowledge. This systematic mapping study aims to classify and analyze the literature related to automotive software engineering in order to provide a structured body-of-knowledge, identify well-established topics and potential research gaps. The review includes 679 articles from multiple research sub-area, published between 1990 and 2015. The primary studies were analyzed and classified with respect to five different dimensions. Furthermore, potential research gaps and recommendations for future research are presented. Three areas, namely system/software architecture and design, qualification testing, and reuse were the most frequently addressed topics in the literature. There were fewer comparative and validation studies, and the literature lacks practitioner-oriented guidelines. Overall, research activity on automotive software engineering seems to have high industrial relevance but is relatively lower in its scientific rigor. },
  doi      = {https://doi.org/10.1016/j.jss.2017.03.005},
  keywords = {Automotive software engineering,Automotive systems,Embedded systems,Literature survey,Software-intensive systems,Systematic mapping study},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121217300560},
}

@Article{SHERIF2003159,
  author   = {Sherif, Karma and Vinze, Ajay},
  title    = {{Barriers to adoption of software reuse: A qualitative study}},
  journal  = {Information {\&} Management},
  year     = {2003},
  volume   = {41},
  number   = {2},
  pages    = {159--175},
  issn     = {0378-7206},
  abstract = {With economic pressures to deliver software applications at a faster rate and at lower cost, software reuse is becoming a significant technology for software development. This paper focuses on deriving a descriptive and explanatory theory concerning the individual and organizational barriers associated with the adoption of reuse. A case-study research method was used. A series of five cases were selected on the basis of theoretical replication. The findings, which indicate that barriers occur at both the individual and organizational level, suggest that those at the individual level are actually a consequence of the interaction of barriers caused at the organizational level.},
  doi      = {https://doi.org/10.1016/S0378-7206(03)00045-4},
  keywords = {Barriers to adoption,Case studies,Component development,Grounded theory methodology,Qualitative research,Software reuse},
  url      = {http://www.sciencedirect.com/science/article/pii/S0378720603000454},
}

@Article{Beohar201642,
  author   = {Beohar, Harsh and Varshosaz, Mahsa and Mousavi, Mohammad Reza},
  title    = {{Basic behavioral models for software product lines: Expressiveness and testing pre-orders}},
  journal  = {Science of Computer Programming},
  year     = {2016},
  volume   = {123},
  pages    = {42--60},
  issn     = {0167-6423},
  abstract = {Abstract In order to provide a rigorous foundation for Software Product Lines (SPLs), several fundamental approaches have been proposed to their formal behavioral modeling. In this paper, we provide a structured overview of those formalisms based on labeled transition systems and compare their expressiveness in terms of the set of products they can specify. Moreover, we define the notion of tests for each of these formalisms and show that our notions of testing precisely capture product derivation, i.e., all valid products will pass the set of test cases of the product line and each invalid product fails at least one test case of the product line. },
  annote   = {{\{}SELECTED{\}} {\{}AND{\}} {\{}EXTENDED{\}} {\{}PAPERS{\}} {\{}FROM{\}} {\{}ACM{\}} {\{}SVT{\}} 2014},
  doi      = {https://doi.org/10.1016/j.scico.2015.06.005},
  keywords = {Behavioral specification,Calculus of communicating systems (CCS),Featured transition systems,Formal specification,Labeled transition systems,Modal transition systems,Product line {\{}CCS{\}} (PL-CCS),Software product lines},
  url      = {http://www.sciencedirect.com/science/article/pii/S0167642315001288},
}

@Article{LIASKOS2012767,
  author   = {Liaskos, Sotirios and Khan, Shakil M and Litoiu, Marin and Jungblut, Marina Daoud and Rogozhkin, Vyacheslav and Mylopoulos, John},
  title    = {{Behavioral adaptation of information systems through goal models}},
  journal  = {Information Systems},
  year     = {2012},
  volume   = {37},
  number   = {8},
  pages    = {767--783},
  issn     = {0306-4379},
  abstract = {Customizing software to perfectly fit individual needs is becoming increasingly important in information systems engineering. Users want to be able to customize software behavior through reference to terms familiar to their diverse needs and experience. We present a requirements-driven approach to behavioral customization of software systems. Goal models are constructed to represent alternative behaviors that users can exhibit to achieve their goals. Customization information is then added to restrict the space of possibilities to those that fit specific users, contexts, or situations. Meanwhile, elements of the goal models are mapped to units of source code. This way, customization preferences posed at the requirements level are directly translated into system customizations. Our approach, which we apply to an on-line shopping cart system and an automated teller machine simulator, does not assume adoption of a particular development methodology, platform, or variability implementation technique and keeps the reasoning computation overhead from interfering with the execution of the configured application.},
  annote   = {Special Issue: Advanced Information Systems Engineering (CAiSE'11)},
  doi      = {https://doi.org/10.1016/j.is.2012.05.006},
  keywords = {Adaptive systems,Goal modeling,Information systems engineering,Software customization},
  url      = {http://www.sciencedirect.com/science/article/pii/S0306437912000737},
}

@Article{MartínezFernández201737,
  author        = {Mart{\'{i}}nez-Fern{\'{a}}ndez, Silverio and Ayala, Claudia P and Franch, Xavier and Marques, Helena Martins},
  title         = {{Benefits and drawbacks of software reference architectures: A case study}},
  journal       = {Information and Software Technology},
  year          = {2017},
  volume        = {88},
  pages         = {37--52},
  issn          = {0950-5849},
  abstract      = {AbstractContext Software Reference Architectures (SRAs) play a fundamental role for organizations whose business greatly depends on the efficient development and maintenance of complex software applications. However, little is known about the real value and risks associated with {\{}SRAs{\}} in industrial practice. Objective To investigate the current industrial practice of {\{}SRAs{\}} in a single company from the perspective of different stakeholders. Method An exploratory case study that investigates the benefits and drawbacks perceived by relevant stakeholders in nine {\{}SRAs{\}} designed by a multinational software consulting company. Results The study shows the perceptions of different stakeholders regarding the benefits and drawbacks of {\{}SRAs{\}} (e.g., both {\{}SRA{\}} designers and users agree that they benefit from reduced development costs; on the contrary, only application builders strongly highlighted the extra learning curve as a drawback associated with mastering SRAs). Furthermore, some of the {\{}SRA{\}} benefits and drawbacks commonly highlighted in the literature were remarkably not mentioned as a benefit of {\{}SRAs{\}} (e.g., the use of best practices). Likewise, other aspects arose that are not usually discussed in the literature, such as higher time-to-market for applications when their dependencies on the {\{}SRA{\}} are managed inappropriately. Conclusions This study aims to help practitioners and researchers to better understand real {\{}SRAs{\}} projects and the contexts where these benefits and drawbacks appeared, as well as some {\{}SRA{\}} improvement strategies. This would contribute to strengthening the evidence regarding {\{}SRAs{\}} and support practitioners in making better informed decisions about the expected {\{}SRA{\}} benefits and drawbacks. Furthermore, we make available the instruments used in this study and the anonymized data gathered to motivate others to provide similar evidence to help mature {\{}SRA{\}} research and practice.},
  doi           = {https://doi.org/10.1016/j.infsof.2017.03.011},
  keywords      = {Benefits,Case study,Drawbacks,Empirical software engineering,Reference architecture,Software architecture,case study},
  mendeley-tags = {case study},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584916304438},
}

@Article{Cacho2014117,
  author   = {Cacho, Nelio and Sant'anna, Claudio and Figueiredo, Eduardo and Dantas, Francisco and Garcia, Alessandro and Batista, Thais},
  title    = {{Blending design patterns with aspects: A quantitative study}},
  journal  = {Journal of Systems and Software},
  year     = {2014},
  volume   = {98},
  pages    = {117--139},
  issn     = {0164-1212},
  abstract = {Abstract Design patterns often need to be blended (or composed) when they are instantiated in a software system. The composition of design patterns consists of assigning multiple pattern elements into overlapping sets of classes in a software system. Whenever the modularity of each design pattern is not preserved in the source code, their implementation becomes tangled with each other and with the classes' core responsibilities. As a consequence, the change or removal of each design pattern will be costly or prohibitive as the software system evolves. In fact, composing design patterns is much harder than instantiating them in an isolated manner. Previous studies have found design pattern implementations are naturally crosscutting in object-oriented systems, thereby making it difficult to modularly compose them. Therefore, aspect-oriented programming (AOP) has been pointed out as a natural alternative for modularizing and blending design patterns. However, there is little empirical knowledge on how {\{}AOP{\}} models influence the composability of widely used design patterns. This paper investigates the influence of using {\{}AOP{\}} models for composing the Gang-of-Four design patterns. Our study categorizes different forms of pattern composition and studies the benefits and drawbacks of {\{}AOP{\}} in these contexts. We performed assessments of several pair-wise compositions taken from 3 medium-sized systems implemented in Java and two {\{}AOP{\}} models, namely, AspectJ and Compose*. We also considered complex situations where more than two patterns involved in each composition, and the patterns were interacting with other aspects implementing other crosscutting concerns of the system. In general, we observed two dominant factors impacting the pattern composability with AOP: (i) the category of the pattern composition, and (ii) the AspectJ idioms used to implement the design patterns taking part in the composition. },
  doi      = {https://doi.org/10.1016/j.jss.2014.08.041},
  keywords = {Aspect-oriented programming,Composability,Design patterns,Empirical studies,Metrics},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121214001885},
}

@Article{Pillat201595,
  author   = {Pillat, Raquel M and Oliveira, Toacy C and Alencar, Paulo S C and Cowan, Donald D},
  title    = {{BPMNt: A {\{}BPMN{\}} extension for specifying software process tailoring}},
  journal  = {Information and Software Technology},
  year     = {2015},
  volume   = {57},
  pages    = {95--115},
  issn     = {0950-5849},
  abstract = {AbstractContext Although {\{}SPEM{\}} 2.0 has great potential for software process modeling, it does not provide concepts or formalisms for precise modeling of process behavior. Indeed, {\{}SPEM{\}} fails to address process simulation, execution, monitoring and analysis, which are important activities in process management. On the other hand, {\{}BPMN{\}} 2.0 is a widely used notation to model business processes that has associated tools and techniques to facilitate the aforementioned process management activities. Using {\{}BPMN{\}} to model software development processes can leverage BPMN's infrastructure to improve the quality of these processes. However, {\{}BPMN{\}} lacks an important feature to model software processes: a mechanism to represent process tailoring. Objective This paper proposes BPMNt, a conservative extension to {\{}BPMN{\}} that aims at creating a tailoring representation mechanism similar to the one found in {\{}SPEM{\}} 2.0. Method We have used the {\{}BPMN{\}} 2.0 extensibility mechanism to include the representation of specific tailoring relationships namely suppression, local contribution, and local replacement, which establish links between process elements (such as in the case of SPEM). Moreover, this paper also presents some rules to ensure the consistency of {\{}BPMN{\}} models when using tailoring relationships. Results In order to evaluate our proposal we have implemented a tool to support the {\{}BPMNt{\}} approach and have applied it for representing real process adaptations in the context of an academic management system development project. Results of this study showed that the approach and its support tool can successfully be used to adapt BPMN-based software processes in real scenarios. Conclusion We have proposed an approach to enable reuse and adaptation of BPMN-based software process models as well as derivation traceability between models through tailoring relationships. We believe that bringing such capabilities into {\{}BPMN{\}} will open new perspectives to software process management. },
  doi      = {https://doi.org/10.1016/j.infsof.2014.09.004},
  keywords = {BPMN,Process modeling,Software process tailoring},
  url      = {http://www.sciencedirect.com/science/article/pii/S0950584914002031},
}

@Article{Bécan20161794,
  author   = {B{\'{e}}can, G and Acher, M and Baudry, B and Nasr, S B},
  title    = {{Breathing ontological knowledge into feature model synthesis: an empirical study}},
  journal  = {Empirical Software Engineering},
  year     = {2016},
  volume   = {21},
  number   = {4},
  pages    = {1794--1841},
  abstract = {Feature Models (FMs) are a popular formalism for modeling and reasoning about the configurations of a software product line. As the manual construction of an FM is time-consuming and error-prone, management operations have been developed for reverse engineering, merging, slicing, or refactoring FMs from a set of configurations/dependencies. Yet the synthesis of meaningless ontological relations in the FM – as defined by its feature hierarchy and feature groups – may arise and cause severe difficulties when reading, maintaining or exploiting it. Numerous synthesis techniques and tools have been proposed, but only a few consider both configuration and ontological semantics of an FM. There are also few empirical studies investigating ontological aspects when synthesizing FMs. In this article, we define a generic, ontologic-aware synthesis procedure that computes the likely siblings or parent candidates for a given feature. We develop six heuristics for clustering and weighting the logical, syntactical and semantical relationships between feature names. We then perform an empirical evaluation on hundreds of FMs, coming from the SPLOT repository and Wikipedia. We provide evidence that a fully automated synthesis (i.e., without any user intervention) is likely to produce FMs far from the ground truths. As the role of the user is crucial, we empirically analyze the strengths and weaknesses of heuristics for computing ranking lists and different kinds of clusters. We show that a hybrid approach mixing logical and ontological techniques outperforms state-of-the-art solutions. We believe our approach, environment, and empirical results support researchers and practitioners working on reverse engineering and management of FMs. {\textcopyright} 2015, Springer Science+Business Media New York.},
  annote   = {cited By 2},
  doi      = {10.1007/s10664-014-9357-1},
  keywords = {Computer software; Reverse engineering; Semantics;,Feature modeling; Model management; Refactorings;,Ontology},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924266514{\&}doi=10.1007{\%}2Fs10664-014-9357-1{\&}partnerID=40{\&}md5=a8bb299ac1e292ee13e93d3b3aaf87ad},
}

@Article{Schäler2012597,
  author   = {Sch{\"{a}}ler, M and Leich, T and Rosenm{\"{u}}ller, M and Saake, G},
  title    = {{Building information system variants with tailored database schemas using features}},
  journal  = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year     = {2012},
  volume   = {7328 LNCS},
  pages    = {597--612},
  abstract = {Database schemas are an integral part of many information systems (IS). New software-engineering methods, such as software product lines, allow engineers to create a high number of different programs tailored to the customer needs from a common code base. Unfortunately, these engineering methods usually do not take the database schema into account. Particularly, a tailored client program requires a tailored database schema as well to form a consistent IS. In this paper, we show the challenges of tailoring relational database schemas in software product lines. Furthermore, we present an approach to treat the client and database part of an IS in the same way using a variable database schema. Additionally, we show the benefits and discuss disadvantages of the approach during the evolution of an industrial case study, covering a time span of more than a year. {\textcopyright} 2012 Springer-Verlag Berlin Heidelberg.},
  annote   = {cited By 5},
  doi      = {10.1007/978-3-642-31095-9_39},
  keywords = {Building information system; Client programs; Cust,Database systems,Industrial applications; Information systems; Sys},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867778420{\&}doi=10.1007{\%}2F978-3-642-31095-9{\_}39{\&}partnerID=40{\&}md5=fe5e71afb4fa5e1b7a8c410ca5825877},
}

@Article{Pessoa201754,
  author   = {Pessoa, Leonardo and Fernandes, Paula and Castro, Thiago and Alves, Vander and Rodrigues, Gena{\'{i}}na N and Carvalho, Hervaldo},
  title    = {{Building reliable and maintainable Dynamic Software Product Lines: An investigation in the Body Sensor Network domain}},
  journal  = {Information and Software Technology},
  year     = {2017},
  volume   = {86},
  pages    = {54--70},
  issn     = {0950-5849},
  abstract = {Abstract Context: Dependability is a key requirement, especially in safety-critical applications. Many of these applications have changing context and configurations at runtime to achieve functional and quality goals and can be realized as Dynamic Software Product Lines (DSPLs). {\{}DSPL{\}} constitutes an emerging but promising research area. Nevertheless, ensuring dependability in {\{}DSPLs{\}} remains insufficiently explored, especially in terms of reliability and maintainability. This compromises quality assurance and applicability of {\{}DSPLs{\}} in safety-critical domains, such as Body Sensor Network (BSN). Objective: To address this issue, we propose an approach to developing reliable and maintainable {\{}DSPLs{\}} in the context of the {\{}BSN{\}} domain. Method: Adaptation plans are instances of a Domain Specific Language (DSL) describing reliability goals and adaptability at runtime. These instances are automatically checked for reliability goal satisfiability before being deployed and interpreted at runtime to provide more suitable adaptation goals complying with evolving needs perceived by a domain specialist. Results: The approach is evaluated in the {\{}BSN{\}} domain. Results show that reliability and maintainability could be provided with execution and reconfiguration times of around 30 ms, notification and adaptation plan update time over the network around 5 s, and space consumption around 5 MB. Conclusion: The method is feasible at reasonable cost. The incurred benefits are reliable vital signal monitoring for the patient—thus providing early detection of serious health issues and the possibility of proactive treatment—and a maintainable infrastructure allowing medical {\{}DSL{\}} instance update to suit the needs of the domain specialist and ultimately of the patient. },
  doi      = {https://doi.org/10.1016/j.infsof.2017.02.002},
  keywords = {Adaptiveness,Body Sensor Network,Context-awareness,Dynamic Software Product Lines,Maintainability,Reliability},
  url      = {http://www.sciencedirect.com/science/article/pii/S0950584917301386},
}

@Article{Chabridon20131912,
  author   = {Chabridon, Sophie and Conan, Denis and Abid, Zied and Taconet, Chantal},
  title    = {{Building ubiquitous QoC-aware applications through model-driven software engineering}},
  journal  = {Science of Computer Programming},
  year     = {2013},
  volume   = {78},
  number   = {10},
  pages    = {1912--1929},
  issn     = {0167-6423},
  abstract = {As every-day mobile devices can easily be equipped with multiple sensing capabilities, ubiquitous applications are expected to exploit the richness of the context information that can be collected by these devices in order to provide the service that is the most appropriate to the situation of the user. However, the design and implementation of such context-aware ubiquitous appplications remain challenging as there exist very few models and tools to guide application designers and developers in mastering the complexity of context information. This becomes even more crucial as context is by nature imperfect. One way to address this issue is to associate to context information meta-data representing its quality. We propose a generic and extensible design process for context-aware applications taking into account the quality of context (QoC). We demonstrate its use on a prototype application for sending flash sale offers to mobile users. We present extensive performance results in terms of memory and processing time of both elementary context management operations and the whole context policy implementing the Flash sale application. The cost of adding QoC management is also measured and appears to be limited to a few milliseconds. We show that a context policy with 120 QoC-aware nodes can be processed in less than 100 ms on a mobile phone. Moreover, a policy of almost 3000 nodes can be instantiated before exhausting the resources of the phone. This enables very rich application scenarios enhancing the user experience and will favor the development of new ubiquitous applications. },
  annote   = {Special section on Language Descriptions Tools and Applications (LDTA'08 {\&} '09) {\&} Special section on Software Engineering Aspects of Ubiquitous Computing and Ambient Intelligence (UCAmI 2011)},
  doi      = {https://doi.org/10.1016/j.scico.2012.07.019},
  keywords = {Context,Domain specific language,Model-driven software engineering,Pervasive computing,Quality of context,Ubiquitous computing},
  url      = {http://www.sciencedirect.com/science/article/pii/S0167642312001475},
}

@Article{Henard2014,
  author  = {Henard, Christopher and Papadakis, Mike and Perrouin, Gilles and Klein, Jacques and Heymans, Patrick and {Le Traon}, Yves},
  title   = {{Bypassing the Combinatorial Explosion: Using Similarity to Generate and Prioritize T-Wise Test Configurations for Software Product Lines}},
  journal = {IEEE Transactions on Software Engineering},
  year    = {2014},
  volume  = {40},
  number  = {7},
  pages   = {650--670},
  month   = {jul},
  issn    = {0098-5589},
  doi     = {10.1109/TSE.2014.2327020},
  file    = {:Users/mac/Library/Application Support/Mendeley Desktop/Downloaded/Henard et al. - 2014 - Bypassing the combinatorial explosion Using similarity to generate and prioritize t-wise test configurations f(2).pdf:pdf},
  url     = {http://ieeexplore.ieee.org/document/6823132/},
}

@Article{KUZ2007687,
  author   = {Kuz, Ihor and Liu, Yan and Gorton, Ian and Heiser, Gernot},
  title    = {{CAmkES: A component model for secure microkernel-based embedded systems}},
  journal  = {Journal of Systems and Software},
  year     = {2007},
  volume   = {80},
  number   = {5},
  pages    = {687--699},
  issn     = {0164-1212},
  abstract = {Component-based software engineering promises to provide structure and reusability to embedded-systems software. At the same time, microkernel-based operating systems are being used to increase the reliability and trustworthiness of embedded systems. Since the microkernel approach to designing systems is partially based on the componentisation of system services, component-based software engineering is a particularly attractive approach to developing microkernel-based systems. While a number of widely used component architectures already exist, they are generally targeted at enterprise computing rather than embedded systems. Due to the unique characteristics of embedded systems, a component architecture for embedded systems must have low overhead, be able to address relevant non-functional issues, and be flexible to accommodate application specific requirements. In this paper we introduce a component architecture aimed at the development of microkernel-based embedded systems. The key characteristics of the architecture are that it has a minimal, low-overhead, core but is highly modular and therefore flexible and extensible. We have implemented a prototype of this architecture and confirm that it has very low overhead and is suitable for implementing both system-level and application level services.},
  annote   = {Component-Based Software Engineering of Trustworthy Embedded Systems},
  doi      = {https://doi.org/10.1016/j.jss.2006.08.039},
  keywords = {Component architecture,Embedded system,Microkernel},
  url      = {http://www.sciencedirect.com/science/article/pii/S016412120600224X},
}

@Article{CLARKE2017754,
  author   = {Clarke, Roger},
  title    = {{Can small users recover from the cloud?}},
  journal  = {Computer Law {\&} Security Review},
  year     = {2017},
  volume   = {33},
  number   = {6},
  pages    = {754--767},
  issn     = {0267-3649},
  abstract = {Large numbers of small organisations and prosumers have shifted away from managing data on their own devices and are now heavily reliant on service-providers for both storage and processing of their data. Most such entities are also dependent on those service-providers to perform backups and enable data recovery. Prior work defining users' backup needs was applied to this context in order to establish specifications for appropriate backup arrangements. A sample of service-providers was assessed against those specifications. Their backup and recovery mechanisms were found to fall seriously short of the need.},
  doi      = {https://doi.org/10.1016/j.clsr.2017.08.004},
  keywords = {Backup,Cloud computing,Consumers,Recovery,SaaS,Small business},
  url      = {http://www.sciencedirect.com/science/article/pii/S0267364917302789},
}

@Article{vanAngeren2016430,
  author   = {van Angeren, Joey and Alves, Carina and Jansen, Slinger},
  title    = {{Can we ask you to collaborate? Analyzing app developer relationships in commercial platform ecosystems}},
  journal  = {Journal of Systems and Software},
  year     = {2016},
  volume   = {113},
  pages    = {430--445},
  issn     = {0164-1212},
  abstract = {Abstract Previous studies have emphasized the necessity for software platform owners to govern their platform ecosystem in order to create durable opportunities for themselves and the app developers that surround the platform. To date, platform ecosystems have been widely analyzed from the perspective of platform owners. However, how and to what extent app developers collaborate with their peers needs to be investigated further. In this article, we study the interfirm relationships among app developers in commercial platform ecosystems and explore the causes of variation in the network structure of these ecosystems. By means of a comparative study of four commercial platform ecosystems of Google (Google Apps and Google Chrome) and Microsoft (Microsoft Office365 and Internet Explorer), we illustrate substantial variation in the extent to which app developers initiated interfirm relationships. Further, we analyze how the degree of enforced entry barriers to the app store, the use of a partnership model, and the domain of the software platform that underpins the ecosystem affect the properties of these commercial platform ecosystems. We present subsequent explanations as a set of propositions that can be tested in future empirical research. },
  doi      = {https://doi.org/10.1016/j.jss.2015.11.025},
  keywords = {Case study,Interfirm network analysis,Software ecosystem},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121215002502},
}

@Article{Zdravkovic201571,
  author   = {Zdravkovic, J and Svee, E.-O. and Giannoulis, C},
  title    = {{Capturing consumer preferences as requirements for software product lines}},
  journal  = {Requirements Engineering},
  year     = {2015},
  volume   = {20},
  number   = {1},
  pages    = {71--90},
  abstract = {Delivering great consumer experiences in competitive market conditions requires software vendors to move away from traditional modes of thinking to an outside-in perspective, one that shifts their business to becoming consumer-centric. Requirements engineers operating in these conditions thus need new means to both capture real preferences of consumers and then relate them to requirements for software customized in different ways to fit anyone. Additionally, because system development models require inputs that are more concrete than abstract, the indistinct values of consumers need to be classified and formalized. To address this challenge, this study aims to establish a conceptual link between preferences of consumers and system requirements, using software product line (SPL) as a means for systematically accommodating the variations within the preferences. The novelty of this study is a conceptual model of consumer preference, which integrates generic value frameworks from both psychology and marketing, and a method for its transformation to requirements for SPL using a goal-oriented RE framework as the mediator. The presented artifacts are grounded in an empirical study related to the development of a system for online education. {\textcopyright} 2013, The Author(s).},
  annote   = {cited By 4},
  doi      = {10.1007/s00766-013-0187-2},
  keywords = {Commerce; Computer software; Distance education; R,Consumer value; Features; Goal modeling; Requirem,Software design},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924223166{\&}doi=10.1007{\%}2Fs00766-013-0187-2{\&}partnerID=40{\&}md5=427f356ae79e2831846fe0a519b4f852},
}

@Article{Niemelä20071107,
  author   = {Niemel{\"{a}}, E and Immonen, A},
  title    = {{Capturing quality requirements of product family architecture}},
  journal  = {Information and Software Technology},
  year     = {2007},
  volume   = {49},
  number   = {11-12},
  pages    = {1107--1120},
  abstract = {Software quality is one of the major issues with software intensive systems. Moreover, quality is a critical success factor in software product families exploiting shared architecture and common components in a set of products. Our contribution is the QRF (Quality Requirements of a software Family) method, which explicitly focuses on how quality requirements have to be defined, represented and transformed to architectural models. The method has been applied to two experiments; one in a laboratory environment and the other in industry. The use of the QRF method is exemplified by the Distribution Service Platform (DiSeP), the laboratory experiment. The lessons learned are also based on our experiences of applying the method in industrial settings. {\textcopyright} 2006 Elsevier B.V. All rights reserved.},
  annote   = {cited By 35},
  doi      = {10.1016/j.infsof.2006.11.003},
  file     = {:Users/mac/Library/Application Support/Mendeley Desktop/Downloaded/Niemel{\"{a}}, Immonen - 2007 - Capturing quality requirements of product family architecture.pdf:pdf},
  keywords = {Computer software; Distribution functions; Industr,Industrial settings; Quality requirement; Softwar,Quality assurance},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-34848888028{\&}doi=10.1016{\%}2Fj.infsof.2006.11.003{\&}partnerID=40{\&}md5=e13e12d15019d1467a408d04910818f6},
}

@Article{vanderAalst2005129,
  author   = {van der Aalst, Wil M P and Weske, Mathias and Gr{\"{u}}nbauer, Dolf},
  title    = {{Case handling: a new paradigm for business process support}},
  journal  = {Data {\&} Knowledge Engineering},
  year     = {2005},
  volume   = {53},
  number   = {2},
  pages    = {129--162},
  issn     = {0169-023X},
  abstract = {Case handling is a new paradigm for supporting flexible and knowledge intensive business processes. It is strongly based on data as the typical product of these processes. Unlike workflow management, which uses predefined process control structures to determine what should be done during a workflow process, case handling focuses on what can be done to achieve a business goal. In case handling, the knowledge worker in charge of a particular case actively decides on how the goal of that case is reached, and the role of a case handling system is assisting rather than guiding her in doing so. In this paper, case handling is introduced as a new paradigm for supporting flexible business processes. It is motivated by comparing it to workflow management as the traditional way to support business processes. The main entities of case handling systems are identified and classified in a meta model. Finally, the basic functionality and usage of a case handling system is illustrated by an example. },
  doi      = {https://doi.org/10.1016/j.datak.2004.07.003},
  keywords = {Adaptive workflow,Business process management,Case handling,Flexibility,Workflow management systems},
  url      = {http://www.sciencedirect.com/science/article/pii/S0169023X04001296},
}

@Article{Bashroush:2017:CTS:3058791.3034827,
  author    = {Bashroush, Rabih and Garba, Muhammad and Rabiser, Rick and Groher, Iris and Botterweck, Goetz},
  title     = {{CASE Tool Support for Variability Management in Software Product Lines}},
  journal   = {ACM Comput. Surv.},
  year      = {2017},
  volume    = {50},
  number    = {1},
  pages     = {14:1----14:45},
  issn      = {0360-0300},
  address   = {New York, NY, USA},
  doi       = {10.1145/3034827},
  keywords  = {Software engineering,computer-aided software engineering,software variability},
  publisher = {ACM},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/3034827},
}

@Article{PEREZ20081438,
  author   = {P{\'{e}}rez, Francisco and de Lara, Juan and Conde, Luis and Alfonseca, Manuel and Gal{\'{a}}n, Luis and Raboso, David},
  title    = {{CEST and MEST: Tools for the simulation of radio frequency electric discharges in waveguides}},
  journal  = {Simulation Modelling Practice and Theory},
  year     = {2008},
  volume   = {16},
  number   = {9},
  pages    = {1438--1452},
  issn     = {1569-190X},
  abstract = {In this paper we present two software tools for the simulation of electron multiplication processes in radio frequency (RF) waveguides. The electric discharges are caused by the multiplication of a small initial number of electrons. These are accelerated by the RF field and produce new electrons either by collisions with the walls of the waveguide (ripping new electrons from them), or by ionization of the neutral atoms of a gas inside the device. MEST allows simulating the Multipactor effect, a discharge produced in vacuum and generated by the collision of the electrons with the walls. CEST simulates the discharge when in addition a neutral gas is present in the waveguide, at pressures lower than ground levels (often denominated Corona discharge). The main characteristic of both tools is that they implement individual-based, microscopic models, where every electron is individually represented and tracked. In the case of MEST, the simulation is discrete-event, as the trajectory of each electron can be computed analytically. In CEST we use a hybrid simulation approach. The trajectory of each electron is governed by the Langevin stochastic differential equations that take into account a deterministic RF electric force and the random interaction with the neutral atom background. In addition, wall and ionizing collisions are modelled as discrete events. The tools allow performing batches of simulations with different wall coating materials and gases, and have produced results in good agreement with experimental and theoretical data. The different output forms generated at run-time have proven to be very useful in order to analyze the different discharge processes. The tools are valuable for the selection of the most promising coating materials for the construction of the waveguide, as well as for the identification of safe operating parameters.},
  doi      = {https://doi.org/10.1016/j.simpat.2008.08.002},
  keywords = {Corona,Discrete-event simulation,Electric discharges,Hybrid simulation,Multipactor,Simulation tools},
  url      = {http://www.sciencedirect.com/science/article/pii/S1569190X08001457},
}

@Article{terBeek2014351,
  author   = {ter Beek, M H and Fantechi, A and Gnesi, S},
  title    = {{Challenges in modelling and analyzing quantitative aspects of bike-sharing systems}},
  journal  = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year     = {2014},
  volume   = {8802},
  pages    = {351--367},
  abstract = {Bike-sharing systems are becoming popular not only as a sustainable means of transportation in the urban environment, but also as a challenging case study that presents interesting run-time optimization problems. As a side-study within a research project aimed at quantitative analysis that used such a case study, we have observed how the deployed systems enjoy a wide variety of different features. We have therefore applied variability analysis to define a family of bike-sharing systems, and we have sought support in available tools. We have so established a tool chain that includes (academic) tools that provide different functionalities regarding the analysis of software product lines, from feature modelling to product derivation and from quantitative evaluation of the attributes of products to model checking value-passing modal specifications. The tool chain is currently experimented inside the mentioned project as a complement to more sophisticated product-based analysis techniques. {\textcopyright} Springer-Verlag Berlin Heidelberg 2014.},
  annote   = {cited By 8},
  keywords = {Analysis techniques; Means of transportations; Pr,Bicycles; Chains; Formal methods; Model checking;,Time sharing systems},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84910613014{\&}partnerID=40{\&}md5=e5550e42d36a4fb96dd5b584ee979d72},
}

@Article{Castro2012463,
  author   = {Castro, Jaelson and Lucena, Marcia and Silva, Carla and Alencar, Fernanda and Santos, Emanuel and Pimentel, Jo{\~{a}}o},
  title    = {{Changing attitudes towards the generation of architectural models}},
  journal  = {Journal of Systems and Software},
  year     = {2012},
  volume   = {85},
  number   = {3},
  pages    = {463--479},
  issn     = {0164-1212},
  abstract = {Architectural design is an important activity, but the understanding of how it is related to requirements modeling is rather limited. It is worth noting that goal orientation is an increasingly recognized paradigm for eliciting, modeling, specifying, and analyzing software requirements. However, it is not clear how goal models are related to architectural models. In this paper we present an approach based on model transformations to derive architectural structural specifications from system goals. The source and target languages are respectively the i* (iStar) modeling language and the Acme architectural description language. A real case study is used to show the feasibility of our approach. },
  annote   = {Novel approaches in the design and implementation of systems/software architecture},
  doi      = {https://doi.org/10.1016/j.jss.2011.05.047},
  keywords = {Architectural design,Model driven development,Model transformations,Requirements engineering},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121211001415},
}

@Article{Axelsson20141457,
  author        = {Axelsson, Jakob and Papatheocharous, Efi and Andersson, Jesper},
  title         = {{Characteristics of software ecosystems for Federated Embedded Systems: A case study}},
  journal       = {Information and Software Technology},
  year          = {2014},
  volume        = {56},
  number        = {11},
  pages         = {1457--1475},
  issn          = {0950-5849},
  abstract      = {AbstractContext Traditionally, Embedded Systems (ES) are tightly linked to physical products, and closed both for communication to the surrounding world and to additions or modifications by third parties. New technical solutions are however emerging that allow addition of plug-in software, as well as external communication for both software installation and data exchange. These mechanisms in combination will allow for the construction of Federated Embedded Systems (FES). Expected benefits include the possibility of third-party actors developing add-on functionality; a shorter time to market for new functions; and the ability to upgrade existing products in the field. This will however require not only new technical solutions, but also a transformation of the software ecosystems for ES. Objective This paper aims at providing an initial characterization of the mechanisms that need to be present to make a {\{}FES{\}} ecosystem successful. This includes identification of the actors, the possible business models, the effects on product development processes, methods and tools, as well as on the product architecture. Method The research was carried out as an explorative case study based on interviews with 15 senior staff members at 9 companies related to {\{}ES{\}} that represent different roles in a future ecosystem for FES. The interview data was analyzed and the findings were mapped according to the Business Model Canvas (BMC). Results The findings from the study describe the main characteristics of a {\{}FES{\}} ecosystem, and identify the challenges for future research and practice. Conclusions The case study indicates that new actors exist in the {\{}FES{\}} ecosystem compared to a traditional supply chain, and that their roles and relations are redefined. The business models include new revenue streams and services, but also create the need for trade-offs between, e.g., openness and dependability in the architecture, as well as new ways of working.},
  annote        = {Special issue on Software Ecosystems},
  doi           = {https://doi.org/10.1016/j.infsof.2014.03.011},
  keywords      = {Architecture,Case study,Embedded Systems,Software ecosystems,Systems-of-Systems,case study},
  mendeley-tags = {case study},
  url           = {http://www.sciencedirect.com/science/article/pii/S095058491400072X},
}

@Article{Raatikainen2005,
  author   = {Raatikainen, Mikko and Soininen, Timo and M??nnist??, Tomi and Mattila, Antti},
  title    = {{Characterizing configurable software product families and their derivation}},
  journal  = {Software Process Improvement and Practice},
  year     = {2005},
  volume   = {10},
  number   = {1},
  pages    = {41--60},
  issn     = {10774866},
  doi      = {10.1002/spip.211},
  file     = {:Users/mac/Library/Application Support/Mendeley Desktop/Downloaded/Raatikainen et al. - 2005 - Characterizing configurable software product families and their derivation.pdf:pdf},
  keywords = {Case study,Configurable product base,Configurable software product family,Product derivation,Software product family},
}

@Article{Saraiva201585,
  author   = {{de A.G. Saraiva}, Juliana and de Fran{\c{c}}a, Micael S and Soares, S{\'{e}}rgio C B and Filho, Fernando J C L and de Souza, Renata M C R},
  title    = {{Classifying metrics for assessing Object-Oriented Software Maintainability: A family of metrics' catalogs}},
  journal  = {Journal of Systems and Software},
  year     = {2015},
  volume   = {103},
  pages    = {85--101},
  issn     = {0164-1212},
  abstract = {Abstract Object-Oriented Programming is one of the most used paradigms. Complementarily, the software maintainability is considered a software attribute playing an important role in quality level. In this context, Object-Oriented Software Maintainability (OOSM) has been studied through years, and many researchers have proposed a large number of metrics to measure it. Consequently, the decision-making process about which metrics can be adopted in experiments on {\{}OOSM{\}} is a hard task. Therefore, a metrics' categorization has been proposed to facilitate this process. As result, 7 categories and 17 subcategories were identified. These categories represent the scenarios of {\{}OOSM{\}} metrics adoption, and a family of {\{}OOSM{\}} metrics catalog was generated based on the selection of a metrics' categorization. Additionally, a quasi-experiment was conducted to check the coverage index of the catalogs generated using our approach over the catalogs suggested by experts. 90{\%} of coverage was obtained with 99{\%} of confidential level using the Wilcoxon Test. Complementarily, a survey was conducted to check the experts' opinion about the catalog generated by the portal when they were compared by the catalogs suggested by them. Therefore, this evaluation can be the first evidences of the usefulness of the family of the catalogs based on the metrics' categorization. },
  doi      = {https://doi.org/10.1016/j.jss.2015.01.014},
  keywords = {Metrics,Object-Oriented Software Development,Software maintainability},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121215000126},
}

@Article{SINNEMA2007717,
  author   = {Sinnema, Marco and Deelstra, Sybren},
  title    = {{Classifying variability modeling techniques}},
  journal  = {Information and Software Technology},
  year     = {2007},
  volume   = {49},
  number   = {7},
  pages    = {717--739},
  issn     = {0950-5849},
  abstract = {Variability modeling is important for managing variability in software product families, especially during product derivation. In the past few years, several variability modeling techniques have been developed, each using its own concepts to model the variability provided by a product family. The publications regarding these techniques were written from different viewpoints, use different examples, and rely on a different technical background. This paper sheds light on the similarities and differences between six variability modeling techniques, by exemplifying the techniques with one running example, and classifying them using a framework of key characteristics for variability modeling. It furthermore discusses the relation between differences among those techniques, and the scope, size, and application domain of product families.},
  doi      = {https://doi.org/10.1016/j.infsof.2006.08.001},
  keywords = {Classification,Software product family,Variability management,Variability modeling},
  url      = {http://www.sciencedirect.com/science/article/pii/S0950584906001042},
}

@Article{Rubin2015627,
  author   = {Rubin, J and Czarnecki, K and Chechik, M},
  title    = {{Cloned product variants: from ad-hoc to managed software product lines}},
  journal  = {International Journal on Software Tools for Technology Transfer},
  year     = {2015},
  volume   = {17},
  number   = {5},
  pages    = {627--646},
  abstract = {We focus on the problem of managing a collection of related software product variants realized via cloning. By analyzing three industrial case studies of organizations with cloned product lines, we conclude that an efficient management of clones relies on both refactoring cloned variants into a single-copy product line representation and improving development experience when maintaining existing clones. We propose a framework that consists of seven conceptual operators for cloned product line management and show that these operators are adequate to realize development activities we observed in the analyzed case studies. We discuss options for implementing the operators and benefits of the operator-based view. {\textcopyright} 2014, Springer-Verlag Berlin Heidelberg.},
  annote   = {cited By 3},
  doi      = {10.1007/s10009-014-0347-9},
  keywords = {Cloning,Computer software,Development activity; Development experiences; Ef},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941423205{\&}doi=10.1007{\%}2Fs10009-014-0347-9{\&}partnerID=40{\&}md5=7472e5524c685cc539f316498ae8f0c1},
}

@Article{Horcas2014106,
  author   = {Horcas, J.-M. and Pinto, M and Fuentes, L},
  title    = {{Closing the gap between the specification and enforcement of security policies}},
  journal  = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year     = {2014},
  volume   = {8647 LNCS},
  pages    = {106--118},
  abstract = {Security policies are enforced through the deployment of certain security functionalities within the applications. Applications can have different levels of security and thus each security policy is enforced by different security functionalities. Thus, the secure deployment of an application is not an easy task, being more complicated due to the existing gap between the specification of a security policy and the deployment, inside the application, of the security functionalities that are required to enforce that security policy. The main goal of this paper is to close this gap. This is done by using the paradigms of Software Product Lines and Aspect-Oriented Programming in order to: (1) link the security policies with the security functionalities, (2) generate a configuration of the security functionalities that fit a security policy, and (3) weave the selected security functionalities into an application. We qualitatively evaluate our approach, and discuss its benefits using a case study. {\textcopyright} 2014 Springer International Publishing.},
  annote   = {cited By 2},
  doi      = {10.1007/978-3-319-09770-1_10},
  keywords = {Aspect oriented programming; Computer software; Sp,Security enforcement; Security policy; Software P,Security systems},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906739535{\&}doi=10.1007{\%}2F978-3-319-09770-1{\_}10{\&}partnerID=40{\&}md5=ce8a5ec191a26dc73244892638bde52e},
}

@Article{Gholami201631,
  author   = {Gholami, Mahdi Fahmideh and Daneshgar, Farhad and Low, Graham and Beydoun, Ghassan},
  title    = {{Cloud migration process—A survey, evaluation framework, and open challenges}},
  journal  = {Journal of Systems and Software},
  year     = {2016},
  volume   = {120},
  pages    = {31--69},
  issn     = {0164-1212},
  abstract = {Abstract Moving mission-oriented enterprise software applications to cloud environments is a crucial {\{}IT{\}} task and requires a systematic approach. The foci of this paper is to provide a detailed review of extant cloud migration approaches from the perspective of the process model. To this aim, an evaluation framework is proposed and used to appraise and compare existing approaches for highlighting their features, similarities, and key differences. The survey distills the status quo and makes a rich inventory of important activities, recommendations, techniques, and concerns that are common in a typical cloud migration process in one place. This enables both academia and practitioners in the cloud computing community to get an overarching view of the process of the legacy application migration to the cloud. Furthermore, the survey identifies a number challenges that have not been yet addressed by existing approaches, developing opportunities for further research endeavours. },
  doi      = {https://doi.org/10.1016/j.jss.2016.06.068},
  keywords = {Cloud computing,Cloud migration,Evaluation framework,Legacy application,Migration methodology,Process model},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121216300966},
}

@Article{Lee20132154,
  author   = {Lee, Seonah and Kang, Sungwon},
  title    = {{Clustering navigation sequences to create contexts for guiding code navigation}},
  journal  = {Journal of Systems and Software},
  year     = {2013},
  volume   = {86},
  number   = {8},
  pages    = {2154--2165},
  issn     = {0164-1212},
  abstract = {Abstract To guide programmer code navigation, previous approaches such as TeamTracks recommend pieces of code to visit by mining the associations between pieces of code in programmer interaction histories. However, these result in low recommendation accuracy. To create more accurate recommendations, we propose NavClus an approach that clusters navigation sequences from programmer interaction histories. NavClus automatically forms collections of code that are relevant to the tasks performed by programmers, and then retrieves the collections best matched to a programmer's current navigation path. This makes it possible to recommend the collections of code that are relevant to the programmer's given task. We compare NavClus' recommendation accuracy with TeamTracks' by simulating recommendations using 4397 interaction histories. The comparative experiment shows that the recommendation accuracy of NavClus is twice as high as that of TeamTracks. },
  doi      = {https://doi.org/10.1016/j.jss.2013.03.103},
  keywords = {Code navigation,Context aware code recommender,Data clustering techniques,Data stream mining,Programmer interaction histories},
  url      = {http://www.sciencedirect.com/science/article/pii/S016412121300085X},
}

@Article{Graves20122633,
  author   = {Graves, Daniel and Noppen, Joost and Pedrycz, Witold},
  title    = {{Clustering with proximity knowledge and relational knowledge}},
  journal  = {Pattern Recognition},
  year     = {2012},
  volume   = {45},
  number   = {7},
  pages    = {2633--2644},
  issn     = {0031-3203},
  abstract = {In this article, a proximity fuzzy framework for clustering relational data is presented, where the relationships between the entities of the data are given in terms of proximity values. We offer a comprehensive and in-depth comparison of our clustering framework with proximity relational knowledge to clustering with distance relational knowledge, such as the well known relational Fuzzy C-Means (FCM). We conclude that proximity can provide a richer description of the relationships among the data and this offers a significant advantage when realizing clustering. We further motivate clustering relational proximity data and provide both synthetic and real-world experiments to demonstrate both the usefulness and advantage offered by clustering proximity data. Finally, a case study of relational clustering is introduced where we apply proximity fuzzy clustering to the problem of clustering a set of trees derived from software requirements engineering. The relationships between trees are based on the degree of closeness in both the location of the nodes in the trees and the semantics associated with the type of connections between the nodes. },
  doi      = {https://doi.org/10.1016/j.patcog.2011.12.019},
  keywords = {Fuzzy clustering,Knowledge representation,Proximity,Relational clustering,Software requirements},
  url      = {http://www.sciencedirect.com/science/article/pii/S0031320311005206},
}

@Article{Schulze2011103,
  author   = {Schulze, S and Apel, S and K{\"{a}}stner, C},
  title    = {{Code clones in feature-oriented software product lines}},
  journal  = {ACM SIGPLAN Notices},
  year     = {2011},
  volume   = {46},
  number   = {2},
  pages    = {103--112},
  abstract = {Some limitations of object-oriented mechanisms are known to cause code clones (e.g., extension using inheritance). Novel programming paradigms such as feature-oriented programming (FOP) aim at alleviating these limitations. However, it is an open issue whether FOP is really able to avoid code clones or whether it even facilitates (FOP-related) clones. To address this issue, we conduct an empirical analysis on ten feature-oriented software product lines with respect to code cloning. We found that there is a considerable number of clones in feature-oriented software product lines and that a large fraction of these clones is FOP-related (i.e., caused by limitations of feature-oriented mechanisms). Based on our results, we initiate a discussion on the reasons for FOP-related clones and on how to cope with them. We show by means of examples how such clones can be removed by applying refactorings. Copyright {\textcopyright} 2010 ACM.},
  annote   = {cited By 1},
  doi      = {10.1145/1942788.1868310},
  keywords = {Cloning,Code clone; Code cloning; Empirical analysis; Feat},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-79951743431{\&}doi=10.1145{\%}2F1942788.1868310{\&}partnerID=40{\&}md5=f151bb4f4d206bf539e21a1b3fc92e58},
}

@Article{Heradio200925,
  author   = {Heradio, R and Cerrada, J A and Lopez, J C and Coz, J R},
  title    = {{Code Generation with the Exemplar Flexibilization Language}},
  journal  = {Electronic Notes in Theoretical Computer Science},
  year     = {2009},
  volume   = {238},
  number   = {2},
  pages    = {25--34},
  issn     = {1571-0661},
  abstract = {Code Generation is an increasing popular technique for implementing Software Product Lines that produces code from abstract specifications written in Domain Specific Languages (DSLs). This paper proposes to take advantage of the similitude among the products in a domain to generate them by analogy. That is, instead of synthesizing the final code from scratch or transforming the {\{}DSL{\}} specifications, the final products are obtained by adapting a previously developed domain product. The paper also discusses the capabilities and limitations of several currently available tools and languages to implement this kind of generators and introduce a new language to overcome the limitations. },
  annote   = {Proceedings of the First Workshop on Generative Technologies (WGT) 2008.},
  doi      = {https://doi.org/10.1016/j.entcs.2009.05.004},
  keywords = {Code Generation,Domain Specific Language,Software Product Line},
  url      = {http://www.sciencedirect.com/science/article/pii/S1571066109001261},
}

@Article{Lorenz2011167,
  author   = {Lorenz, D H and Rosenan, B},
  title    = {{Code reuse with language oriented programming}},
  journal  = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year     = {2011},
  volume   = {6727 LNCS},
  pages    = {167--182},
  abstract = {There is a gap between our ability to reuse high-level concepts in software design and our ability to reuse the code implementing them. Language Oriented Programming (LOP) is a software development paradigm that aims to close this gap, through extensive use of Domain Specific Languages (DSLs). With LOP, the high-level reusable concepts become reusable DSL constructs, and their translation into code level concepts is done in the DSL implementation. Particular products are implemented using DSL code, thus reusing only high-level concepts. In this paper we provide a comparison between two implementation approaches for LOP: (a),using external DSLs with a projectional language workbench (MPS); and (b),using internal DSLs with an LOP language (Cedalion). To demonstrate how reuse is achieved in each approach, we present a small case study, where LOP is used to build a Software Product Line (SPL) of calculator software. {\textcopyright} 2011 Springer-Verlag.},
  annote   = {cited By 0},
  doi      = {10.1007/978-3-642-21347-2_13},
  keywords = {Code reuse; Domain specific languages; Implementat,Computer software reusability,DSL; High level languages; Software design},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-79959648051{\&}doi=10.1007{\%}2F978-3-642-21347-2{\_}13{\&}partnerID=40{\&}md5=2211ab4c1299a3d7ba9d55095cccbe74},
}

@Article{Demuth2016281,
  author   = {Demuth, Andreas and Riedl-Ehrenleitner, Markus and Lopez-Herrejon, Roberto E and Egyed, Alexander},
  title    = {{Co-evolution of metamodels and models through consistent change propagation}},
  journal  = {Journal of Systems and Software},
  year     = {2016},
  volume   = {111},
  pages    = {281--297},
  issn     = {0164-1212},
  abstract = {Abstract In model-driven engineering (MDE), metamodels and domain-specific languages are key artifacts as they are used to define syntax and static semantics of domain models. However, metamodels are evolving over time, requiring existing domain models to be co-evolved. Though approaches have been proposed for performing such co-evolution automatically, those approaches typically support only specific metamodel changes. In this paper, we present a vision of co-evolution between metamodels and models through consistent change propagation. The approach addresses co-evolution issues without being limited to specific metamodels or evolution scenarios. It relies on incremental management of metamodel-based constraints that are used to detect co-evolution failures (i.e., inconsistencies between metamodel and model). After failure detection, the approach automatically generates suggestions for correction (i.e., repairs for inconsistencies). A case study with the {\{}UML{\}} metamodel and 23 {\{}UML{\}} models shows that the approach is technically feasible and also scalable. },
  doi      = {https://doi.org/10.1016/j.jss.2015.03.003},
  keywords = {Consistency checking,Consistent change propagation,Metamodel co-evolution},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121215000564},
}

@Article{Magdaleno2015452,
  author   = {Magdaleno, Andr{\'{e}}a Magalh{\~{a}}es and {de Oliveira Barros}, Marcio and Werner, Cl{\'{a}}udia Maria Lima and de Araujo, Renata Mendes and Batista, Carlos Freud Alves},
  title    = {{Collaboration optimization in software process composition}},
  journal  = {Journal of Systems and Software},
  year     = {2015},
  volume   = {103},
  pages    = {452--466},
  issn     = {0164-1212},
  abstract = {Abstract Purpose: The purpose of this paper is to describe an optimization approach to maximize collaboration in software process composition. The research question is: how to compose a process for a specific software development project context aiming to maximize collaboration among team members? The optimization approach uses heuristic search algorithms to navigate the solution space and look for acceptable solutions. Design/methodology/approach: The process composition approach was evaluated through an experimental study conducted in the context of a large oil company in Brazil. The objective was to evaluate the feasibility of composing processes for three software development projects. We have also compared genetic algorithm (GA) and hill climbing (HC) algorithms driving the optimization with a simple random search (RS) in order to determine which would be more effective in addressing the problem. In addition, human specialist point-of-view was explored to verify if the composed processes were in accordance with his/her expectations regarding size, complexity, diversity, and reasonable sequence of components. Findings: The main findings indicate that {\{}GA{\}} is more effective (best results regarding the fitness function) than {\{}HC{\}} and {\{}RS{\}} in the search of solutions for collaboration optimization in software process composition for large instances. However, all algorithms are competitive for small instances and even brute force can be a feasible alternative in such a context. These {\{}SBSE{\}} results were complemented by the feedback given by specialist, indicating his satisfaction with the correctness, diversity, adherence to the project context, and support to the project manager during the decision making in process composition. Research limitations: This work was evaluated in the context of a single company and used only three project instances. Due to confidentiality restrictions, the data describing these instances could not be disclosed to be used in other research works. The reduced size of the sample prevents generalization for other types of projects or different contexts. Implications: This research is important for practitioners who are facing challenges to handle diversity in software process definition, since it proposes an approach based on context, reuse and process composition. It also contributes to research on collaboration by presenting a collaboration management solution (COMPOOTIM) that includes both an approach to introduce collaboration in organizations through software processes and a collaboration measurement strategy. From the standpoint of software engineering looking for collaborative solutions in distributed software development, free/open source software, agile, and ecosystems initiatives, the results also indicate how to increase collaboration in software development. Originality/value: This work proposes a systematic strategy to manage collaboration in software development process composition. Moreover, it brings together a mix of computer-oriented and human-oriented studies on the search-based software engineering (SBSE) research area. Finally, this work expands the body of knowledge in {\{}SBSE{\}} to the field of software process which has not been properly explored by former research. },
  doi      = {https://doi.org/10.1016/j.jss.2014.11.036},
  keywords = {Collaboration,SBSE,Software process},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121214002672},
}

@Article{Brosch20103,
  author   = {Brosch, Franz and Gitzel, Ralf and Koziolek, Heiko and Krug, Simone},
  title    = {{Combining Architecture-based Software Reliability Predictions with Financial Impact Calculations}},
  journal  = {Electronic Notes in Theoretical Computer Science},
  year     = {2010},
  volume   = {264},
  number   = {1},
  pages    = {3--17},
  issn     = {1571-0661},
  abstract = {Software failures can lead to substantial costs for the user. Existing models for software reliability prediction do not provide much insight into this financial impact. Our approach presents a first step towards the integration of reliability prediction from the {\{}IT{\}} perspective and the business perspective. We show that failure impact should be taken into account not only at their date of occurrence but already in the design stage of the development. First we model cost relevant business processes as well as the associated {\{}IT{\}} layer and then connect them to failure probabilities. Based on this we conduct a reliability and cost estimation. The method is illustrated by a case study. },
  annote   = {Proceedings of the 7th International Workshop on Formal Engineering approaches to Software Components and Architectures (FESCA 2010)},
  doi      = {https://doi.org/10.1016/j.entcs.2010.07.002},
  keywords = {Cost,Reliability,Software failure},
  url      = {http://www.sciencedirect.com/science/article/pii/S1571066110000617},
}

@Article{WEHRMEISTER2014844,
  author   = {Wehrmeister, Marco Aur{\'{e}}lio and de Freitas, Edison Pignaton and Binotto, Al{\'{e}}cio Pedro Delazari and Pereira, Carlos Eduardo},
  title    = {{Combining aspects and object-orientation in model-driven engineering for distributed industrial mechatronics systems}},
  journal  = {Mechatronics},
  year     = {2014},
  volume   = {24},
  number   = {7},
  pages    = {844--865},
  issn     = {0957-4158},
  abstract = {Recent advances in technology enable the creation of complex industrial systems comprising mechanical, electrical, and logical – software – components. It is clear that new project techniques are demanded to support the design of such systems. At design phase, it is extremely important to raise abstraction level in earlier stages of product development in order to deal with such a complexity in an efficient way. This paper discusses Model Driven Engineering (MDE) applied to design industrial mechatronics systems. An aspect-oriented MDE approach is presented by means of a real-world case study, comprising requirements engineering up to code generation. An assessment of two well-known high-level paradigms, namely Aspect- and Object-Oriented paradigms, is deeply presented. Their concepts are applied at every design step of an embedded and real-time mechatronics system, specifically for controlling a product assembler industrial cell. The handling of functional and non-functional requirements (at modeling level) using aspects and objects is further emphasized. Both designs are compared using a set of software engineering metrics, which were adapted to be applied at modeling level. Particularly, the achieved results show the suitability of each paradigm for the system specification in terms of reusability quality of model elements. Focused on the generated code for each case study, statistics depicted an improvement in number of lines using aspects.},
  annote   = {1. Model-Based Mechatronic System Design 2. Model Based Engineering},
  doi      = {https://doi.org/10.1016/j.mechatronics.2013.12.008},
  keywords = {Aspect Oriented Software Development (AOSD),Code generation,Design automation,Embedded and real-time system,Industrial mechatronics system,Model-Driven Engineering (MDE)},
  url      = {http://www.sciencedirect.com/science/article/pii/S0957415813002420},
}

@Article{SCHATZ2007171,
  author   = {Sch{\"{a}}tz, Bernhard},
  title    = {{Combining Product Lines and Model-Based Development}},
  journal  = {Electronic Notes in Theoretical Computer Science},
  year     = {2007},
  volume   = {182},
  pages    = {171--186},
  issn     = {1571-0661},
  abstract = {Using model-based development has shown to increase efficiency and effectiveness of software production. However, with software as an integral part of products with customized functionalities, explicit treatment of product lines is increasingly becoming necessary to cope with this additional complexity. To combine these aspects, which are generally considered only in isolation, a conceptual model addressing both the aspects of product-line engineering as well as aspects of component systems is introduced, and the consequences concerning product line identification and instantiation are illustrated.},
  annote   = {Proceedings of the Third International Workshop on Formal Aspects of Component Software (FACS 2006)},
  doi      = {https://doi.org/10.1016/j.entcs.2006.09.038},
  keywords = {Product line,conceptual model,consistency,meta model,model-based,variability},
  url      = {http://www.sciencedirect.com/science/article/pii/S1571066107003933},
}

@Article{Bettini2013218,
  author   = {Bettini, Lorenzo and Damiani, Ferruccio and Geilmann, Kathrin and Sch{\"{a}}fer, Jan},
  title    = {{Combining traits with boxes and ownership types in a Java-like setting}},
  journal  = {Science of Computer Programming},
  year     = {2013},
  volume   = {78},
  number   = {2},
  pages    = {218--247},
  issn     = {0167-6423},
  abstract = {The box model is a lightweight component model for the object-oriented paradigm, which structures the flat object-heap into hierarchical runtime components called boxes. Boxes have clear runtime boundaries that divide the objects of a box into objects that can be used to interact with the box (the boundary objects) and objects that are encapsulated and represent the state of the box (the local objects). The distinction into local and boundary objects is statically achieved by an ownership type system for boxes that uses domain annotations to classify objects into local and boundary objects and that guarantees that local objects can never be directly accessed by the context of a box. A trait is a set of methods divorced from any class hierarchy. Traits are units of fine-grained reuse that can be composed together to form classes or other traits. This paper integrates traits into an ownership type system for boxes. This combination is fruitful in two ways: it can statically guarantee encapsulation of objects and still provide fine-grained reuse among classes that goes beyond the possibilities of standard inheritance. It also solves a specific problem of the box ownership type system: namely that box classes cannot inherit from standard classes (and vice versa), and thus code sharing between these two kinds of classes was not possible in this setting so far. We present an ownership type system and the corresponding soundness proofs that guarantee encapsulation of objects in an object-oriented language with traits. },
  annote   = {Coordination 2010},
  doi      = {https://doi.org/10.1016/j.scico.2011.10.006},
  keywords = {Boxes,Featherweight Java,Ownership types,Traits},
  url      = {http://www.sciencedirect.com/science/article/pii/S0167642311001833},
}

@Article{Soares20131006,
  author   = {Soares, Gustavo and Gheyi, Rohit and Murphy-Hill, Emerson and Johnson, Brittany},
  title    = {{Comparing approaches to analyze refactoring activity on software repositories}},
  journal  = {Journal of Systems and Software},
  year     = {2013},
  volume   = {86},
  number   = {4},
  pages    = {1006--1022},
  issn     = {0164-1212},
  abstract = {Some approaches have been used to investigate evidence on how developers refactor their code, whether refactorings activities may decrease the number of bugs, or improve developers' productivity. However, there are some contradicting evidence in previous studies. For instance, some investigations found evidence that if the number of refactoring changes increases in the preceding time period the number of defects decreases, different from other studies. They have used different approaches to evaluate refactoring activities. Some of them identify committed behavior-preserving transformations in software repositories by using manual analysis, commit messages, or dynamic analysis. Others focus on identifying which refactorings are applied between two programs by using manual inspection or static analysis. In this work, we compare three different approaches based on manual analysis, commit message (Ratzinger's approach) and dynamic analysis (SafeRefactor's approach) to detect whether a pair of versions determines a refactoring, in terms of behavioral preservation. Additionally, we compare two approaches (manual analysis and Ref-Finder) to identify which refactorings are performed in each pair of versions. We perform both comparisons by evaluating their accuracy, precision, and recall in a randomly selected sample of 40 pairs of versions of JHotDraw, and 20 pairs of versions of Apache Common Collections. While the manual analysis presents the best results in both comparisons, it is not as scalable as the automated approaches. Ratzinger's approach is simple and fast, but presents a low recall; differently, SafeRefactor is able to detect most applied refactorings, although limitations in its test generation backend results for some kinds of subjects in low precision values. Ref-Finder presented a low precision and recall in our evaluation. },
  annote   = {{\{}SI{\}} : Software Engineering in Brazil: Retrospective and Prospective Views},
  doi      = {https://doi.org/10.1016/j.jss.2012.10.040},
  keywords = {Automated analysis,Manual analysis,Refactoring,Repository},
  url      = {http://www.sciencedirect.com/science/article/pii/S016412121200297X},
}

@Article{ReinhartzBerger2013320,
  author   = {Reinhartz-Berger, Iris and Sturm, Arnon and Wand, Yair},
  title    = {{Comparing functionality of software systems: An ontological approach}},
  journal  = {Data {\&} Knowledge Engineering},
  year     = {2013},
  volume   = {87},
  pages    = {320--338},
  issn     = {0169-023X},
  abstract = {Abstract Organizations can reduce the costs and enhance the quality of required software by adapting existing software systems. Software adaptation decisions often involve comparing alternatives on two criteria: (1) how well a system meets users' requirements and (2) the effort required for adapting the system. These criteria reflect two points of view — of users and of developers. Common to both views is the notion of functionality, which software developers have traditionally used for effort estimation utilizing concepts such as function points. However, users involved in selecting systems are not necessarily familiar with such concepts. We propose an approach for comparing software functionality from users' point of view. The approach employs ontological concepts to define functionality in terms of system behaviors. To evaluate whether or not the approach is also usable by software developers, we conducted an exploratory experiment. In the experiment, software engineering students ranked descriptions of software systems on the amount of changes needed to adapt the systems to given requirements. The results demonstrated that the ontological approach was usable after a short training and provided results comparable to ranking done by expert software developers. We also compared the ontological approach to a method which employed function point concepts. The results showed no statistically significant differences in performance, but there seemed to be an advantage to the ontological approach for cases that were difficult to analyze. Moreover, it took less time to apply the ontological approach than the function point-based approach, and the difference was statistically significant. },
  doi      = {https://doi.org/10.1016/j.datak.2012.09.005},
  keywords = {Development effort estimation,Function point analysis,Ontologies,Requirements engineering,Software comparison,Variability management},
  url      = {http://www.sciencedirect.com/science/article/pii/S0169023X12001073},
}

@Article{Bauer2016,
  author   = {Bauer, Veronika and Vetro', Antonio},
  title    = {{Comparing reuse practices in two large software-producing companies}},
  journal  = {Journal of Systems and Software},
  year     = {2016},
  volume   = {117},
  pages    = {545--582},
  issn     = {01641212},
  abstract = {Context Reuse can improve productivity and maintainability in software development. Research has proposed a wide range of methods and techniques. Are these successfully adopted in practice? Objective We propose a preliminary answer by integrating two in-depth empirical studies on software reuse at two large software-producing companies. Method We compare and interpret the study results with a focus on reuse practices, effects, and context. Results Both companies perform pragmatic reuse of code produced within the company, not leveraging other available artefacts. Reusable entities are retrieved from a central repository, if present. Otherwise, direct communication with trusted colleagues is crucial for access. Reuse processes remain implicit and reflect the development style. In a homogeneous infrastructure-supported context, participants strongly agreed on higher development pace and less maintenance effort as reuse benefits. In a heterogeneous context with fragmented infrastructure, these benefits did not materialize. Neither case reports statistically significant evidence of negative side effects of reuse nor inhibitors. In both cases, a lack of reuse led to duplicate implementations. Conclusion Technological advances have improved the way reuse concepts can be applied in practice. Homogeneity in development process and tool support seem necessary preconditions. Developing and adopting adequate reuse strategies in heterogeneous contexts remains challenging.},
  doi      = {10.1016/j.jss.2016.03.067},
  file     = {:Users/mac/Downloads/Comparing reuse practices in two large software-producing companies.pdf:pdf},
  isbn     = {0307-904X},
  keywords = {Empirical,Software engineering,Software reuse,Survey research,Technology transfer},
}

@Article{Marcolino2017617,
  author   = {Marcolino, A S and {Oliveira E.}, Jr.},
  title    = {{Comparing SMarty and PLUS for variability identification and representation at product-line UML class level: A controlled quasi-experiment}},
  journal  = {Journal of Computer Science},
  year     = {2017},
  volume   = {13},
  number   = {11},
  pages    = {617--632},
  abstract = {Although variability management is one of the main activities of software product lines, current literature provides almost no empirical evaluations on variability management approaches based on UML. This paper aims at experimentally comparing two approaches and picks SMarty and PLUS as representative examples. Such comparison takes into account their effectiveness of expressing correctly and incorrectly variabilities in UML class diagrams. We used a 2×2 factorial design for this study. We calculated and analyzed data from participants using the T-Test. The Spearman technique supported correlation of the effectiveness of the approaches and the participants prior variability knowledge. In general, PLUS was more effective than SMarty. Generalization of results is not possible as this is an incipient evidence of PLUS and SMarty effectiveness based on graduate students and lecturers. However, counting on students and lecturers provides several contributions as we discuss in this paper. {\textcopyright} 2017 Anderson S. Marcolino and Edson OliveiraJr.},
  annote   = {cited By 0},
  doi      = {10.3844/jcssp.2017.617.632},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037031346{\&}doi=10.3844{\%}2Fjcssp.2017.617.632{\&}partnerID=40{\&}md5=bedec9dff77e55b39451bebcba04df39},
}

@Article{Taulavuori2004535,
  author   = {Taulavuori, Anne and Niemel{\"{a}}, Eila and Kallio, P{\"{a}}ivi},
  title    = {{Component documentation—a key issue in software product lines}},
  journal  = {Information and Software Technology},
  year     = {2004},
  volume   = {46},
  number   = {8},
  pages    = {535--546},
  issn     = {0950-5849},
  abstract = {Product lines embody a strategic reuse of both intellectual effort and existing artefacts, such as software architectures and components. Third-party components are increasingly being used in product line based software engineering, in which case the integration is controlled by the product line architecture. However, the software integrators have difficulties in finding out the capabilities of components, because components are not documented in a standard way. Documentation is often the only way of assessing the applicability, credibility and quality of a third-party component. Our contribution is a standard documentation pattern for software components. The pattern provides guidelines and structure for component documentation and ensures the quality of documentation. The pattern has been validated by applying and analysing it in practice. },
  doi      = {https://doi.org/10.1016/j.infsof.2003.10.004},
  file     = {:Users/mac/Library/Application Support/Mendeley Desktop/Downloaded/Taulavuori, Niemel{\"{a}}, Kallio - 2004 - Component documentation—a key issue in software product lines.pdf:pdf},
  keywords = {Component documentation,Software product line,Third-party component},
  url      = {http://www.sciencedirect.com/science/article/pii/S0950584903002131},
}

@Article{Tizzei2011121,
  author   = {Tizzei, Leonardo P and Dias, Marcelo and Rubira, Cec{\'{i}}lia M F and Garcia, Alessandro and Lee, Jaejoon},
  title    = {{Components meet aspects: Assessing design stability of a software product line}},
  journal  = {Information and Software Technology},
  year     = {2011},
  volume   = {53},
  number   = {2},
  pages    = {121--136},
  issn     = {0950-5849},
  abstract = {Context It is important for Product Line Architectures (PLA) to remain stable accommodating evolutionary changes of stakeholder's requirements. Otherwise, architectural modifications may have to be propagated to products of a product line, thereby increasing maintenance costs. A key challenge is that several features are likely to exert a crosscutting impact on the {\{}PLA{\}} decomposition, thereby making it more difficult to preserve its stability in the presence of changes. Some researchers claim that the use of aspects can ameliorate instabilities caused by changes in crosscutting features. Hence, it is important to understand which aspect-oriented (AO) and non-aspect-oriented techniques better cope with {\{}PLA{\}} stability through evolution. Objective This paper evaluates the positive and negative change impact of component and aspect based design on PLAs. The objective of the evaluation is to assess how aspects and components promote {\{}PLA{\}} stability in the presence of various types of evolutionary change. To support a broader analysis, we also evaluate the {\{}PLA{\}} stability of a hybrid approach (i.e. combined use of aspects and components) against the isolated use of component-based, OO, and {\{}AO{\}} approaches. Method An quantitative and qualitative analysis of {\{}PLA{\}} stability which involved four different implementations of a PLA: (i) an {\{}OO{\}} implementation, (ii) an {\{}AO{\}} implementation, (iii) a component-based implementation, and (iv) a hybrid implementation where both components and aspects are employed. Each implementation has eight releases and they are functionally equivalent. We used conventional metrics suites for change impact and modularity to measure the architecture stability evaluation of the 4 implementations. Results The combination of aspects and components promotes superior {\{}PLA{\}} resilience than the other {\{}PLAs{\}} in most of the circumstances. Conclusion It is concluded that the combination of aspects and components supports the design of high cohesive and loosely coupled PLAs. It also contributes to improve modularity by untangling feature implementation. },
  doi      = {https://doi.org/10.1016/j.infsof.2010.08.007},
  keywords = {Aspect-Oriented Software Development,Component-based Development,Design stability,Product Line Architecture},
  url      = {http://www.sciencedirect.com/science/article/pii/S0950584910001564},
}

@Article{Krüger2018402,
  author   = {Kr{\"{u}}ger, J and Pinnecke, M and Kenner, A and Kruczek, C and Benduhn, F and Leich, T and Saake, G},
  title    = {{Composing annotations without regret? Practical experiences using FeatureC}},
  journal  = {Software - Practice and Experience},
  year     = {2018},
  volume   = {48},
  number   = {3},
  pages    = {402--427},
  abstract = {Software product lines enable developers to derive similar products from a common code base. Existing implementation techniques can be categorized as composition-based and annotation-based approaches, with both approaches promising complementary benefits. However, annotation-based approaches are commonly used in practice despite composition allowing physical separation of features and, thus, improving traceability and maintenance. A main hindrance to migrate annotated systems toward a composition-based product line is the challenging and time-consuming transformation task. For a company, it is difficult to predict the corresponding costs, and a successful outcome is uncertain. To overcome such problems, a solution proposed by the previous work is to use a hybrid approach, utilizing composition and annotation simultaneously. Based on this idea, we introduce a stepwise migration process from annotation-based toward composition-based approaches to lower the adoption barrier of composition. This process itself is independent of used implementation techniques and enables developers to incrementally migrate toward composition. We support our approach with detailed examples by partially migrating a real-world system. In detail, we describe the following: (1) our migration process, (2) its application on a real-world system, and (3) discuss practical challenges we face. We implemented the proposed approach and show that appropriate tool support helps to migrate toward composition-based product lines. Based on the case study, we show that the hybrid product lines work correctly and can compete with the performance of the original annotated system. However, the results also illustrate open issues that have to be solved to apply such migrations in practice. Copyright {\textcopyright} 2017 John Wiley {\&} Sons, Ltd.},
  annote   = {cited By 3},
  doi      = {10.1002/spe.2525},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029425150{\&}doi=10.1002{\%}2Fspe.2525{\&}partnerID=40{\&}md5=3822d0535163ebd0fd64133619fb2359},
}

@Article{Cledou201751,
  author   = {Cledou, G and Proen{\c{c}}a, J and {Soares Barbosa}, L},
  title    = {{Composing Families of Timed Automata}},
  journal  = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year     = {2017},
  volume   = {10522 LNCS},
  pages    = {51--66},
  abstract = {Featured Timed Automata (FTA) is a formalism that enables the verification of an entire Software Product Line (SPL), by capturing its behavior in a single model instead of product-by-product. However, it disregards compositional aspects inherent to SPL development. This paper introduces Interface FTA (IFTA), which extends FTA with variable interfaces that restrict the way automata can be composed, and with support for transitions with atomic multiple actions, simplifying the design. To support modular composition, a set of Reo connectors are modelled as IFTA. This separation of concerns increases reusability of functionality across products, and simplifies modelling, maintainability, and extension of SPLs. We show how IFTA can be easily translated into FTA and into networks of Timed Automata supported by UPPAAL. We illustrate this with a case study from the electronic government domain. {\textcopyright} 2017, IFIP International Federation for Information Processing.},
  annote   = {cited By 2; Conference of 7th International Conference on Fundamentals of Software Engineering, FSEN 2017 ; Conference Date: 26 April 2017 Through 28 April 2017; Conference Code:200239},
  doi      = {10.1007/978-3-319-68972-2_4},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032860604{\&}doi=10.1007{\%}2F978-3-319-68972-2{\_}4{\&}partnerID=40{\&}md5=b64f1611442effcb0511e108e1f86faf},
}

@Article{Cubo2013326,
  author   = {Cubo, J and Gamez, N and Fuentes, L and Pimentel, E},
  title    = {{Composition and self-adaptation of service-based systems with feature models}},
  journal  = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year     = {2013},
  volume   = {7925 LNCS},
  pages    = {326--342},
  abstract = {The adoption of mechanisms for reusing software in pervasive systems has not yet become standard practice. This is because the use of pre-existing software requires the selection, composition and adaptation of prefabricated software parts, as well as the management of some complex problems such as guaranteeing high levels of efficiency and safety in critical domains. In addition to the wide variety of services, pervasive systems are composed of many networked heterogeneous devices with embedded software. In this work, we promote the safe reuse of services in service-based systems using two complementary technologies, Service-Oriented Architecture and Software Product Lines. In order to do this, we extend both the service discovery and composition processes defined in the DAMASCo framework, which currently does not deal with the service variability that constitutes pervasive systems. We use feature models to represent the variability and to self-adapt the services during the composition in a safe way taking context changes into consideration. We illustrate our proposal with a case study related to the driving domain of an Intelligent Transportation System, handling the context information of the environment. {\textcopyright} 2013 Springer-Verlag Berlin Heidelberg.},
  annote   = {cited By 7},
  doi      = {10.1007/978-3-642-38977-1_25},
  keywords = {Computer software reusability; Information servic,Context information; Feature models; Heterogeneous,Software design},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884877705{\&}doi=10.1007{\%}2F978-3-642-38977-1{\_}25{\&}partnerID=40{\&}md5=f81d9530e644ff736d29215fe0d9a293},
}

@Article{Boucké20102108,
  author   = {Bouck{\'{e}}, Nelis and Weyns, Danny and Holvoet, Tom},
  title    = {{Composition of architectural models: Empirical analysis and language support}},
  journal  = {Journal of Systems and Software},
  year     = {2010},
  volume   = {83},
  number   = {11},
  pages    = {2108--2127},
  issn     = {0164-1212},
  abstract = {Managing the architectural description (AD) of a complex software system and maintaining consistency among the different models is a demanding task. To understand the underlying problems, we analyse several non-trivial software architectures. The empirical study shows that a substantial amount of information of {\{}ADs{\}} is repeated, mainly by integrating information of different models in new models. Closer examination reveals that the absence of rigorously specified dependencies among models and the lack of support for automated composition of models are primary causes of management and consistency problems in software architecture. To tackle these problems, we introduce an approach in which compositions of models, together with relations among models, are explicitly supported in the ADL. We introduce these concepts formally and discuss a proof-of-concept instantiation of composition in xADL and its supporting tools. The approach is evaluated by comparing the original and revised {\{}ADs{\}} in an empirical study. The study indicates that our approach reduces the number of manually specified elements by 29{\%}, and reduces the number of manual changes to elements for several realistic change scenarios by 52{\%}. },
  annote   = {Interplay between Usability Evaluation and Software Development},
  doi      = {https://doi.org/10.1016/j.jss.2010.06.011},
  keywords = {Architectural description language (ADL),Architectural models,Composition,Empirical analysis,Relations,Software architecture},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121210001639},
}

@Article{Reinhartz-Berger2014678,
  author   = {Reinhartz-Berger, I and Sturm, A},
  title    = {{Comprehensibility of UML-based software product line specifications A controlled experiment}},
  journal  = {Empirical Software Engineering},
  year     = {2014},
  volume   = {19},
  number   = {3},
  pages    = {678--713},
  abstract = {Software Product Line Engineering (SPLE) deals with developing artifacts that capture the common and variable aspects of software product families. Domain models are one kind of such artifacts. Being developed in early stages, domain models need to specify commonality and variability and guide the reuse of the artifacts in particular software products. Although different modeling methods have been proposed to manage and support these activities, the assessment of these methods is still in an inceptive stage. In this work, we examined the comprehensibility of domain models specified in ADOM, a UML-based SPLE method. In particular, we conducted a controlled experiment in which 116 undergraduate students were required to answer comprehension questions regarding a domain model that was equipped with explicit reuse guidance and/or variability specification. We found that explicit specification of reuse guidance within the domain model helped understand the model, whereas explicit specification of variability increased comprehensibility only to a limited extent. Explicit specification of both reuse guidance and variability often provided intermediate results, namely, results that were better than specification of variability without reuse guidance, but worse than specification of reuse guidance without variability. All these results were perceived in different UML diagram types, namely, use case, class, and sequence diagrams and for different commonality-, variability-, and reuse-related aspects. {\textcopyright} 2012 Springer Science+Business Media, LLC.},
  annote   = {cited By 3},
  doi      = {10.1007/s10664-012-9234-8},
  keywords = {Computer software reusability,Domain model; Empirical evaluations; Software pro,Experiments; Query languages; Software design; Spe},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899943828{\&}doi=10.1007{\%}2Fs10664-012-9234-8{\&}partnerID=40{\&}md5=8c6715fe527f714fb9a98aa9dd230798},
}

@Article{Echeverría2016476,
  author   = {Echeverr{\'{i}}a, J and P{\'{e}}rez, F and Cetina, C and Pastor, {\'{O}}},
  title    = {{Comprehensibility of variability in model fragments for product configuration}},
  journal  = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year     = {2016},
  volume   = {9694},
  pages    = {476--490},
  abstract = {The ability to manage variability in software has become crucial to overcome the complexity and variety of systems. To this end, a comprehensible representation of variability is important. Nevertheless, in previous works, difficulties have been detected to understand variability in an industrial environment. Specifically, domain experts had difficulty understanding variability in model fragments to produce the software for their products. Hence, the aim of this paper is to further investigate these difficulties by conducting an experiment in which participants deal with variability in order to achieve their desired product configurations. Our results show new insights into product configuration which suggest next steps to improve general variability modeling approaches, and therefore promoting the adoption of these approaches in industry. {\textcopyright} Springer International Publishing Switzerland 2016.},
  annote   = {cited By 0},
  doi      = {10.1007/978-3-319-39696-529},
  keywords = {Domain experts; Industrial environments; Product,Information systems,Software design; Systems engineering},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976647589{\&}doi=10.1007{\%}2F978-3-319-39696-529{\&}partnerID=40{\&}md5=cc20ce4b870548c351a0f40e87a08572},
}

@Article{Farahani2016301,
  author   = {Farahani, E D and Habibi, J},
  title    = {{Comprehensive configuration management model for software product line}},
  journal  = {International Journal of Control Theory and Applications},
  year     = {2016},
  volume   = {9},
  number   = {25},
  pages    = {301--322},
  abstract = {In Software Product Line (SPL), Configuration Management (CM) is a multi-dimensional problem. On the one hand, the Core Assets that constitute a configuration need to be managed, and on the other hand, each product in the product line that is built using a configuration must be managed, and furthermore, the management of all these configurations must be coordinated under a single process. Therefore, CM for product lines is more complex than for single systems. The CM of any software system involves four closely related activities: Change Management (ChM), Version Management (VM), Build Management (BM) and Release Management (RM). The aim of this paper is to provide a comprehensive CM model comprising four main sub-models for all CM-related activities required for evolutionary based SPL system development and maintenance. The proposed models support any level of aggregation in SPLs and have been applied to Mobile SPL as a case study. {\textcopyright} International Science Press.},
  annote   = {cited By 0},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007028727{\&}partnerID=40{\&}md5=3164e6050cad5ccf1db519d1ff64c1c8},
}

@Article{Tan:2009:CDM:1571629.1571630,
  author    = {Tan, Hee Beng Kuan and Zhao, Yuan and Zhang, Hongyu},
  title     = {{Conceptual Data Model-based Software Size Estimation for Information Systems}},
  journal   = {ACM Trans. Softw. Eng. Methodol.},
  year      = {2009},
  volume    = {19},
  number    = {2},
  pages     = {4:1----4:37},
  issn      = {1049-331X},
  address   = {New York, NY, USA},
  doi       = {10.1145/1571629.1571630},
  keywords  = {Software sizing,conceptual data model,line of code (LOC),multiple linear regression model},
  publisher = {ACM},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/1571629.1571630},
}

@Article{Svahnberg2002143,
  author   = {Svahnberg, M and Mattsson, M},
  title    = {{Conditions and restrictions for product line generation migration}},
  journal  = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year     = {2002},
  volume   = {2290},
  pages    = {143--154},
  abstract = {In this paper we describe a case study of a company in the domain of automatic guided vehicles (AGVs) that is in the process of migrating from a previous generation of software product line, which has mainly been centered around hardware, into a new product line generation, which will be software-centered. We describe the issues motivating this transition, and the factors that complicate it. Moreover, we present a three stage process for migrating into a new software product line. This process is currently initiated in collaboration with the aforementioned company. {\textcopyright} Springer-Verlag Berlin Heidelberg 2002.},
  annote   = {cited By 2},
  keywords = {Automatic guided vehicles; Computer software,Network architecture,Product-lines; Software Product Line; Three-stage},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-21644455855{\&}partnerID=40{\&}md5=94b9321031ab3b8a77cf09a3cc6a56ff},
}

@Article{LaRosa2011313,
  author   = {Rosa, Marcello La and Dumas, Marlon and ter Hofstede, Arthur H M and Mendling, Jan},
  title    = {{Configurable multi-perspective business process models}},
  journal  = {Information Systems},
  year     = {2011},
  volume   = {36},
  number   = {2},
  pages    = {313--340},
  issn     = {0306-4379},
  abstract = {A configurable process model provides a consolidated view of a family of business processes. It promotes the reuse of proven practices by providing analysts with a generic modeling artifact from which to derive individual process models. Unfortunately, the scope of existing notations for configurable process modeling is restricted, thus hindering their applicability. Specifically, these notations focus on capturing tasks and control-flow dependencies, neglecting equally important ingredients of business processes such as data and resources. This research fills this gap by proposing a configurable process modeling notation incorporating features for capturing resources, data and physical objects involved in the performance of tasks. The proposal has been implemented in a toolset that assists analysts during the configuration phase and guarantees the correctness of the resulting process models. The approach has been validated by means of a case study from the film industry. },
  annote   = {Special Issue: Semantic Integration of Data, Multimedia, and Services},
  doi      = {https://doi.org/10.1016/j.is.2010.07.001},
  keywords = {Business process,Configurable process model,EPC},
  url      = {http://www.sciencedirect.com/science/article/pii/S0306437910000633},
}

@Article{Farahani2016433,
  author   = {Farahani, E D and Habibi, J},
  title    = {{Configuration Management Model in Evolutionary Software Product Line}},
  journal  = {International Journal of Software Engineering and Knowledge Engineering},
  year     = {2016},
  volume   = {26},
  number   = {3},
  pages    = {433--455},
  abstract = {In Software Product Line (SPL), Configuration Management (CM) is a multi-dimensional problem. On the one hand, the Core Assets that constitute a configuration need to be managed, and on the other hand, each product in the product line that is built using a configuration must be managed, and furthermore, the management of all these configurations must be coordinated under a single process. Therefore, CM for product lines is more complex than for single systems. The CM of any software system involves four closely related activities: Change Management (ChM), Version Management (VM), System Building (SB) and Release Management (RM) [I. Sommerville, Software Engineering, 9th edn. (Addison-Wesley, 2010)]. The aim of this paper is to provide ChM and VM models for evolutionary-based SPL system development and maintenance. The proposed models support any level of aggregation in SPLs and have been applied to Mobile SPL as a case study. {\textcopyright} 2016 World Scientific Publishing Company.},
  annote   = {cited By 0},
  doi      = {10.1142/S0218194016500182},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84968548424{\&}doi=10.1142{\%}2FS0218194016500182{\&}partnerID=40{\&}md5=04b82211594b36b88b4f29690caba20f},
}

@Article{KOEHLER200877,
  author   = {Koehler, Christian and Lazovik, Alexander and Arbab, Farhad},
  title    = {{Connector Rewriting with High-Level Replacement Systems}},
  journal  = {Electronic Notes in Theoretical Computer Science},
  year     = {2008},
  volume   = {194},
  number   = {4},
  pages    = {77--92},
  issn     = {1571-0661},
  abstract = {Reo is a language for coordinating autonomous components in distributed environments. Coordination in Reo is performed by circuit-like connectors, which are constructed from primitive channels with well-defined behavior. These channels are mobile, i.e. can be dynamically created and reconfigured at run-time. Based on these language features, we introduce a high-level transformation system for Reo. We show how transformations of Reo connectors can be defined using the theory of high-level replacement (HLR) systems. This leads to a powerful notion of dynamic connector reconfiguration in Reo. Moreover, the rewrite rules are naturally expressed in Reo's visual syntax for connectors. Applications of this framework are manifold, due to the generality of the field of coordination. In this paper we provide an example from the area of Service-oriented Computing.},
  annote   = {Proceedings of the 6th International Workshop on the Foundations of Coordination Languages and Software Architectures (FOCLASA 2007)},
  doi      = {https://doi.org/10.1016/j.entcs.2008.03.100},
  keywords = {Reo,adhesive categories,coordination,high-level replacement systems,model transformation,service composition},
  url      = {http://www.sciencedirect.com/science/article/pii/S1571066108002053},
}

@Article{Guo20124987,
  author   = {Guo, Jianmei and Wang, Yinglin and Trinidad, Pablo and Benavides, David},
  title    = {{Consistency maintenance for evolving feature models}},
  journal  = {Expert Systems with Applications},
  year     = {2012},
  volume   = {39},
  number   = {5},
  pages    = {4987--4998},
  issn     = {0957-4174},
  abstract = {Software product line (SPL) techniques handle the construction of customized systems. One of the most common representations of the decisions a customer can make in {\{}SPLs{\}} is feature models (FMs). An {\{}FM{\}} represents the relationships among common and variable features in an SPL. Features are a representation of the characteristics in a system that are relevant to customers. {\{}FMs{\}} are subject to change since the set of features and their relationships can change along an {\{}SPL{\}} lifecycle. Due to this evolution, the consistency of {\{}FMs{\}} may be compromised. There exist some approaches to detect and explain inconsistencies in FMs, however this process can take a long time for large FMs. In this paper we present a complementary approach to dealing with inconsistencies in {\{}FM{\}} evolution scenarios that improves the performance for existing approaches reducing the impact of change to the smallest part of an {\{}FM{\}} that changes. To achieve our goal, we formalize {\{}FMs{\}} from an ontological perspective and define constraints that must be satisfied in {\{}FMs{\}} to be consistent. We define a set of primitive operations that modify {\{}FMs{\}} and which are responsible for the {\{}FM{\}} evolution, analyzing their impact on the {\{}FM{\}} consistency. We propose a set of predefined strategies to keep the consistency for error-prone operations. As a proof-of-concept we present the results of our experiments, where we check for the effectiveness and efficiency of our approach in {\{}FMs{\}} with thousands of features. Although our approach is limited by the kinds of consistency constraints and the primitive operations we define, the experiments present a significant improvement in performance results in those cases where they are applicable. },
  doi      = {https://doi.org/10.1016/j.eswa.2011.10.014},
  keywords = {Consistency maintenance,Evolution,Feature models,Ontology,Semantics,Software product lines},
  url      = {http://www.sciencedirect.com/science/article/pii/S0957417411014990},
}

@Article{Dam2016137,
  author   = {Dam, Hoa Khanh and Egyed, Alexander and Winikoff, Michael and Reder, Alexander and Lopez-Herrejon, Roberto E},
  title    = {{Consistent merging of model versions}},
  journal  = {Journal of Systems and Software},
  year     = {2016},
  volume   = {112},
  pages    = {137--155},
  issn     = {0164-1212},
  abstract = {Abstract While many engineering tasks can, and should be, manageable independently, it does place a great burden on explicit collaboration needs—including the need for frequent and incremental merging of artifacts that software engineers manipulate using these tools. State-of-the-art merging techniques are often limited to textual artifacts (e.g., source code) and they are unable to discover and resolve complex merging issues beyond simple conflicts. This work focuses on the merging of models where we consider not only conflicts but also arbitrary syntactic and semantic consistency issues. Consistent artifacts are merged fully automatically and only inconsistent/conflicting artifacts are brought to the users' attention, together with a systematic proposal of how to resolve them. Our approach is neutral with regard to who made the changes and hence reduces the bias caused by any individual engineer's limited point of view. Our approach also applies to arbitrary design or models, provided that they follow a well-defined metamodel with explicit constraints—the norm nowadays. The extensive empirical evaluation suggests that our approach scales to practical settings. },
  doi      = {https://doi.org/10.1016/j.jss.2015.06.044},
  keywords = {Inconsistency management,Model merging,Model versioning},
  url      = {http://www.sciencedirect.com/science/article/pii/S016412121500134X},
}

@Article{Galster2013428,
  author        = {Galster, Matthias and Avgeriou, Paris and Tofan, Dan},
  title         = {{Constraints for the design of variability-intensive service-oriented reference architectures – An industrial case study}},
  journal       = {Information and Software Technology},
  year          = {2013},
  volume        = {55},
  number        = {2},
  pages         = {428--441},
  issn          = {0950-5849},
  abstract      = {Context Service-oriented architecture has become a widely used concept in software industry. However, we currently lack support for designing variability-intensive service-oriented systems. Such systems could be used in different environments, without the need to design them from scratch. To support the design of variability-intensive service-oriented systems, reference architectures that facilitate variability in instantiated service-oriented architectures can help. Objective The design of variability-intensive service-oriented reference architectures is subject to specific constraints. Architects need to know these constraints when designing such reference architectures. Our objective is to identify these constraints. Method An exploratory case study was performed in the context of local e-government in the Netherlands to study constraints from the perspective of (a) the users of a variability-intensive service-oriented system (municipalities that implement national laws), and (b) the implementing organizations (software vendors). We collected data through interviews with representatives from five organizations, document analyses and expert meetings. Results We identified ten constraints (e.g., organizational constraints, integration-related constraints) which affect the process of designing reference architectures for variability-intensive service-oriented systems. Also, we identified how stakeholders are affected by these constraints, and how constraints are specific to the case study domain. Conclusions Our results help design variability-intensive service-oriented reference architectures. Furthermore, our results can be used to define processes to design such reference architectures.},
  annote        = {Special Section: Component-Based Software Engineering (CBSE), 2011},
  doi           = {https://doi.org/10.1016/j.infsof.2012.09.011},
  keywords      = {Case study,Reference architectures,SOA,Service-oriented architecture,Variability,case study,e-Government},
  mendeley-tags = {case study},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584912002054},
}

@Article{Wulf-Hadash2013354,
  author   = {Wulf-Hadash, O and Reinhartz-Berger, I},
  title    = {{Constructing domain knowledge through cross product line analysis}},
  journal  = {Lecture Notes in Business Information Processing},
  year     = {2013},
  volume   = {147 LNBIP},
  pages    = {354--369},
  abstract = {Nowadays many companies develop and maintain families of systems, termed product lines (PL), rather than individual systems. Furthermore, due to increase in market competition and the dynamic nature of companies' emergence, several PLs may exist under the same roof. These PLs may be independently developed taking into consideration different sets of products and requirements. Thus the developed artifacts potentially have a different and partial view of the domain. Moreover, future development and maintenance of the different PLs may require consolidating the various artifacts into a single coherent one. In this work, we present a method for constructing domain knowledge through cross PL analysis. This method uses similarity metrics, text clustering, and mining techniques in order to create domain models and recommend on improvements to the existing PLs artifacts. Preliminary results reveal that the method outcomes reflect human perception of the examined domain. {\textcopyright} 2013 Springer-Verlag.},
  annote   = {cited By 2},
  doi      = {10.1007/978-3-642-38484-4_25},
  keywords = {Competition; Systems analysis; Systems engineering,Domain analysis; Empirical evaluations; Feature c,Industry},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879855069{\&}doi=10.1007{\%}2F978-3-642-38484-4{\_}25{\&}partnerID=40{\&}md5=dd7ff26179d87acb7e982ace6933682d},
}

@Article{Horcas201915,
  author   = {Horcas, J.-M. and Pinto, M and Fuentes, L},
  title    = {{Context-Aware Energy-Efficient Applications for Cyber-Physical Systems}},
  journal  = {Ad Hoc Networks},
  year     = {2019},
  volume   = {82},
  pages    = {15--30},
  abstract = {Software systems have a strong impact on the energy consumption of the hardware they use. This is especially important for cyber-physical systems where power consumption strongly influences the battery life. For this reason, software developers should be more aware of the energy consumed by their systems. Moreover, software systems should be developed to adapt their behavior to minimize the energy consumed during their execution. This can be done by monitoring the usage context of the system and having runtime support to react to those changes that impact the energy footprint negatively. Although both the hardware and the software parts of cyber-physical systems can be adapted to reduce its energy consumption, this paper focuses on software adaptation. Concretely, the paper illustrates how to address the problem of developing context-aware energy-efficient applications using a Green Eco-Assistant that makes use of advanced software engineering methods, such as Dynamic Software Product Lines and Separation of Concerns. The main steps of our approach are illustrated by applying them to a cyber-physical system case study. {\textcopyright} 2018 Elsevier B.V.},
  annote   = {cited By 0},
  doi      = {10.1016/j.adhoc.2018.08.004},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051643345{\&}doi=10.1016{\%}2Fj.adhoc.2018.08.004{\&}partnerID=40{\&}md5=d7dfa3119ecae9d24de0bc4a4401a92a},
}

@Article{Ubayashi20132331,
  author   = {Ubayashi, Naoyasu and Nakajima, Shin and Hirayama, Masayuki},
  title    = {{Context-dependent product line engineering with lightweight formal approaches}},
  journal  = {Science of Computer Programming},
  year     = {2013},
  volume   = {78},
  number   = {12},
  pages    = {2331--2346},
  issn     = {0167-6423},
  abstract = {This paper proposes a new style of product line engineering methods. It focuses on constructing embedded systems that take into account the contexts such as the external physical environments. In current product line development projects, Feature Analysis is mainly conducted from the viewpoint of system configurations: how hardware and software components are configured to constitute a system. In most cases, contexts are not considered explicitly. As a result, unexpected and unfavorable behavior might emerge in a system if a developer does not recognize any possible conflicting combinations between the system and contexts. To deal with this problem, this paper provides the notion of a context-dependent product line, which is composed of the system and context lines. The former is obtained by analyzing a family of systems. The latter is obtained by analyzing features of contexts associated to the systems. The system and context lines contain reusable core assets. The configuration of selected system components and contexts can be formally checked at the specification level. In this paper, we show a development process that includes the creation of both product line assets as well as context assets. },
  annote   = {Special Section on International Software Product Line Conference 2010 and Fundamentals of Software Engineering (selected papers of {\{}FSEN{\}} 2011)},
  doi      = {https://doi.org/10.1016/j.scico.2012.06.006},
  keywords = {Context analysis,Formal methods,Product line engineering},
  url      = {http://www.sciencedirect.com/science/article/pii/S0167642312001177},
}

@Article{Salvaneschi201520,
  author   = {Salvaneschi, Guido and Ghezzi, Carlo and Pradella, Matteo},
  title    = {{ContextErlang: A language for distributed context-aware self-adaptive applications}},
  journal  = {Science of Computer Programming},
  year     = {2015},
  volume   = {102},
  pages    = {20--43},
  issn     = {0167-6423},
  abstract = {Abstract Self-adaptive software modifies its behavior at run time to satisfy changing requirements in a dynamic environment. Context-oriented programming (COP) has been recently proposed as a specialized programming paradigm for context-aware and adaptive systems. {\{}COP{\}} mostly focuses on run time adaptation of the application's behavior by supporting modular descriptions of behavioral variations. However, self-adaptive applications must satisfy additional requirements, such as distribution and concurrency, support for unforeseen changes and enforcement of correct behavior in the presence of dynamic change. Addressing these issues at the language level requires a holistic design that covers all aspects and takes into account the possibly cumbersome interaction of those features, for example concurrency and dynamic change. We present ContextErlang, a {\{}COP{\}} programming language in which adaptive abstractions are seamlessly integrated with distribution and concurrency. We define ContextErlang's formal semantics, validated through an executable prototype, and we show how it supports formal proofs that the language design ensures satisfaction of certain safety requirements. We provide empirical evidence that ContextErlang is an effective solution through case studies and a performance assessment. We also show how the same design principles that lead to the development of ContextErlang can be followed to systematically design contextual extensions of other languages. A concrete example is presented concerning ContextScala. },
  doi      = {https://doi.org/10.1016/j.scico.2014.11.016},
  keywords = {Concurrency,Context,Context-oriented programming,Distribution,Self-adaptive software},
  url      = {http://www.sciencedirect.com/science/article/pii/S0167642314005577},
}

@Article{Salvaneschi20121801,
  author   = {Salvaneschi, Guido and Ghezzi, Carlo and Pradella, Matteo},
  title    = {{Context-oriented programming: A software engineering perspective}},
  journal  = {Journal of Systems and Software},
  year     = {2012},
  volume   = {85},
  number   = {8},
  pages    = {1801--1817},
  issn     = {0164-1212},
  abstract = {The implementation of context-aware systems can be supported through the adoption of techniques at the architectural level such as middlewares or component-oriented architectures. It can also be supported by suitable constructs at the programming language level. Context-oriented programming (COP) is emerging as a novel paradigm for the implementation of this kind of software, in particular in the field of mobile and ubiquitous computing. The {\{}COP{\}} paradigm tackles the issue of developing context-aware systems at the language-level, introducing ad hoc language abstractions to manage adaptations modularization and their dynamic activation. In this paper we review the state of the art in the field of {\{}COP{\}} in the perspective of the benefits that this technique can provide to software engineers in the design and implementation of context-aware applications. },
  doi      = {https://doi.org/10.1016/j.jss.2012.03.024},
  keywords = {Context,Context-awareness,Context-oriented programming},
  url      = {http://www.sciencedirect.com/science/article/pii/S016412121200074X},
}

@Article{Chen201772,
  author   = {Chen, Lianping},
  title    = {{Continuous Delivery: Overcoming adoption challenges}},
  journal  = {Journal of Systems and Software},
  year     = {2017},
  volume   = {128},
  pages    = {72--86},
  issn     = {0164-1212},
  abstract = {Abstract Continuous Delivery (CD) is a relatively new software development approach. Companies that have adopted {\{}CD{\}} have reported significant benefits. Motivated by these benefits, many companies would like to adopt CD. However, adopting {\{}CD{\}} can be very challenging for a number of reasons, such as obtaining buy-in from a wide range of stakeholders whose goals may seemingly be different from—or even conflict with—our own; gaining sustained support in a dynamic complex enterprise environment; maintaining an application development team's momentum when their application's migration to {\{}CD{\}} requires an additional strenuous effort over a long period of time; and so on. To help overcome the adoption challenges, I present six strategies: (1) selling {\{}CD{\}} as a painkiller; (2) establishing a dedicated team with multi-disciplinary members; (3) continuous delivery of continuous delivery; (4) starting with the easy but important applications; (5) visual {\{}CD{\}} pipeline skeleton; (6) expert drop. These strategies were derived from four years of experience in implementing {\{}CD{\}} at a multi-billion-euro company. Additionally, our experience led to the identification of eight further challenges for research. The information contributes toward building a body of knowledge for {\{}CD{\}} adoption. },
  doi      = {https://doi.org/10.1016/j.jss.2017.02.013},
  keywords = {Adoption,Agile Software Development,Continuous Delivery,Continuous Deployment,Continuous Software Engineering,DevOps},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121217300353},
}

@Article{SMR:SMR337,
  author        = {{Del Rosso}, Christian},
  title         = {{Continuous evolution through software architecture evaluation: a case study}},
  journal       = {Journal of Software Maintenance and Evolution: Research and Practice},
  year          = {2006},
  volume        = {18},
  number        = {5},
  pages         = {351--383},
  issn          = {1532-0618},
  doi           = {10.1002/smr.337},
  keywords      = {case study,experience-based software assessment,scenario-based software architecture assessment,software architecture assessments,software performance assessment,software product family},
  mendeley-tags = {case study},
  publisher     = {John Wiley {\&} Sons, Ltd.},
  url           = {http://dx.doi.org/10.1002/smr.337},
}

@Article{Lucena2013890,
  author   = {Lucena, Carlos and Nunes, Ingrid},
  title    = {{Contributions to the emergence and consolidation of Agent-oriented Software Engineering}},
  journal  = {Journal of Systems and Software},
  year     = {2013},
  volume   = {86},
  number   = {4},
  pages    = {890--904},
  issn     = {0164-1212},
  abstract = {Many of the issues addressed with multi-agent approaches, such as distributed coordination and self-organization, are now becoming part of industrial and business systems. However, Multiagent Systems (MASs) are still not widely adopted in industry owing to the lack of a connection between {\{}MAS{\}} and software engineering. Since 2000, there is an effort to bridge this gap and to produce software engineering techniques for agent-based systems that guide the processes of design, development and maintenance. In Brazil, Agent-oriented Software Engineering (AOSE) was first investigated by the research group in the Software Engineering Laboratory (LES) at PUC-Rio, which after one decade of study in this area has built an {\{}AOSE{\}} community. This paper presents the history of {\{}AOSE{\}} at {\{}LES{\}} by discussing the sub-areas of {\{}MAS{\}} Software Engineering research and development that have been focus of the {\{}LES{\}} research group. We give examples of relevant results and present a subset of the extensive literature the group has produced during the last decade. We also report how we faced the challenges that emerged from our research by organizing and developing a research community at the intersection of software engineering, programming and {\{}MASs{\}} with a concern for scalability of solutions. },
  annote   = {{\{}SI{\}} : Software Engineering in Brazil: Retrospective and Prospective Views},
  doi      = {https://doi.org/10.1016/j.jss.2012.09.016},
  keywords = {Agent-oriented Software Engineering,LES,Multiagent systems,PUC-Rio,SBES 25 years},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121212002567},
}

@Article{Accioly2014615,
  author   = {Accioly, P and Borba, P and Bonif{\'{a}}cio, R},
  title    = {{Controlled experiments comparing black-box testing strategies for software product lines}},
  journal  = {Journal of Universal Computer Science},
  year     = {2014},
  volume   = {20},
  number   = {5},
  pages    = {615--639},
  abstract = {SPL testing has been considered a challenging task, mainly due to the diversity of products that might be generated from an SPL. To deal with this problem, techniques for specifying and deriving product specific functional test cases have been proposed. However, there is little empirical evidence of the benefits and drawbacks of these techniques. To provide this kind of evidence, in a previous work we conducted an empirical study that compared two design techniques for black-box manual testing, a generic technique that we have observed in an industrial test execution environment, and a product specific technique whose functional test cases could be derived using any SPL approach that considers variations in functional tests. Besides revisiting the first study, here we present a second study that reinforce our findings and brings new insights to our investigation. Both studies indicate that specific test cases improve test execution productivity and quality. {\textcopyright} J.UCS.},
  annote   = {cited By 1},
  file     = {:Users/mac/ownCloud/Anita/SLR/REVISION DE JOURNALS/PAPERS{\_}D/SELECT DAVID/checked/analizados/experimentos analizados/15 Controlled experiments comparing black-box testing strategies for software product lines.pdf:pdf},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904764309{\&}partnerID=40{\&}md5=112aba25b5f4f038fd0f7ce17b051afe},
}

@Article{Coman2014124,
  author   = {Coman, Irina D and Robillard, Pierre N and Sillitti, Alberto and Succi, Giancarlo},
  title    = {{Cooperation, collaboration and pair-programming: Field studies on backup behavior}},
  journal  = {Journal of Systems and Software},
  year     = {2014},
  volume   = {91},
  pages    = {124--134},
  issn     = {0164-1212},
  abstract = {Abstract Considering that pair programming has been extensively studied for more than a decade, it can seem quite surprising that there is such a lack of consensus on both its best use and its benefits. We argue that pair programming is not a replacement of usual developer interactions, but rather a formalization and enhancement of naturally occurring interactions. Consequently, we study and classify a broader range of developer interactions, evaluating them for type, purpose and patterns of occurrence, with the aim to identify situations in which pair programming is likely to be truly needed and thus most beneficial. We study the concrete pair programming practices in both academic and industrial settings. All interactions between teammates were recorded as backup behavior activities. In each of these two projects, developers were free to interact when needed. All team interactions were self-recorded by the teammates. The analysis of the interaction tokens shows two salient features: solo work is an important component of teamwork and team interactions have two main purposes, namely cooperation and collaboration. Cooperative backup behavior occurs when a developer provides help to a teammate. Collaborative backup behavior occurs when the teammates are sharing the same goal toward solving an issue. We found that collaborative backup behavior, which occurred much less often, is close to the formal definition of pair programming. This study suggests that mandatory pair programming may be less efficient in organizations where solo work could be done and when some interactions are for cooperative activities. Based on these results, we discussed the potential implications concerning the best use of pair programming in practice, a more effective evaluation of its use, its potential benefits and emerging directions of future research. },
  doi      = {https://doi.org/10.1016/j.jss.2013.12.037},
  keywords = {Backup-behavior,Field study,Pair-programming},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121214000107},
}

@Article{Strode20121222,
  author   = {Strode, Diane E and Huff, Sid L and Hope, Beverley and Link, Sebastian},
  title    = {{Coordination in co-located agile software development projects}},
  journal  = {Journal of Systems and Software},
  year     = {2012},
  volume   = {85},
  number   = {6},
  pages    = {1222--1238},
  issn     = {0164-1212},
  abstract = {Agile software development provides a way to organise the complex task of multi-participant software development while accommodating constant project change. Agile software development is well accepted in the practitioner community but there is little understanding of how such projects achieve effective coordination, which is known to be critical in successful software projects. A theoretical model of coordination in the agile software development context is presented based on empirical data from three cases of co-located agile software development. Many practices in these projects act as coordination mechanisms, which together form a coordination strategy. Coordination strategy in this context has three components: synchronisation, structure, and boundary spanning. Coordination effectiveness has two components: implicit and explicit. The theoretical model of coordination in agile software development projects proposes that an agile coordination strategy increases coordination effectiveness. This model has application for practitioners who want to select appropriate practices from agile methods to ensure they achieve coordination coverage in their project. For the field of information systems development, this theory contributes to knowledge of coordination and coordination effectiveness in the context of agile software development. },
  annote   = {Special Issue: Agile Development},
  doi      = {https://doi.org/10.1016/j.jss.2012.02.017},
  keywords = {Agile methods,Agile software development project,Coordination Theory,Coordination effectiveness,Coordination strategy,Extreme Programming,Scrum},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121212000465},
}

@Article{Wang2015370,
  author   = {Wang, Shuai and Ali, Shaukat and Gotlieb, Arnaud},
  title    = {{Cost-effective test suite minimization in product lines using search techniques}},
  journal  = {Journal of Systems and Software},
  year     = {2015},
  volume   = {103},
  pages    = {370--391},
  issn     = {0164-1212},
  abstract = {Abstract Cost-effective testing of a product in a product line requires obtaining a set of relevant test cases from the entire test suite via test selection and minimization techniques. In this paper, we particularly focus on test minimization for product lines, which identifies and eliminates redundant test cases from test suites in order to reduce the total number of test cases to execute, thereby improving the efficiency of testing. However, such minimization may result in the minimized test suite with low test coverage, low fault revealing capability, low priority test cases, and require more time than the allowed testing budget (e.g., time) as compared to the original test suite. To deal with the above issues, we formulated the minimization problem as a search problem and defined a fitness function considering various optimization objectives based on the above issues. To assess the performance of our fitness function, we conducted an extensive empirical evaluation by investigating the fitness function with three weight-based Genetic Algorithms (GAs) and seven multi-objective search algorithms using an industrial case study and 500 artificial problems inspired from the industrial case study. The results show that Random-Weighted Genetic Algorithm (RWGA) significantly outperforms the other algorithms since {\{}RWGA{\}} can balance all the objectives together by dynamically updating weights during each generation. Based on the results of our empirical evaluation, we also implemented a tool called {\{}TEst{\}} Minimization using Search Algorithms (TEMSA) to support test minimization using various search algorithms in the context of product lines. },
  doi      = {https://doi.org/10.1016/j.jss.2014.08.024},
  keywords = {Product line,Search algorithm,Test suite minimization},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121214001757},
}

@Article{Rincón2018418,
  author   = {Rinc{\'{o}}n, L and Mart{\'{i}}nez, J.-C. and Pab{\'{o}}n, M C and Mogoll{\'{o}}n, J and Caballero, A},
  title    = {{Creating a software product line of mini-games to support language therapy}},
  journal  = {Communications in Computer and Information Science},
  year     = {2018},
  volume   = {885},
  pages    = {418--431},
  abstract = {During the therapy for the treatment of language disorders, a technology-based material approach, such as mini-games, could be introduced to motivate and interact with the patients, especially when they are children. However, adapting the therapy to the needs of each patient is essential to maintain the patient's progress and interest, which makes it difficult to use just any kind of mini-game. This paper describes the creation of a Software Product Line (SPL) to produce customized mini-games that children could use during their therapy sessions. Our experience shows that software product lines are feasible to provide the customization required in the therapy. {\textcopyright} Springer Nature Switzerland AG 2018.},
  annote   = {cited By 0; Conference of 13th Colombian Conference on Computing, CCC 2018 ; Conference Date: 26 September 2018 Through 28 September 2018; Conference Code:218769},
  doi      = {10.1007/978-3-319-98998-3_32},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054344870{\&}doi=10.1007{\%}2F978-3-319-98998-3{\_}32{\&}partnerID=40{\&}md5=b5291225399bcf4464923817650d41ca},
}

@Article{LAKKA2015132,
  author   = {Lakka, Spyridoula and Stamati, Teta and Michalakelis, Christos and Anagnostopoulos, Dimosthenis},
  title    = {{Cross-national analysis of the relation of eGovernment maturity and OSS growth}},
  journal  = {Technological Forecasting and Social Change},
  year     = {2015},
  volume   = {99},
  pages    = {132--147},
  issn     = {0040-1625},
  abstract = {The aims of this research are to explore and evaluate the nature of the relationship between open source software (OSS) and eGovernment maturity, as well as the factors impacting their development at a national level. The study proposes a theoretical framework, under the prism of which socio-economic, technological and institutional factors critical to eGovernment and OSS are revealed. The hypotheses are evaluated by means of an econometric model of simultaneous equations. In order to better gauge the results of the hypotheses, the model is evaluated over economic environments at different stages of development. Social development and OSS growth were found to be the most important facilitators for eGovernment maturity, across countries of all stages of development. Institutional quality, technological openness, freedom in press and the macro-economic environment exerted different weights of importance across different country groupings. Findings also suggest that technological infrastructure and innovation are important drivers for OSS growth across countries at all stages of development. Research results can provide useful input for research in eGov, as they open up new directions in the study of the relation with OSS.},
  doi      = {https://doi.org/10.1016/j.techfore.2015.06.024},
  keywords = {Endogenous growth theory,Exogenous growth theory,Institutionalism,Open source software,Simultaneous equations,eGovernment},
  url      = {http://www.sciencedirect.com/science/article/pii/S0040162515001912},
}

@Article{Thörn2010411,
  author   = {Th{\"{o}}rn, C},
  title    = {{Current state and potential of variability management practices in software-intensive SMEs: Results from a regional industrial survey}},
  journal  = {Information and Software Technology},
  year     = {2010},
  volume   = {52},
  number   = {4},
  pages    = {411--421},
  abstract = {Context: More and more, small and medium-sized enterprises (SMEs) are using software to augment the functionality of their products and offerings. Variability management of software is becoming an interesting topic for SMEs with expanding portfolios and increasingly complex product structures. While the use of software product lines to resolve high variability is well known in larger organizations, there is less known about the practices in SMEs. Objective: This paper presents results from a survey of software developing SMEs. The purpose of the paper is to provide a snapshot of the current awareness and practices of variability modeling in organizations that are developing software with the constraints present in SMEs. Method: A survey with questions regarding the variability practices was distributed to software developing organizations in a region of Sweden that has many SMEs. The response rate was 13{\%} and 25 responses are used in this analysis. Results: We find that, although there are SMEs that develop implicit software product lines and have relatively sophisticated variability structures for their solution space, the structures of the problem space and the product space have room for improvement. Conclusions: The answers in the survey indicate that SMEs are in situations where they can benefit from more structured variability management, but the awareness need to be raised. Even though the problem space similarity is high, there is little reuse and traceability activities performed. The existence of SMEs with qualified variability management and product line practices indicates that small organizations are capable to apply such practices. {\textcopyright} 2009 Elsevier B.V. All rights reserved.},
  annote   = {cited By 10},
  doi      = {10.1016/j.infsof.2009.10.009},
  keywords = {Complex products; High variability; Industrial sur,Computer software,Societies and institutions; Surveys},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-77049124496{\&}doi=10.1016{\%}2Fj.infsof.2009.10.009{\&}partnerID=40{\&}md5=c6c79551ab7aa0aaf04e6ead9774ed78},
}

@Article{Fortier2010915,
  author   = {Fortier, Andr{\'{e}}s and Rossi, Gustavo and Gordillo, Silvia E and Challiol, Cecilia},
  title    = {{Dealing with variability in context-aware mobile software}},
  journal  = {Journal of Systems and Software},
  year     = {2010},
  volume   = {83},
  number   = {6},
  pages    = {915--936},
  issn     = {0164-1212},
  abstract = {Mobile context-aware software pose a set of challenging requirements to developers as these applications exhibit novel features, such as handling varied sensing devices and dynamically adapting to the user's context (e.g. his or her location), and evolve quickly according to technological advances. In this paper, we discuss how to handle variability both across different domains and during the evolution of a single application. We present a set of design structures for solving different problems related with mobility (such as location sensing, behaviour adaptation, etc.), together with the design rationale underlying them, and show how these sound micro-architectural constructs impact on variability. Our presentation is illustrated with case studies in different domains. },
  annote   = {Software Architecture and Mobility},
  doi      = {https://doi.org/10.1016/j.jss.2009.11.002},
  keywords = {Architecture,Context-awareness,Mobile software,Software variability},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121209002830},
}

@Article{Ullah20102496,
  author   = {Ullah, Muhammad Irfan and Ruhe, G{\"{u}}nther and Garousi, Vahid},
  title    = {{Decision support for moving from a single product to a product portfolio in evolving software systems}},
  journal  = {Journal of Systems and Software},
  year     = {2010},
  volume   = {83},
  number   = {12},
  pages    = {2496--2512},
  issn     = {0164-1212},
  abstract = {Successful software systems continuously evolve to accommodate ever-changing needs of customers. Accommodating the feature requests of all the customers in a single product increases the risks and costs of software maintenance. A possible approach to mitigate these risks is to transition the evolving software system (ESS) from a single system to a portfolio of related product variants, each addressing a specific customers' segment. This evolution should be conducted such that the extent of modifications required in ESS's structure is reduced. The proposed method COPE+ uses preferences of customers on product features to generate multiple product portfolios each containing one product variant per segment of customers. Recommendations are given to the decision maker to update the product portfolios based on structural analysis of ESS. Product portfolios are compared with the {\{}ESS{\}} using statechart representations to identify the level of similarity in their behaviors. A proof of concept is presented by application to an open-source text editing system. Structural and behavioral analysis of candidate portfolios helped the decision maker to select one portfolio out of three candidates. },
  annote   = {{\{}TAIC{\}} {\{}PART{\}} 2009 - Testing: Academic {\&} Industrial Conference - Practice And Research Techniques},
  doi      = {https://doi.org/10.1016/j.jss.2010.07.049},
  keywords = {Behavioral analysis,Decision support,Open-source systems,Software product evolution,Software product lines,Software product management},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121210002062},
}

@Article{Mendonca2010311,
  author   = {Mendonca, Marcilio and Cowan, Donald},
  title    = {{Decision-making coordination and efficient reasoning techniques for feature-based configuration}},
  journal  = {Science of Computer Programming},
  year     = {2010},
  volume   = {75},
  number   = {5},
  pages    = {311--332},
  issn     = {0167-6423},
  abstract = {Software Product Lines is a contemporary approach to software development that exploits the similarities and differences within a family of systems in a particular domain of interest in order to provide a common infrastructure for deriving members of this family in a timely fashion, with high-quality standards, and at lower costs. In Software Product Lines, feature-based product configuration is the process of selecting the desired features for a given software product from a repository of features called a feature model. This process is usually carried out collaboratively by people with distinct skills and interests called stakeholders. Collaboration benefits stakeholders by allowing them to directly intervene in the configuration process. However, collaboration also raises an important side effect, i.e., the need of stakeholders to cope with decision conflicts. Conflicts arise when decisions that are locally consistent cannot be applied globally because they violate one or more constraints in the feature model. Unfortunately, current product configuration systems are typically single-user-based in the sense that they do not provide means to coordinate concurrent decision-making on the feature model. As a consequence, configuration is carried out by a single person that is in charge of representing the interests of all stakeholders and managing decision conflicts on their own. This results in an error-prone and time-consuming process that requires past decisions to be revisited continuously either to correct misinterpreted stakeholder requirements or to handle decision conflicts. Yet another challenging issue related to configuration problems is the typically high computational cost of configuration algorithms. In fact, these algorithms frequently fall into the category of NP-hard and thus can become intractable in practice. In this paper, our goal is two-fold. First, we revisit our work on Collaborative Product Configuration (CPC) in which we proposed an approach to describe and validate collaborative configuration scenarios. We discuss how collaborative configuration can be described in terms of a workflow-like plan that safely guides stakeholders during the configuration process. Second, we propose a preliminary set of reasoning algorithms tailored to the feature modelling domain that can be used to provide automated support for product configuration. In addition, we compare empirically the performance of the proposed algorithms to that of a general-purpose solution. We hope that the insights provided in this paper will encourage other researchers to develop new algorithms in the near future. },
  annote   = {Coordination Models, Languages and Applications (SAC'08)},
  doi      = {https://doi.org/10.1016/j.scico.2009.12.004},
  keywords = {Automated reasoning,Constraint-based reasoning,Decision-making coordination,Feature modelling,Feature models,Product configuration,Software Product Lines},
  url      = {http://www.sciencedirect.com/science/article/pii/S0167642309001713},
}

@Article{deOliveira2014666,
  author   = {de Oliveira, R P and Blanes, D and Gonzalez-Huerta, J and Insfran, E and Abrah{\~{a}}o, S and Cohen, S and de Almeida, E S},
  title    = {{Defining and validating a feature-driven requirements engineering approach}},
  journal  = {Journal of Universal Computer Science},
  year     = {2014},
  volume   = {20},
  number   = {5},
  pages    = {666--691},
  abstract = {The specification of requirements is a key activity for achieving the goals of any software project and it has long been established and recognized by researchers and practitioners. Within Software Product Lines (SPL), this activity is even more critical owing to the need to deal with common, variable, and product-specific requirements, not only for a single product but for the whole set of products. In this paper, we present a Feature-Driven Requirements Engineering approach (FeDRE) that provides support to the requirements specification of SPL. The approach realizes features into functional requirements by considering the variability captured in a feature model. It also provides detailed guidelines on how to associate chunks of features from a feature model and to consider them as the context for the Use Case specification. The evaluation of the approach is illustrated in a case study for developing an SPL of mobile applications for emergency notifications. This case study was applied within 14 subjects, 8 subjects from Universitat Polit{\`{e}}cnica de Val{\`{e}}ncia and 6 subjects from Federal University of Bahia. Evaluations concerning the perceived ease of use, perceived usefulness, effectiveness and efficiency as regards requirements analysts using the approach are also presented. The results show that FeDRE was perceived as easy to learn and useful by the participants. {\textcopyright} J.UCS.},
  annote   = {cited By 1},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904769020{\&}partnerID=40{\&}md5=31d21d19a0249965fa4b5f0742277f51},
}

@Article{González-Huerta2013388,
  author   = {Gonz{\'{a}}lez-Huerta, J and Insfr{\'{a}}n, E and Abrah{\~{a}}o, S},
  title    = {{Defining and validating a multimodel approach for product architecture derivation and improvement}},
  journal  = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year     = {2013},
  volume   = {8107 LNCS},
  pages    = {388--404},
  abstract = {Software architectures are the key to achieving the non-functional requirements (NFRs) in any software project. In software product line (SPL) development, it is crucial to identify whether the NFRs for a specific product can be attained with the built-in architectural variation mechanisms of the product line architecture, or whether additional architectural transformations are required. This paper presents a multimodel approach for quality-driven product architecture derivation and improvement (QuaDAI). A controlled experiment is also presented with the objective of comparing the effectiveness, efficiency, perceived ease of use, intention to use and perceived usefulness with regard to participants using QuaDAI as opposed to the Architecture Tradeoff Analysis Method (ATAM). The results show that QuaDAI is more efficient and perceived as easier to use than ATAM, from the perspective of novice software architecture evaluators. However, the other variables were not found to be statistically significant. Further replications are needed to obtain more conclusive results. {\textcopyright} 2013 Springer-Verlag.},
  annote   = {cited By 5},
  doi      = {10.1007/978-3-642-41533-3_24},
  keywords = {Architectural pattern; Controlled experiment; Mode,Computer software; Experiments; Models; Software,Quality control},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886848508{\&}doi=10.1007{\%}2F978-3-642-41533-3{\_}24{\&}partnerID=40{\&}md5=470a1e83060cea2a038d5adeb4f1f478},
}

@Article{HEUER20132414,
  author   = {Heuer, Andr{\'{e}} and Stricker, Vanessa and Budnik, Christof J and Konrad, Sascha and Lauenroth, Kim and Pohl, Klaus},
  title    = {{Defining variability in activity diagrams and Petri nets}},
  journal  = {Science of Computer Programming},
  year     = {2013},
  volume   = {78},
  number   = {12},
  pages    = {2414--2432},
  issn     = {0167-6423},
  abstract = {Control flow models, such as UML activity diagrams or Petri nets, are widely accepted modeling languages used to support quality assurance activities in single system engineering as well as software product line (SPL) engineering. Quality assurance in product line engineering is a challenging task since a defect in a domain artifact may affect several products of the product line. Thus, proper quality assurance approaches need to pay special attention to the product line variability. Automation is essential to support quality assurance approaches. A prerequisite for automation is a profound formalization of the underlying control flow models and, in the context of SPLs, of the variability therein. In this paper, we propose a formal syntax and semantics for defining variability in Petri nets. We use these extended Petri nets as a foundation to formally define variability in UML activity diagrams; UML activity diagrams serve as a basis for several testing techniques in product line engineering. We illustrate the contribution of such a formalization to assurance activities in product line engineering by describing its usage in three application examples.},
  annote   = {Special Section on International Software Product Line Conference 2010 and Fundamentals of Software Engineering (selected papers of FSEN 2011)},
  doi      = {https://doi.org/10.1016/j.scico.2012.06.003},
  keywords = {Documenting variability,Quality assurance for product line engineering,Variability,Variability in activity diagrams},
  url      = {http://www.sciencedirect.com/science/article/pii/S0167642312001141},
}

@Article{Mohagheghi20091646,
  author   = {Mohagheghi, Parastoo and Dehlen, Vegard and Neple, Tor},
  title    = {{Definitions and approaches to model quality in model-based software development – A review of literature}},
  journal  = {Information and Software Technology},
  year     = {2009},
  volume   = {51},
  number   = {12},
  pages    = {1646--1669},
  issn     = {0950-5849},
  abstract = {More attention is paid to the quality of models along with the growing importance of modelling in software development. We performed a systematic review of studies discussing model quality published since 2000 to identify what model quality means and how it can be improved. From forty studies covered in the review, six model quality goals were identified; i.e., correctness, completeness, consistency, comprehensibility, confinement and changeability. We further present six practices proposed for developing high-quality models together with examples of empirical evidence. The contributions of the article are identifying and classifying definitions of model quality and identifying gaps for future research. },
  annote   = {Quality of {\{}UML{\}} Models},
  doi      = {https://doi.org/10.1016/j.infsof.2009.04.004},
  keywords = {Model quality,Model-driven development,Modelling,Systematic review,UML},
  url      = {http://www.sciencedirect.com/science/article/pii/S0950584909000457},
}

@Article{Lochau201463,
  author   = {Lochau, Malte and Lity, Sascha and Lachmann, Remo and Schaefer, Ina and Goltz, Ursula},
  title    = {{Delta-oriented model-based integration testing of large-scale systems}},
  journal  = {Journal of Systems and Software},
  year     = {2014},
  volume   = {91},
  pages    = {63--84},
  issn     = {0164-1212},
  abstract = {Abstract Software architecture specifications are of growing importance for coping with the complexity of large-scale systems. They provide an abstract view on the high-level structural system entities together with their explicit dependencies and build the basis for ensuring behavioral conformance of component implementations and interactions, e.g., using model-based integration testing. The increasing inherent diversity of such large-scale variant-rich systems further complicates quality assurance. In this article, we present a combination of architecture-driven model-based testing principles and regression-inspired testing strategies for efficient, yet comprehensive variability-aware conformance testing of variant-rich systems. We propose an integrated delta-oriented architectural test modeling and testing approach for component as well as integration testing that allows the generation and reuse of test artifacts among different system variants. Furthermore, an automated derivation of retesting obligations based on accurate delta-oriented architectural change impact analysis is provided. Based on a formal conceptual framework that guarantees stable test coverage for every system variant, we present a sample implementation of our approach and an evaluation of the validity and efficiency by means of a case study from the automotive domain. },
  doi      = {https://doi.org/10.1016/j.jss.2013.11.1096},
  keywords = {Large-scale systems,Model-based testing,Regression testing,Variable software architectures},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121213002781},
}

@Article{Bodden2012162,
  author   = {Bodden, E and Falzon, K and Pun, K I and Stolz, V},
  title    = {{Delta-oriented monitor specification}},
  journal  = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year     = {2012},
  volume   = {7609 LNCS},
  number   = {PART 1},
  pages    = {162--177},
  abstract = {Delta-oriented programming allows software developers to define software product lines as variations of a common code base, where variations are expressed as so-called program deltas. Monitor-oriented programming (MOP) provides a mechanism to execute functionality based on the execution history of the program; this is useful, e.g., for the purpose of runtime verification and for enforcing security policies. In this work we discuss how delta-oriented programming and MOP can benefit from each other in the Abstract Behavior Specification Language (ABS) through a new approach we call Delta-oriented Monitor Specification (DMS). We use deltas over monitor definitions to concisely capture protocol changes induced by feature combinations, and propose a notation to denote these deltas. In addition, we explore the design space for expressing runtime monitors as program deltas in ABS. A small case study shows that our approach successfully avoids code duplication in monitor specifications and that those specifications can evolve hand in hand with feature definitions. {\textcopyright} 2012 Springer-Verlag.},
  annote   = {cited By 1},
  doi      = {10.1007/978-3-642-34026-0_13},
  keywords = {Behavior specifications; Code duplication; Design,Specification languages,Specifications},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868275919{\&}doi=10.1007{\%}2F978-3-642-34026-0{\_}13{\&}partnerID=40{\&}md5=81a914f78956478166d71c4d9472c433},
}

@Article{Rodrigues2012112,
  author        = {Rodrigues, Gena{\'{i}}na Nunes and Alves, Vander and Silveira, Renato and Laranjeira, Luiz A},
  title         = {{Dependability analysis in the Ambient Assisted Living Domain: An exploratory case study}},
  journal       = {Journal of Systems and Software},
  year          = {2012},
  volume        = {85},
  number        = {1},
  pages         = {112--131},
  issn          = {0164-1212},
  abstract      = {Ambient Assisted Living (AAL) investigates the development of systems involving the use of different types of sensors, which monitor activities and vital signs of lonely elderly people in order to detect emergency situations or deviations from desirable medical patterns. Instead of requiring the elderly person to manually push a button to request assistance, state-of-the-art {\{}AAL{\}} solutions automate the process by ‘perceiving' lonely elderly people in their home environment through various sensors and performing appropriate actions under the control of the underlying software. Dependability in the {\{}AAL{\}} domain is a critical requirement, since poor system availability, reliability, safety, or integrity may cause inappropriate emergency assistance to potentially have fatal consequences. Nevertheless, contemporary research has not focused on assessing dependability in this domain. This work attempts to fill this gap presenting an approach which relies on modern quantitative and qualitative dependability analysis techniques based on software architecture. The analysis method presented in this paper consists of conversion patterns from Unified Modeling Language (UML) behavior models of the {\{}AAL{\}} software architecture into a formal executable specification, based on a probabilistic process algebra description language, which enables a sound quantitative and qualitative analysis. The {\{}UML{\}} models specify system component interactions and are annotated with component failure probabilities and system usage profile information. The resulting formal specification is executed on PRISM, a model checking tool adequate for the purpose of our analysis in order to identify a set of domain-specific dependability properties expressed declaratively in Probabilistic Computational Tree Logic (PCTL). The benefits of using these techniques are twofold. Firstly, they allow us to seamlessly integrate the analysis during subsequent software lifecycle stages in critical scenarios. Secondly, we identify the components which have the highest impact on software system dependability, and therefore, be able to address software architecture and individual software component problems prior to implementation and the occurrence of critical errors.},
  annote        = {Dynamic Analysis and Testing of Embedded Software},
  doi           = {https://doi.org/10.1016/j.jss.2011.07.037},
  keywords      = {Ambient Assisted Living,Component-based system,Dependability analysis,case study},
  mendeley-tags = {case study},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121211002056},
}

@Article{Khalil201494,
  author   = {Khalil, Issa M and Khreishah, Abdallah and Ahmed, Faheem and Shuaib, Khaled},
  title    = {{Dependable wireless sensor networks for reliable and secure humanitarian relief applications}},
  journal  = {Ad Hoc Networks},
  year     = {2014},
  volume   = {13, Part A},
  pages    = {94--106},
  issn     = {1570-8705},
  abstract = {Disasters such as flooding, earthquake, famine and terrorist attacks might occur any time anywhere without prior warnings. In most cases it is difficult to predict when a disaster might occur however, well-planned disaster recovery procedures will reduce the intensity of expected consequences. When a disaster occurs, infrastructure based communications are most likely to be crippled, worsening the critical situation on hand. Wireless ad hoc and sensor network (WASN) technologies are proven to be valuable in coordinating and managing rescue operations during disasters. However, the increasing reliance on {\{}WASNs{\}} make them attractive to malicious attackers, especially terrorist groups, in a bid to hamper rescue operations amplifying the damage and increasing the number of casualties. Therefore, it is necessary to ensure the fidelity of data traffic through {\{}WASN{\}} against malicious traffic disruption attacks. In this paper, we first demonstrate how {\{}WASN{\}} can be used in a well-planned disaster recovery effort. Then, we introduce and analyze one of the most severe traffic disruption attacks against WASNs, called Identity Delegation, and its countermeasures. Its severity lies in its capability to evade detection by even state-of-the-art intrusion detection techniques such as the neighbor monitoring based mechanisms. Through identity delegation, an adversary can drop packets, evade detection, and frame innocent nodes for dropping the traffic. We introduce a technique to mitigate identity delegation attack, dubbed Sadec, and compare it with the state-of-the-art mitigation technique namely Basic Local Monitoring (BLM) under a wide range of network scenarios. Our analysis which is validated by extensive ns-2 simulation scenarios show that {\{}BLM{\}} fails to efficiently mitigate packet drop through identity delegation attacks while Sadec successfully mitigates them. The results also show that Sadec achieves higher delivery ratios of data packets compared to BLM. On the other hand, the results show similar behavior in framing probabilities between Sadec and BLM. However, the desirable features of Sadec come at the expense of higher false isolation probabilities in networks with heavy traffic load and poor communication links. },
  annote   = {(1)Special Issue : Wireless Technologies for Humanitarian Relief {\&} (2)Special Issue: Models And Algorithms For Wireless Mesh Networks},
  doi      = {https://doi.org/10.1016/j.adhoc.2012.06.002},
  keywords = {Identity delegation,Local monitoring,Multi-hop wireless networks,Packet dropping,Security attacks},
  url      = {http://www.sciencedirect.com/science/article/pii/S1570870512001102},
}

@Article{Abate20122228,
  author   = {Abate, Pietro and Cosmo, Roberto Di and Treinen, Ralf and Zacchiroli, Stefano},
  title    = {{Dependency solving: A separate concern in component evolution management}},
  journal  = {Journal of Systems and Software},
  year     = {2012},
  volume   = {85},
  number   = {10},
  pages    = {2228--2240},
  issn     = {0164-1212},
  abstract = {Maintenance of component-based software platforms often has to face rapid evolution of software components. Component dependencies, conflicts, and package managers with dependency solving capabilities are the key ingredients of prevalent software maintenance technologies that have been proposed to keep software installations synchronized with evolving component repositories. We review state-of-the-art package managers and their ability to keep up with evolution at the current growth rate of popular component-based platforms, and conclude that their dependency solving abilities are not up to the task. We show that the complexity of the underlying upgrade planning problem is NP-complete even for seemingly simple component models, and argue that the principal source of complexity lies in multiple available versions of components. We then discuss the need of expressive languages for user preferences, which makes the problem even more challenging. We propose to establish dependency solving as a separate concern from other upgrade aspects, and present {\{}CUDF{\}} as a formalism to describe upgrade scenarios. By analyzing the result of an international dependency solving competition, we provide evidence that the proposed approach is viable. },
  annote   = {Automated Software Evolution},
  doi      = {https://doi.org/10.1016/j.jss.2012.02.018},
  keywords = {Competition,Component,Dependency solving,Open source,Package management,Software evolution},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121212000477},
}

@Article{Pinto2012525,
  author   = {Pinto, M{\'{o}}nica and Fuentes, Lidia and Fern{\'{a}}ndez, Luis},
  title    = {{Deriving detailed design models from an aspect-oriented {\{}ADL{\}} using {\{}MDD{\}}}},
  journal  = {Journal of Systems and Software},
  year     = {2012},
  volume   = {85},
  number   = {3},
  pages    = {525--545},
  issn     = {0164-1212},
  abstract = {Software architects can separate crosscutting concerns more appropriately by using an aspect-oriented ADL, concretely AO-ADL. This paper illustrates how aspect-orientation and model-driven development technologies can be used to enhance the system design phase; by automatically deriving detailed designs that take into account the “aspects” identified at the architectural level. Specifically, we have defined model-to-model transformation rules to automatically generate either aspect-oriented or object-oriented {\{}UML{\}} 2.0 models, closing the gap between {\{}ADLs{\}} and the notations used at the detailed design phase. By using AO-ADL it is possible to specify separately crosscutting concerns and base functionality. Another advantage of using AO-ADL is that it allows the specification of parameterizable architectures, promoting the definition of architectural templates. AO-ADL, then, enforces the specification of crosscutting concerns as separate architectural templates, which can be later instantiated and integrated with the core functionality of the system being developed. The AO-ADL language and the transformation rules from AO-ADL to {\{}UML{\}} 2.0 are available throughout the AO-ADL Tool Suite, which can be used to progressively refine and elaborate aspect-oriented software architectures. These refined architectures are the starting point of the detailed design phase. This means that our approach provides support to automatically generate a skeleton of the detailed design that preserves the information about the crosscutting and the non-crosscutting functionalities identified and modelled at the architecture level. },
  annote   = {Novel approaches in the design and implementation of systems/software architecture},
  doi      = {https://doi.org/10.1016/j.jss.2011.05.026},
  keywords = {AO-ADL,ATL,Aspect-oriented software development,Model-driven development,Software architectures,Theme/UML,UML 2.0},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121211001269},
}

@Article{Strickler20161232,
  author   = {Strickler, Andrei and Lima, Jackson A Prado and Vergilio, Silvia R and Pozo, Aurora T R},
  title    = {{Deriving products for variability test of Feature Models with a hyper-heuristic approach}},
  journal  = {Applied Soft Computing},
  year     = {2016},
  volume   = {49},
  pages    = {1232--1242},
  issn     = {1568-4946},
  abstract = {Abstract Deriving products from a Feature Model (FM) for testing Software Product Lines (SPLs) is a hard task. It is important to select a minimum number of products but, at the same time, to consider the coverage of testing criteria such as pairwise, among other factors. To solve such problems Multi-Objective Evolutionary Algorithms (MOEAs) have been successfully applied. However, to design a solution for this and other software engineering problems can be very difficult, because it is necessary to choose among different search operators and parameters. Hyper-heuristics can help in this task, and have raised interest in the Search-Based Software Engineering (SBSE) field. Considering the growing adoption of {\{}SPL{\}} in the industry and crescent demand for {\{}SPL{\}} testing approaches, this paper introduces a hyper-heuristic approach to automatically derive products to variability testing of SPLs. The approach works with {\{}MOEAs{\}} and two selection methods, random and based on FRR-MAB (Fitness Rate Rank based Multi-Armed Bandit). It was evaluated with real {\{}FMs{\}} and the results show that the proposed approach outperforms the traditional algorithms used in the literature, and that both selection methods present similar performance. },
  doi      = {https://doi.org/10.1016/j.asoc.2016.07.059},
  keywords = {Hyper-heuristic,Software Product Line,Software testing},
  url      = {http://www.sciencedirect.com/science/article/pii/S1568494616303994},
}

@Article{Marinho2017111,
  author   = {Marinho, A and de Oliveira, D and Ogasawara, E and Silva, V and Oca{\~{n}}a, K and Murta, L and Braganholo, V and Mattoso, M},
  title    = {{Deriving scientific workflows from algebraic experiment lines: A practical approach}},
  journal  = {Future Generation Computer Systems},
  year     = {2017},
  volume   = {68},
  pages    = {111--127},
  abstract = {The exploratory nature of a scientific computational experiment involves executing variations of the same workflow with different approaches, programs, and parameters. However, current approaches do not systematize the derivation process from the experiment definition to the concrete workflows and do not track the experiment provenance down to the workflow executions. Therefore, the composition, execution, and analysis for the entire experiment become a complex task. To address this issue, we propose the Algebraic Experiment Line (AEL). AEL uses a data-centric workflow algebra, which enriches the experiment representation by introducing a uniform data model and its corresponding operators. This representation and the AEL provenance model map concepts from the workflow execution data to the AEL derived workflows with their corresponding experiment abstract definitions. We show how AEL has improved the understanding of a real experiment in the bioinformatics area. By combining provenance data from the experiment and its corresponding executions, AEL provenance queries navigate from experiment concepts defined at high abstraction level to derived workflows and their execution data. It also shows a direct way of querying results from different trials involving activity variations and optionalities, only present at the experiment level of abstraction. {\textcopyright} 2016 Elsevier B.V.},
  annote   = {cited By 0},
  doi      = {10.1016/j.future.2016.08.016},
  keywords = {Abstracting; Bioinformatics,Algebra,Computational experiment; Concrete workflows; Dat},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84989866090{\&}doi=10.1016{\%}2Fj.future.2016.08.016{\&}partnerID=40{\&}md5=b9f956816a38755d563acf6b407b90f6},
}

@Article{Gadeyne2014198,
  author   = {Gadeyne, Klaas and Pinte, Gregory and Berx, Kristof},
  title    = {{Describing the design space of mechanical computational design synthesis problems}},
  journal  = {Advanced Engineering Informatics},
  year     = {2014},
  volume   = {28},
  number   = {3},
  pages    = {198--207},
  issn     = {1474-0346},
  abstract = {Abstract An important challenge in mechatronic system design is to select a feasible system architecture that satisfies all requirements. This article describes (i) the necessary concepts that a system architect needs to be able to formally and declaratively describe the design space of mechanical design synthesis problems, thereby minimizing accidental complexity; (ii) how a Domain Specific Language based on the SysML modeling language and the Object Constraint Language (OCL) can be used to create this model of the design space; and (iii) an iterative process to come up with a formal model of the design space. This model describes the design space independent of any (knowledge of a) particular solving technology for the Design Space Exploration. Furthermore, the information in the model allows to select the most appropriate solving strategy for a particular design synthesis problem. The different concepts are illustrated on the example of automated synthesis of a gearbox. },
  annote   = {Multiview Modeling for Mechatronic Design},
  doi      = {https://doi.org/10.1016/j.aei.2014.03.004},
  keywords = {Computational design synthesis,Configuration design,Design Space Exploration,Embodiment design,Gearbox architecture,Variant design},
  url      = {http://www.sciencedirect.com/science/article/pii/S1474034614000329},
}

@Article{vanGurp2002105,
  author   = {van Gurp, Jilles and Bosch, Jan},
  title    = {{Design erosion: problems and causes}},
  journal  = {Journal of Systems and Software},
  year     = {2002},
  volume   = {61},
  number   = {2},
  pages    = {105--119},
  issn     = {0164-1212},
  abstract = {Design erosion is a common problem in software engineering. We have found that invariably, no matter how ambitious the intentions of the designers were, software designs tend to erode over time to the point that redesigning from scratch becomes a viable alternative compared to prolonging the life of the existing design. In this paper, we illustrate how design erosion works by presenting the evolution of the design of a small software system. In our analysis of this example, we show how design decisions accumulate and become invalid because of new requirements. Also it is argued that even an optimal strategy for designing the system (i.e. no compromises with respect to e.g. cost are made) does not lead to an optimal design because of unforeseen requirement changes that invalidate design decisions that were once optimal. },
  doi      = {https://doi.org/10.1016/S0164-1212(01)00152-2},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121201001522},
}

@Article{Peña200771,
  author   = {Pe{\~{n}}a, Joaquin and Hinchey, Michael G and Resinas, Manuel and Sterritt, Roy and Rash, James L},
  title    = {{Designing and managing evolving systems using a {\{}MAS{\}} product line approach}},
  journal  = {Science of Computer Programming},
  year     = {2007},
  volume   = {66},
  number   = {1},
  pages    = {71--86},
  issn     = {0167-6423},
  abstract = {We view an evolutionary system as being a software product line. The core architecture is the unchanging part of the system, and each version of the system may be viewed as a product from the product line. Each “product” may be described as the core architecture with some agent-based additions. The result is a multiagent system software product line. We describe an approach to such a software product line-based approach using the MaCMAS agent-oriented methodology. The approach scales to enterprise architectures as a multiagent system is an appropriate means of representing a changing enterprise architecture and the interaction between components in it. In addition, we reduce the gap between the enterprise architecture and the software architecture. },
  annote   = {Special Issue on the 5th International Workshop on System/Software Architectures (IWSSA'06)},
  doi      = {https://doi.org/10.1016/j.scico.2006.10.007},
  keywords = {Enterprise architecture evolution,Multiagent systems product lines,Swarm-based systems},
  url      = {http://www.sciencedirect.com/science/article/pii/S0167642306002498},
}

@Article{Cetina2010331,
  author   = {Cetina, C and Giner, P and Fons, J and Pelechano, V},
  title    = {{Designing and prototyping dynamic software product lines: Techniques and guidelines}},
  journal  = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year     = {2010},
  volume   = {6287 LNCS},
  pages    = {331--345},
  abstract = {Dynamic Software Product Lines (DSPL) encompass systems that are capable of modifying their own configuration with respect to changes in their operating environment by using run-time reconfigurations. A failure in these reconfigurations can directly impact the user experience since the reconfigurations are performed when the system is already under the users control. Prototyping DSPLs at an early development stage can help to pinpoint potential issues and optimize design. In this work, we identify and addresses two challenges associated with the involvement of human subjects in DSPL prototyping: enabling DSPL users to (1) trigger the run-time reconfigurations and to (2) understand the effects of the reconfigurations. These techniques have been applied with the participation of human subjects by means of a Smart Hotel case study which was deployed with real devices. The application of these techniques reveals DSPL-design issues with recovering from a failed reconfiguration or a reconfiguration triggered by mistake. To address these issues, we discuss some guidelines learned in the Smart Hotel case study. {\textcopyright} 2010 Springer-Verlag Berlin Heidelberg.},
  annote   = {cited By 4},
  doi      = {10.1007/978-3-642-15579-6_23},
  keywords = {Design issues; Development stages; Dynamic softwar,Software prototyping},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-78049359003{\&}doi=10.1007{\%}2F978-3-642-15579-6{\_}23{\&}partnerID=40{\&}md5=81df8821082a001b4b31b20a5abe04d6},
}

@Article{GOEDICKE2004353,
  author   = {Goedicke, Michael and K{\"{o}}llmann, Carsten and Zdun, Uwe},
  title    = {{Designing runtime variation points in product line architectures: three cases}},
  journal  = {Science of Computer Programming},
  year     = {2004},
  volume   = {53},
  number   = {3},
  pages    = {353--380},
  issn     = {0167-6423},
  abstract = {Software product lines provide a common architecture, reusable code, and other common assets for a set of related software products. Variation is a central requirement in this context, as the product line components have to be instantiated, composed, and configured in the context of the products. In many approaches either static composition techniques or dynamic composition techniques based on loose relationships, such as association, aggregation, and replacement of entities, are proposed to design the variation points. If the domain of the product requires runtime variation, however, these approaches do not provide any central management facility for the runtime variation points. As a solution to this problem, we propose a pattern language that provides a domain-specific variation language and runtime variation point management facilities as part of the product line. We present three case studies from the areas of interactive digital television and document archiving in which we have applied this pattern language.},
  annote   = {Software Variability Management},
  doi      = {https://doi.org/10.1016/j.scico.2003.04.006},
  url      = {http://www.sciencedirect.com/science/article/pii/S0167642304000966},
}

@Article{SouzaNeto201684,
  author   = {Neto, Pl{\'{a}}cido A Souza and Vargas-Solar, Genoveva and da Costa, Umberto Souza and Musicante, Martin A},
  title    = {{Designing service-based applications in the presence of non-functional properties: A mapping study}},
  journal  = {Information and Software Technology},
  year     = {2016},
  volume   = {69},
  pages    = {84--105},
  issn     = {0950-5849},
  abstract = {AbstractContext The development of distributed software systems has become an important problem for the software engineering community. Service-based applications are a common solution for this kind of systems. Services provide a uniform mechanism for discovering, integrating and using these resources. In the development of service based applications not only the functionality of services and compositions should be considered, but also conditions in which the system operates. These conditions are called non-functional requirements (NFR). The conformance of applications to {\{}NFR{\}} is crucial to deliver software that meets the expectations of its users. Objective This paper presents the results of a systematic mapping carried out to analyze how {\{}NFR{\}} have been addressed in the development of service-based applications in the last years, according to different points of view. Method Our analysis applies the systematic mapping approach. It focuses on the analysis of publications organized by categories called facets, which are combined to answer specific research questions. The facets compose a classification schema which is part of the contribution and results. Results This paper presents our findings on how {\{}NFR{\}} have been supported in the development of service-based applications by proposing a classification scheme consisting in five facets: (i) programming paradigm (object/service oriented); (ii) contribution (methodology, system, middleware); (iii) software process phase; (iv) technique or mathematical model used for expressing NFR; and (v) the types of {\{}NFR{\}} addressed by the papers, based on the classification proposed by the ISO/IEC 9126 specification. The results of our systematic mapping are presented as bubble charts that provide a quantitative analysis to show the frequencies of publications for each facet. The paper also proposes a qualitative analysis based on these plots. This analysis discusses how {\{}NFR{\}} (quality properties) have been addressed in the design and development of service-based applications, including methodologies, languages and tools devised to support different phases of the software process. Conclusion This systematic mapping showed that {\{}NFR{\}} are not fully considered in all software engineering phases for building service based applications. The study also let us conclude that work has been done for providing models and languages for expressing {\{}NFR{\}} and associated middleware for enforcing them at run time. An important finding is that {\{}NFR{\}} are not fully considered along all software engineering phases and this opens room for proposing methodologies that fully model NFR. The data collected by our work and used for this systematic mapping are available in https://github.com/placidoneto/systematic-mapping{\_}service-based-app{\_}nfr. },
  doi      = {https://doi.org/10.1016/j.infsof.2015.09.004},
  keywords = {Non-functional requirements,Service-based software process,Systematic mapping},
  url      = {http://www.sciencedirect.com/science/article/pii/S0950584915001573},
}

@Article{KHATCHADOURIAN201756,
  author   = {Khatchadourian, Raffi and Rashid, Awais and Masuhara, Hidehiko and Watanabe, Takuya},
  title    = {{Detecting broken pointcuts using structural commonality and degree of interest}},
  journal  = {Science of Computer Programming},
  year     = {2017},
  volume   = {150},
  pages    = {56--74},
  issn     = {0167-6423},
  abstract = {Pointcut fragility is a well-documented problem in Aspect-Oriented Programming; changes to the base-code can lead to join points incorrectly falling in or out of the scope of pointcuts. Deciding which pointcuts have broken due to base-code changes is a daunting venture, especially in large and complex systems. We present an automated approach that recommends pointcuts that are likely to require modification due to a particular base-code change, as well as ones that do not. Our hypothesis is that join points selected by a pointcut exhibit common structural characteristics. Patterns describing such commonality are used to recommend pointcuts that have potentially broken with a degree of confidence as the developer is typing. The approach is implemented as an extension to the popular Mylyn Eclipse IDE plug-in, which maintains focused contexts of entities relevant to the task at hand using a Degree of Interest (DOI) model. We show that it is accurate in revealing broken pointcuts by applying it to multiple versions of several open source projects and evaluating the quality of the recommendations produced against actual modifications. We found that our tool made broken pointcuts 2.14 times more interesting in the DOI model than unbroken ones, with a p-value under 0.1, indicating a significant difference in final DOI value between the two kinds of pointcuts (i.e., broken and unbroken).},
  doi      = {https://doi.org/10.1016/j.scico.2017.06.011},
  keywords = {Software development environments,Software maintenance,Software tools},
  url      = {http://www.sciencedirect.com/science/article/pii/S0167642317301326},
}

@Article{Lopez-Herrejon2010217,
  author   = {Lopez-Herrejon, R E and Egyed, A},
  title    = {{Detecting inconsistencies in multi-view models with variability}},
  journal  = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year     = {2010},
  volume   = {6138 LNCS},
  pages    = {217--232},
  abstract = {Multi-View Modeling (MVM) is a common modeling practice that advocates the use of multiple, different and yet related models to represent the needs of diverse stakeholders. Of crucial importance in MVM is consistency checking - the description and verification of semantic relationships amongst the views. Variability is the capacity of software artifacts to vary, and its effective management is a core tenet of the research in Software Product Lines (SPL). MVM has proven useful for developing one-of-a-kind systems; however, to reap the potential benefits of MVM in SPL it is vital to provide consistency checking mechanisms that cope with variability. In this paper we describe how to address this need by applying Safe Composition - the guarantee that all programs of a product line are type safe. We evaluate our approach with a case study. {\textcopyright} 2010 Springer-Verlag.},
  annote   = {cited By 17},
  doi      = {10.1007/978-3-642-13595-8_18},
  keywords = {Computer software,Consistency checking; Effective management; Multi-},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954628413{\&}doi=10.1007{\%}2F978-3-642-13595-8{\_}18{\&}partnerID=40{\&}md5=7b0566671c141c7b6ac3e8239fb83b26},
}

@Article{Wang2013206,
  author   = {Wang, Renzhong and Dagli, Cihan H},
  title    = {{Developing a Holistic Modeling Approach for Search-based System Architecting}},
  journal  = {Procedia Computer Science},
  year     = {2013},
  volume   = {16},
  pages    = {206--215},
  issn     = {1877-0509},
  abstract = {This paper proposes a holistic modeling approach that combines the capabilities of Object Process Methodology (OPM), Colored Petri Net (CPN), and feature model. The resultant holistic model not only can capture the structural, behavioral, and dynamic aspects of a system, allowing simulation and strong analysis methods to be applied, it can also specify the architectural design space. This modeling approach is developed to facilitate the implementation of search-based system architecting where search algorithms are used to explore design trade space for good architecture alternatives. Such architecting approach integrates certain model construction, alternative generation, simulation, and assessment processes into a coherent and automated framework. Both the proposed holistic modeling approach and the search-based architecting framework are generic. They are targeted at systems that can be specified by conceptual models using object-oriented or process-oriented paradigms. The broad applicability of the proposed approach is demonstrated with the configuration of reconfigurable manufacturing systems (RMSs) under multi- objective optimization as an example. The test results showed that the proposed modeling approach could cover a huge number of architecture alternatives and supported the assessment of several performance measures. A set of quality results was obtained after running the optimization algorithm following the proposed search-based architecting framework. },
  annote   = {2013 Conference on Systems Engineering Research},
  doi      = {https://doi.org/10.1016/j.procs.2013.01.022},
  keywords = {Object oriented modeling,Simulation,System analysis and design,System architecting},
  url      = {http://www.sciencedirect.com/science/article/pii/S1877050913000239},
}

@Article{Svendsen2010106,
  author        = {Svendsen, A and Zhang, X and Lind-Tviberg, R and Fleurey, F and Haugen, {\O} and M{\o}ller-Pedersen, B and Olsen, G K},
  title         = {{Developing a software product line for train control: A case study of CVL}},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year          = {2010},
  volume        = {6287 LNCS},
  pages         = {106--120},
  abstract      = {This paper presents a case study of creating a software product line for the train signaling domain. The Train Control Language (TCL) is a DSL which automates the production of source code for computers controlling train stations. By applying the Common Variability Language (CVL), which is a separate and generic language to define variability on base models, we form a software product line of stations. We discuss the process and experience of using CVL to automate the production of three real train stations. A brief discussion about the verification needed for the generated products is also included. {\textcopyright} 2010 Springer-Verlag Berlin Heidelberg.},
  annote        = {cited By 10},
  doi           = {10.1007/978-3-642-15579-6_8},
  keywords      = {Base models,DSL,Network architecture,Signaling,Software Product Line,Software design,Source codes,case study},
  mendeley-tags = {case study},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-78049365870{\&}doi=10.1007{\%}2F978-3-642-15579-6{\_}8{\&}partnerID=40{\&}md5=4fee3eb9c1d9378b4e41d2e9c202d08a},
}

@Article{deJonge200963,
  author   = {de Jonge, Merijn},
  title    = {{Developing Product Lines with Third-Party Components}},
  journal  = {Electronic Notes in Theoretical Computer Science},
  year     = {2009},
  volume   = {238},
  number   = {5},
  pages    = {63--80},
  issn     = {1571-0661},
  abstract = {The trends toward product line development and toward adopting more third-party software are hard to combine. The reason is that product lines demand fine control over the software (e.g., for diversity management), while third-party software (almost by definition) provides only little or no control. A growing use of third-party software may therefore lead to less control over the product development process or, vice-versa, requiring large control over the software may limit the ability to use third-party components. Since both are means to reduce costs and to shorten time to market, the question is whether they can be combined effectively. In this paper, we describe our solution to this problem which combines the Koala component model developed within Philips with the concept of build-level components. We show that by lifting component granularity of Koala components from individual C files to build-level components, both trends can be united. The Koala architectural description language is used to orchestrate product composition and to manage diversity, while build-level components form the unit of third-party component composition. },
  annote   = {Proceedings of the 8th Workshop on Language Descriptions, Tools and Applications (LDTA 2008)},
  doi      = {https://doi.org/10.1016/j.entcs.2009.09.041},
  keywords = {Koala,build-level components,software composition,software product lines,third-party sofware},
  url      = {http://www.sciencedirect.com/science/article/pii/S1571066109003958},
}

@Article{Asadi201473,
  author   = {Asadi, Mohsen and Mohabbati, Bardia and Gr{\"{o}}ner, Gerd and Gasevic, Dragan},
  title    = {{Development and validation of customized process models}},
  journal  = {Journal of Systems and Software},
  year     = {2014},
  volume   = {96},
  pages    = {73--92},
  issn     = {0164-1212},
  abstract = {Abstract Configurable reference process models encompass common and variable processes of organizations from different business domains. These reference process models are designed and reused to guide and derive customized business processes according to the requirements of stakeholders. The customization process is generally initiated by a configuration step, selecting a subset of the reference process model. Configuration is followed by a customization step, which assumes adapting or extending the configured business process based on the specific or unforeseen requirements. Hence, it is crucial to validate the correctness and compliance of the final customized business process with respect to the patterns and business constraints that are specified in the reference model. In this paper, we firstly introduce a technique to develop a customized process model and then present a set of identified inconsistency patterns that may happen during the configuration of a reference model and the customization of configured process models. Furthermore, we describe our proposed approach including formal representations and algorithms that provide logical reasoning and enable automatic inconsistency detection by leveraging description logic. In order to explore the scalability of the approach, we designed the experiments with various process models sizes and inconsistency distributions. The results of the experiments revealed the scalability of our approach with large size process models (500 activities). },
  doi      = {https://doi.org/10.1016/j.jss.2014.05.063},
  keywords = {Description logics,Feature models,Reference process models},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121214001344},
}

@Article{Feigenspan2013699,
  author   = {Feigenspan, J and K{\"{a}}stner, C and Apel, S and Liebig, J and Schulze, M and Dachselt, R and Papendieck, M and Leich, T and Saake, G},
  title    = {{Do background colors improve program comprehension in the {\#}ifdef hell?}},
  journal  = {Empirical Software Engineering},
  year     = {2013},
  volume   = {18},
  number   = {4},
  pages    = {699--745},
  abstract = {Software-product-line engineering aims at the development of variable and reusable software systems. In practice, software product lines are often implemented with preprocessors. Preprocessor directives are easy to use, and many mature tools are available for practitioners. However, preprocessor directives have been heavily criticized in academia and even referred to as "{\#}ifdef hell", because they introduce threats to program comprehension and correctness. There are many voices that suggest to use other implementation techniques instead, but these voices ignore the fact that a transition from preprocessors to other languages and tools is tedious, erroneous, and expensive in practice. Instead, we and others propose to increase the readability of preprocessor directives by using background colors to highlight source code annotated with ifdef directives. In three controlled experiments with over 70 subjects in total, we evaluate whether and how background colors improve program comprehension in preprocessor-based implementations. Our results demonstrate that background colors have the potential to improve program comprehension, independently of size and programming language of the underlying product. Additionally, we found that subjects generally favor background colors. We integrate these and other findings in a tool called FeatureCommander, which facilitates program comprehension in practice and which can serve as a basis for further research. {\textcopyright} 2012 Springer Science+Business Media, LLC.},
  annote   = {cited By 35},
  doi      = {10.1007/s10664-012-9208-x},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879846095{\&}doi=10.1007{\%}2Fs10664-012-9208-x{\&}partnerID=40{\&}md5=76165de9ec8580474e8390c79a7ad728},
}

@Article{Jansen2008536,
  author   = {Jansen, Anton and Bosch, Jan and Avgeriou, Paris},
  title    = {{Documenting after the fact: Recovering architectural design decisions}},
  journal  = {Journal of Systems and Software},
  year     = {2008},
  volume   = {81},
  number   = {4},
  pages    = {536--557},
  issn     = {0164-1212},
  abstract = {Software architecture documentation helps people in understanding the software architecture of a system. In practice, software architectures are often documented after the fact, i.e. they are maintained or created after most of the design decisions have been made and implemented. To keep the architecture documentation up-to-date an architect needs to recover and describe these decisions. This paper presents ADDRA, an approach an architect can use for recovering architectural design decisions after the fact. {\{}ADDRA{\}} uses architectural deltas to provide the architect with clues about these design decisions. This allows the architect to systematically recover and document relevant architectural design decisions. The recovered architectural design decisions improve the documentation of the architecture, which increases traceability, communication, and general understanding of a system. },
  annote   = {Selected papers from the 10th Conference on Software Maintenance and Reengineering (CSMR 2006)},
  doi      = {https://doi.org/10.1016/j.jss.2007.08.025},
  keywords = {Architectural design decisions,Software architecture recovery},
  url      = {http://www.sciencedirect.com/science/article/pii/S016412120700194X},
}

@Article{Strasunskas20121239,
  author   = {Strasunskas, Darijus and Hakkarainen, Sari E},
  title    = {{Domain model-driven software engineering: A method for discovery of dependency links}},
  journal  = {Information and Software Technology},
  year     = {2012},
  volume   = {54},
  number   = {11},
  pages    = {1239--1249},
  issn     = {0950-5849},
  abstract = {Context Dependency management often suffers from labor intensity and complexity in creating and maintaining the dependency relations in practice. This is even more critical in a distributed development, in which developers are geographically distributed and a wide variety of tools is used. In those settings, different interpretations of software requirements or usage of different terminologies make it challenging to predict the change impact. Objective is (a) to describe a method facilitating change management in geographically distributed software engineering by effective discovery and establishment of dependency links using domain models; (b) to evaluate the effectiveness of the proposed method. Method A domain model, providing a common reference point, is used to manage development objects and to automatically support dependency discovery. We propose to associate (annotate) development objects with the concepts from the model. These associations are used to compute dependency among development objects, and are stepwise refined to direct dependency links (i.e. enabling product traceability). To evaluate the method, we conducted a laboratory-based randomized experiment on two real cases. Six participants were using an implemented prototype and two comparable tools to perform simulated tasks. Results In the paper we elaborate on the proposed method discussing its functional steps. Results from the experiment show that the method can be effectively used to assist in discovery of dependency links. Users have discovered on average fourteen percent more dependency links than by using the comparable tools. Conclusions The proposed method advocates the use of domain models throughout the whole development life-cycle and is apt to facilitate multi-site software engineering. The experimental study and results suggest that the method is effective in the discovery of dependencies among development objects. },
  doi      = {https://doi.org/10.1016/j.infsof.2012.06.004},
  keywords = {Dependency management,Domain model-based information system design,Information systems development,Randomized experiment,Software engineering},
  url      = {http://www.sciencedirect.com/science/article/pii/S0950584912001073},
}

@Article{vanDeursen20021,
  author   = {van Deursen, A and Klint, P},
  title    = {{Domain-specific language design requires feature descriptions}},
  journal  = {Journal of Computing and Information Technology},
  year     = {2002},
  volume   = {10},
  number   = {1},
  pages    = {1--17},
  abstract = {A domain-specific language (DSL) provides a notation tailored towards an application domain and is based on the relevant concepts and features of that domain. As such, a DSL is a means to describe and generate members of a family of programs in the domain. A prerequisite for the design of a DSL is a detailed analysis and structuring of the application domain. Graphical feature diagrams have been proposed to organize the dependencies between such features, and to indicate which ones are common to all family members and which ones vary. In this paper, we study feature diagrams in more details, as well as their relationship to domain-specific languages. We propose the Feature Description Language (FDL), a textual language to describe features. We explore automated manipulation of feature descriptions such as normalization, expansion to disjunctive normal form, variability computation and constraint satisfaction. Feature descriptions can be directly mapped to UML diagrams which in their turn can be used for Java code generation. The value of FDL is assessed via a case study in the use and expressiveness of feature descriptions for the area of documentation generators.},
  annote   = {cited By 142},
  doi      = {10.2498/cit.2002.01.01},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84928037990{\&}doi=10.2498{\%}2Fcit.2002.01.01{\&}partnerID=40{\&}md5=71a33563d42f75dfa75532e5a936e15e},
}

@Article{Kosar201677,
  author   = {Kosar, Toma{\v{z}} and Bohra, Sudev and Mernik, Marjan},
  title    = {{Domain-Specific Languages: A Systematic Mapping Study}},
  journal  = {Information and Software Technology},
  year     = {2016},
  volume   = {71},
  pages    = {77--91},
  issn     = {0950-5849},
  abstract = {Abstract Context: In this study we report on a Systematic Mapping Study (SMS) for Domain-Specific Languages (DSLs), based on an automatic search including primary studies from journals, conferences, and workshops during the period from 2006 until 2012. Objective: The main objective of the described work was to perform an {\{}SMS{\}} on {\{}DSLs{\}} to better understand the {\{}DSL{\}} research field, identify research trends, and any possible open issues. The set of research questions was inspired by a {\{}DSL{\}} survey paper published in 2005. Method: We conducted a {\{}SMS{\}} over 5 stages: defining research questions, conducting the search, screening, classifying, and data extraction. Our {\{}SMS{\}} included 1153 candidate primary studies from the {\{}ISI{\}} Web of Science and {\{}ACM{\}} Digital Library, 390 primary studies were classified after screening. Results: This {\{}SMS{\}} discusses two main research questions: research space and trends/demographics of the literature within the field of DSLs. Both research questions are further subdivided into several research sub-questions. The results from the first research question clearly show that the {\{}DSL{\}} community focuses more on the development of new techniques/methods rather than investigating the integrations of {\{}DSLs{\}} with other software engineering processes or measuring the effectiveness of {\{}DSL{\}} approaches. Furthermore, there is a clear lack of evaluation research. Amongst different {\{}DSL{\}} development phases more attention is needed in regard to domain analysis, validation, and maintenance. The second research question revealed that the number of publications remains stable, and has not increased over the years. Top cited papers and venues are mentioned, as well as identifying the more active institutions carrying {\{}DSL{\}} research. Conclusion: The statistical findings regarding research questions paint an interesting picture about the mainstreams of the {\{}DSL{\}} community, as well as open issues where researchers can improve their research in their future work. },
  doi      = {https://doi.org/10.1016/j.infsof.2015.11.001},
  keywords = {Domain-Specific Languages,Systematic Mapping Study,Systematic Review},
  url      = {http://www.sciencedirect.com/science/article/pii/S0950584915001858},
}

@Article{Viana20133123,
  author   = {Viana, Matheus C and Penteado, Ros{\^{a}}ngela A D and do Prado, Ant{\^{o}}nio F},
  title    = {{Domain-Specific Modeling Languages to improve framework instantiation}},
  journal  = {Journal of Systems and Software},
  year     = {2013},
  volume   = {86},
  number   = {12},
  pages    = {3123--3139},
  issn     = {0164-1212},
  abstract = {Abstract Frameworks are reusable software composed of concrete and abstract classes that implement the functionality of a domain. Applications reuse frameworks to enhance quality and development efficiency. However, frameworks are hard to learn and reuse. Application developers must understand the complex class hierarchy of the framework to instantiate it properly. In this paper, we present an approach to build a Domain-Specific Modeling Language (DSML) of a framework and use it to facilitate framework reuse during application development. The {\{}DSML{\}} of a framework is built by identifying the features of this framework and the information required to instantiate them. Application generators transform models created with the {\{}DSML{\}} into application code, hiding framework complexities. In this paper, we illustrate the use of our approach in a framework for the domain of business resource transactions and a experiment that evaluated the efficiency obtained with our approach. },
  doi      = {https://doi.org/10.1016/j.jss.2013.07.030},
  keywords = {Domain-Specific Modeling Language,Framework,Reuse},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121213001775},
}

@Article{Goedicke2002384,
  author   = {Goedicke, M and Pohl, K and Zdun, U},
  title    = {{Domain-specific runtime variability in product line architectures}},
  journal  = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year     = {2002},
  volume   = {2425},
  pages    = {384--396},
  abstract = {A software product line primarily structures the software architecture around the commonalities of a set of products within a specific organization.Common alities can be implemented in prefabricated components, and product differences are typically treated by well-defined variation points that are actualized later on.Dyna mic, domain-specific aspects, such as ad hoc customization by domian experts, are hard to model with static extension techniques.In this paper, we will discuss open issues for dynamic and domain-specific customizations of product line architectures.W e will also present an indirection architecture based on Component Wrapper objects and message redirection for dynamically composing and customizing generic components for the use in concrete products.As a case study, we will discuss two designs from a Multimedia Home Platform product line: end-user personalization across different new media platforms and customization of interactive applications by content editors. {\textcopyright} Springer-Verlag Berlin Heidelberg 2002},
  annote   = {cited By 5},
  keywords = {Application programs; Concrete products; Informati,Architecture-based; Generic components; Interacti,Object oriented programming},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944033745{\&}partnerID=40{\&}md5=01db694c08604b3248980fd35eddc119},
}

@Article{Turner201425,
  author   = {Turner, Hamilton and Dougherty, Brian and White, Jules and Kegley, Russell and Preston, Jonathan and Schmidt, Douglas C and Gokhale, Aniruddha},
  title    = {{{\{}DRE{\}} system performance optimization with the {\{}SMACK{\}} cache efficiency metric}},
  journal  = {Journal of Systems and Software},
  year     = {2014},
  volume   = {98},
  pages    = {25--43},
  issn     = {0164-1212},
  abstract = {Abstract System performance improvements are critical for the resource-limited environment of multiple integrated applications executing inside a single distributed real-time and embedded (DRE) system, such as integrated avionics platform or vehtronics systems. While processor caches can effectively reduce execution time there are several factors, such as cache size, system data sharing, and task execution schedule, which make it hard to quantify, predict, and optimize the cache usage of a {\{}DRE{\}} system. This article presents SMACK, a novel heuristic for estimating the hardware cache usage of a {\{}DRE{\}} system, and describes a method of varying the runtime behavior of {\{}DRE{\}} system software without (1) requiring extensive safety recertification or (2) violating the real-time scheduling deadlines. By using {\{}SMACK{\}} as a maximization target, we were able to reduce integrated {\{}DRE{\}} system execution time by an average of 2.4{\%} and a maximum of 4.34{\%}. },
  doi      = {https://doi.org/10.1016/j.jss.2014.08.031},
  keywords = {Cache,DRE,Deployment,Heuristic,Optimization},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121214001836},
}

@Article{Alférez201424,
  author   = {Alf{\'{e}}rez, G H and Pelechano, V and Mazo, R and Salinesi, C and Diaz, D},
  title    = {{Dynamic adaptation of service compositions with variability models}},
  journal  = {Journal of Systems and Software},
  year     = {2014},
  volume   = {91},
  pages    = {24--47},
  issn     = {0164-1212},
  abstract = {Abstract Web services run in complex contexts where arising events may compromise the quality of the whole system. Thus, it is desirable to count on autonomic mechanisms to guide the self-adaptation of service compositions according to changes in the computing infrastructure. One way to achieve this goal is by implementing variability constructs at the language level. However, this approach may become tedious, difficult to manage, and error-prone. In this paper, we propose a solution based on a semantically rich variability model to support the dynamic adaptation of service compositions. When a problematic event arises in the context, this model is leveraged for decision-making. The activation and deactivation of features in the variability model result in changes in a composition model that abstracts the underlying service composition. These changes are reflected into the service composition by adding or removing fragments of Business Process Execution Language (WS-BPEL) code, which can be deployed at runtime. In order to reach optimum adaptations, the variability model and its possible configurations are verified at design time using Constraint Programming. An evaluation demonstrates several benefits of our approach, both at design time and at runtime. },
  doi      = {https://doi.org/10.1016/j.jss.2013.06.034},
  keywords = {Autonomic computing,Constraint programming,Dynamic adaptation,Dynamic software product line,Models at runtime,Variability,Verification,Web service composition},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121213001465},
}

@Article{Bagheri2014187,
  author   = {Bagheri, E and Ensan, F},
  title    = {{Dynamic decision models for staged software product line configuration}},
  journal  = {Requirements Engineering},
  year     = {2014},
  volume   = {19},
  number   = {2},
  pages    = {187--212},
  abstract = {Software product line engineering practices offer desirable characteristics such as rapid product development, reduced time-to-market, and more affordable development costs as a result of systematic representation of the variabilities of a domain of discourse that leads to methodical reuse of software assets. The development lifecycle of a product line consists of two main phases: domain engineering, which deals with the understanding and formally modeling of the target domain, and application engineering that is concerned with the configuration of a product line into one concrete product based on the preferences and requirements of the stakeholders. The work presented in this paper focuses on the application engineering phase and builds both the theoretical and technological tools to assist the stakeholders in (a) understanding the complex interactions of the features of a product line; (b) eliciting the utility of each feature for the stakeholders and hence exposing the stakeholders' otherwise implicit preferences in a way that they can more easily make decisions; and (c) dynamically building a decision model through interaction with the stakeholders and by considering the structural characteristics of software product line feature models, which will guide the stakeholders through the product configuration process. Initial exploratory empirical experiments that we have performed show that our proposed approach for helping stakeholders understand their feature preferences and its associated staged feature model configuration process is able to positively impact the quality of the end results of the application engineering process within the context of the limited number of participants. In addition, it has been observed that the offered tooling support is able to ease the staged feature model configuration process. {\textcopyright} 2013 Springer-Verlag London.},
  annote   = {cited By 7},
  doi      = {10.1007/s00766-013-0165-8},
  keywords = {Computer software reusability; Concrete products;,Feature models; Rapid product development; Softwa,Software design},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84900872964{\&}doi=10.1007{\%}2Fs00766-013-0165-8{\&}partnerID=40{\&}md5=8dc990fc51cc488a06456ca6e52fbe4a},
}

@Article{Bashari2017191,
  author   = {Bashari, M and Bagheri, E and Du, W},
  title    = {{Dynamic Software Product Line Engineering: A Reference Framework}},
  journal  = {International Journal of Software Engineering and Knowledge Engineering},
  year     = {2017},
  volume   = {27},
  number   = {2},
  pages    = {191--234},
  abstract = {Runtime adaptive systems are able to dynamically transform their internal structure, and hence their behavior, in response to internal or external changes. Such transformations provide the basis for new functionalities or improvements of the non-functional properties that match operational requirements and standards. Software Product Line Engineering (SPLE) has introduced several models and mechanisms for variability modeling and management. Dynamic software product lines (DSPL) engineering exploits the knowledge acquired in SPLE to develop systems that can be context-aware, post-deployment reconfigurable, or runtime adaptive. This paper focuses on DSPL engineering approaches for developing runtime adaptive systems and proposes a framework for classifying and comparing these approaches from two distinct perspectives: adaptation properties and adaptation realization. These two perspectives are linked together by a series of guidelines that help to select a suitable adaptation realization approach based on desired adaptation types. {\textcopyright} 2017 World Scientific Publishing Company.},
  annote   = {cited By 0},
  doi      = {10.1142/S0218194017500085},
  keywords = {Adaptive systems; Computer software; Surveying,Dynamic software product lines; Non functional pr,Software design},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016321904{\&}doi=10.1142{\%}2FS0218194017500085{\&}partnerID=40{\&}md5=9e3707cf66288cf1fe8eb164fb6575b5},
}

@Article{Marchione2010357,
  author   = {Marchione, F G and Fantinato, M and {De Toledo}, M B F and {De Souza Gimenes}, I M},
  title    = {{E-contracting with price configuration for Web services and QoS}},
  journal  = {International Journal of Web and Grid Services},
  year     = {2010},
  volume   = {6},
  number   = {4},
  pages    = {357--384},
  abstract = {The large amount of information in electronic contracts hampers their establishment due to high complexity. An approach inspired in Software Product Line (PL) and based on feature modelling was proposed to make this process more systematic through information reuse and structuring. By assessing the feature-based approach in relation to a proposed set of requirements, it was showed that the approach does not allow the price of services and of Quality of Services (QoS) attributes to be considered in the negotiation and included in the electronic contract. Thus, this paper also presents an extension of such approach in which prices and price types associated to Web services and QoS levels are applied. An extended toolkit prototype is also presented as well as an experiment example of the proposed approach. Copyright {\textcopyright} 2010 Inderscience Enterprises Ltd.},
  annote   = {cited By 3},
  doi      = {10.1504/IJWGS.2010.036403},
  keywords = {Business Process; E-contracting; Electronic servic,Commerce,Computer software reusability; Costs; Web service},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-78049489331{\&}doi=10.1504{\%}2FIJWGS.2010.036403{\&}partnerID=40{\&}md5=dc25e161202646c1cab1d910e99b89f6},
}

@Article{Adam2013362,
  author   = {Adam, S and Schmid, K},
  title    = {{Effective requirements elicitation in product line application engineering - An experiment}},
  journal  = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year     = {2013},
  volume   = {7830 LNCS},
  pages    = {362--378},
  abstract = {[Context {\&} Motivation] Developing new software systems based on a software product line (SPL) is still a time-consuming task and the benefits of using such an approach are often smaller than expected. One important reason for this are difficulties in systematically mapping customer requirements to characteristics of the SPL. [Question/problem] Even though it has been recognized that the success of reuse strongly depends on how requirements are treated, it remains unclear how to perform this in an optimal way. [Principal ideas/results] In this paper, we present a controlled experiment performed with 26 students that compared two requirements elicitation approaches when instantiating a given SPL. [Contribution] Our findings indicate that a novel, problem-oriented requirements approach that explicitly integrates the reuse of SPL requirements into the elicitation of customer-specific requirements is more effective than a traditional SPL requirements approach, which distinguishes requirements reuse and additional elicitation customer-specific requirements. {\textcopyright} 2013 Springer-Verlag.},
  annote   = {cited By 0},
  doi      = {10.1007/978-3-642-37422-7_26},
  keywords = {Application engineering; Controlled experiment; Cu,Computer software selection and evaluation; Exper,Requirements engineering},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875865790{\&}doi=10.1007{\%}2F978-3-642-37422-7{\_}26{\&}partnerID=40{\&}md5=a8e9142d33bf17142e7cc1a548e61cae},
}

@Article{Li20141775,
  author   = {Li, Wei and Delicato, Fl{\'{a}}via C and Pires, Paulo F and Lee, Young Choon and Zomaya, Albert Y and Miceli, Claudio and Pirmez, Luci},
  title    = {{Efficient allocation of resources in multiple heterogeneous Wireless Sensor Networks}},
  journal  = {Journal of Parallel and Distributed Computing},
  year     = {2014},
  volume   = {74},
  number   = {1},
  pages    = {1775--1788},
  issn     = {0743-7315},
  abstract = {Abstract Wireless Sensor Networks (WSNs) are useful for a wide range of applications, from different domains. Recently, new features and design trends have emerged in the {\{}WSN{\}} field, making those networks appealing not only to the scientific community but also to the industry. One such trend is the running different applications on heterogeneous sensor nodes deployed in multiple {\{}WSNs{\}} in order to better exploit the expensive physical network infrastructure. Another trend deals with the capability of accessing sensor generated data from the Web, fitting {\{}WSNs{\}} in novel paradigms of Internet of Things (IoT) and Web of Things (WoT). Using well-known and broadly accepted Web standards and protocols enables the interoperation of heterogeneous {\{}WSNs{\}} and the integration of their data with other Web resources, in order to provide the final user with value-added information and applications. Such emergent scenarios where multiple networks and applications interoperate to meet high level requirements of the user will pose several changes in the design and execution of {\{}WSN{\}} systems. One of these challenges regards the fact that applications will probably compete for the resources offered by the underlying sensor nodes through the Web. Thus, it is crucial to design mechanisms that effectively and dynamically coordinate the sharing of the available resources to optimize resource utilization while meeting application requirements. However, it is likely that Quality of Service (QoS) requirements of different applications cannot be simultaneously met, while efficiently sharing the scarce networks resources, thus bringing the need of managing an inherent tradeoff. In this paper, we argue that a middleware platform is required to manage heterogeneous {\{}WSNs{\}} and efficiently share their resources while satisfying user needs in the emergent scenarios of WoT. Such middleware should provide several services to control running application as well as to distribute and coordinate nodes in the execution of submitted sensing tasks in an energy-efficient and QoS-enabled way. As part of the middleware provided services we present the Resource Allocation in Heterogeneous {\{}WSNs{\}} (SACHSEN) algorithm. {\{}SACHSEN{\}} is a new resource allocation heuristic for systems composed of heterogeneous {\{}WSNs{\}} that effectively deals with the tradeoff between possibly conflicting QoS requirements and exploits heterogeneity of multiple WSNs. },
  doi      = {https://doi.org/10.1016/j.jpdc.2013.09.012},
  keywords = {Resource allocation,Task allocation,Wireless sensor network},
  url      = {http://www.sciencedirect.com/science/article/pii/S0743731513002104},
}

@Article{Walraven201448,
  author   = {Walraven, Stefan and Landuyt, Dimitri Van and Truyen, Eddy and Handekyn, Koen and Joosen, Wouter},
  title    = {{Efficient customization of multi-tenant Software-as-a-Service applications with service lines}},
  journal  = {Journal of Systems and Software},
  year     = {2014},
  volume   = {91},
  pages    = {48--62},
  issn     = {0164-1212},
  abstract = {Abstract Application-level multi-tenancy is an architectural approach for Software-as-a-Service (SaaS) applications which enables high operational cost efficiency by sharing one application instance among multiple customer organizations (the so-called tenants). However, the focus on increased resource sharing typically results in a one-size-fits-all approach. In principle, the shared application instance satisfies only the requirements common to all tenants, without supporting potentially different and varying requirements of these tenants. As a consequence, multi-tenant SaaS applications are inherently limited in terms of flexibility and variability. This paper presents an integrated service engineering method, called service line engineering, that supports co-existing tenant-specific configurations and that facilitates the development and management of customizable, multi-tenant SaaS applications, without compromising scalability. Specifically, the method spans the design, implementation, configuration, composition, operations and maintenance of a SaaS application that bundles all variations that are based on a common core. We validate this work by illustrating the benefits of our method in the development of a real-world SaaS offering for document processing. We explicitly show that the effort to configure and compose an application variant for each individual tenant is significantly reduced, though at the expense of a higher initial development effort. },
  doi      = {https://doi.org/10.1016/j.jss.2014.01.021},
  keywords = {Multi-tenancy,SaaS,Variability},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121214000326},
}

@Article{SHE20141122,
  author   = {She, Steven and Ryssel, Uwe and Andersen, Nele and W{\c{a}}sowski, Andrzej and Czarnecki, Krzysztof},
  title    = {{Efficient synthesis of feature models}},
  journal  = {Information and Software Technology},
  year     = {2014},
  volume   = {56},
  number   = {9},
  pages    = {1122--1143},
  issn     = {0950-5849},
  abstract = {Context
Variability modeling, and in particular feature modeling, is a central element of model-driven software product line architectures. Such architectures often emerge from legacy code, but, creating feature models from large, legacy systems is a long and arduous task. We describe three synthesis scenarios that can benefit from the algorithms in this paper.
Objective
This paper addresses the problem of automatic synthesis of feature models from propositional constraints. We show that the decision version of the problem is NP-hard. We designed two efficient algorithms for synthesis of feature models from CNF and DNF formulas respectively.
Method
We performed an experimental evaluation of the algorithms against a binary decision diagram (BDD)-based approach and a formal concept analysis (FCA)-based approach using models derived from realistic models.
Results
Our evaluation shows a 10 to 1,000-fold performance improvement for our algorithms over the BDD-based approach. The performance of the DNF-based algorithm was similar to the FCA-based approach, with advantages for both techniques. We identified input properties that affect the runtimes of the CNF- and DNF-based algorithms.
Conclusions
Our algorithms are the first known techniques that are efficient enough to be used on dependencies extracted from real systems, opening new possibilities of creating reverse engineering and model management tools for variability models.},
  annote   = {Special Sections from “Asia-Pacific Software Engineering Conference (APSEC), 2012” and “ Software Product Line conference (SPLC), 2012”},
  doi      = {https://doi.org/10.1016/j.infsof.2014.01.012},
  keywords = {Feature models,Software product lines,Variability models},
  url      = {http://www.sciencedirect.com/science/article/pii/S0950584914000238},
}

@Article{Ramos20081207,
  author   = {Ramos, M A and Penteado, R A D},
  title    = {{Embedded software revitalization through component mining and software product line techniques}},
  journal  = {Journal of Universal Computer Science},
  year     = {2008},
  volume   = {14},
  number   = {8},
  pages    = {1207--1227},
  abstract = {The mining of generic software components from legacy systems can be used as an auxiliary technique to revitalize systems. This paper presents a software maintenance approach that uses such technique to revitalize one or more embedded legacy systems simultaneously and, in addition, create a core of reusable assets that can be used to support the development of new similar products. Software Product Line techniques are used to support the tasks of domain modelling and software component development. A real case study in the domain of Point of Sale (POS) terminals is presented and it illustrates the use of the proposed approach to revitalize three similar embedded legacy systems, simultaneously. It also shows how it is possible, through the created core of reusable assets, to deliver variations of these systems to meet the requirements of a wide family of POS terminals with different hardware configurations. {\textcopyright} J.UCS.},
  annote   = {cited By 2},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-46049088149{\&}partnerID=40{\&}md5=3196ea29d6331ce88e4ea332ebd0a55a},
}

@Article{Bonifácio201797,
  author   = {Bonif{\'{a}}cio, R and Borba, P and Ferraz, C and Accioly, P},
  title    = {{Empirical assessment of two approaches for specifying software product line use case scenarios}},
  journal  = {Software and Systems Modeling},
  year     = {2017},
  volume   = {16},
  number   = {1},
  pages    = {97--123},
  abstract = {Modularity benefits, including the independent maintenance and comprehension of individual modules, have been widely advocated. However, empirical assessments to investigate those benefits have mostly focused on source code, and thus, the relevance of modularity to earlier artifacts is still not so clear (such as requirements and design models). In this paper, we use a multimethod technique, including designed experiments, to empirically evaluate the benefits of modularity in the context of two approaches for specifying product line use case scenarios: PLUSS and MSVCM. The first uses an annotative approach for specifying variability, whereas the second relies on aspect-oriented constructs for separating common and variant scenario specifications. After evaluating these approaches through the specifications of several systems, we find out that MSVCM reduces feature scattering and improves scenario cohesion. These results suggest that evolving a product line specification using MSVCM requires only localized changes. On the other hand, the results of six experiments reveal that MSVCM requires more time to derive the product line specifications and, contrasting with the modularity results, reduces the time to evolve a product line specification only when the subjects have been well trained and are used to the task of evolving product line specifications. {\textcopyright} 2015, Springer-Verlag Berlin Heidelberg.},
  annote   = {cited By 0},
  doi      = {10.1007/s10270-015-0471-3},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929900234{\&}doi=10.1007{\%}2Fs10270-015-0471-3{\&}partnerID=40{\&}md5=f2635739cdc8744117607019ed6aa88a},
}

@Article{Tüzün201577,
  author   = {T{\"{u}}z{\"{u}}n, E and Tekinerdogan, B and Kalender, M E and Bilgen, S},
  title    = {{Empirical evaluation of a decision support model for adopting software product line engineering}},
  journal  = {Information and Software Technology},
  year     = {2015},
  volume   = {60},
  pages    = {77--101},
  abstract = {Context The software product line engineering (SPLE) community has provided several different approaches for assessing the feasibility of SPLE adoption and selecting transition strategies. These approaches usually include many rules and guidelines which are very often implicit or scattered over different publications. Hence, for the practitioners it is not always easy to select and use these rules to support the decision making process. Even in case the rules are known, the lack of automated support for storing and executing the rules seriously impedes the decision making process. Objective We aim to evaluate the impact of a decision support system (DSS) on decision-making in SPLE adoption. In alignment with this goal, we provide a decision support model (DSM) and the corresponding DSS. Method First, we apply a systematic literature review (SLR) on the existing primary studies that discuss and present approaches for analyzing the feasibility of SPLE adoption and transition strategies. Second, based on the data extraction and synthesis activities of the SLR, the required questions and rules are derived and implemented in the DSS. Third, for validation of the approach we conduct multiple case studies. Results In the course of the SLR, 31 primary studies were identified from which we could construct 25 aspects, 39 questions and 312 rules. We have developed the DSS tool Transit-PL that embodies these elements. Conclusions The multiple case study validation showed that the adoption of the developed DSS tool is justified to support the decision making process in SPLE adoption. {\textcopyright}2015 Elsevier B.V. All rights reserved.},
  annote   = {cited By 2},
  doi      = {10.1016/j.infsof.2014.12.007},
  file     = {:Users/mac/Library/Application Support/Mendeley Desktop/Downloaded/T{\"{u}}z{\"{u}}n et al. - 2015 - Empirical evaluation of a decision support model for adopting software product line engineering.pdf:pdf},
  keywords = {Artificial intelligence; Computer software; Decisi,Decision support models; Decision support system,Decision support systems},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921772938{\&}doi=10.1016{\%}2Fj.infsof.2014.12.007{\&}partnerID=40{\&}md5=8aa699041661142533209386bfcf15cd},
}

@Article{SAEED20161,
  author   = {Saeed, Mazin and Saleh, Faisal and Al-Insaif, Sadiq and El-Attar, Mohamed},
  title    = {{Empirical validating the cognitive effectiveness of a new feature diagrams visual syntax}},
  journal  = {Information and Software Technology},
  year     = {2016},
  volume   = {71},
  pages    = {1--26},
  issn     = {0950-5849},
  abstract = {Context
Feature models are commonly used to capture and communicate the commonality and variability of features in a Software Product Line. The core component of Feature models is feature diagrams, which graphically depict features in a hierarchical form. In previous work we have proposed a new notation that aims to improve the cognitive effectiveness of feature diagrams.
Objective
The objective of this paper is to empirically validate the cognitive effectiveness of the new feature diagrams notation in comparison to its original form.
Methods
We use two distinct empirical user-studies to validate the new notation. The first empirical study uses the survey approach while the second study is a subject-based experiment. The survey study investigates the semantic transparency of the new notation while the second study investigates the speed and accuracy of reading the notation.
Results
The results of the studies indicate that the proposed changes have significantly improved its cognitive effectiveness.
Conclusions
The cognitive effectiveness of feature diagrams has been improved, however there remains further research for full acceptance of the new notation by its potential user community.},
  doi      = {https://doi.org/10.1016/j.infsof.2015.10.012},
  keywords = {Feature models,Software product lines,Visual syntax evaluation},
  url      = {http://www.sciencedirect.com/science/article/pii/S0950584915001780},
}

@Article{Fernandez2013161,
  author   = {Fernandez, Adrian and Abrah{\~{a}}o, Silvia and Insfran, Emilio},
  title    = {{Empirical validation of a usability inspection method for model-driven Web development}},
  journal  = {Journal of Systems and Software},
  year     = {2013},
  volume   = {86},
  number   = {1},
  pages    = {161--186},
  issn     = {0164-1212},
  abstract = {Web applications should be usable in order to be accepted by users and to improve their success probability. Despite the fact that this requirement has promoted the emergence of several usability evaluation methods, there is a need for empirically validated methods that provide evidence about their effectiveness and that can be properly integrated into early stages of Web development processes. Model-driven Web development processes have grown in popularity over the last few years, and offer a suitable context in which to perform early usability evaluations due to their intrinsic traceability mechanisms. These issues have motivated us to propose a Web Usability Evaluation Process (WUEP) which can be integrated into model-driven Web development processes. This paper presents a family of experiments that we have carried out to empirically validate WUEP. The family of experiments was carried out by 64 participants, including PhD and Master's computer science students. The objective of the experiments was to evaluate the participants' effectiveness, efficiency, perceived ease of use and perceived satisfaction when using {\{}WUEP{\}} in comparison to an industrial widely used inspection method: Heuristic Evaluation (HE). The statistical analysis and meta-analysis of the data obtained separately from each experiment indicated that {\{}WUEP{\}} is more effective and efficient than {\{}HE{\}} in the detection of usability problems. The evaluators were also more satisfied when applying WUEP, and found it easier to use than HE. Although further experiments must be carried out to strengthen these results, {\{}WUEP{\}} has proved to be a promising usability inspection method for Web applications which have been developed by using model-driven development processes. },
  doi      = {https://doi.org/10.1016/j.jss.2012.07.043},
  keywords = {Family of experiments,Model-driven development,Usability inspection,Web applications},
  url      = {http://www.sciencedirect.com/science/article/pii/S016412121200218X},
}

@Article{Binkley201530,
  author   = {Binkley, Dave and Lawrie, Dawn and Uehlinger, Christopher and Heinz, Daniel},
  title    = {{Enabling improved IR-based feature location}},
  journal  = {Journal of Systems and Software},
  year     = {2015},
  volume   = {101},
  pages    = {30--42},
  issn     = {0164-1212},
  abstract = {Abstract Recent solutions to software engineering problems have incorporated tools and techniques from information retrieval (IR). The use of {\{}IR{\}} requires choosing an appropriate retrieval model and deciding on a query that best captures a particular information need. Taking feature location as a representative example, three research questions are investigated: (1) the impact of query preprocessing, (2) the impact that different scraping techniques for queries have on retrieval performance, (3) the performance impact that the underlying retrieval model has on identifying the correct source-code functions (the correct documents). These research questions are addressed using the five open source projects released as part of the {\{}SEMERU{\}} dataset. In the experiments, five methods of scraping queries from modification requests and seven retrieval model instances are considered. Using the standard evaluation metric Mean Reciprocal Rank (MRR), the experimental analysis reveals that better retrieval models are not the ones commonly used by software engineering researchers. Results find that models based on query-likelihood perform about twice as well as models in common use in software engineering such as {\{}LSI{\}} and thus deserve greater attention. Furthermore, corpus preprocessing has a significant impact as the top performing setting is over 100{\%} better than the average. },
  doi      = {https://doi.org/10.1016/j.jss.2014.11.013},
  keywords = {Feature location,Information retrieval models,Query formulation},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121214002428},
}

@Article{Alegre201655,
  author   = {Alegre, Unai and Augusto, Juan Carlos and Clark, Tony},
  title    = {{Engineering context-aware systems and applications: A survey}},
  journal  = {Journal of Systems and Software},
  year     = {2016},
  volume   = {117},
  pages    = {55--83},
  issn     = {0164-1212},
  abstract = {Abstract Context-awareness is an essential component of systems developed in areas like Intelligent Environments, Pervasive {\&} Ubiquitous Computing and Ambient Intelligence. In these emerging fields, there is a need for computerized systems to have a higher understanding of the situations in which to provide services or functionalities, to adapt accordingly. The literature shows that researchers modify existing engineering methods in order to better fit the needs of context-aware computing. These efforts are typically disconnected from each other and generally focus on solving specific development issues. We encourage the creation of a more holistic and unified engineering process that is tailored for the demands of these systems. For this purpose, we study the state-of-the-art in the development of context-aware systems, focusing on: (A) Methodologies for developing context-aware systems, analyzing the reasons behind their lack of adoption and features that the community wish they can use; (B) Context-aware system engineering challenges and techniques applied during the most common development stages; (C) Context-aware systems conceptualization. },
  doi      = {https://doi.org/10.1016/j.jss.2016.02.010},
  keywords = {Ambient Intelligence,Context-Aware Systems Engineering,Context-aware computing,Context-awareness,Context-sensitive,Intelligent Environments,Pervasive {\&} Ubiquitous Computing,Sentient computing,Software engineering},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121216000467},
}

@Article{Zhang2010198,
  author   = {Zhang, Weishan and Hansen, Klaus Marius and Kunz, Thomas},
  title    = {{Enhancing intelligence and dependability of a product line enabled pervasive middleware}},
  journal  = {Pervasive and Mobile Computing},
  year     = {2010},
  volume   = {6},
  number   = {2},
  pages    = {198--217},
  issn     = {1574-1192},
  abstract = {To provide good support for user-centered application scenarios in pervasive computing environments, pervasive middleware must react to context changes and prepare services accordingly. At the same time, pervasive middleware should provide extended dependability via self-management capabilities, to conduct self-diagnosis of possible malfunctions using the current runtime context, and self-configuration and self-adaptation when there are service mismatches. In this article, we present an approach to combine the power of {\{}BDI{\}} practical reasoning and OWL/SWRL ontologies theoretical reasoning in order to improve the intelligence of pervasive middleware, supported by a set of Self-Management Pervasive Service (SeMaPS) ontologies featuring dynamic context, complex context, and self-management rules modeling. In this approach, belief sets are enriched with the results of OWL/SWRL theoretical reasoning to derive beliefs that cannot be obtained directly or explicitly. This is demonstrated with agents negotiating sports appointments. To cope with self-management, the corresponding monitoring, configuration, adaptation and diagnosis rules are developed based on {\{}OWL{\}} and {\{}SWRL{\}} utilizing SeMaPS ontologies. Evaluations show this combined reasoning approach can perform well, and that Semantic Web-based self-management is promising for pervasive computing environments. },
  annote   = {Context Modelling, Reasoning and Management},
  doi      = {https://doi.org/10.1016/j.pmcj.2009.07.002},
  keywords = {Middleware,OWL (Web Ontology Language),SWRL (Semantic Web Rule Language),Self-diagnosis,Self-management,XVCL (XML-based Variant Configuration Language),{\{}BDI{\}} (Belief-Desire-Intention) agents},
  url      = {http://www.sciencedirect.com/science/article/pii/S1574119209000637},
}

@Article{Stallinger201321,
  author   = {Stallinger, Fritz and Neumann, Robert},
  title    = {{Enhancing ISO/IEC 15288 with reuse and product management: An add-on process reference model}},
  journal  = {Computer Standards {\&} Interfaces},
  year     = {2013},
  volume   = {36},
  number   = {1},
  pages    = {21--32},
  issn     = {0920-5489},
  abstract = {Abstract To support the transformation of system engineering from the project-based development of highly customer-specific solutions to the reuse and customization of ‘system products', we integrate a process reference model for reuse- and product-oriented industrial engineering and a process reference model extending ISO/IEC 12207 on software life cycle processes with software- and system-level product management. We synthesize the key process elements of both models to enhance ISO/IEC 15288 on system life cycle processes with product- and reuse-oriented engineering and product management practices as an integrated framework for process assessment and improvement in contexts where systems are developed and evolved as products. },
  doi      = {https://doi.org/10.1016/j.csi.2013.07.006},
  keywords = {ISO/IEC 15288,Process reference model,Product management,Reuse,Systems engineering},
  url      = {http://www.sciencedirect.com/science/article/pii/S0920548913000718},
}

@Article{AlDallal2008595,
  author   = {Dallal, Jehad Al and Sorenson, Paul},
  title    = {{Estimating the coverage of the framework application reusable cluster-based test cases}},
  journal  = {Information and Software Technology},
  year     = {2008},
  volume   = {50},
  number   = {6},
  pages    = {595--604},
  issn     = {0950-5849},
  abstract = {Object-oriented frameworks support both software code and design reusability. In addition, it is found that providing class-based tests with the framework reduces considerably the class-based testing time and effort of the applications developed using the frameworks. Similarly, reusable cluster-based test cases can be generated using the framework hooks, and they, too, can be provided with the framework to reduce the cluster testing time and effort of the framework applications. In this paper, we introduce a methodology to estimate the possible coverage of the cluster-based reusable test cases for framework applications prior to suggesting and applying a specific technique to produce the test cases. An experimental case study is conducted to demonstrate the practical issues in applying the introduced methodology and to give insights on the possible coverage of the framework reusable cluster-based test cases. The results of applying the methodology on five framework applications show that, on average, the reusable cluster-based test cases cover at least one-third of the cluster testing areas of the interface classes created during the framework application engineering stage. },
  doi      = {https://doi.org/10.1016/j.infsof.2007.07.006},
  keywords = {Case study,Cluster testing,Framework interface classes,Hooks,Object-oriented framework,Object-oriented framework application,Reusable test cases},
  url      = {http://www.sciencedirect.com/science/article/pii/S0950584907000808},
}

@Article{Hubaux2011337,
  author        = {Hubaux, A and Boucher, Q and Hartmann, H and Michel, R and Heymans, P},
  title         = {{Evaluating a textual feature modelling language: Four industrial case studies}},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year          = {2011},
  volume        = {6563 LNCS},
  pages         = {337--356},
  abstract      = {Feature models are commonly used in software product line engineering as a means to document variability. Since their introduction, feature models have been extended and formalised in various ways. The majority of these extensions are variants of the original tree-based graphical notation. But over time, textual dialects have also been proposed. The textual variability language (TVL) was proposed to combine the advantages of both graphical and textual notations. However, its benefits and limitations have not been empirically evaluated up to now. In this paper, we evaluate TVL with four cases from companies of different sizes and application domains. The study shows that practitioners can benefit from TVL. The participants appreciated the notation, the advantages of a textual language and considered the learning curve to be gentle. The study also reveals some limitations of the current version of TVL. {\textcopyright} 2011 Springer-Verlag Berlin Heidelberg.},
  annote        = {cited By 7},
  doi           = {10.1007/978-3-642-19440-5_23},
  keywords      = {Application domains,Different sizes,Document var,Models,Production engineering,Software design,case study},
  mendeley-tags = {case study},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952272925{\&}doi=10.1007{\%}2F978-3-642-19440-5{\_}23{\&}partnerID=40{\&}md5=0157326f6bab790b5a139316da08a7c1},
}

@Article{Erdweg201524,
  author   = {Erdweg, Sebastian and van der Storm, Tijs and V{\"{o}}lter, Markus and Tratt, Laurence and Bosman, Remi and Cook, William R and Gerritsen, Albert and Hulshout, Angelo and Kelly, Steven and Loh, Alex and Konat, Gabri{\"{e}}l and Molina, Pedro J and Palatnik, Martin and Pohjonen, Risto and Schindler, Eugen and Schindler, Klemens and Solmi, Riccardo and Vergu, Vlad and Visser, Eelco and van der Vlist, Kevin and Wachsmuth, Guido and van der Woning, Jimi},
  title    = {{Evaluating and comparing language workbenches: Existing results and benchmarks for the future}},
  journal  = {Computer Languages, Systems {\&} Structures},
  year     = {2015},
  volume   = {44, Part A},
  pages    = {24--47},
  issn     = {1477-8424},
  abstract = {Abstract Language workbenches are environments for simplifying the creation and use of computer languages. The annual Language Workbench Challenge (LWC) was launched in 2011 to allow the many academic and industrial researchers in this area an opportunity to quantitatively and qualitatively compare their approaches. We first describe all four {\{}LWCs{\}} to date, before focussing on the approaches used, and results generated, during the third LWC. We give various empirical data for ten approaches from the third LWC. We present a generic feature model within which the approaches can be understood and contrasted. Finally, based on our experiences of the existing LWCs, we propose a number of benchmark problems for future LWCs. },
  annote   = {Special issue on the 6th and 7th International Conference on Software Language Engineering (SLE 2013 and {\{}SLE{\}} 2014)},
  doi      = {https://doi.org/10.1016/j.cl.2015.08.007},
  keywords = {Benchmarks,Domain-specific languages,Language workbenches,Questionnaire language,Survey},
  url      = {http://www.sciencedirect.com/science/article/pii/S1477842415000573},
}

@Article{OLIVEIRA2017347,
  author   = {de Oliveira, Raphael Pereira and Santos, Alcemir Rodrigues and de Almeida, Eduardo Santana and {da Silva Gomes}, Gecynalda Soares},
  title    = {{Evaluating Lehman's Laws of software evolution within software product lines industrial projects}},
  journal  = {Journal of Systems and Software},
  year     = {2017},
  volume   = {131},
  pages    = {347--365},
  issn     = {0164-1212},
  abstract = {The evolution of a single system is a task where we deal with the modification of a single product. Lehman's Laws of software evolution were broadly evaluated within this type of system and the results shown that these single systems evolve according to his stated laws over time. However, considering Software Product Lines (SPL), we need to deal with the modification of several products which include common, variable, and product specific assets. Because of the several assets within SPL, each stated law may have a different behavior for each asset kind. Nonetheless, we do not know if all of the stated laws are still valid for SPL since they were partially evaluated in this context. Thus, this paper details an empirical investigation where Lehman's Laws (LL) of Software Evolution were used in two SPL industrial projects to understand how the SPL assets evolve over time. These projects are related to an application in the medical domain and another in the financial domain, developed by medium-size companies in Brazil. They contain a total of 71 modules and a total of 71.442 bug requests in their tracking system, gathered along the total of more than 10 years. We employed two techniques - the KPSS Test and linear regression analysis, to assess the relationship between LL and SPL assets. Results showed that one law was completely supported (conservation of organizational stability) for all assets within both empirical studies. Two laws were partially supported for both studies depending on the asset type (continuous growth and conservation of familiarity). Finally, the remaining laws had differences among their results for all assets (continuous change, increasing complexity, and declining quality).},
  doi      = {https://doi.org/10.1016/j.jss.2016.07.038},
  keywords = {Empirical study,Lehman's Laws of software evolution,Software evolution,Software product lines},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121216301339},
}

@Article{Cunha2016234,
  author   = {Cunha, J{\'{a}}come and Fernandes, Jo{\~{a}}o Paulo and Martins, Pedro and Mendes, Jorge and Pereira, Rui and Saraiva, Jo{\~{a}}o},
  title    = {{Evaluating refactorings for spreadsheet models}},
  journal  = {Journal of Systems and Software},
  year     = {2016},
  volume   = {118},
  pages    = {234--250},
  issn     = {0164-1212},
  abstract = {Abstract Software refactoring is a well-known technique that provides transformations on software artifacts with the aim of improving their overall quality. We have previously proposed a catalog of refactorings for spreadsheet models expressed in the ClassSheets modeling language, which allows us to specify the business logic of a spreadsheet in an object-oriented fashion. Reasoning about spreadsheets at the model level enhances a model-driven spreadsheet environment where a ClassSheet model and its conforming instance (spreadsheet data) automatically co-evolves after applying a refactoring at the model level. Research motivation was to improve the model and its conforming instance: the spreadsheet data. In this paper we define such refactorings using previously proposed evolution steps for models and instances. We also present an empirical study we designed and conducted in order to confirm our original intuition that these refactorings have a positive impact on end-user productivity, both in terms of effectiveness and efficiency. The results are not only presented in terms of productivity changes between refactored and non-refactored scenarios, but also the overall user satisfaction, relevance, and experience. In almost all cases the refactorings improved end-users productivity. Moreover, in most cases users were more engaged with the refactored version of the spreadsheets they worked with. },
  doi      = {https://doi.org/10.1016/j.jss.2016.04.043},
  keywords = {Empirical study,Model-driven engineering,Software refactoring,Spreadsheets},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121216300280},
}

@Article{Costa2016156,
  author   = {Costa, Bruno and Pires, Paulo F and Delicato, Fl{\'{a}}via C and Merson, Paulo},
  title    = {{Evaluating {\{}REST{\}} architectures—Approach, tooling and guidelines}},
  journal  = {Journal of Systems and Software},
  year     = {2016},
  volume   = {112},
  pages    = {156--180},
  issn     = {0164-1212},
  abstract = {Abstract Architectural decisions determine the ability of the implemented system to satisfy functional and quality attribute requirements. The Representational State Transfer (REST) architectural style has been extensively used recently for integrating services and applications. Its adoption to build SOA-based distributed systems brings several benefits, but also poses new challenges and risks. Particularly important among those risks are failures to effectively address quality attribute requirements such as security, reliability, and performance. A proved efficient technique to identify and help mitigate those risks is the architecture evaluation. In this paper we propose an approach, tooling, and guidelines to aid architecture evaluation activities in REST-based systems. These guidelines can be systematically used along with evaluation methods to reason about design considerations and tradeoffs. To demonstrate how the guidelines can help architecture evaluators, we present a proof of concept describing how to use the guidelines in an {\{}ATAM{\}} (Architecture Tradeoff Analysis Method) evaluation. We also present the results of a survey conducted with industry specialists who have performed architecture evaluations in real world REST-based systems in order to gauge the suitability and utility of the proposed guidelines. Finally, the paper describes a Web tool developed to facilitate the use of the evaluation guidelines. },
  doi      = {https://doi.org/10.1016/j.jss.2015.09.039},
  keywords = {REST,Scenario-based evaluation guidelines,Software architecture evaluation},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121215002150},
}

@Article{Alférez2014355,
  author   = {Alf{\'{e}}rez, M and Bonif{\'{a}}cio, R and Teixeira, L and Accioly, P and Kulesza, U and Moreira, A and Ara{\'{u}}jo, J and Borba, P},
  title    = {{Evaluating scenario-based SPL requirements approaches: the case for modularity, stability and expressiveness}},
  journal  = {Requirements Engineering},
  year     = {2014},
  volume   = {19},
  number   = {4},
  pages    = {355--376},
  abstract = {Software product lines (SPL) provide support for productivity gains through systematic reuse. Among the various quality attributes supporting these goals, modularity, stability and expressiveness of feature specifications, their composition and configuration knowledge emerge as strategic values in modern software development paradigms. This paper presents a metric-based evaluation aiming at assessing how well the chosen qualities are supported by scenario-based SPL requirementsapproaches. The selected approaches for this study span from type of notation (textual or graphical based), style to support variability (annotation or composition based), and specification expressiveness. They are compared using the metrics developed in a set of releases from an exemplar case study. Our major findings indicate that composition-based approaches have greater potential to support modularity and stability, and that quantification mechanisms simplify and increase expressiveness of configuration knowledge and composition specifications. {\textcopyright} 2013, Springer-Verlag London.},
  annote   = {cited By 3},
  doi      = {10.1007/s00766-013-0184-5},
  keywords = {Computer software reusability; Productivity; Speci,Productivity gain; Quality attributes; Requiremen,Software design},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84920253202{\&}doi=10.1007{\%}2Fs00766-013-0184-5{\&}partnerID=40{\&}md5=26bd7eec7ee2caf73ab4fc950066efee},
}

@Article{Maazoun201842,
  author   = {Maazoun, J and Bouassida, N and Ben-Abdallah, H},
  title    = {{Evaluating SPL Quality with Metrics}},
  journal  = {Advances in Intelligent Systems and Computing},
  year     = {2018},
  volume   = {736},
  pages    = {42--51},
  abstract = {A Software Product Line (SPL) is a set of systems that share a group of manageable features and satisfy the specific needs of a particular domain. The features of an SPL can be used in variable combinations to derive product variants in the SPL domain. Because SPLs promote product development through reuse, it is vital to have a means to measure their quality in terms of quality attributes like complexity, reusability,{\ldots} In this paper, we propose a set of metrics to evaluate the quality of an SPL at three levels: the feature model, design and code. We adapted a set of metrics for software quality and defined new metrics to deal with the inherent characteristics of SPLs, specifically the feature model and the traceability between features, design and code. Furthermore, to assist in interpreting the quality of a given SPL, we conducted an empirical study over ten open source SPLs to identify thresholds for the proposed metrics. {\textcopyright} 2018, Springer International Publishing AG, part of Springer Nature.},
  annote   = {cited By 0; Conference of 17th International Conference on Intelligent Systems Design and Applications, ISDA 2017 ; Conference Date: 14 December 2017 Through 16 December 2017; Conference Code:212209},
  doi      = {10.1007/978-3-319-76348-4_5},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044475814{\&}doi=10.1007{\%}2F978-3-319-76348-4{\_}5{\&}partnerID=40{\&}md5=e2e162871deb7bcb48c7baca54c9b48e},
}

@Article{Saeed2014180,
  author   = {Saeed, M and Saleh, F and Al-Insaif, S and El-Attar, M},
  title    = {{Evaluating the Cognitive Effectiveness of the Visual Syntax of Feature Diagrams}},
  journal  = {Communications in Computer and Information Science},
  year     = {2014},
  volume   = {432 CCIS},
  pages    = {180--194},
  abstract = {[Context and Motivation] Feature models are widely used in the Software Product Line (SPL) domain to capture and communicate the commonality and variability of features in a product line. Feature models contain feature diagrams that graphically depict features in a hierarchical form. [Problem/Question] Many research works have been devoted to enriching the visual syntax of feature diagrams to extend its expressiveness to capture additional types of semantics, however, there is a lack of research that evaluates the visual perception of feature models by its readers. Models serve a dual purpose: to brainstorm and communicate. A very sophisticated yet unreadable model is arguably useless. To date, there has not been a scientific evaluation of the cognitive effectiveness of the visual syntax of feature diagrams. [Principle Ideas] This paper presents a scientific evaluation of the cognitive effectiveness of feature diagrams. The evaluation approach is based on theory and empirical evidence mainly from the cognitive science field. [Contribution] The evaluation reveals drawbacks in the visual notation of feature diagrams. The paper concludes with some recommendations for improvement to remedy the identified flaws. {\textcopyright} Springer-Verlag Berlin Heidelberg 2014.},
  annote   = {cited By 1},
  doi      = {10.1007/978-3-662-43610-3_14},
  keywords = {Cognitive science; Commonality and variability; E,Computer software; Requirements engineering; Seman,Graphic methods},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904750779{\&}doi=10.1007{\%}2F978-3-662-43610-3{\_}14{\&}partnerID=40{\&}md5=9604755a9b0ee53a178a597d51bd687a},
}

@Article{Sturm20141390,
  author   = {Sturm, Arnon and Kramer, Oded},
  title    = {{Evaluating the productivity of a reference-based programming approach: A controlled experiment}},
  journal  = {Information and Software Technology},
  year     = {2014},
  volume   = {56},
  number   = {10},
  pages    = {1390--1402},
  issn     = {0950-5849},
  abstract = {AbstractContext Domain engineering aims at facilitating software development in an efficient and economical way. One way to measure that is through productivity indicators, which refer to the ability of creating a quality software product in a limited period and with limited resources. Many approaches have been devised to increase productivity; however, these approaches seem to suffer from a tension between expressiveness on the one hand, and applicability (or the lack of it) in providing guidance for developers. Objective This paper evaluates the applicability and efficiency of adopting a domain engineering approach, called Application-based {\{}DOmain{\}} Modeling (ADOM), in the context of the programming task with Java, and thus termed ADOM-Java, for improving productivity in terms of code quality and development time. Method To achieve that objective we have qualitatively evaluate the approach using questionnaires and following a text analysis procedure. We also set a controlled experiment in which 50 undergraduate students performed a Java-based programming task using either ADOM-Java or Java alone. Results The qualitative evaluation reveal that the approach is easy to uses and provides valuable guidance. Nevertheless, it requires training. The outcomes of the experiment indicate that the approach is applicable and that the students that used ADOM-Java achieved better code quality, as well as better functionality and within less time than the students who used only Java. Conclusion The results of the experiments imply that by providing a code base equipped with reuse guidelines for programmers can increase programming productivity in terms of quality and development time. These guidelines may also enforce coding standards and architectural design. },
  doi      = {https://doi.org/10.1016/j.infsof.2014.05.003},
  keywords = {Domain engineering,Productivity,Programming,Software quality,Software reusability},
  url      = {http://www.sciencedirect.com/science/article/pii/S0950584914001062},
}

@Article{Jaksic2014122,
  author   = {Jaksic, A and France, R B and Collet, P and Ghosh, S},
  title    = {{Evaluating the usability of a visual feature modeling notation}},
  journal  = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year     = {2014},
  volume   = {8706},
  pages    = {122--140},
  abstract = {Feature modeling is a popular Software Product Line Engineering (SPLE) technique used to describe variability in a product family. A usable feature modeling tool environment should enable SPLE practitioners to produce good quality models, in particular, models that effectively communicate modeled information. FAMILIAR is a text-based environment for manipulating and composing Feature Models (FMs). In this paper we present extensions we made to FAMILIAR to enhance its usability. The extensions include a visualization of FMs, or more precisely, a feature diagram rendering mechanism that supports the use of a combination of text and graphics to describe FMs, their configurations, and the results of FM analyses. We also present the results of a preliminary evaluation of the environment's usability. The evaluation involves comparing the use of the extended environment with the previous text-based console-driven version. The preliminary experiment provides some evidence that use of the new environment results in increased cognitive effectiveness of novice users and improved quality of new FMs. {\textcopyright} Springer International Publishing Switzerland 2014.},
  annote   = {cited By 1},
  keywords = {Computer graphics; Computer software; Visualizatio,FAMILIAR; Feature modeling; Model driven developm,Software design},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921862446{\&}partnerID=40{\&}md5=2ad230a54434045936f51004b1c8b0e6},
}

@Article{Villela2010113,
  author   = {Villela, K and D{\"{o}}rr, J and John, I},
  title    = {{Evaluation of a method for proactively managing the evolving scope of a software product line}},
  journal  = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year     = {2010},
  volume   = {6182 LNCS},
  pages    = {113--127},
  abstract = {[Context and motivation] PLEvo-Scoping is a method intended to help Product Line (PL) scoping teams anticipate emergent features and distinguish unstable from stable features, with the aim of preparing their PL for likely future adaptation needs. [Question/problem]This paper describes a quasi-experiment performed to characterize PLEvo-Scoping in terms of adequacy and feasibility. [Principal ideas/results] This quasi-experiment was performed by two scoping teams in charge of scoping the same PL, where one scoping team applied first an existing PL scoping approach and then PLEvo-Scoping, while the other scoping team interweaved activities from both. The two approaches achieved similar results: The method could be applied in just one day, and it was considered adequate and feasible. [Contribution] Ideas on how to improve the method and its tool support have been obtained, and similar results are expected from other professionals facing the problem of evolution-centered PL scoping. However, further empirical studies should be performed. {\textcopyright} 2010 Springer-Verlag.},
  annote   = {cited By 5},
  doi      = {10.1007/978-3-642-14192-8_13},
  keywords = {Biology; Computer software selection and evaluati,Empirical studies; Product Line Scoping; Product-l,Experiments},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955447396{\&}doi=10.1007{\%}2F978-3-642-14192-8{\_}13{\&}partnerID=40{\&}md5=8190e353b8258f66db2decb3c020113b},
}

@Article{Giron2018108,
  author   = {Giron, A A and Gimenes, I M S and {Oliveira E.}, Jr.},
  title    = {{Evaluation of test case generation based on a Software Product Line for model transformation}},
  journal  = {Journal of Computer Science},
  year     = {2018},
  volume   = {14},
  number   = {1},
  pages    = {108--121},
  abstract = {Model-Driven Engineering (MDE) supports model evolution and refinement by means of model transformations at several abstraction levels. Validating these transformations is essential to ensure the quality and correctness of such models. However, MDE transformations become more complex to validate, for example, when they are implemented in different languages. One particular example is the transformation of the SyMPLES approach. SyMPLES is a development approach for embedded systems, which is based on concepts of both Software Product Lines (SPL) and MDE. SyMPLES has a model transformation process which creates Simulink models from SysML models. This paper presents a case study which applies test case generation based on SPL to validate this model transformation. An SPL was used to generate a set of test cases based on coverage criteria. The results showed that the test cases generated uncovered errors in the transformation of SyMPLES. In addition, a comparison with the test case generation based on metamodel is presented, in order to analyze the effectiveness of the techniques. The coverage criteria made it possible to reduce the number of test cases generated, thus minimizing test effort and time. {\textcopyright} 2018 Alexandre Augusto Giron, Itana Maria de Souza Gimenes and Edson OliveiraJr.},
  annote   = {cited By 0},
  doi      = {10.3844/jcssp.2018.108.121},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041612613{\&}doi=10.3844{\%}2Fjcssp.2018.108.121{\&}partnerID=40{\&}md5=ec87bb404092cb7ed4c40ad5e01f7d57},
}

@Article{Abramov20121029,
  author   = {Abramov, Jenny and Sturm, Arnon and Shoval, Peretz},
  title    = {{Evaluation of the Pattern-based method for Secure Development (PbSD): A controlled experiment}},
  journal  = {Information and Software Technology},
  year     = {2012},
  volume   = {54},
  number   = {9},
  pages    = {1029--1043},
  issn     = {0950-5849},
  abstract = {Context Security in general, and database protection from unauthorized access in particular, are crucial for organizations. Although it has been long accepted that the important system requirements should be considered from the early stages of the development process, non-functional requirements such as security tend to get neglected or dealt with only at later stages of the development process. Objective We present an empirical study conducted to evaluate a Pattern-based method for Secure Development – PbSD – that aims to help developers, in particular database designers, to design database schemata that comply with the organizational security policies regarding authorization, from the early stages of development. The method provides a complete framework to guide, enforce and verify the correct implementation of security policies within a system design, and eventually generate a database schema from that design. Method The PbSD method was evaluated in comparison with a popular existing method that directly specifies the security requirements in {\{}SQL{\}} and Oracle's VPD. The two methods were compared with respect to the quality of the created access control specifications, the time it takes to complete the specification, and the perceived quality of the methods. Results We found that the quality of the access control specifications using the PbSD method for secure development were better with respect to privileges granted in the table, column and row granularity levels. Moreover, subjects who used the PbSD method completed the specification task in less time compared to subjects who used SQL. Finally, the subjects perceived the PbSD method clearer and more easy to use. Conclusion The pattern-based method for secure development can enhance the quality of security specification of databases, and decrease the software development time and cost. The results of the experiment may also indicate that the use of patterns in general has similar benefits; yet this requires further examinations. },
  doi      = {https://doi.org/10.1016/j.infsof.2012.04.001},
  keywords = {Authorization,Controlled experiment,Database design,Model driven development,Secure software development,Security patterns},
  url      = {http://www.sciencedirect.com/science/article/pii/S0950584912000729},
}

@Article{Lemos2013951,
  author   = {Lemos, Ot{\'{a}}vio Augusto Lazzarini and Ferrari, Fabiano Cutigi and Eler, Marcelo Medeiros and Maldonado, Jos{\'{e}} Carlos and Masiero, Paulo Cesar},
  title    = {{Evaluation studies of software testing research in Brazil and in the world: A survey of two premier software engineering conferences}},
  journal  = {Journal of Systems and Software},
  year     = {2013},
  volume   = {86},
  number   = {4},
  pages    = {951--969},
  issn     = {0164-1212},
  abstract = {This paper reports on a historical perspective of the evaluation studies present in software testing research published in the Brazilian Symposium on Software Engineering (SBES) in comparison to the International Conference on Software Engineering (ICSE). The survey characterizes the software testing-related papers published in the 25-year history of SBES, investigates the types of evaluation presented in these publications, and how the rate of evaluations has evolved over the years. A similar analysis within the same period is made for ICSE, allowing for a comparison between the national and international scenario. Results show that the rate of papers that present evaluation studies in {\{}SBES{\}} has significantly increased over the years. However, among the papers that described some kind of evaluation, only around 20{\%} performed more rigorous evaluations (i.e. case studies, quasi experiments, or controlled experiments). Such percentage is low when compared to ICSE, which presented 40{\%} of papers with more rigorous evaluations within the same period. Nevertheless, we noticed that both venues still lack the publication of research reporting controlled experiments: only a single paper in each conference presented this type of evaluation. },
  annote   = {{\{}SI{\}} : Software Engineering in Brazil: Retrospective and Prospective Views},
  doi      = {https://doi.org/10.1016/j.jss.2012.11.040},
  keywords = {Evaluation studies,Software testing,Software testing research in Brazil},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121212003238},
}

@Article{Souza2013,
  author    = {Souza, Iuri Santos and {Da Silva Gomes}, Gecynalda Soares and {Da Mota Silveira Neto}, Paulo Anselmo and {Do Carmo Machado}, Ivan and {De Almeida}, Eduardo Santana and {De Lemos Meira}, Silvio Romero},
  title     = {{Evidence of software inspection on feature specification for software product lines}},
  journal   = {Journal of Systems and Software},
  year      = {2013},
  volume    = {86},
  number    = {5},
  pages     = {1172--1190},
  issn      = {01641212},
  abstract  = {In software product lines (SPL), scoping is a phase responsible for capturing, specifying and modeling features, and also their constraints, interactions and variations. The feature specification task, performed in this phase, is usually based on natural language, which may lead to lack of clarity, non-conformities and defects. Consequently, scoping analysts may introduce ambiguity, inconsistency, omissions and non-conformities. In this sense, this paper aims at gathering evidence about the effects of applying an inspection approach to feature specification for SPL. Data from a SPL reengineering project were analyzed in this work and the analysis indicated that the correction activity demanded more effort. Also, Pareto's principle showed that incompleteness and ambiguity reported higher non-conformity occurrences. Finally, the Poisson regression analysis showed that sub-domain risk information can be a good indicator for prioritization of sub-domains in the inspection activity.{\textcopyright} 2012 Elsevier Inc. All rights reserved.},
  doi       = {10.1016/j.jss.2012.11.044},
  file      = {:Users/mac/ownCloud/Anita/SLR/REVISION DE JOURNALS/PAPERS{\_}A/selected/checked/casos de estudios analizados/2 Evidence of software inspection on feature specification for software product lines.pdf:pdf},
  keywords  = {Empirical study,Software inspection,Software product lines,Software quality control},
  publisher = {Elsevier Inc.},
}

@Article{VogelHeuser201554,
  author   = {Vogel-Heuser, Birgit and Fay, Alexander and Schaefer, Ina and Tichy, Matthias},
  title    = {{Evolution of software in automated production systems: Challenges and research directions}},
  journal  = {Journal of Systems and Software},
  year     = {2015},
  volume   = {110},
  pages    = {54--84},
  issn     = {0164-1212},
  abstract = {Abstract Coping with evolution in automated production systems implies a cross-disciplinary challenge along the system's life-cycle for variant-rich systems of high complexity. The authors from computer science and automation provide an interdisciplinary survey on challenges and state of the art in evolution of automated production systems. Selected challenges are illustrated on the case of a simple pick and place unit. In the first part of the paper, we discuss the development process of automated production systems as well as the different type of evolutions during the system's life-cycle on the case of a pick and place unit. In the second part, we survey the challenges associated with evolution in the different development phases and a couple of cross-cutting areas and review existing approaches addressing the challenges. We close with summarizing future research directions to address the challenges of evolution in automated production systems. },
  doi      = {https://doi.org/10.1016/j.jss.2015.08.026},
  keywords = {Automated production systems,Automation,Evolution,Software engineering},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121215001818},
}

@Article{Karimpour2017189,
  author   = {Karimpour, Reza and Ruhe, Guenther},
  title    = {{Evolutionary robust optimization for software product line scoping: An explorative study}},
  journal  = {Computer Languages, Systems {\&} Structures},
  year     = {2017},
  volume   = {47, Part 2},
  pages    = {189--210},
  issn     = {1477-8424},
  abstract = {Abstract Background: Software product line (SPL) scoping is an important phase when planning for product line adoption. An {\{}SPL{\}} scope specifies: (1) the extent of the domain supported by the product line, (2) portfolio of products in the product line and (3) list of assets to be developed for reuse across the family of products. Issue: {\{}SPL{\}} scope planning is usually based on estimates about the state of the market and the engineering capabilities of the development team. One challenge with these estimates is that there are inaccuracies due to uncertainty in the environment or accuracy of measurement. This may result in issues ranging from suboptimal plans to infeasible plans. Objective: To address the above, we propose to include uncertainty as part of the {\{}SPL{\}} scoping model. Plans developed in consideration of uncertainty would be more robust against possible fluctuations in estimates. Approach: In this paper, a method to incorporate uncertainty in scoping optimization and its application to generate robust solutions is proposed. We capture uncertainty as part of the formulation and model scoping optimization as a multi-objective problem with profit and stability as fitness functions. Profit stability and feasibility stability are considered to represent stability concerns. Results: Results show that, compared to other scope optimization approaches, both performance stability and feasibility stability are improved while maintaining near optimal performance for profit objective. Also, generated results consist of solutions with trade-offs between profit and stability, providing the decision maker with enhanced decision support. Conclusion: Multi-objective optimization with stability consideration for {\{}SPL{\}} scoping provides project managers with a robust and flexible way to address uncertainty in the process of {\{}SPL{\}} scoping. },
  doi      = {https://doi.org/10.1016/j.cl.2016.07.007},
  keywords = {Evolutionary optimization,Robust optimization,Search-based software engineering,Software product line scoping},
  url      = {http://www.sciencedirect.com/science/article/pii/S1477842416301063},
}

@Article{Wijnstra2004111,
  author   = {Wijnstra, J G},
  title    = {{Evolving a product family in a changing context}},
  journal  = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year     = {2004},
  volume   = {3014},
  pages    = {111--128},
  abstract = {The notion of software product families is becoming more and more popular, both in research and in industry. There is no single best product family approach that is suitable for all, since each product family has its unique context. Such a context comprises elements such as scope, organization, and business strategy. As these elements can change over time, the product family approach may have to evolve with them. In this paper we describe our ideas for a method to assess the variability approach of an existing product family, and to improve that approach to match the changing context. This is illustrated in a case study from the medical imaging domain. This product family in question started out with only a few family members, but over time, the growth in the number of different applications and new application domains have put higher variability demands on the family. These changes also require an evolution in the product family approach. We will describe the current product family approach and the changing requirements on this approach. We also performed a partially automated analysis of the variation to give us a good overview of the way variation is currently handled in the system. Based on that, a direction for evolving the product family approach is proposed. {\textcopyright} Springer-Verlag Berlin Heidelberg 2004.},
  annote   = {cited By 5},
  keywords = {Architecture; Artificial intelligence; Computer sc,Automated analysis; Business strategy; Evolution;,Medical imaging},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-33646178917{\&}partnerID=40{\&}md5=e3cfbc922130b5306062cd710b4b2adb},
}

@Article{Haber2012183,
  author   = {Haber, A and Rendel, H and Rumpe, B and Schaefer, I},
  title    = {{Evolving delta-oriented software product line architectures}},
  journal  = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year     = {2012},
  volume   = {7539 LNCS},
  pages    = {183--208},
  abstract = {Diversity is prevalent in modern software systems. Several system variants exist at the same time in order to adapt to changing user requirements. Additionally, software systems evolve over time in order to adjust to unanticipated changes in their application environment. In modern software development, software architecture modeling is an important means to deal with system complexity by architectural decomposition. This leads to the need of architectural description languages that can represent spatial and temporal variability. In this paper, we present delta modeling of software architectures as a uniform modeling formalism for architectural variability in space and in time. In order to avoid degeneration of the product line model under system evolution, we present refactoring techniques to maintain and improve the quality of the variability model. Using a running example from the automotive domain, we evaluate our approach by carrying out a case study that compares delta modeling with annotative variability modeling. {\textcopyright} 2012 Springer-Verlag.},
  annote   = {cited By 9},
  doi      = {10.1007/978-3-642-34059-8_10},
  keywords = {Application environment; Architectural description,Computer software; Software architecture,Information technology},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868357096{\&}doi=10.1007{\%}2F978-3-642-34059-8{\_}10{\&}partnerID=40{\&}md5=572ca2236365029622667b4844f59dc6},
}

@Article{White2014119,
  author   = {White, J and Galindo, J A and Saxena, T and Dougherty, B and Benavides, D and Schmidt, D C},
  title    = {{Evolving feature model configurations in software product lines}},
  journal  = {Journal of Systems and Software},
  year     = {2014},
  volume   = {87},
  number   = {1},
  pages    = {119--136},
  abstract = {The increasing complexity and cost of software-intensive systems has led developers to seek ways of reusing software components across development projects. One approach to increasing software reusability is to develop a software product-line (SPL), which is a software architecture that can be reconfigured and reused across projects. Rather than developing software from scratch for a new project, a new configuration of the SPL is produced. It is hard, however, to find a configuration of an SPL that meets an arbitrary requirement set and does not violate any configuration constraints in the SPL. Existing research has focused on techniques that produce a configuration of an SPL in a single step. Budgetary constraints or other restrictions, however, may require multi-step configuration processes. For example, an aircraft manufacturer may want to produce a series of configurations of a plane over a span of years without exceeding a yearly budget to add features. This paper provides three contributions to the study of multi-step configuration for SPLs. First, we present a formal model of multi-step SPL configuration and map this model to constraint satisfaction problems (CSPs). Second, we show how solutions to these SPL configuration problems can be automatically derived with a constraint solver by mapping them to CSPs. Moreover, we show how feature model changes can be mapped to our approach in a multi-step scenario by using feature model drift. Third, we present empirical results demonstrating that our CSP-based reasoning technique can scale to SPL models with hundreds of features and multiple configuration steps. {\textcopyright} 2013 Elsevier Inc.},
  annote   = {cited By 11},
  doi      = {10.1016/j.jss.2013.10.010},
  keywords = {Aircraft manufacturers; Budgetary constraints; Con,Budget control; Computer software reusability; So,Constraint satisfaction problems},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84888645293{\&}doi=10.1016{\%}2Fj.jss.2013.10.010{\&}partnerID=40{\&}md5=8ba32b418ee9714829c54449d3e18c41},
}

@Article{Koziolek2009177,
  author        = {Koziolek, H and Weiss, R and Doppelhamer, J},
  title         = {{Evolving industrial software architectures into a software product line: A case study}},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year          = {2009},
  volume        = {5581 LNCS},
  pages         = {177--193},
  abstract      = {Industrial software applications have high requirements on performance, availability, and maintainability. Additionally, diverse application landscapes of large corporate companies require systematic planning for reuse, which can be fostered by a software product-line approach. Analyses at the software architecture level can help improving the structure of the systems to account for extra-functional requirements and reuse. This paper reports a case study of product-line development for ABB's robotics PC software. We analysed the software architectures of three existing robotics applications and identified their core assets. As a result, we designed a new product-line architecture, which targets at fulfilling various extra-functional requirements. This paper describes experiences and lessons learned during the project. {\textcopyright} 2009 Springer Berlin Heidelberg.},
  annote        = {cited By 6},
  doi           = {10.1007/978-3-642-02351-4_12},
  keywords      = {Computer software,Computer software reusability,Core asset,Diverse applications,Functional requi,Software architecture,case study},
  mendeley-tags = {case study},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-70350640677{\&}doi=10.1007{\%}2F978-3-642-02351-4{\_}12{\&}partnerID=40{\&}md5=96cbc5e5189c1cb1b1be6ac4bffc1aa6},
}

@Article{HERADIO201212885,
  author   = {Heradio, Ruben and Fernandez-Amoros, David and de la Torre, Luis and Abad, Ismael},
  title    = {{Exemplar driven development of software product lines}},
  journal  = {Expert Systems with Applications},
  year     = {2012},
  volume   = {39},
  number   = {17},
  pages    = {12885--12896},
  issn     = {0957-4174},
  abstract = {The benefits of following a product line approach to develop similar software systems are well documented. Nevertheless, some case studies have revealed significant barriers to adopt such approach. In order to minimize the paradigm shift between conventional software engineering and software product line engineering, this paper presents a new development process where the products of a domain are made by analogy to an existing product. Furthermore, this paper discusses the capabilities and limitations of different techniques to implement the analogy relation and proposes a new language to overcome such limitations.},
  doi      = {https://doi.org/10.1016/j.eswa.2012.05.004},
  keywords = {Code generation,Domain engineering,Domain specific language,Software product line},
  url      = {http://www.sciencedirect.com/science/article/pii/S0957417412007026},
}

@Article{Andersson2013595,
  author   = {Andersson, Henric and Herzog, Erik and {\"{O}}lvander, Johan},
  title    = {{Experience from model and software reuse in aircraft simulator product line engineering}},
  journal  = {Information and Software Technology},
  year     = {2013},
  volume   = {55},
  number   = {3},
  pages    = {595--606},
  issn     = {0950-5849},
  abstract = {Context “Reuse” and “Model Based Development” are two prominent trends for improving industrial development efficiency. Product lines are used to reduce the time to create product variants by reusing components. The model based approach provides the opportunity to enhance knowledge capture for a system in the early stages in order to be reused throughout its lifecycle. This paper describes how these two trends are combined to support development and support of a simulator product line for the {\{}SAAB{\}} 39 Gripen fighter aircraft. Objective The work aims at improving the support (in terms of efficiency and quality) when creating simulation model configurations. Software based simulators are flexible so variants and versions of included models may easily be exchanged. The objective is to increase the reuse when combining models for usage in a range of development and training simulators. Method The research has been conducted with an interactive approach using prototyping and demonstrations, and the evaluation is based on an iterative and a retrospective method. Results A product line of simulator models for the {\{}SAAB{\}} 39 Gripen aircraft has been analyzed and defined in a Product Variant Master. A configurator system has been implemented for creation, integration, and customization of stringent simulator model configurations. The system is currently under incorporation in the standard development process at {\{}SAAB{\}} Aeronautics. Conclusion The explicit and visual description of products and their variability through a configurator system enables better insights and a common understanding so that collaboration on possible product configurations improves and the potential of software reuse increases. The combination of application fields imposes constraints on how traditional tools and methods may be utilized. Solutions for Design Automation and Knowledge Based Engineering are available, but their application has limitations for Software Product Line engineering and the reuse of simulation models. },
  annote   = {Special Issue on Software Reuse and Product LinesSpecial Issue on Software Reuse and Product Lines},
  doi      = {https://doi.org/10.1016/j.infsof.2012.06.014},
  keywords = {Configurator,Knowledge Based Engineering,Model Based Development,PDM,SPL,Software Product Line},
  url      = {http://www.sciencedirect.com/science/article/pii/S0950584912001280},
}

@Article{Lee2009137,
  author   = {Lee, H and Choi, H and Kang, K C and Kim, D and Lee, Z},
  title    = {{Experience report on using a domain model-based extractive approach to software product line asset development}},
  journal  = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year     = {2009},
  volume   = {5791 LNCS},
  pages    = {137--149},
  abstract = {When we attempted to introduce an extractive approach to a company, we were faced with a challenging project situation where legacy applications did not have many commonalities among their implementations as they were developed independently by different teams without sharing a common code base. Although there were not many structural similarities, we expected to find similarities if we view them from the domain model perspective as they were in the same domain and were developed with the object-oriented paradigm. Therefore, we decided to place the domain model at the center of extraction and reengineering, thus developing a domain model-based extractive method. The method has been successfully applied to introduce software product line to a set-top box manufacturing company. {\textcopyright} 2009 Springer Berlin Heidelberg.},
  annote   = {cited By 9},
  doi      = {10.1007/978-3-642-04211-9_14},
  keywords = {Component extraction; Domain model; Experience rep,Computer software reusability,Feature extraction; Production engineering},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-70350417614{\&}doi=10.1007{\%}2F978-3-642-04211-9{\_}14{\&}partnerID=40{\&}md5=76fc90edc472d461632563b61a170d17},
}

@Article{SPE:SPE613,
  author    = {Schwanke, Robert W and Lutz, Robyn R},
  title     = {{Experience with the architectural design of a modest product family}},
  journal   = {Software: Practice and Experience},
  year      = {2004},
  volume    = {34},
  number    = {13},
  pages     = {1273--1296},
  issn      = {1097-024X},
  doi       = {10.1002/spe.613},
  file      = {:Users/mac/Library/Application Support/Mendeley Desktop/Downloaded/Schwanke, Lutz - 2004 - Experience with the architectural design of a modest product family.pdf:pdf},
  keywords  = {medical image analysis,modest product family,product line architecture,software architecture,software product family,software product line},
  publisher = {John Wiley {\&} Sons, Ltd.},
  url       = {http://dx.doi.org/10.1002/spe.613},
}

@Article{Deelstra2004165,
  author   = {Deelstra, S and Sinnema, M and Bosch, J},
  title    = {{Experiences in Software Product Families: Problems and Issues during Product Derivation}},
  journal  = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year     = {2004},
  volume   = {3154},
  pages    = {165--182},
  abstract = {A fundamental reason for investing in product families is to minimize the application engineering costs. Several organizations that employ product families, however, are becoming increasingly aware of the fact that, despite the efforts in domain engineering, deriving individual products from their shared software assets is a time- and effort-consuming activity. In this paper, we present a collection of product derivation problems that we identified during a case study at two large and mature industrial organizations. These problems are attributed to the lack of methodological support for application engineering, and to underlying causes of complexity and implicit properties. For each problem, we provide a description and an example, while for each cause we present a description, consequences, solutions, and research issues. The discussions in this paper are relevant outside the context of the two companies, as the challenges they face arise in, for example, comparable or less mature organizations. {\textcopyright} Springer-Verlag 2004.},
  annote   = {cited By 24},
  keywords = {Application engineering; Domain engineering; Indu,Cost engineering,Societies and institutions},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-35048863852{\&}partnerID=40{\&}md5=27964013b1c42ae6e016636d1126754e},
}

@Article{Lassing200247,
  author   = {Lassing, Nico and Bengtsson, PerOlof and van Vliet, Hans and Bosch, Jan},
  title    = {{Experiences with ALMA: Architecture-Level Modifiability Analysis}},
  journal  = {Journal of Systems and Software},
  year     = {2002},
  volume   = {61},
  number   = {1},
  pages    = {47--57},
  issn     = {0164-1212},
  abstract = {Modifiability is an important quality for software systems, because a large part of the costs associated with these systems is spent on modifications. The effort, and therefore cost, that is required for these modifications is largely determined by a system's software architecture. Analysis of software architectures is therefore an important technique to achieve modifiability and reduce maintenance costs. However, few techniques for software architecture analysis currently exist. Based on our experiences with software architecture analysis of modifiability, we have developed ALMA, an architecture-level modifiability analysis method consisting of five steps. In this paper we report on our experiences with ALMA. We illustrate our experiences with examples from two case studies of software architecture analysis of modifiability. These case studies concern a system for mobile positioning at Ericsson Software Technology {\{}AB{\}} and a system for freight handling at {\{}DFDS{\}} Fraktarna. Our experiences are related to each step of the analysis process. In addition, we made some observations on software architecture analysis of modifiability in general. },
  doi      = {https://doi.org/10.1016/S0164-1212(01)00113-3},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121201001133},
}

@Article{AJILA200774,
  author   = {Ajila, Samuel A and Dumitrescu, Razvan T},
  title    = {{Experimental use of code delta, code churn, and rate of change to understand software product line evolution}},
  journal  = {Journal of Systems and Software},
  year     = {2007},
  volume   = {80},
  number   = {1},
  pages    = {74--91},
  issn     = {0164-1212},
  abstract = {This research is a longitudinal study of change processes. It links changes in the product line architecture of a large telecommunications equipment supplier with the company's customers, inner context, and eight line card products over six-year period. There are three important time related constructs in this study: the time it takes to develop a new product line release; the frequency in which a metric is collected; and the frequency at which financial results and metrics related to the customer layer are collected and made available. Data collection has been organized by product release. The original goal of this research is to study the economic impact of market reposition on the product line and identify metrics that can be used to records changes in product line. We later look at the product line evolution vis-{\`{a}}-vis the changes in the products that form the product line. Our results show that there is no relationship between the size of the code added to the product line and the number of designers required to develop and test it; and there is a positive relationship between designer turnover and impact of change.},
  doi      = {https://doi.org/10.1016/j.jss.2006.05.034},
  keywords = {Code churn,Code delta,Impact analysis,Rate of change,Software evolution,Software metric,Software product line},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121206001658},
}

@Article{MAGILL2016133,
  author   = {Magill, Evan and Blum, Jesse},
  title    = {{Exploring conflicts in rule-based sensor networks}},
  journal  = {Pervasive and Mobile Computing},
  year     = {2016},
  volume   = {27},
  pages    = {133--154},
  issn     = {1574-1192},
  abstract = {This paper addresses rule conflicts within wireless sensor networks. The work is situated within psychiatric ambulatory assessment settings where patients are monitored in and around their homes. Detecting behaviours within these settings favours sensor networks, while scalability and resource concerns favour processing data on smart nodes incorporating rule engines. Such monitoring involves personalisation, thereby becoming important to program node rules on the fly. Since rules may originate from distinct sources and change over time, methods are required to maintain rule consistency. Drawing on lessons from Feature Interaction, the paper contributes novel approaches for detecting and resolving rule-conflict across sensor networks.},
  doi      = {https://doi.org/10.1016/j.pmcj.2015.08.005},
  keywords = {Feature interactions,Rule conflict,Rule-based systems,Run-time programming},
  url      = {http://www.sciencedirect.com/science/article/pii/S1574119215001650},
}

@Article{Wnuk2015647,
  author   = {Wnuk, K and Kabbedijk, J and Brinkkemper, S and Regnell, B and Callele, D},
  title    = {{Exploring factors affecting decision outcome and lead time in large-scale requirements engineering}},
  journal  = {Journal of Software: Evolution and Process},
  year     = {2015},
  volume   = {27},
  number   = {9},
  pages    = {647--673},
  abstract = {Optimizing decision lead time and outcome is important for successful product management. This work identifies decision lead time and outcome factors in large-scale requirements engineering. Our investigation brings supporting evidence that complex changes have longer lead time and that important customers more likely get what they request. The results provide input into the discussion of whether a large company should focus on only a few of its large customers and disregard its significantly larger group of small customers.
Lead time, defined as the duration between the moment a request was filed and the moment the decision was made, is an important aspect of decision making in market-driven requirements engineering. Minimizing lead time allows software companies to focus their resources on the most profitable functionality and enables them to remain competitive within the quickly changing software market. Achieving and sustaining low decision lead time and the resulting high decision efficiency require a better understanding of factors that may affect both decision lead time and outcome. In order to identify possible factors, we conducted an exploratory two-stage case study that combines the statistical analysis of seven possible relationships among decision characteristics at a large company with a survey of industry participants. Our results show that the number of products affected by a decision increases the time needed to make a decision. Practitioners should take this aspect into consideration when planning for efficient decision making and possibly reducing the complexity of decisions. Our results also show that when a change request originates from an important customer, the request is more often accepted. The results provide input into the discussion of whether a large company should focus on only a few of its large customers and disregard its significantly larger group of small customers. The results provide valuable insights for researchers, who can use them to plan research of decision-making processes and methods, and for practitioners, who can use them to optimize their decision-making processes. In future work, we plan to investigate other decision characteristics, such as the number of stakeholders involved in the discussion about the potential change or the number of dependencies between software components. {\textcopyright} 2015 John Wiley {\&} Sons, Ltd.},
  annote   = {cited By 0},
  doi      = {10.1002/smr.1721},
  keywords = {Commerce; Requirements engineering; Sales; Surveys,Decision making,Decision making process; Decision outcome; Large-},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941654310{\&}doi=10.1002{\%}2Fsmr.1721{\&}partnerID=40{\&}md5=499f3f223a5e1a9a599d1570af30de82},
}

@Article{Dang2014135,
  author   = {Dang, Han-Hing and Gl{\"{u}}ck, Roland and M{\"{o}}ller, Bernhard and Roocks, Patrick and Zelend, Andreas},
  title    = {{Exploring modal worlds}},
  journal  = {Journal of Logical and Algebraic Methods in Programming},
  year     = {2014},
  volume   = {83},
  number   = {2},
  pages    = {135--153},
  issn     = {2352-2208},
  abstract = {Abstract Modal idempotent semirings cover a large set of different applications. The paper presents a small collection of these, ranging from algebraic logics for program correctness over bisimulation refinement, formal concept analysis, database preferences to feature oriented software development. We provide new results and/or views on these domains; the modal semiring setting allows a concise and unified treatment, while being more general than, e.g., standard relation algebra. },
  annote   = {Festschrift in Honour of Gunther Schmidt on the Occasion of his 75th Birthday},
  doi      = {https://doi.org/10.1016/j.jlap.2014.02.004},
  keywords = {Bisimulation,Formal concept analysis,Pareto front,Rectangles,Separation logic,Software product lines},
  url      = {http://www.sciencedirect.com/science/article/pii/S1567832614000058},
}

@Article{BEZERRA2017366,
  author   = {Bezerra, Carla I M and Andrade, Rossana M C and Monteiro, Jose Maria},
  title    = {{Exploring quality measures for the evaluation of feature models: a case study}},
  journal  = {Journal of Systems and Software},
  year     = {2017},
  volume   = {131},
  pages    = {366--385},
  issn     = {0164-1212},
  abstract = {Evaluating the quality of a feature model is essential to ensure that errors in the early stages do not spread throughout the Software Product Line (SPL). One way to evaluate the feature model is to use measures that could be associated with the feature model quality characteristics and their quality attributes. In this paper, we aim at investigating how measures can be applied to the quality assessment of SPL feature models. We performed an exploratory case study using the COfFEE maintainability measures catalog and the S.P.L.O.T. feature models repository. In order to support this case study, we built a dataset (denoted by MAcchiATO) containing the values of 32 measures from COfFEE for 218 software feature models, extracted from S.P.L.O.T. This research approach allowed us to explore three different data analysis techniques. First, we applied the Spearman's rank correlation coefficient in order to identify relationships between the measures. This analysis showed that not all 32 measures in COfFEE are necessary to reveal the quality of a feature model and just 15 measures could be used. Next, the 32 measures in COfFEE were grouped by applying the Principal Component Analysis and a set of 9 new grouped measures were defined. Finally, we used the Tolerance Interval technique to define statistical thresholds for these 9 new grouped measures. So, our findings suggest that measures can be effectively used to support the quality evaluation of SPL feature models.},
  doi      = {https://doi.org/10.1016/j.jss.2016.07.040},
  keywords = {Feature models,Measures,Quality evaluation,Software product line},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121216301340},
}

@Article{Jaring2004449,
  author   = {Jaring, M and Bosch, J},
  title    = {{Expressing product diversification - Categorizing and classifying variability in software product family engineering}},
  journal  = {International Journal of Software Engineering and Knowledge Engineering},
  year     = {2004},
  volume   = {14},
  number   = {5},
  pages    = {449--470},
  abstract = {In a software product family context, software architects design architectures that support product diversification in both space (multiple contexts) and time (changing contexts). Product diversification is based on the concept of variability: a single architecture and a set of components support a family of products. Software product families have to support increasing amounts of variability, thereby making variability engineering a primary concern in software product family development. The first part of this paper (1) suggests a two-dimensional, orthogonal categorization of variability realization techniques and classifies these variability categories into system maturity levels. The second part (2) discusses a case study of an industrial software product family of mobile communication infrastructure for professional markets such as the military. The study categorizes and classifies the variability in this product family according to criteria common to virtually all software development projects.},
  annote   = {cited By 3},
  doi      = {10.1142/S0218194004001804},
  keywords = {Computer aided design; Computer architecture; Mark,Product diversification; Software product familie,Software engineering},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-10044283207{\&}doi=10.1142{\%}2FS0218194004001804{\&}partnerID=40{\&}md5=d4be64864295692ddc6446ec441911ea},
}

@Article{HOFNER2016952,
  author   = {H{\"{o}}fner, Peter and M{\"{o}}ller, Bernhard},
  title    = {{Extended Feature Algebra}},
  journal  = {Journal of Logical and Algebraic Methods in Programming},
  year     = {2016},
  volume   = {85},
  number   = {5, Part 2},
  pages    = {952--971},
  issn     = {2352-2208},
  abstract = {Feature Algebra was introduced as an abstract framework for feature-oriented software development. One goal is to provide a common, clearly defined basis for the key ideas of feature-orientation. The algebra captures major aspects of feature-orientation, such as the hierarchical structure of features and feature composition. However, as we will show, it is not able to model aspects at the level of code, i.e., situations where code fragments of different features have to be merged. In other words, it does not reflect details of concrete implementations. In this paper we first present concrete models for the original axioms of Feature Algebra which represent the main concepts of feature-oriented programs. This shows that the abstract Feature Algebra can be interpreted in different ways. We then use these models to show that the axioms of Feature Algebra do not properly reflect all aspects of feature-orientation from the level of directory structures down to the level of actual code. This gives motivation to extend the abstract algebra, which is the second main contribution of the paper. We modify the axioms and introduce the concept of an Extended Feature Algebra. As third contribution, we introduce more operators to cover concepts like overriding in the abstract setting.},
  annote   = {Articles dedicated to Prof. J. N. Oliveira on the occasion of his 60th birthday},
  doi      = {https://doi.org/10.1016/j.jlamp.2015.12.002},
  keywords = {Algebraic characterisation of FOSD,Feature Algebra,Feature-orientation},
  url      = {http://www.sciencedirect.com/science/article/pii/S2352220815001480},
}

@Article{Stallinger201293,
  author   = {Stallinger, F and Neumann, R},
  title    = {{Extending ISO/IEC 12207 with software product management: A process reference model proposal}},
  journal  = {Communications in Computer and Information Science},
  year     = {2012},
  volume   = {290 CCIS},
  pages    = {93--106},
  abstract = {Software product management is generally expected to link and integrate business and product related goals with core software engineering and software life cycle activities. Empirical research demonstrates the positive effect of mature software product management practices on key software development performance indicators. Nevertheless, the various frameworks available for software product management have distinct and diverse focus points, are often linked or incorporated with specific development paradigms, or lack integration with or addressing of core software engineering activities. On the other hand, traditional software process improvement approaches generally lack the provision of explicit or detailed software product management activities. - In this paper we build on the results of preceding research on identifying a lack of software product management practices within ISO/IEC 12207 and on deriving key outcomes of software product management activities from selected software product management frameworks. Based on these results we propose a process reference model for software product management that can be integrated with the process reference model as defined in ISO/IEC 12207 for software life cycle processes. {\textcopyright} 2012 Springer-Verlag.},
  annote   = {cited By 7},
  doi      = {10.1007/978-3-642-30439-2_9},
  keywords = {Benchmarking; Life cycle; Models; Software design,ISO/IEC; ISO/IEC 15504; Process assessments; Proce,Management},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862188967{\&}doi=10.1007{\%}2F978-3-642-30439-2{\_}9{\&}partnerID=40{\&}md5=bc10ccce40d79f40f8b241c020b2da7c},
}

@Article{Bakar20161297,
  author   = {Bakar, N H and Kasirun, Z M and Salleh, N and Jalab, H A},
  title    = {{Extracting features from online software reviews to aid requirements reuse}},
  journal  = {Applied Soft Computing Journal},
  year     = {2016},
  volume   = {49},
  pages    = {1297--1315},
  abstract = {Sets of common features are essential assets to be reused in fulfilling specific needs in software product line methodology. In Requirements Reuse (RR), the extraction of software features from Software Requirement Specifications (SRS) is viable only to practitioners who have access to these software artefacts. Due to organisational privacy, SRS are always kept confidential and not easily available to the public. As alternatives, researchers opted to use the publicly available software descriptions such as product brochures and online software descriptions to identify potential software features to initiate the RR process. The aim of this paper is to propose a semi-automated approach, known as Feature Extraction for Reuse of Natural Language requirements (FENL), to extract phrases that can represent software features from software reviews in the absence of SRS as a way to initiate the RR process. FENL is composed of four stages, which depend on keyword occurrences from several combinations of nouns, verbs, and/or adjectives. In the experiment conducted, phrases that could reflect software features, which reside within online software reviews were extracted by utilising the techniques from information retrieval (IR) area. As a way to demonstrate the feature groupings phase, a semi-automated approach to group the extracted features were then conducted with the assistance of a modified word overlap algorithm. As for the evaluation, the proposed extraction approach is evaluated through experiments against the truth data set created manually. The performance results obtained from the feature extraction phase indicates that the proposed approach performed comparably with related works in terms of recall, precision, and F-Measure. {\textcopyright} 2016 Elsevier B.V.},
  annote   = {cited By 0},
  doi      = {10.1016/j.asoc.2016.07.048},
  keywords = {Automation; Computer software selection and evalua,Computer software reusability,Extracting features; Latent Semantic Analysis; NA},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997124422{\&}doi=10.1016{\%}2Fj.asoc.2016.07.048{\&}partnerID=40{\&}md5=e7c61585c2eae8eaf3f9c8cca858e5a2},
}

@Article{Deb2016265,
  author   = {Deb, Novarun and Chaki, Nabendu and Ghose, Aditya},
  title    = {{Extracting finite state models from i* models}},
  journal  = {Journal of Systems and Software},
  year     = {2016},
  volume   = {121},
  pages    = {265--280},
  issn     = {0164-1212},
  abstract = {Abstract i* models are inherently sequence agnostic. This makes the process of cross-checking i* models against temporal properties quite impossible. There is an immediate industrial need to bridge the gap between such a sequence agnostic model and a standardized model verifier so that model checking can be performed in the requirement analysis phase itself. In this paper, we first spell out the Naive Algorithm that generates all possible finite state models corresponding to a given i* model. The growth of the finite state model space can be mapped to the problem of finding the number of possible paths between the Least Upper Bound (LUB) and the Greatest Lower Bound (GLB) of a k-dimensional hypercube lattice structure. The mathematics for doing a quantitative analysis of the space growth has also been presented. The Naive Algorithm has its main drawback in the hyperexponential growth of the model space. The Semantic Implosion Algorithm is proposed as a solution to the hyperexponential problem. This algorithm exploits the temporal information embedded within the i* model of an enterprise to reduce the rate of growth of the finite state model space. A comparative quantitative analysis between the two approaches concludes the superiority of the Semantic Implosion Algorithm. },
  doi      = {https://doi.org/10.1016/j.jss.2016.03.038},
  keywords = {Model checking,Model transformation,i* model},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121216300048},
}

@Article{Sobernig2016140,
  author   = {Sobernig, Stefan and Hoisl, Bernhard and Strembeck, Mark},
  title    = {{Extracting reusable design decisions for UML-based domain-specific languages: A multi-method study}},
  journal  = {Journal of Systems and Software},
  year     = {2016},
  volume   = {113},
  pages    = {140--172},
  issn     = {0164-1212},
  abstract = {Abstract When developing domain-specific modeling languages (DSMLs), software engineers have to make a number of important design decisions on the {\{}DSML{\}} itself, or on the software-development process that is applied to develop the DSML. Thus, making well-informed design decisions is a critical factor in developing DSMLs. To support this decision-making process, the model-driven development community has started to collect established design practices in terms of patterns, guidelines, story-telling, and procedural models. However, most of these documentation practices do not capture the details necessary to reuse the rationale behind these decisions in other {\{}DSML{\}} projects. In this paper, we report on a three-year research effort to compile and to empirically validate a catalog of structured decision descriptions (decision records) for UML-based DSMLs. This catalog is based on design decisions extracted from 90 {\{}DSML{\}} projects. These projects were identified—among others—via an extensive systematic literature review (SLR) for the years 2005–2012. Based on more than 8,000 candidate publications, we finally selected 84 publications for extracting design-decision data. The extracted data were evaluated quantitatively using a frequent-item-set analysis to obtain characteristic combinations of design decisions and qualitatively to document recurring documentation issues for UML-based DSMLs. We revised the collected decision records based on this evidence and made the decision-record catalog for developing UML-based {\{}DSMLs{\}} publicly available. Furthermore, our study offers insights into {\{}UML{\}} usage (e.g. diagram types) and into the adoption of {\{}UML{\}} extension techniques (e.g. metamodel extensions, profiles). },
  doi      = {https://doi.org/10.1016/j.jss.2015.11.037},
  keywords = {Design decision,Design rationale,Domain-specific language,Domain-specific modeling,Model-driven development,Unified modeling language},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121215002617},
}

@Article{Ghanam201043,
  author   = {Ghanam, Y and Maurer, F},
  title    = {{Extreme product line engineering - Refactoring for variability: A test-driven approach}},
  journal  = {Lecture Notes in Business Information Processing},
  year     = {2010},
  volume   = {48 LNBIP},
  pages    = {43--57},
  abstract = {Software product lines - families of similar but not identical software products - need to address the issue of feature variability. That is, a single feature might require various implementations for different customers. Also, features might need optional extensions that are needed by some but not all products. Software product line engineering manages variability by conducting a thorough domain analysis upfront during the planning phases. However, upfront, heavyweight planning approaches are not well-aligned with the values of minimalistic practices like XP where bottom-up, incremental development is common. In this paper, we introduce a bottom-up, test-driven approach to introduce variability to systems by reactively refactoring existing code. We support our approach with an eclipse plug-in to automate the refactoring process. We evaluate our approach by a case study to determine the feasibility and practicality of the approach. {\textcopyright} Springer-Verlag Berlin Heidelberg 2010.},
  annote   = {cited By 8},
  doi      = {10.1007/978-3-642-13054-0_4},
  keywords = {Agile methods,Computer programming,Computer software,Feature variability,Incremental d,Production engineering,Softwar},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876252634{\&}doi=10.1007{\%}2F978-3-642-13054-0{\_}4{\&}partnerID=40{\&}md5=322d60a770930761dd517f417e6d38cf},
}

@Article{Cámara2009116,
  author   = {C{\'{a}}mara, J and Kobsa, A},
  title    = {{Facilitating controlled tests of website design changes using aspect-oriented software development and software product lines}},
  journal  = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year     = {2009},
  volume   = {5740 LNCS},
  pages    = {116--135},
  abstract = {Controlled online experiments in which envisaged changes to a website are first tested live with a small subset of site visitors have proven to predict the effects of these changes quite accurately. However, these experiments often require expensive infrastructure and are costly in terms of development effort. This paper advocates a systematic approach to the design and implementation of such experiments in order to overcome the aforementioned drawbacks by making use of Aspect-Oriented Software Development and Software Product Lines. {\textcopyright} 2009 Springer.},
  annote   = {cited By 1},
  doi      = {10.1007/978-3-642-03722-1_5},
  keywords = {Aspect oriented software development; Controlled t,Computer software,Computer systems programming; Experiments; Softwa},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-70350587132{\&}doi=10.1007{\%}2F978-3-642-03722-1{\_}5{\&}partnerID=40{\&}md5=f0c491bcf7c149426d1da832cf42dee3},
}

@Article{Fronza20132,
  author   = {Fronza, Ilenia and Sillitti, Alberto and Succi, Giancarlo and Terho, Mikko and Vlasenko, Jelena},
  title    = {{Failure prediction based on log files using Random Indexing and Support Vector Machines}},
  journal  = {Journal of Systems and Software},
  year     = {2013},
  volume   = {86},
  number   = {1},
  pages    = {2--11},
  issn     = {0164-1212},
  abstract = {Research problem The impact of failures on software systems can be substantial since the recovery process can require unexpected amounts of time and resources. Accurate failure predictions can help in mitigating the impact of failures. Resources, applications, and services can be scheduled to limit the impact of failures. However, providing accurate predictions sufficiently ahead is challenging. Log files contain messages that represent a change of system state. A sequence or a pattern of messages may be used to predict failures. Contribution We describe an approach to predict failures based on log files using Random Indexing (RI) and Support Vector Machines (SVMs). Method {\{}RI{\}} is applied to represent sequences: each operation is characterized in terms of its context. {\{}SVMs{\}} associate sequences to a class of failures or non-failures. Weighted {\{}SVMs{\}} are applied to deal with imbalanced datasets and to improve the true positive rate. We apply our approach to log files collected during approximately three months of work in a large European manufacturing company. Results According to our results, weighted {\{}SVMs{\}} sacrifice some specificity to improve sensitivity. Specificity remains higher than 0.80 in four out of six analyzed applications. Conclusions Overall, our approach is very reliable in predicting both failures and non-failures. },
  doi      = {https://doi.org/10.1016/j.jss.2012.06.025},
  keywords = {Event sequence data,Failure prediction,Log files,Random Indexing,Support Vector Machine (SVM)},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121212001732},
}

@Article{Acher2013657,
  author   = {Acher, Mathieu and Collet, Philippe and Lahire, Philippe and France, Robert B},
  title    = {{FAMILIAR: A domain-specific language for large scale management of feature models}},
  journal  = {Science of Computer Programming},
  year     = {2013},
  volume   = {78},
  number   = {6},
  pages    = {657--681},
  issn     = {0167-6423},
  abstract = {The feature model formalism has become the de facto standard for managing variability in software product lines (SPLs). In practice, developing an {\{}SPL{\}} can involve modeling a large number of features representing different viewpoints, sub-systems or concerns of the software system. This activity is generally tedious and error-prone. In this article, we present {\{}FAMILIAR{\}} a Domain-Specific Language (DSL) that is dedicated to the large scale management of feature models and that complements existing tool support. The language provides a powerful support for separating concerns in feature modeling, through the provision of composition and decomposition operators, reasoning facilities and scripting capabilities with modularization mechanisms. We illustrate how an {\{}SPL{\}} consisting of medical imaging services can be practically managed using reusable {\{}FAMILIAR{\}} scripts that implement reasoning mechanisms. We also report on various usages and applications of {\{}FAMILIAR{\}} and its operators, to demonstrate their applicability to different domains and use for different purposes. },
  annote   = {Special section: The Programming Languages track at the 26th {\{}ACM{\}} Symposium on Applied Computing (SAC 2011) {\&} Special section on Agent-oriented Design Methods and Programming Techniques for Distributed Computing in Dynamic and Complex Environments},
  doi      = {https://doi.org/10.1016/j.scico.2012.12.004},
  keywords = {Domain-specific language,Feature model,Model management,Software product lines,Variability},
  url      = {http://www.sciencedirect.com/science/article/pii/S0167642312002158},
}

@Article{Thüm201311,
  author   = {Th{\"{u}}m, T and Schaefer, I and Apel, S and Hentschel, M},
  title    = {{Family-based deductive verification of software product lines}},
  journal  = {ACM SIGPLAN Notices},
  year     = {2013},
  volume   = {48},
  number   = {3},
  pages    = {11--20},
  abstract = {A software product line is a set of similar software products that share a common code base. While software product lines can be implemented efficiently using feature-oriented programming, verifying each product individually does not scale, especially if human effort is required (e.g., as in interactive theorem proving). We present a family-based approach of deductive verification to prove the correctness of a software product line efficiently. We illustrate and evaluate our approach for software product lines written in a feature-oriented dialect of Java and specified using the Java Modeling Language. We show that the theorem prover KeY can be used off-the-shelf for this task, without any modifications. Compared to the individual verification of each product, our approach reduces the verification time needed for our case study by more than 85 {\%}. Copyright 2012 ACM.},
  annote   = {cited By 4},
  doi      = {10.1145/2480361.2371404},
  keywords = {Computer software,Deductive verification; Feature-oriented programmi,Software design; Theorem proving},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877914932{\&}doi=10.1145{\%}2F2480361.2371404{\&}partnerID=40{\&}md5=040800419faa1099e7b9d3d297b62583},
}

@Article{Han2015133,
  author   = {Han, Sangyup and Kim, Myungchul and Lee, Ben and Kang, Sungwon},
  title    = {{Fast Directional Handoff and lightweight retransmission protocol for enhancing multimedia quality in indoor {\{}WLANs{\}}}},
  journal  = {Computer Networks},
  year     = {2015},
  volume   = {79},
  pages    = {133--147},
  issn     = {1389-1286},
  abstract = {Abstract More and more mobile devices such as smartphones are being used with {\{}IEEE{\}} 802.11 wireless {\{}LANs{\}} (WLANs or Wi-Fi). However, mobile users are still experiencing poor service quality on the move due to the large handoff delay and packet loss problem. In order to reduce the delay, a new handoff scheme using the geomagnetic sensor embedded in mobile devices is proposed in this paper. The proposed scheme predicts the movement direction of a Mobile Station (MS) from the currently associated Access Point (AP) and performs active scanning with a reduced number of channels. In terms of the packet loss, a lightweight retransmission protocol is also proposed to minimize lost packets on Wi-Fi without producing a lot of acknowledgement packets. The proposed approaches are implemented on Android smartphones, and their performance is evaluated in a real indoor {\{}WLAN{\}} environment. The evaluation results demonstrate that the proposed schemes maintain seamless quality for real-time video even in an environment with frequent handoffs. Note that the proposed schemes are a client-only solution and do not require modification of the existing APs, which renders them very practical. },
  doi      = {https://doi.org/10.1016/j.comnet.2014.12.019},
  keywords = {Digital compass,Fast Directional Handoff,Geomagnetic sensor,Lightweight retransmission protocol,{\{}IEEE{\}} 802.11},
  url      = {http://www.sciencedirect.com/science/article/pii/S1389128615000031},
}

@Article{Nasir20159,
  author   = {Nasir, S Zafar and Mahmood, Tariq and Shaikh, M Shahid and Shaikh, Zubair A},
  title    = {{Fault-tolerant context development and requirement validation in {\{}ERP{\}} systems}},
  journal  = {Computer Standards {\&} Interfaces},
  year     = {2015},
  volume   = {37},
  pages    = {9--19},
  issn     = {0920-5489},
  abstract = {Abstract This paper presents context development and requirement validation to overcome maintenance problems in Enterprise Resource Planning (ERP) systems. Using {\{}ERP{\}} data of a local petroleum firm, we employ knowledge integration to dynamically validate users' requirements, and to gather, analyze, and represent context through knowledge models. We also employ context-awareness to model the {\{}ERP{\}} context, along with a user requirement model. We employ context affinity to determine impact of these models on requirements' validation. We apply fault-tolerance on these models by using data mining to pre-identify delays in delivery of petroleum products, and to predict faulty contextual {\{}ERP{\}} product configuration. },
  doi      = {https://doi.org/10.1016/j.csi.2014.05.001},
  keywords = {Context development,Data Mining,Enterprise Resource Planning,Fault tolerance,Requirement validation},
  url      = {http://www.sciencedirect.com/science/article/pii/S0920548914000695},
}

@Article{ABRANTES2014874,
  author   = {Abrantes, Rui and Figueiredo, Jos{\'{e}}},
  title    = {{Feature based process framework to manage scope in dynamic NPD portfolios}},
  journal  = {International Journal of Project Management},
  year     = {2014},
  volume   = {32},
  number   = {5},
  pages    = {874--884},
  issn     = {0263-7863},
  abstract = {The need to develop new products in increasingly frequent cycles of innovation drives organizations to form new product development (NPD) portfolios. In such dynamic environments, organizations need to reinforce their capabilities to deal with the simultaneity of multiple NPD projects, as well as with the frequent changes of the product scope. Many organizations, that have adopted the typical NPD process enforcing a streamlined product development process, are challenged beyond strict planning and rigorous control of their NPD projects. This paper identifies the challenges to manage the scope of a complete portfolio of NPD projects within the dynamic context that organizations face today, and using existing scope management practices. This paper suggests a novel approach to structuring the scope in dynamic NPD portfolios using feature modeling, and illustrates its use in an action-research case.},
  doi      = {https://doi.org/10.1016/j.ijproman.2013.10.014},
  keywords = {Action research,Feature modeling,New product development,Process framework,Project portfolio management,Scope management},
  url      = {http://www.sciencedirect.com/science/article/pii/S0263786313001531},
}

@Article{Hall2000449,
  author   = {Hall, Robert J},
  title    = {{Feature combination and interaction detection via foreground/background models}},
  journal  = {Computer Networks},
  year     = {2000},
  volume   = {32},
  number   = {4},
  pages    = {449--469},
  issn     = {1389-1286},
  abstract = {One approach to building complex software product families is to partition the possible functions of the system into conceptual chunks called features. Ideally, system instances are rapidly assembled by combining features desired by the particular customer. Unfortunately, features often interact, meaning their combination causes unintended undesirable behavior even though in isolation the features work fine. This paper describes an approach to feature combination and interaction detection via foreground/background models, which allows expressing features as augmentations to the behavior of a base model. It also classifies interactions into three categories, based on how they can be detected, and describes implemented tools which can detect interactions from the three categories. I show why this approach avoids falsely detecting the spurious Type I interactions to which many existing approaches are prone. The tools and methodology, as well as the prevalence of spurious interactions in existing approaches, are illustrated through application to telephony features from the feature interaction contest associated with FIW'98. This data provides evidence that the foreground/background approach catches more nonspurious interactions, with less human effort, than competing approaches. },
  doi      = {https://doi.org/10.1016/S1389-1286(00)00010-4},
  keywords = {Feature interaction,Formal methods,Software validation},
  url      = {http://www.sciencedirect.com/science/article/pii/S1389128600000104},
}

@Article{Cafeo201637,
  author   = {Cafeo, Bruno B P and Cirilo, Elder and Garcia, Alessandro and Dantas, Francisco and Lee, Jaejoon},
  title    = {{Feature dependencies as change propagators: An exploratory study of software product lines}},
  journal  = {Information and Software Technology},
  year     = {2016},
  volume   = {69},
  pages    = {37--49},
  issn     = {0950-5849},
  abstract = {AbstractContext A Software Product Line (SPL) is a set of software systems that share common functionalities, so-called features. When features are related, we consider this relation a feature dependency. Whenever a new feature is added, the presence of feature dependencies in the source code may increase the maintenance effort. In particular, along the maintenance of {\{}SPL{\}} implementation, added features may induce changes in other features, the so-called change propagation. Change propagation is the set of ripple changes required to other features whenever a particular feature is added or changed. Objective The relationship between feature dependency and change propagation is not well understood. Therefore, the objective of our study is to examine the relation between feature dependency and change propagation. Method We investigate change propagation through feature dependencies in additive changes on five evolving SPLs. We analysed a wide range of additive changes in 21 representations of those SPLs. This analysis enabled us to understand whether and how features dependencies and change propagations are related. Results The results have empirically confirmed for the first time the strong relation between feature dependency and change propagation. We also identified what are the circumstances involving dependent features that are more likely to cause change propagation. Surprisingly, the results also suggested that the extent of change propagation across {\{}SPL{\}} features might be higher than the one found in previous studies of dependent modules in non-SPLs. We also found a concentration of change propagation in a few feature dependencies. Conclusion Even though the results show that there is a strong relation between feature dependencies and change propagation, such relation is not alike for all dependencies. This indicates that (i) a general feature dependency minimisation might not ameliorate the change propagation, and (ii) feature dependency properties must be analysed beforehand to drive maintenance effort to important dependencies. },
  doi      = {https://doi.org/10.1016/j.infsof.2015.08.009},
  keywords = {Change propagation,Feature dependency,Maintenance,Software Product Line},
  url      = {http://www.sciencedirect.com/science/article/pii/S0950584915001512},
}

@Article{Bakar2015132,
  author   = {Bakar, Noor Hasrina and Kasirun, Zarinah M and Salleh, Norsaremah},
  title    = {{Feature extraction approaches from natural language requirements for reuse in software product lines: A systematic literature review}},
  journal  = {Journal of Systems and Software},
  year     = {2015},
  volume   = {106},
  pages    = {132--149},
  issn     = {0164-1212},
  abstract = {Abstract Requirements for implemented system can be extracted and reused for a production of a new similar system. Extraction of common and variable features from requirements leverages the benefits of the software product lines engineering (SPLE). Although various approaches have been proposed in feature extractions from natural language (NL) requirements, no related literature review has been published to date for this topic. This paper provides a systematic literature review (SLR) of the state-of-the-art approaches in feature extractions from {\{}NL{\}} requirements for reuse in SPLE. We have included 13 studies in our synthesis of evidence and the results showed that hybrid natural language processing approaches were found to be in common for overall feature extraction process. A mixture of automated and semi-automated feature clustering approaches from data mining and information retrieval were also used to group common features, with only some approaches coming with support tools. However, most of the support tools proposed in the selected studies were not made available publicly and thus making it hard for practitioners' adoption. As for the evaluation, this {\{}SLR{\}} reveals that not all studies employed software metrics as ways to validate experiments and case studies. Finally, the quality assessment conducted confirms that practitioners' guidelines were absent in the selected studies. },
  doi      = {https://doi.org/10.1016/j.jss.2015.05.006},
  keywords = {Feature extractions,Natural language requirements,Requirements reuse,Software product lines,Systematic literature review},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121215001004},
}

@Article{Kamoun2018239,
  author   = {Kamoun, A and {Hadj Kacem}, M and {Hadj Kacem}, A and Drira, K},
  title    = {{Feature model as a design-pattern-based service contract for the service provider in the service oriented architecture}},
  journal  = {Lecture Notes in Business Information Processing},
  year     = {2018},
  volume   = {321},
  pages    = {239--264},
  abstract = {In Service Oriented Architecture (SOA), many feature modeling approaches of Service Provider (SP) have been proposed, notably: the two widely used service contracts WSDL and WADL. By studying these approaches, we found that they suffer from several problems, notably: they only work for specific communication technologies (e.g., SOAP or REST) and they do not explicitly model SOA Design Pattern (DPs) and their compounds. One major benefit of using a DP or a compound DP is to develop SPs with proven design solutions. In this paper, in order to overcome these problems, we propose an approach that integrates Software Product Line (SPL) techniques in the development of SPs. Essentially, we propose a Feature Model (FM), which is the defacto standard for variability modeling in SPL, for the feature modeling of SP. This FM, named FMSP, is designed as a DP-based service contract for SP that models different features including 16 SOA DPs and their compounds that are related to the service messaging category. Its objective to enable developers to generate fully functional, valid, DP-based and highly customized SPs for different communication technologies. Through a practical case study and a developed tool, we validate our FMSP and demonstrate that it reduces the development costs (effort and time) of SPs. {\textcopyright} Springer International Publishing AG, part of Springer Nature 2018.},
  annote   = {cited By 0; Conference of 19th International Conference on Enterprise Information Systems, ICEIS 2017 ; Conference Date: 26 April 2017 Through 29 April 2017; Conference Code:214609},
  doi      = {10.1007/978-3-319-93375-7_12},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048953762{\&}doi=10.1007{\%}2F978-3-319-93375-7{\_}12{\&}partnerID=40{\&}md5=f27ee629950920442dfe8e1b3bda2af7},
}

@Article{Acher2012629,
  author   = {Acher, M and Heymans, P and Collet, P and Quinton, C and Lahire, P and Merle, P},
  title    = {{Feature model differences}},
  journal  = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year     = {2012},
  volume   = {7328 LNCS},
  pages    = {629--645},
  abstract = {Feature models are a widespread means to represent commonality and variability in software product lines. As is the case for other kinds of models, computing and managing feature model differences is useful in various real-world situations. In this paper, we propose a set of novel differencing techniques that combine syntactic and semantic mechanisms, and automatically produce meaningful differences. Practitioners can exploit our results in various ways: to understand, manipulate, visualize and reason about differences. They can also combine them with existing feature model composition and decomposition operators. The proposed automations rely on satisfiability algorithms. They come with a dedicated language and a comprehensive environment. We illustrate and evaluate the practical usage of our techniques through a case study dealing with a configurable component framework. {\textcopyright} 2012 Springer-Verlag Berlin Heidelberg.},
  annote   = {cited By 12},
  doi      = {10.1007/978-3-642-31095-9_41},
  keywords = {Commonality and variability; Component framework;,Information systems; Semantics; Systems engineeri,Software design},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867776734{\&}doi=10.1007{\%}2F978-3-642-31095-9{\_}41{\&}partnerID=40{\&}md5=80cb45d216ca14a21f459ac1bbecafdb},
}

@Article{Shen2009170,
  author   = {Shen, L and Peng, X and Zhao, W},
  title    = {{Feature-driven and incremental variability generalization in software product line}},
  journal  = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year     = {2009},
  volume   = {5791 LNCS},
  pages    = {170--180},
  abstract = {In the lifecycle of a software product line (SPL), incremental generalization is usually required to extend the variability of existing core assets to support the new or changed application requirements. In addition, the generalization should conform to the evolved SPL requirements which are usually represented by a feature model. In this paper, we propose a feature-driven and incremental variability generalization method based on the aspect-oriented variability implementation techniques. It addresses a set of basic scenarios where program-level JBoss-AOP based reference implementations respond to the feature-level variability generalization patterns. It also provides the corresponding guidance to compose these patterns in more complex cases. Based on the method, we present a case study and related discussions. {\textcopyright} 2009 Springer Berlin Heidelberg.},
  annote   = {cited By 0},
  doi      = {10.1007/978-3-642-04211-9_17},
  keywords = {Application requirements; Aspect-oriented; Core as,Computer software reusability,Speech recognition},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-70350381125{\&}doi=10.1007{\%}2F978-3-642-04211-9{\_}17{\&}partnerID=40{\&}md5=3b8d03018048714ba3902c69c190962f},
}

@Article{Lanna201859,
  author   = {Lanna, A and Castro, T and Alves, V and Rodrigues, G and Schobbens, P.-Y. and Apel, S},
  title    = {{Feature-family-based reliability analysis of software product lines}},
  journal  = {Information and Software Technology},
  year     = {2018},
  volume   = {94},
  pages    = {59--81},
  abstract = {Context Verification techniques are being applied to ensure that software systems achieve desired quality levels and fulfill functional and non-functional requirements. However, applying these techniques to software product lines is challenging, given the exponential blowup of the number of products. Current product-line verification techniques leverage symbolic model checking and variability information to optimize the analysis, but still face limitations that make them costly or infeasible. In particular, state-of-the-art verification techniques for product-line reliability analysis are enumerative which hinders their applicability, given the latent exponential blowup of the configuration space. Objective The objectives of this paper are the following: (a) we present a method to efficiently compute the reliability of all configurations of a compositional or annotation-based software product line from its UML behavioral models, (b) we provide a tool that implements the proposed method, and (c) we report on an empirical study comparing the performance of different reliability analysis strategies for software product lines. Method We present a novel feature-family-based analysis strategy to compute the reliability of all products of a (compositional or annotation-based) software product line. The feature-based step of our strategy divides the behavioral models into smaller units that can be analyzed more efficiently. The family-based step performs the reliability computation for all configurations at once by evaluating reliability expressions in terms of a suitable variational data structure. Results Our empirical results show that our feature-family-based strategy for reliability analysis outperforms, in terms of time and space, four state-of-the-art strategies (product-based, family-based, feature-product-based, and family-product-based) for the same property. It is the only one that could be scaled to a 220-fold increase in the size of the configuration space. Conclusion Our feature-family-based strategy leverages both feature- and family-based strategies by taming the size of the models to be analyzed and by avoiding the products enumeration inherent to some state-of-the-art analysis methods. {\textcopyright} 2017 Elsevier B.V.},
  annote   = {cited By 0},
  doi      = {10.1016/j.infsof.2017.10.001},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031715221{\&}doi=10.1016{\%}2Fj.infsof.2017.10.001{\&}partnerID=40{\&}md5=95561d47e8bbc8279031b08250c08189},
}

@Article{THUM201470,
  author   = {Th{\"{u}}m, Thomas and K{\"{a}}stner, Christian and Benduhn, Fabian and Meinicke, Jens and Saake, Gunter and Leich, Thomas},
  title    = {{FeatureIDE: An extensible framework for feature-oriented software development}},
  journal  = {Science of Computer Programming},
  year     = {2014},
  volume   = {79},
  pages    = {70--85},
  issn     = {0167-6423},
  abstract = {FeatureIDE is an open-source framework for feature-oriented software development (FOSD) based on Eclipse. FOSD is a paradigm for the construction, customization, and synthesis of software systems. Code artifacts are mapped to features, and a customized software system can be generated given a selection of features. The set of software systems that can be generated is called a software product line (SPL). FeatureIDE supports several FOSD implementation techniques such as feature-oriented programming, aspect-oriented programming, delta-oriented programming, and preprocessors. All phases of FOSD are supported in FeatureIDE, namely domain analysis, requirements analysis, domain implementation, and software generation.},
  annote   = {Experimental Software and Toolkits (EST 4): A special issue of the Workshop on Academic Software Development Tools and Techniques (WASDeTT-3 2010)},
  doi      = {https://doi.org/10.1016/j.scico.2012.06.002},
  keywords = {Aspect-oriented programming,Delta-oriented programming,Feature modeling,Feature-oriented programming,Feature-oriented software development,Preprocessors,Software product lines,Tool support},
  url      = {http://www.sciencedirect.com/science/article/pii/S0167642312001128},
}

@Article{Apel20132399,
  author   = {Apel, Sven and von Rhein, Alexander and Th{\"{u}}m, Thomas and K{\"{a}}stner, Christian},
  title    = {{Feature-interaction detection based on feature-based specifications}},
  journal  = {Computer Networks},
  year     = {2013},
  volume   = {57},
  number   = {12},
  pages    = {2399--2409},
  issn     = {1389-1286},
  abstract = {Abstract Formal specification and verification techniques have been used successfully to detect feature interactions. We investigate whether feature-based specifications can be used for this task. Feature-based specifications are a special class of specifications that aim at modularity in open-world, feature-oriented systems. The question we address is whether modularity of specifications impairs the ability to detect feature interactions, which cut across feature boundaries. In an exploratory study on 10 feature-oriented systems, we found that the majority of feature interactions could be detected based on feature-based specifications, but some specifications have not been modularized properly and require undesirable workarounds to modularization. Based on the study, we discuss the merits and limitations of feature-based specifications, as well as open issues and perspectives. A goal that underlies our work is to raise awareness of the importance and challenges of feature-based specification. },
  annote   = {Feature Interaction in Communications and Software Systems},
  doi      = {https://doi.org/10.1016/j.comnet.2013.02.025},
  keywords = {Feature interaction,Feature orientation,Feature-based specification,Modularity,Software product lines},
  url      = {http://www.sciencedirect.com/science/article/pii/S1389128613001102},
}

@Article{Kang200545,
  author        = {Kang, K C and Kim, M and Lee, J and Kim, B},
  title         = {{Feature-oriented re-engineering of legacy systems into product line assets - A case study}},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year          = {2005},
  volume        = {3714 LNCS},
  pages         = {45--56},
  abstract      = {Home service robots have a wide range of potential applications, such as home security, patient caring, cleaning, etc. The services provided by the robots in each application area are being defined as markets are formed and, therefore, they change constantly. Thus, robot applications need to evolve both quickly and flexibly adopting frequently changing requirements. This makes software product line framework ideal for the domain of home service robots. Unfortunately, however, robot manufacturers often focus on developing technical components (e.g., vision recognizer and speech processor) and then attempt to develop robots by integrating these components in an ad-hoc way. This practice produces robot applications that are hard to re-use and evolve when requirements change. We believe that re-engineering legacy robot applications into product line assets can significantly enhance reusability and evolvability. In this paper, we present our experience of re-engineering legacy home service robot applications into product line assets through feature modeling and analysis. First, through reverse engineering, we recovered architectures and components of the legacy applications. Second, based on the recovered information and domain knowledge, we reconstructed a feature model for the legacy applications. Anticipating changes in business opportunities or technologies, we restructured and refined the feature model to produce a feature model for the product line. Finally, based on the refined feature model and engineering principles we adopted for asset development, we designed a new architecture and components for robot applications. {\textcopyright} Springer-Verlag Berlin Heidelberg 2005.},
  annote        = {cited By 19},
  doi           = {10.1007/11554844_6},
  keywords      = {Artificial intelligence,Asset development,Business opportunities,Co,Computer architecture,Evolva,Reverse engineering,case study},
  mendeley-tags = {case study},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-33646181380{\&}doi=10.1007{\%}2F11554844{\_}6{\&}partnerID=40{\&}md5=9c88e06d6a7ef9f72602c858dceb5f92},
}

@Article{Schwägerl201619,
  author   = {Schw{\"{a}}gerl, F and Buchmann, T and Westfechtel, B},
  title    = {{Filtered model-driven product line engineering with superMod: The home automation case}},
  journal  = {Communications in Computer and Information Science},
  year     = {2016},
  volume   = {586},
  pages    = {19--41},
  abstract = {Software Product Line Engineering promises to increase the productivity of software development. In the literature, a plan-driven process has been established that is divided up into domain and application engineering. We argue that the strictly sequential order of its process activities implies several disadvantages such as increased complexity, late customer feedback, and duplicate maintenance. SuperMod is a novel model-driven tool based upon a filtered editing model oriented towards version control. The tool provides integrated support for domain and application engineering, offering an iterative and incremental style of development. In this paper, we apply SuperMod to a well-known case study, the Home Automation System product line. We learn that the tool supports a broad variety of iterative and incremental development processes, ranging from phase-structured to feature-driven. Furthermore, it can mitigate the disadvantages of the traditional software product line development process. {\textcopyright} Springer International Publishing Switzerland 2016.},
  annote   = {cited By 1},
  doi      = {10.1007/978-3-319-30142-6_2},
  keywords = {Automation; Computer software; Software engineerin,Filtered editing; Home automation; Model-driven E,Software design},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960436440{\&}doi=10.1007{\%}2F978-3-319-30142-6{\_}2{\&}partnerID=40{\&}md5=b27a8d001069fd67759b48dbcc44f29b},
}

@Article{Eichelberger2014163,
  author   = {Eichelberger, Holger and Schmid, Klaus},
  title    = {{Flexible resource monitoring of Java programs}},
  journal  = {Journal of Systems and Software},
  year     = {2014},
  volume   = {93},
  pages    = {163--186},
  issn     = {0164-1212},
  abstract = {Abstract Monitoring resource consumptions is fundamental in software engineering, e.g., in validation of quality requirements, performance engineering, or adaptive software systems. However, resource monitoring does not come for free as it typically leads to overhead in the observed program. Minimizing this overhead and increasing the reliability of the monitored data is a major goal in realizing resource monitoring tools. Typically, this is achieved by limiting capabilities, e.g., supported resources, granularity of the monitoring focus, or runtime access to results. Thus, in practice often several approaches must be combined to obtain relevant information. We describe SPASS-meter, a novel resource monitoring approach for Java and Android Apps, which combines these conflicting capabilities with low overhead. SPASS-meter supports a large set of resources, flexible configuration of the monitoring scope even for user-defined semantic units (components), runtime analysis and online access to monitoring results in a platform-independent way. We discuss the concepts of SPASS-meter, its architecture, realization and validation, the latter in terms of case studies and an overhead analysis based on performance experiments with SPASS-meter, OpenCore and Kieker. SPASS-meter provides a detailed view of the runtime resource consumption at reasonable overhead of less than 3{\%} processing power and 0.5{\%} memory consumption in our experiments. },
  doi      = {https://doi.org/10.1016/j.jss.2014.02.022},
  keywords = {Empirical analysis,Java,Monitoring overhead,Performance engineering,Resource monitoring,Software components},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121214000533},
}

@Article{Kuhrmann201649,
  author   = {Kuhrmann, Marco and Ternit{\'{e}}, Thomas and Friedrich, Jan and Rausch, Andreas and Broy, Manfred},
  title    = {{Flexible software process lines in practice: A metamodel-based approach to effectively construct and manage families of software process models}},
  journal  = {Journal of Systems and Software},
  year     = {2016},
  volume   = {121},
  pages    = {49--71},
  issn     = {0164-1212},
  abstract = {Abstract Process flexibility and adaptability is a frequently discussed topic in literature, and several approaches propose techniques to improve and optimize software processes for a given organization- or project context. A software process line (SPrL) is an instrument to systematically construct and manage variable software processes, by combining pre-defined and standardized process assets that can be reused, modified, and extended using a well-defined customization approach. Hence, process engineers can ground context-specific process variants in a standardized or domain-specific reference model that can be adapted to the respective context. In this article, we present an approach to construct flexible software process lines and show its practical application in the German V-Modell XT. The presented approach emerges from a 10-year research endeavor and was used to enhance the metamodel of the V-Modell XT and to allow for improved process variability and lifecycle management. Practical dissemination and complementing empirical research show the suitability of the concept. We therefore contribute a proven approach that is presented as metamodel fragment for reuse and implementation in further process modeling approaches. },
  doi      = {https://doi.org/10.1016/j.jss.2016.07.031},
  keywords = {Process design,Process realisation,Software process,Software process lines,Software process metamodel,V-Modell {\{}XT{\}} metamodel},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121216301236},
}

@Article{WESTMAN20172,
  author   = {Westman, Jonas and Nyberg, Mattias and Gustavsson, Joakim and Gurov, Dilian},
  title    = {{Formal architecture modeling of sequential non-recursive C programs}},
  journal  = {Science of Computer Programming},
  year     = {2017},
  volume   = {146},
  pages    = {2--27},
  issn     = {0167-6423},
  abstract = {To manage the complexity of C programs, architecture models are used as high-level descriptions, allowing developers to understand, assess, and manage the C programs without having to understand the intricate complexity of the code implementations. However, for the architecture models to serve their purpose, they must be accurate representations of the C programs. In order to support creating accurate architecture models, the present paper presents a mapping from the domain of sequential non-recursive C programs to a domain of formal architecture models, each being a hierarchy of components with well-defined interfaces. The hierarchically organized components and their interfaces, which capture both data and function call dependencies, are shown to both enable high-level assessment and analysis of the C program and provide a foundation for organizing and expressing specifications for compositional verification.},
  annote   = {Special issue with extended selected papers from FACS 2015},
  doi      = {https://doi.org/10.1016/j.scico.2017.03.007},
  keywords = {Architecture,C program,Component,Interfaces,Modeling},
  url      = {http://www.sciencedirect.com/science/article/pii/S0167642317300564},
}

@Article{Classen2014416,
  author   = {Classen, Andreas and Cordy, Maxime and Heymans, Patrick and Legay, Axel and Schobbens, Pierre-Yves},
  title    = {{Formal semantics, modular specification, and symbolic verification of product-line behaviour}},
  journal  = {Science of Computer Programming},
  year     = {2014},
  volume   = {80, Part B},
  pages    = {416--439},
  issn     = {0167-6423},
  abstract = {Abstract Formal techniques for specifying and verifying Software Product Lines (SPL) are actively studied. While the foundations of this domain recently made significant progress with the introduction of Featured Transition Systems (FTSs) and associated algorithms, {\{}SPL{\}} model checking still faces the well-known state explosion problem. Moreover, there is a need for high-level specification languages usable in industry. We address the state explosion problem by applying the principles of symbolic model checking to FTS-based verification of SPLs. In order to specify properties on specific products only, we extend the temporal logic {\{}CTL{\}} with feature quantifiers. Next, we show how {\{}SPL{\}} behaviour can be specified with fSMV, a variant of SMV, the specification language of the industry-strength model checker NuSMV. fSMV is a feature-oriented extension of {\{}SMV{\}} originally introduced by Plath and Ryan. We prove that fSMV and {\{}FTSs{\}} are expressively equivalent. Finally, we connect these results to a NuSMV extension we developed for verifying {\{}SPLs{\}} against {\{}CTL{\}} properties. },
  doi      = {https://doi.org/10.1016/j.scico.2013.09.019},
  keywords = {Feature,Language,Software product line,Specification,Verification},
  url      = {http://www.sciencedirect.com/science/article/pii/S0167642313002517},
}

@Article{HARUVY2005513,
  author   = {Haruvy, Ernan and Prasad, Ashutosh},
  title    = {{Freeware as a competitive deterrent}},
  journal  = {Information Economics and Policy},
  year     = {2005},
  volume   = {17},
  number   = {4},
  pages    = {513--534},
  issn     = {0167-6245},
  abstract = {Using perfect foresight and adaptive models, this paper examines the effect of competitor asymmetry, consumer sensitivity to incentives and adaptive processes on freeware strategies and competitive outcomes. Four roles played by freeware in competitive markets are identified – it can be a mechanism to build or speed up the growth of a network without the need to lower prices on the commercial version, a deterrence mechanism, a hindrance to a rival's network building efforts, and a coordination device in the presence of forward looking consumers. We determine the optimal prices of the commercial version, the decisions to introduce freeware and the freeware qualities for both competing firms.},
  doi      = {https://doi.org/10.1016/j.infoecopol.2005.03.002},
  keywords = {Competitive strategy,Evolutionary dynamics,Freeware,Product line design,Software},
  url      = {http://www.sciencedirect.com/science/article/pii/S0167624505000259},
}

@Article{Carromeu201639,
  author   = {Carromeu, C and Paiva, D B and Cagnin, M I},
  title    = {{From e-Gov Web SPL to e-Gov Mobile SPL}},
  journal  = {International Journal of Web Information Systems},
  year     = {2016},
  volume   = {12},
  number   = {1},
  pages    = {39--61},
  abstract = {Purpose-This paper aims to discuss the motivation and present the evolution from a Software Product Line (SPL) in the e-Gov Web (e-Gov Web SPL) domain to a SPL in the mobile domain (e-Gov Mobile SPL). Design/methodology/approach-The evolution was supported by the Product Line UML-Based Software Engineering approach and the feature model. Findings-The authors were able to observe that it is feasible to evolve from a SPL for the Web platform to a SPL for the mobile platform, with the intent to port existing Web applications to mobile platforms such that users can have access to the main information and are able to interact with the most important functionalities of Web applications in a mobile device. Research limitations/implications-As for the main limitations, the authors can point out the small number of instantiations performed until the moment with the support of the e-Gov Mobile SPL, what prevented the conduction of an empirical study. Practical implications-Using e-Gov Mobile SPL, it is possible to reduce development time and cost. Originality/value-The existing SPLs do not worry about supporting the development of mobile applications corresponding to existing Web applications, as it is desirable to have access to the information and main features of these applications in mobile devices. We obtained some e-Gov Mobile SPL instantiations corresponding to e-Gov Web SPL instantiations to attend the demands of the Brazilian Agricultural Research Corporation Unit situated at Campo Grande, MS, Brazil. {\textcopyright} Emerald Group Publishing Limited.},
  annote   = {cited By 0},
  doi      = {10.1108/IJWIS-10-2015-0036},
  keywords = {Agricultural research; Empirical studies; Evoluti,Computer software; Mobile devices; Mobile phones;,World Wide Web},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84971351807{\&}doi=10.1108{\%}2FIJWIS-10-2015-0036{\&}partnerID=40{\&}md5=fd88b8ca73aeeaa55fe4c41f927b8a9e},
}

@Article{Karataş20132295,
  author   = {Karataş, Ahmet Serkan and Oğuzt{\"{u}}z{\"{u}}n, Halit and Doğru, Ali},
  title    = {{From extended feature models to constraint logic programming}},
  journal  = {Science of Computer Programming},
  year     = {2013},
  volume   = {78},
  number   = {12},
  pages    = {2295--2312},
  issn     = {0167-6423},
  abstract = {Since feature models for realistic product families may be quite complicated, the automated analysis of feature models is desirable. Although several approaches reported in the literature address this issue, complex cross-tree relationships involving attributes in extended feature models have not been handled. In this article, we introduce a mapping from extended feature models to constraint logic programming over finite domains. This mapping is used to translate into constraint logic programs; basic, cardinality-based and extended feature models, which can include complex cross-tree relationships involving attributes. This translation enables the use of off-the-shelf constraint solvers for the automated analysis of extended feature models involving such complex relationships. We also present the performance results of some well-known analysis operations on an example translated model. },
  annote   = {Special Section on International Software Product Line Conference 2010 and Fundamentals of Software Engineering (selected papers of {\{}FSEN{\}} 2011)},
  doi      = {https://doi.org/10.1016/j.scico.2012.06.004},
  keywords = {Constraint logic programming,Extended feature model,Feature attribute,Variability modeling},
  url      = {http://www.sciencedirect.com/science/article/pii/S0167642312001153},
}

@Article{Bosch2010,
  author    = {Bosch, Jan and Bosch-Sijtsema, Petra},
  title     = {{From integration to composition: On the impact of software product lines, global development and ecosystems}},
  journal   = {Journal of Systems and Software},
  year      = {2010},
  volume    = {83},
  number    = {1},
  pages     = {67--76},
  issn      = {01641212},
  abstract  = {Three trends accelerate the increase in complexity of large-scale software development, i.e. software product lines, global development and software ecosystems. For the case study companies we studied, these trends caused several problems, which are organized around architecture, process and organization, and the problems are related to the efficiency and effectiveness of software development as these companies used too integration-centric approaches. We present five approaches to software development, organized from integration-centric to composition-oriented and describe the areas of applicability. {\textcopyright} 2009 Elsevier Inc. All rights reserved.},
  doi       = {10.1016/j.jss.2009.06.051},
  file      = {:Users/mac/Library/Application Support/Mendeley Desktop/Downloaded/Bosch, Bosch-Sijtsema - 2010 - From integration to composition On the impact of software product lines, global development and ecosystem.pdf:pdf},
  isbn      = {0164-1212},
  keywords  = {Global development,Software composition,Software ecosystems,Software integration,Software product lines},
  publisher = {Elsevier Inc.},
  url       = {http://dx.doi.org/10.1016/j.jss.2009.06.051},
}

@Article{Kilamo20121467,
  author   = {Kilamo, Terhi and Hammouda, Imed and Mikkonen, Tommi and Aaltonen, Timo},
  title    = {{From proprietary to open source—Growing an open source ecosystem}},
  journal  = {Journal of Systems and Software},
  year     = {2012},
  volume   = {85},
  number   = {7},
  pages    = {1467--1478},
  issn     = {0164-1212},
  abstract = {In today's business and software arena, Free/Libre/Open Source Software has emerged as a promising platform for software ecosystems. Following this trend, more and more companies are releasing their proprietary software as open source, forming a software ecosystem of related development projects complemented with a social ecosystem of community members. Since the trend is relatively recent, there are few guidelines on how to create and maintain a sustainable open source ecosystem for a proprietary software. This paper studies the problem of building open source communities for industrial software that was originally developed as closed source. Supporting processes, guidelines and best practices are discussed and illustrated through an industrial case study. The research is paving the road for new directions in growing a thriving open source ecosystem. },
  annote   = {Software Ecosystems},
  doi      = {https://doi.org/10.1016/j.jss.2011.06.071},
  keywords = {Open source,Open source engineering,Opening proprietary software,Software ecosystem},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121211001683},
}

@Article{Sánchez20122504,
  author   = {S{\'{a}}nchez, Pedro and Alonso, Diego and Morales, Jos{\'{e}} Miguel and Navarro, Pedro Javier},
  title    = {{From Teleo-Reactive specifications to architectural components: A model-driven approach}},
  journal  = {Journal of Systems and Software},
  year     = {2012},
  volume   = {85},
  number   = {11},
  pages    = {2504--2518},
  issn     = {0164-1212},
  abstract = {The Teleo-Reactive approach designed by N.J. Nilsson offers a high-level programming model that permits the development of reactive systems, such as robotic vehicles. Teleo-Reactive programs are written in a manner that allows engineers to define the behaviour of the system while taking into account goals and changes in the state of the environment. This article presents a systematic approach that makes it possible to derive architectural models, with structural descriptions and behaviour, from Teleo-Reactive Programs. The development of reactive systems can therefore benefit significantly from a combination of two approaches: (1) the Teleo-Reactive approach, which is oriented towards a description of the system from the standpoint of the goals identified and the state of the environment and (2) the architectural approach, which is oriented towards the design of component-based software, in which decisions are conditioned by the need to reuse already tested solutions. The integration of this work into a development environment that allows code to be generated via model transformations opens up new possibilities in the development of this type of systems. The proposal is validated through a case study that is representative of the domain, and a survey carried out with post-graduate students. },
  doi      = {https://doi.org/10.1016/j.jss.2012.05.067},
  keywords = {Component-based software development,Model-driven software development,Reactive systems,Robotics,Teleo-Reactive programs},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121212001537},
}

@Article{Castro2011342,
  author   = {Castro, J and Pimentel, J and Lucena, M and Santos, E and Dermeval, D},
  title    = {{F-STREAM: A flexible process for deriving architectures from requirements models}},
  journal  = {Lecture Notes in Business Information Processing},
  year     = {2011},
  volume   = {83 LNBIP},
  pages    = {342--353},
  abstract = {Some quality attributes are known to have an impact on the overall architecture of a system, requiring to be properly handled from the early stages of the software development. This led to the creation of different and unrelated approaches to handle specific attributes, such as security, performance, adaptability, etc. The challenge is to propose a flexible approach that could be configured to address multiple attributes of interest, promoting the reuse of best practices and reduction of development costs. We advocate the use of Software Product Line (SPL) principles to manage and customize variability in software processes targeted for the generation of architectural models from requirements models. Hence, in this paper we propose F-STREAM, a flexible and systematic process to derive architecture models from requirements. We define a common core process, its variation and extension points. The definition of this process was performed based on a survey of the existing approaches. As example, we instantiate a process for adaptive systems. {\textcopyright} 2011 Springer-Verlag.},
  annote   = {cited By 1},
  doi      = {10.1007/978-3-642-22056-2_37},
  keywords = {Adaptive systems; Information systems; Software de,Architectural models; Architecture models; Develo,Models},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960303560{\&}doi=10.1007{\%}2F978-3-642-22056-2{\_}37{\&}partnerID=40{\&}md5=974f17abd2abf9f728cda7c67a294855},
}

@Article{GarcíaMagariño2015161,
  author   = {Garc{\'{i}}a-Magari{\~{n}}o, Iv{\'{a}}n and Plaza, Inmaculada},
  title    = {{FTS-SOCI: An agent-based framework for simulating teaching strategies with evolutions of sociograms}},
  journal  = {Simulation Modelling Practice and Theory},
  year     = {2015},
  volume   = {57},
  pages    = {161--178},
  issn     = {1569-190X},
  abstract = {Abstract Teaching strategies have been proven to influence the academic performance of students, as well as group sociometrics have been proven to be directly related to group performance. Although in the literature there are examples of teaching strategies and the resulting sociograms, there is not any detailed technique, tool or simulator that predicts the influence of a new teaching strategy on group sociometrics. The current work is aimed at covering this gap in the literature, by providing a framework for programming teaching strategies to simulate their influence on group sociometrics. In particular, this framework is called FTS-SOCI (Framework for simulating Teaching Strategies with evolutions of SOCIograms). This framework includes an agent-based simulator that simulates the evolution of sociograms taking five models of the literature into account. In this framework, students and teacher are modelled as agents, and the teacher agent can have any teaching strategy defined by the user. In order to test the current approach, this work simulates existing teaching strategies in (1) nursing education and (2) sport lessons. The resulting sociograms of FTS-SOCI for these strategies have been compared with the corresponding real sociograms reported in the literature. This works shows that the group sociometric values provided by FTS-SOCI are quite similar to the real cases. For instance, the mean squared error of the group cohesion sociometric (i.e. {\{}IAg{\}} metric) is only 0.00024 and 0.00068 respectively for the teaching strategies of both fields. },
  doi      = {https://doi.org/10.1016/j.simpat.2015.07.003},
  keywords = {Agent-based framework,Agent-based simulator,Multi-agent system,Sociogram,Teaching strategy},
  url      = {http://www.sciencedirect.com/science/article/pii/S1569190X15001070},
}

@Article{Lazreg:2018:FFA:3284971.3284975,
  author    = {Lazreg, Sami and Collet, Philippe and Mosser, S{\'{e}}bastien},
  title     = {{Functional Feasibility Analysis of Variability-intensive Data Flow-oriented Applications over Highly-configurable Platforms}},
  journal   = {SIGAPP Appl. Comput. Rev.},
  year      = {2018},
  volume    = {18},
  number    = {3},
  pages     = {32--48},
  issn      = {1559-6915},
  address   = {New York, NY, USA},
  doi       = {10.1145/3284971.3284975},
  keywords  = {behavioral product lines model checking,embedded system design engineering,feature model,variability modeling},
  publisher = {ACM},
  url       = {http://doi.acm.org/10.1145/3284971.3284975},
}

@Article{Gencel:2008:FSM:1363102.1363106,
  author    = {Gencel, Cigdem and Demirors, Onur},
  title     = {{Functional Size Measurement Revisited}},
  journal   = {ACM Trans. Softw. Eng. Methodol.},
  year      = {2008},
  volume    = {17},
  number    = {3},
  pages     = {15:1----15:36},
  issn      = {1049-331X},
  address   = {New York, NY, USA},
  doi       = {10.1145/1363102.1363106},
  keywords  = {COSMIC FFP,Functional size measurement,MkII FPA,software benchmarking,software estimation},
  publisher = {ACM},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/1363102.1363106},
}

@Article{AHMED20082780,
  author   = {Ahmed, F and Capretz, L F and Samarabandu, J},
  title    = {{Fuzzy inference system for software product family process evaluation}},
  journal  = {Information Sciences},
  year     = {2008},
  volume   = {178},
  number   = {13},
  pages    = {2780--2793},
  issn     = {0020-0255},
  abstract = {When developing multiple products within a common application domain, systematic use of a software product family process can yield increased productivity in cost, quality, effort and schedule. Such a process provides the means for the reuse of software assets which can considerably reduce the development time and the cost of software products. A comprehensive strategy for the evaluating the maturity of a software product family process is needed due to growing popularity of this concept in the software industry. In this paper, we propose a five-level maturity scale for software product family process. We also present a fuzzy inference system for evaluating maturity of software product family process using the proposed maturity scale. This research is aimed at establishing a comprehensive and unified strategy for process evaluation of a software product family. Such a process evaluation strategy will enable an organization to discover and monitor the strengths and weaknesses of the various activities performed during development of multiple products within a common application domain.},
  doi      = {https://doi.org/10.1016/j.ins.2008.03.002},
  keywords = {Adaptive neuro-fuzzy inference system,Fuzzy logic,Process maturity,Software engineering,Software process assessment,Software product family},
  url      = {http://www.sciencedirect.com/science/article/pii/S0020025508000844},
}

@Article{Kaur2015842,
  author   = {Kaur, Ramandeep and Arora, Stuti and Jha, P C and Madan, Sushila},
  title    = {{Fuzzy Multi-criteria Approach for Component Selection of Fault Tolerant Software System under Consensus Recovery Block Scheme}},
  journal  = {Procedia Computer Science},
  year     = {2015},
  volume   = {45},
  pages    = {842--851},
  issn     = {1877-0509},
  abstract = {Abstract The vibrant and advanced software development tools not only provide software with versatile functions for radical users but at the same time, an easy to use {\{}GUI{\}} for naive users. {\{}APS{\}} (Application Package Software) has provided a customised approach for developing independent software components which are ready to be integrated with existing software systems. The {\{}APS{\}} along with {\{}CBSE{\}} (Component Based Software Engineering) has an inordinate potential for reducing development time, cost and effort, which otherwise may extend beyond weeks or months' time for integration. Further, the {\{}CBSE{\}} approach promotes software reusability i.e. reusing the available components. A component can be reused after fabrication which will include the fabrication cost and time. For development of economical and reliable software, components can be procured in the form of Commercial off-The Shelf (COTS) components from the vendor or may be developed in-house or can be fabricated. This decision is based on several parameters. The aim of this paper is to select the suitable mix of components using Build-or-buy strategy or considering fabrication and to propose a multi-objective model for software modular system with objective of maximizing reliability while simultaneously minimizing the cost, execution time and Source Lines of Code (SLOC) using Consensus Recovery Block Scheme. },
  annote   = {International Conference on Advanced Computing Technologies and Applications (ICACTA)},
  doi      = {https://doi.org/10.1016/j.procs.2015.03.169},
  keywords = {Build-or-Buy,CBSE,COTS,CRB,Execution Time,Fabrication,SLOC ;,Software Reusability},
  url      = {http://www.sciencedirect.com/science/article/pii/S1877050915004123},
}

@Article{ALONSO2012170,
  author   = {Alonso, Diego and Pastor, Juan {\'{A}}ngel and S{\'{a}}nchez, Pedro and {\'{A}}lvarez, B{\'{a}}rbara and Vicente-Chicote, Cristina},
  title    = {{Generaci{\'{o}}n Autom{\'{a}}tica de Software para Sistemas de Tiempo Real: Un Enfoque basado en Componentes, Modelos y Frameworks}},
  journal  = {Revista Iberoamericana de Autom{\'{a}}tica e Inform{\'{a}}tica Industrial RIAI},
  year     = {2012},
  volume   = {9},
  number   = {2},
  pages    = {170--181},
  issn     = {1697-7912},
  abstract = {Resumen
Los Sistemas de Tiempo-Real poseen caracter{\'{i}}sticas que los hacen particularmente sensibles a las decisiones arquitect{\'{o}}nicas que se adopten. El uso de Frameworks y Componentes ha demostrado ser eficaz en la mejora de la productividad y calidad del software, sobre todo si se combina con enfoques de L{\'{i}}neas de Productos. Sin embargo, los resultados en cuanto a reutilizaci{\'{o}}n y estandarizaci{\'{o}}n dejan patente la ausencia de portabilidad tanto de los dise{\~{n}}os como las implementaciones basadas en componentes. Este art{\'{i}}culo, fundamentado en el Desarrollo de Software Dirigido por Modelos, presenta un enfoque que separa la descripci{\'{o}}n de aplicaciones de tiempo–real basadas en componentes de sus posibles implementaciones para distintas plataformas. Esta separaci{\'{o}}n viene soportada por la integraci{\'{o}}n autom{\'{a}}tica del c{\'{o}}digo obtenido a partir de los modelos de entrada en frameworks implementados usando tecnolog{\'{i}}a orientada a objetos. Asimismo, se detallan las decisiones arquitect{\'{o}}nicas adoptadas en la implementaci{\'{o}}n de uno de estos frameworks, que se utilizar{\'{a}} como caso de estudio para ilustrar los beneficios derivados del enfoque propuesto. Por {\'{u}}ltimo, se realiza una comparativa en t{\'{e}}rminos de coste de desarrollo con otros enfoques alternativos.
Real-Time Systems have some characteristics that make them particularly sensitive to architectural decisions. The use of Frameworks and Components has proven effective in improving productivity and software quality, especially when combined with Software Product Line approaches. However, the results in terms of software reuse and standardization make the lack of portability of both the design and componentbased implementations clear. This article, based on the Model- Driven Software Development paradigm, presents an approach that separates the component-based description of real-time applications from their possible implementations on different platforms. This separation is supported by the automatic integration of the code obtained from the input models into object-oriented frameworks. The article also details the architectural decisions taken in the implementation of one of such frameworks, which is used as a case study to illustrate the proposed approach. Finally, a comparison with other alternative approaches is made in terms of development cost.},
  doi      = {https://doi.org/10.1016/j.riai.2012.02.010},
  keywords = {Component-Based Software Development,Desarrollo de Software Basado en Componentes,Desarrollo de Software Dirigido por Modelos,Framework,Frameworks,Ingenier{\'{i}}a del Software,Model-Driven Software Development,Patrones de Dise{\~{n}}o Software,Real-Time,Software Design Patterns,Software Engineering,Tiempo-Real},
  url      = {http://www.sciencedirect.com/science/article/pii/S169779121200012X},
}

@Article{Diaz20101970,
  author   = {Diaz, Oscar and Villoria, Felipe M},
  title    = {{Generating blogs out of product catalogues: An {\{}MDE{\}} approach}},
  journal  = {Journal of Systems and Software},
  year     = {2010},
  volume   = {83},
  number   = {10},
  pages    = {1970--1982},
  issn     = {0164-1212},
  abstract = {Blogs can be used as a conduit for customer opinions and, in so doing, building communities around products. We attempt to realise this vision by building blogs out of product catalogues. Unfortunately, the immaturity of blog engines makes this endeavour risky. This paper presents a model-driven approach to face this drawback. This implies the introduction of (meta)models: the catalogue model, based on the standard Open Catalog Format, and blog models, that elaborate on the use of blogs as conduits for virtual communities. Blog models end up being realised through blog engines. Specifically, we focus on two types of engines: a hosted blog platform and a standalone blog platform, both in Blojsom. However, the lack of standards in a broad and constantly evolving blog-engine space, hinders both the portability and the maintainability of the solution. Hence, we resort to the notion of “abstract platform” as a way to depart from the peculiarities of specific blog engines. Additionally, the paper measures the reuse gains brought by {\{}MDE{\}} in comparison with the manual coding of blogs. },
  doi      = {https://doi.org/10.1016/j.jss.2010.05.075},
  keywords = {Blog,Catalogue,Model-Driven Engineering,PSM evolution},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121210001469},
}

@Article{Seidl201789,
  author   = {Seidl, C and Schuster, S and Schaefer, I},
  title    = {{Generative software product line development using variability-aware design patterns}},
  journal  = {Computer Languages, Systems and Structures},
  year     = {2017},
  volume   = {48},
  pages    = {89--111},
  abstract = {Software Product Lines (SPLs) are an approach to reuse in-the-large that models a set of closely related software systems in terms of commonalities and variabilities. Design patterns are best practices for addressing recurring design problems in object-oriented source code. In the practice of implementing SPL, instances of certain design patterns are employed to handle variability, which makes these “variability-aware design patterns” a best practice for SPL design. However, currently there is no dedicated method for proactively developing SPLs using design patterns suitable for realizing variable functionality. In this paper, we present a method to perform generative SPL development with design patterns. We use role models to capture design patterns and their relation to a variability model. We further allow mapping of individual design pattern roles to (parts of) implementation elements to be generated (e.g., classes, methods) and check the conformance of the realization with the specification of the pattern. We provide definitions for the variability-aware versions of the design patterns Observer, Strategy, Template Method and Composite. Furthermore, we support generation of realizations in Java, C++ and UML class diagrams utilizing annotative, compositional and transformational variability realization mechanisms. Hence, we support proactive development of SPLs using design patterns to apply best practices for the realization of variability. We realize our concepts within the Eclipse IDE and demonstrate them within a case study. {\textcopyright} 2016 Elsevier Ltd},
  annote   = {cited By 0},
  doi      = {10.1016/j.cl.2016.08.006},
  keywords = {C++ (programming language); Computer software; Com,Product design,Software product line (SPLs); Software product li},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995752793{\&}doi=10.1016{\%}2Fj.cl.2016.08.006{\&}partnerID=40{\&}md5=85cb107ea1b92dcce82154a31efd6979},
}

@Article{Schobbens2007456,
  author   = {Schobbens, P.-Y. and Heymans, P and Trigaux, J.-C. and Bontemps, Y},
  title    = {{Generic semantics of feature diagrams}},
  journal  = {Computer Networks},
  year     = {2007},
  volume   = {51},
  number   = {2},
  pages    = {456--479},
  abstract = {Feature Diagrams (FDs) are a family of popular modelling languages used to address the feature interaction problem, particularly in software product lines, FDs were first introduced by Kang as part of the FODA (Feature-Oriented Domain Analysis) method back in 1990. Afterwards, various extensions of FODA FDs were introduced to compensate for a purported ambiguity and lack of precision and expressiveness. However, they never received a formal semantics, which is the hallmark of precision and unambiguity and a prerequisite for efficient and safe tool automation. The reported work is intended to contribute a more rigorous approach to the definition, understanding, evaluation, selection and implementation of FD languages. First, we provide a survey of FD variants. Then, we give them a formal semantics, thanks to a generic construction that we call Free Feature Diagrams (FFDs). This demonstrates that FDs can be precise and unambiguous. This also defines their expressiveness. Many variants are expressively complete, and thus the endless quest for extensions actually cannot be justified by expressiveness. A finer notion is thus needed to compare these expressively complete languages. Two solutions are well-established: succinctness and embeddability, that express the naturalness of a language. We show that the expressively complete FDs fall into two succinctness classes, of which we of course recommend the most succinct. Among the succinct expressively complete languages, we suggest a new, simple one that is not harmfully redundant: Varied FD (VFD). Finally, we study the execution time that tools will need to solve useful problems in these languages. {\textcopyright} 2006 Elsevier B.V. All rights reserved.},
  annote   = {cited By 158},
  doi      = {10.1016/j.comnet.2006.08.008},
  keywords = {Computer aided software engineering; Computer prog,Feature diagram; Feature interaction; Formal sema,Semantics},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-33750723486{\&}doi=10.1016{\%}2Fj.comnet.2006.08.008{\&}partnerID=40{\&}md5=f069b02d12f436e866959ebaf1a38eb4},
}

@Article{Kim2006926,
  author   = {Kim, Jintae and Kim, Minseong and Park, Sooyong},
  title    = {{Goal and scenario based domain requirements analysis environment}},
  journal  = {Journal of Systems and Software},
  year     = {2006},
  volume   = {79},
  number   = {7},
  pages    = {926--938},
  issn     = {0164-1212},
  abstract = {Identifying and representing domain requirements among products in a product family are crucial activities for a successful software reuse. The domain requirements should be not only identified based on the business goal, which drives marketing plan, product plan, and differences among products, but also represented as familiar notations in order to support developing a particular product in the product family. Thus, our proposal is to identify the domain requirements through goals and scenarios, and represent them as variable use cases for a product family. Especially, for identification of the domain requirements, we propose four abstraction levels of requirements in a product family, and goal and scenario modeling. For representation of them, variable use case model is suggested, and also the use case transfer rules are proposed so as to bridge the gap between the identification and representation activity. The paper illustrates the application of the approach within a supporting tool using the {\{}HIS{\}} (Home Integration System) example. },
  annote   = {Selected papers from the 11th Asia Pacific Software Engineering Conference (APSEC2004)},
  doi      = {https://doi.org/10.1016/j.jss.2005.06.046},
  keywords = {Domain requirements analysis,Goal,Scenario,Use case},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121205001810},
}

@Article{Asadi2016257,
  author   = {Asadi, M and Gr{\"{o}}ner, G and Mohabbati, B and Ga{\v{s}}evi{\'{c}}, D},
  title    = {{Goal-oriented modeling and verification of feature-oriented product lines}},
  journal  = {Software and Systems Modeling},
  year     = {2016},
  volume   = {15},
  number   = {1},
  pages    = {257--279},
  abstract = {Goal models represent requirements and intentions of a software system. They play an important role in the development life cycle of software product lines (SPLs). In the domain engineering phase, goal models guide the development of variability in SPLs by providing the rationale for the variability, while they are used for the configuration of SPLs in the application engineering phase. However, variability in SPLs, which is represented by feature models, usually has design and implementation-induced constraints. When those constraints are not aligned with variability in goal models, the configuration with goal models becomes error prone. To remedy this problem, we propose a description logic (DL)-based approach to represent both models and their relations in a common DL knowledge base. Moreover, we apply reasoning to detect inconsistencies in the variability of goal and feature models. A formal proof is provided to demonstrate the correctness of the reasoning approach. An empirical evaluation shows computational tractability of the inconsistency detection. {\textcopyright} 2014, Springer-Verlag Berlin Heidelberg.},
  annote   = {cited By 3},
  doi      = {10.1007/s10270-014-0402-8},
  keywords = {Application engineering; Computational tractabili,Computation theory; Computer circuits; Computer so,Verification},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84956612126{\&}doi=10.1007{\%}2Fs10270-014-0402-8{\&}partnerID=40{\&}md5=f9e90df907b717cae8579d320572f8ae},
}

@Article{Eichelberger20091686,
  author   = {Eichelberger, Holger and Schmid, Klaus},
  title    = {{Guidelines on the aesthetic quality of {\{}UML{\}} class diagrams}},
  journal  = {Information and Software Technology},
  year     = {2009},
  volume   = {51},
  number   = {12},
  pages    = {1686--1698},
  issn     = {0950-5849},
  abstract = {In the past, formatting guidelines have proved to be a successful method to improve the readability of source code. With the increasing success of visual specification languages such as {\{}UML{\}} for model-driven software engineering visual guidelines are needed to standardize the presentation and the exchange of modeling diagrams with respect to human communication, understandability and readability. In this article, we introduce a new and encompassing taxonomy of visual guidelines capturing the aesthetic quality of {\{}UML{\}} class diagrams. We propose these guidelines as a framework to improve the aesthetic quality and thus the understandability of {\{}UML{\}} class diagrams. To validate this claim, we describe in detail a controlled experiment carried out as a pilot study to gather preliminary insights on the effects of some of the guideline rules on the understandability of {\{}UML{\}} class diagrams. },
  annote   = {Quality of {\{}UML{\}} Models},
  doi      = {https://doi.org/10.1016/j.infsof.2009.04.008},
  keywords = {Aesthetic quality,Automatic layout,Layout guidelines,Modeling tools,Software engineering,UML class diagrams},
  url      = {http://www.sciencedirect.com/science/article/pii/S0950584909000421},
}

@Article{Ahmed201720,
  author   = {Ahmed, Bestoun S and Gambardella, Luca M and Afzal, Wasif and Zamli, Kamal Z},
  title    = {{Handling constraints in combinatorial interaction testing in the presence of multi objective particle swarm and multithreading}},
  journal  = {Information and Software Technology},
  year     = {2017},
  volume   = {86},
  pages    = {20--36},
  issn     = {0950-5849},
  abstract = {AbstractContext Combinatorial testing strategies have lately received a lot of attention as a result of their diverse applications. In its simple form, a combinatorial strategy can reduce several input parameters (configurations) of a system into a small set based on their interaction (or combination). In practice, the input configurations of software systems are subjected to constraints, especially in case of highly configurable systems. To implement this feature within a strategy, many difficulties arise for construction. While there are many combinatorial interaction testing strategies nowadays, few of them support constraints. Objective This paper presents a new strategy, to construct combinatorial interaction test suites in the presence of constraints. Method The design and algorithms are provided in detail. To overcome the multi-judgement criteria for an optimal solution, the multi-objective particle swarm optimisation and multithreading are used. The strategy and its associated algorithms are evaluated extensively using different benchmarks and comparisons. Results Our results are promising as the evaluation results showed the efficiency and performance of each algorithm in the strategy. The benchmarking results also showed that the strategy can generate constrained test suites efficiently as compared to state-of-the-art strategies. Conclusion The proposed strategy can form a new way for constructing of constrained combinatorial interaction test suites. The strategy can form a new and effective base for future implementations. },
  doi      = {https://doi.org/10.1016/j.infsof.2017.02.004},
  keywords = {Constrained combinatorial interaction,Multi-objective particle swarm optimisation,Search-based software engineering,Test case design techniques,Test generation tools},
  url      = {http://www.sciencedirect.com/science/article/pii/S0950584917301349},
}

@Article{ZILLI20151076,
  author   = {Zilli, Massimiliano and Raschke, Wolfgang and Weiss, Reinhold and Loinig, Johannes and Steger, Christian},
  title    = {{Hardware/software co-design for a high-performance Java Card interpreter in low-end embedded systems}},
  journal  = {Microprocessors and Microsystems},
  year     = {2015},
  volume   = {39},
  number   = {8},
  pages    = {1076--1086},
  issn     = {0141-9331},
  abstract = {Java Card is a Java running environment specific for smart cards. In such low-end embedded systems, the execution time of the applications is an issue of first order. One of the components of the Java Card Virtual Machine (JCVM) playing an important role in the execution speed is the bytecode interpreter. In Java systems the main technique for speeding-up the interpreter execution is the Just-In-Time compilation (JIT), but this resource consuming technique is inapplicable in systems with as restricted resources available as in smart cards. This paper presents a hardware/software co-design solution for the performance improvement of the interpreter. In the software domain, we adopted a pseudo-threaded code interpreter that allows a better run-time performance with a small amount of additional code. In the hardware domain, we proceeded moving parts of the interpreter into hardware, giving origin to a Java Card interpreter based on an application specific instruction set processor.},
  doi      = {https://doi.org/10.1016/j.micpro.2015.05.004},
  keywords = {Application specific instruction set processor,Hardware-supported interpreter,Hardware/software co-design,Java Card,Java interpreter,Smart card},
  url      = {http://www.sciencedirect.com/science/article/pii/S0141933115000563},
}

@Article{Lytra201580,
  author   = {Lytra, Ioanna and Tran, Huy and Zdun, Uwe},
  title    = {{Harmonizing architectural decisions with component view models using reusable architectural knowledge transformations and constraints}},
  journal  = {Future Generation Computer Systems},
  year     = {2015},
  volume   = {47},
  pages    = {80--96},
  issn     = {0167-739X},
  abstract = {Abstract Architectural design decisions (ADDs) have been used in recent years for capturing design rationale and documenting architectural knowledge (AK). However, various architectural design views still provide the most common means for describing and communicating architectural design. The evolution of software systems requires that both {\{}ADDs{\}} and architectural design views are documented and maintained, which is a tedious and time-consuming task in the long run. Also, in lack of a systematic and automated support for bridging between {\{}ADDs{\}} and architectural design views, decisions and designs tend to become inconsistent over time. In our proposal, we introduce a reusable {\{}AK{\}} transformation language for supporting the automated transformation of reusable {\{}AK{\}} knowledge to component-and-connector models, the architectural design view used most commonly today. In addition, reusable consistency checking rules verify the consistency between decisions and designs. We evaluate our approach in an industrial case study and show that it offers high reusability, provides automation, and can, in principle, deal with large numbers of recurring decisions. },
  annote   = {Special Section: Advanced Architectures for the Future Generation of Software-Intensive Systems},
  doi      = {https://doi.org/10.1016/j.future.2014.11.010},
  keywords = {AK transformation language,Architectural decisions,Architectural design,Architectural knowledge,Consistency checking},
  url      = {http://www.sciencedirect.com/science/article/pii/S0167739X14002441},
}

@Article{Hähnle2013109,
  author   = {H{\"{a}}hnle, R and Helvensteijn, M and Johnsen, E B and Lienhardt, M and Sangiorgi, D and Schaefer, I and Wong, P Y H},
  title    = {{HATS abstract behavioral specification: The architectural view}},
  journal  = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year     = {2013},
  volume   = {7542 LNCS},
  pages    = {109--132},
  abstract = {The Abstract Behavioral Specification (ABS) language is a formal, executable, object-oriented, concurrent modeling language intended for behavioral modeling of complex software systems that exhibit a high degree of variation, such as software product lines. We give an overview of the architectural aspects of ABS: a feature-driven development workflow, a formal notion of deployment components for specifying environmental constraints, and a dynamic component model that is integrated into the language. We employ an industrial case study to demonstrate how the various aspects work together in practice. {\textcopyright} 2013 Springer-Verlag Berlin Heidelberg.},
  annote   = {cited By 5},
  doi      = {10.1007/978-3-642-35887-6-6},
  keywords = {Architectural views; Behavioral specification; Com,Computer software; Industrial applications,Specifications},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883303962{\&}doi=10.1007{\%}2F978-3-642-35887-6-6{\&}partnerID=40{\&}md5=4e652fb7c5f96ab147171a824e04af6b},
}

@Article{TAIBI2017223,
  author   = {Taibi, Davide and Janes, Andrea and Lenarduzzi, Valentina},
  title    = {{How developers perceive smells in source code: A replicated study}},
  journal  = {Information and Software Technology},
  year     = {2017},
  volume   = {92},
  pages    = {223--235},
  issn     = {0950-5849},
  abstract = {Context. In recent years, smells, also referred to as bad smells, have gained popularity among developers. However, it is still not clear how harmful they are perceived from the developers' point of view. Many developers talk about them, but only few know what they really are, and even fewer really take care of them in their source code. Objective. The goal of this work is to understand the perceived criticality of code smells both in theory, when reading their description, and in practice. Method. We executed an empirical study as a differentiated external replication of two previous studies. The studies were conducted as surveys involving only highly experienced developers (63 in the first study and 41 in the second one). First the perceived criticality was analyzed by proposing the description of the smells, then different pieces of code infected by the smells were proposed, and finally their ability to identify the smells in the analyzed code was tested. Results. According to our knowledge, this is the largest study so far investigating the perception of code smells with professional software developers. The results show that developers are very concerned about code smells in theory, nearly always considering them as harmful or very harmful (17 out of 23 smells). However, when they were asked to analyze an infected piece of code, only few infected classes were considered harmful and even fewer were considered harmful because of the smell. Conclusions. The results confirm our initial hypotheses that code smells are perceived as more critical in theory but not as critical in practice.},
  doi      = {https://doi.org/10.1016/j.infsof.2017.08.008},
  keywords = {Antipatterns,Bad smells,Code smells,Refactoring,Software maintenance},
  url      = {http://www.sciencedirect.com/science/article/pii/S0950584916304128},
}

@Article{Jezek2015129,
  author   = {Jezek, Kamil and Dietrich, Jens and Brada, Premek},
  title    = {{How Java {\{}APIs{\}} break – An empirical study}},
  journal  = {Information and Software Technology},
  year     = {2015},
  volume   = {65},
  pages    = {129--146},
  issn     = {0950-5849},
  abstract = {AbstractContext It has become common practice to build programs by using libraries. While the benefits of reuse are well known, an often overlooked risk are system runtime failures due to {\{}API{\}} changes in libraries that evolve independently. Traditionally, the consistency between a program and the libraries it uses is checked at build time when the entire system is compiled and tested. However, the trend towards partially upgrading systems by redeploying only evolved library versions results in situations where these crucial verification steps are skipped. For Java programs, partial upgrades create additional interesting problems as the compiler and the virtual machine use different rule sets to enforce contracts between the providers and the consumers of APIs. Objective We have studied the extent of the problem in real world programs. We were interested in two aspects: the compatibility of {\{}API{\}} changes as libraries evolve, and the impact this has on programs using these libraries. Method This study is based on the qualitas corpus version 20120401. A data set consisting of 109 Java open-source programs and 564 program versions was used from this corpus. We have investigated two types of library dependencies: explicit dependencies to embedded libraries, and dependencies defined by symbolic references in Maven build files that are resolved at build time. We have used JaCC for {\{}API{\}} analysis, this tool is based on the popular {\{}ASM{\}} byte code analysis library. Results We found that for most of the programs we investigated, {\{}APIs{\}} are unstable as incompatible changes are common. Surprisingly, there are more compatibility problems in projects that use automated dependency resolution. However, we found only a few cases where this has an actual impact on other programs using such an API. Conclusion It is concluded that {\{}API{\}} instability is common and causes problems for programs using these APIs. Therefore, better tools and methods are needed to safeguard library evolution. },
  doi      = {https://doi.org/10.1016/j.infsof.2015.02.014},
  keywords = {API evolution,Backward compatibility,Binary compatibility,Byte-code,Java},
  url      = {http://www.sciencedirect.com/science/article/pii/S0950584915000506},
}

@Article{Fenske:2017:PAA:3170492.3136059,
  author    = {Fenske, Wolfram and Schulze, Sandro and Saake, Gunter},
  title     = {{How Preprocessor Annotations (Do Not) Affect Maintainability: A Case Study on Change-proneness}},
  journal   = {SIGPLAN Not.},
  year      = {2017},
  volume    = {52},
  number    = {12},
  pages     = {77--90},
  issn      = {0362-1340},
  address   = {New York, NY, USA},
  doi       = {10.1145/3170492.3136059},
  keywords  = {annotations,change-proneness,maintenance,preprocessors,variability},
  publisher = {ACM},
  url       = {http://doi.acm.org/10.1145/3170492.3136059},
}

@Article{SHABAH2015223,
  author   = {Shabah, Abdo},
  title    = {{HUMANIT3D for Disaster Response: An Assessment of Mass Customization on Organizational Performance Under Turbulent Environments}},
  journal  = {Procedia Engineering},
  year     = {2015},
  volume   = {107},
  pages    = {223--236},
  issn     = {1877-7058},
  abstract = {Mass customization aims to produce customized goods (allowing economies of scope) at lower cost (to achieve economies of scale) using multiple strategies (modularization and postponement). Mass customization in software and hardware design is becoming more popular for users and researchers. Through a simulation experiment of emergency response organizations under turbulent environment, we aim to compare standardization and mass customization of services and assess the impact of different forms of mass customization (early and late postponement) on performance, quality and consumer satisfaction, on the use of modular dynamic ecosystem based on HUMANIT3D, an integrated collaborative ecosystem composed of UAV management system, data collection system, and 3D Geographic Information System. Our hypothesis is that mass customization performs better and achieves better quality in turbulent environment than standardization, but only when using early postponement strategies. Using mixed methods study, we try to confirm our hypothesis.},
  annote   = {Humanitarian Technology: Science, Systems and Global Impact 2015, HumTech2015},
  doi      = {https://doi.org/10.1016/j.proeng.2015.06.077},
  keywords = {3D GIS,UAV,experiment,mass customization,mobile ecosystem,performance,postponement,quality,satisfaction},
  url      = {http://www.sciencedirect.com/science/article/pii/S1877705815010309},
}

@Article{Xue20161215,
  author   = {Xue, Y and Zhong, J and Tan, T H and Liu, Y and Cai, W and Chen, M and Sun, J},
  title    = {{IBED: Combining IBEA and DE for optimal feature selection in software product line engineering}},
  journal  = {Applied Soft Computing Journal},
  year     = {2016},
  volume   = {49},
  pages    = {1215--1231},
  abstract = {Software configuration, which aims to customize the software for different users (e.g., Linux kernel configuration), is an important and complicated task. In software product line engineering (SPLE), feature oriented domain analysis is adopted and feature model is used to guide the configuration of new product variants. In SPLE, product configuration is an optimal feature selection problem, which needs to find a set of features that have no conflicts and meanwhile achieve multiple design objectives (e.g., minimizing cost and maximizing the number of features). In previous studies, several multi-objective evolutionary algorithms (MOEAs) were used for the optimal feature selection problem and indicator-based evolutionary algorithm (IBEA) was proven to be the best MOEA for this problem. However, IBEA still suffers from the issues of correctness and diversity of found solutions. In this paper, we propose a dual-population evolutionary algorithm, named IBED, to achieve both correctness and diversity of solutions. In IBED, two populations are individually evolved with two different types of evolutionary operators, i.e., IBEA operators and differential evolution (DE) operators. Furthermore, we propose two enhancement techniques for existing MOEAs, namely the feedback-directed mechanism to fast find the correct solutions (e.g., solutions that satisfy the feature model constraints) and the preprocessing method to reduce the search space. Our empirical results have shown that IBED with the enhancement techniques can outperform several state-of-the-art MOEAs on most case studies in terms of correctness and diversity of found solutions. {\textcopyright} 2016 Elsevier B.V.},
  annote   = {cited By 0},
  doi      = {10.1016/j.asoc.2016.07.040},
  keywords = {Computer operating systems; Computer software; Com,Differential Evolution; Differential evolutionary,Evolutionary algorithms},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997496779{\&}doi=10.1016{\%}2Fj.asoc.2016.07.040{\&}partnerID=40{\&}md5=3b3bec8813e78da624b567c95244b62e},
}

@Article{Hauksdóttir2014952,
  author   = {Hauksd{\'{o}}ttir, Dagn{\'{y}} and Mortensen, Niels Henrik and Nielsen, Poul Erik},
  title    = {{Identified adjustability dimensions when generating a product specific requirements specification by requirements reuse}},
  journal  = {Computers in Industry},
  year     = {2014},
  volume   = {65},
  number   = {6},
  pages    = {952--966},
  issn     = {0166-3615},
  abstract = {Abstract A requirements reuse setups typically includes reusable requirement set(s) containing a collection of reusable requirements and a number of product specific requirements sets which are drawn from the reusable set(s). The ideal scenario when reusing requirements is that all the product requirements can be drawn directly from the reusable set. However, this is rarely the case in product development as new requirements are likely to surface. A critical issue in requirements reuse therefore becomes how to enable products to efficiently reuse requirements as well incorporating changes to the product set. In this paper the objective is not to present a specific method for requirements reuse but to introduce and discuss the possible dimensions of adjustability when generating a product requirement set by reusing requirements from a reusable set. Six adjustability dimensions have been identified. An extensive state of the art is included to introduce the presented methods related to each adjustability dimensions. The options for implementing each adjustability dimensions in a requirement reuse approach are illustrated along with a discussion regarding the benefits and issues resulting from each option. This discussion should help practitioners to better understand the possible methods that can be implemented and to design a user friendly and sustainable approach. A case study, describing how the dimensions are incorporated in two requirements reuse approaches, for Danfoss Solar Inverters (SI) and Danfoss Frequency Drives is provided. As a result an overview of how each adjustability dimensions is implemented in each case is presented. The case study demonstrates that all the identified adjustability dimensions were important elements in requirements reuse implementation. The case study furthermore highlights the need, not only to understand the effects of each adjustability dimension but also of the dependencies to case specific criterions. The classification of adjustability dimensions in requirements reuse and the options for their implementation has not been outlined by previous research and should be a useful contribution both to researchers and practitioners working in the field of requirements reuse. },
  doi      = {https://doi.org/10.1016/j.compind.2014.02.011},
  keywords = {Adjustability dimensions,Product development,Requirements reuse,Requirements specification},
  url      = {http://www.sciencedirect.com/science/article/pii/S0166361514000451},
}

@Article{Jha2009181,
  author   = {Jha, M and O'Brien, L},
  title    = {{Identifying issues and concerns in software reuse in software product lines}},
  journal  = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year     = {2009},
  volume   = {5791 LNCS},
  pages    = {181--190},
  abstract = {One of the reasons for introducing software product lines (SPL) is the reduction of costs through reusing common assets for different products. Developing assets to be reused in different products is often not easy. Increasing complexity due to the multitude of different functions and their interactions as well as a rising number of different product variants are just some of the challenges that must be faced when reusing software and other assets. In an attempt to understand the obstacles to implementing software reuse in SPL we have conducted a survey to investigate how software reuse is adopted in SPL so as to provide the necessary degree of support for engineering software product line applications and to identify some of the issues and concerns in software reuse. This survey also gathers information from SPL practitioners on what influences the selection of software to reuse within a software product line. This paper reports the results of that survey. {\textcopyright} 2009 Springer Berlin Heidelberg.},
  annote   = {cited By 1},
  doi      = {10.1007/978-3-642-04211-9_18},
  keywords = {Computer software reusability,Degree of support; Domain Engineering; Engineering,Network architecture; Surveys},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-70350371737{\&}doi=10.1007{\%}2F978-3-642-04211-9{\_}18{\&}partnerID=40{\&}md5=5d9604f338ca236add629049a9dd0a6a},
}

@Article{Wille2016547,
  author   = {Wille, D and Tiede, M and Schulze, S and Seidl, C and Schaefer, I},
  title    = {{Identifying variability in object-oriented code using model-based code mining}},
  journal  = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year     = {2016},
  volume   = {9953 LNCS},
  pages    = {547--562},
  abstract = {A large set of object-oriented programming (OOP) languages exists to realize software for different purposes. Companies often create variants of their existing software by copying and modifying them to changed requirements. While these so-called clone-and-own approaches allow to save money in short-term, they expose the company to severe risks regarding long-term evolution and product quality. The main reason is the high manual maintenance effort which is needed due to the unknown relations between variants. In this paper, we introduce a modelbased approach to identify variability information for OOP code, allowing companies to better understand and manage variability between their variants. This information allows to improve maintenance of the variants and to transition from single variant development to more elaborate reuse strategies such as software product lines. We demonstrate the applicability of our approach by means of a case study analyzing variants generated from an existing software product line and comparing our findings to the managed reuse strategy. {\textcopyright} Springer International Publishing AG 2016.},
  annote   = {cited By 0},
  doi      = {10.1007/978-3-319-47169-3_43},
  keywords = {Codes (symbols); Computer software; Computer softw,Maintenance efforts; Model based approach; Model-,Object oriented programming},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84993995738{\&}doi=10.1007{\%}2F978-3-319-47169-3{\_}43{\&}partnerID=40{\&}md5=8c065ef5e07b363942ab4c1f1e3a46cc},
}

@Article{Nekvi:2014:IRC:2666081.2629432,
  author        = {Nekvi, Md Rashed I and Madhavji, Nazim H},
  title         = {{Impediments to Regulatory Compliance of Requirements in Contractual Systems Engineering Projects: A Case Study}},
  journal       = {ACM Trans. Manage. Inf. Syst.},
  year          = {2014},
  volume        = {5},
  number        = {3},
  pages         = {15:1----15:35},
  issn          = {2158-656X},
  address       = {New York, NY, USA},
  doi           = {10.1145/2629432},
  keywords      = {Legal requirements elicitation,case study,compliance of requirements,effort estimation,rail infrastructure system},
  mendeley-tags = {case study},
  publisher     = {ACM},
  url           = {http://0-doi.acm.org.fama.us.es/10.1145/2629432},
}

@Article{Mohan2008922,
  author   = {Mohan, Kannan and Xu, Peng and Cao, Lan and Ramesh, Balasubramaniam},
  title    = {{Improving change management in software development: Integrating traceability and software configuration management}},
  journal  = {Decision Support Systems},
  year     = {2008},
  volume   = {45},
  number   = {4},
  pages    = {922--936},
  issn     = {0167-9236},
  abstract = {Software configuration management (SCM) and traceability are two prominent practices that support change management in software development. While {\{}SCM{\}} helps manage the evolution of software artifacts and their documentation, traceability helps manage knowledge about the process of the development of software artifacts. In this paper, we present the integration of traceability and {\{}SCM{\}} to help change management during the development and evolution of software artifacts. We developed a traceability model using a case study conducted in a software development organization. This model represents knowledge elements that are essential to comprehensively manage changes tracked within the change management function of {\{}SCM{\}} tools. A tool that supports the integrated practice of {\{}SCM{\}} and traceability is also presented. We illustrate the usefulness of our model and tool using a change management scenario that was drawn from our case study. We also present a qualitative study towards empirically evaluating the usefulness of our approach. },
  annote   = {Information Technology and Systems in the Internet-Era},
  doi      = {https://doi.org/10.1016/j.dss.2008.03.003},
  keywords = {Change management,Process knowledge,Software configuration management,Traceability},
  url      = {http://www.sciencedirect.com/science/article/pii/S0167923608000523},
}

@Article{WANG20149,
  author   = {Wang, Dan and Qi, Chao and Wang, Hongwei},
  title    = {{Improving emergency response collaboration and resource allocation by task network mapping and analysis}},
  journal  = {Safety Science},
  year     = {2014},
  volume   = {70},
  pages    = {9--18},
  issn     = {0925-7535},
  abstract = {Efficient resource allocation and collaboration among involved agencies are two essential prerequisites for successful emergency response. In order to contribute to reasonable resource allocation and targeted collaboration, this paper proposes a method of generating task network for emergency response based on the snowball procedure and an associated method of analyzing task network based on social network analysis. Firstly, the criticality of a task is evaluated using the proposed Weighted Proximity Prestige (WPP) index, which takes into account both the network structure and the dynamic urgency levels of response goals. Secondly, by calculating the WPP and observing its changing trend with time, shared resources for all response goals are identified. Thirdly, for each task, relations sink to it are ranked according their relative importance to provide explicit collaboration guidance. A case study based on the Beijing Flood Emergency Response Plan (2012 Amendment) is carried out to verify the rationality and effectiveness of the proposed method. The case study reveals that the WPP index particularly emphasizes tasks having dominating influence over on-site rescue actions, such as order maintenance, traffic route designing, and transportation resource coordination, which do not attract sufficient attentions in emergency response practices. Resources under severe contention as transit-related and man-power related tasks are identified based on the WPP index. Ranking the relations sinking to each task on a local scale provides more accurate information of working focuses to the agencies responsible for the task.},
  doi      = {https://doi.org/10.1016/j.ssci.2014.05.005},
  keywords = {Emergency response,Relation importance,Shared resources,Task network analysis,Weighted Proximity Prestige},
  url      = {http://www.sciencedirect.com/science/article/pii/S0925753514001106},
}

@Article{CETINA2017261,
  author   = {Cetina, Carlos and Font, Jaime and Arcega, Lorena and P{\'{e}}rez, Francisca},
  title    = {{Improving feature location in long-living model-based product families designed with sustainability goals}},
  journal  = {Journal of Systems and Software},
  year     = {2017},
  volume   = {134},
  pages    = {261--278},
  issn     = {0164-1212},
  abstract = {The benefits of Software Product Lines (SPL) are very appealing: software development becomes better, faster, and cheaper. Unfortunately, these benefits come at the expense of a migration from a family of products to a SPL. Feature Location could be useful in achieving the transition to SPLs. This work presents our FeLLaCaM approach for Feature Location. Our approach calculates similarity to a description of the feature to locate, occurrences where the candidate features remain unchanged, and changes performed to the candidate features throughout the retrospective of the product family. We evaluated our approach in two long-living industrial domains: a model-based family of firmwares for induction hobs that was developed over more than 15 years, and a model-based family of PLC software to control trains that was developed over more than 25 years. In our evaluation, we compare our FeLLaCaM approach with two other approaches for Feature Location: (1) FLL (Feature Location through Latent Semantic Analysis) and (2) FLC (Feature Location through Comparisons). We measure the performance of FeLLaCaM, FLL, and FLC in terms of recall, precision, Matthews Correlation Coefficient, and Area Under the Receiver Operating Characteristics curve. The results show that FeLLaCaM outperforms FLL and FLC.},
  doi      = {https://doi.org/10.1016/j.jss.2017.09.022},
  keywords = {Architecture sustainability,Feature location,Long-Living software systems,Model–Driven engineering,Software product lines},
  url      = {http://www.sciencedirect.com/science/article/pii/S016412121730211X},
}

@Article{Peng2013664,
  author   = {Peng, Xin and Xing, Zhenchang and Tan, Xi and Yu, Yijun and Zhao, Wenyun},
  title    = {{Improving feature location using structural similarity and iterative graph mapping}},
  journal  = {Journal of Systems and Software},
  year     = {2013},
  volume   = {86},
  number   = {3},
  pages    = {664--676},
  issn     = {0164-1212},
  abstract = {Locating program element(s) relevant to a particular feature is an important step in efficient maintenance of a software system. The existing feature location techniques analyse each feature independently and perform a one-time analysis after being provided an initial input. As a result, these techniques are sensitive to the quality of the input. In this paper, we propose to address the above issues in feature location using an iterative context-aware approach. The underlying intuition is that features are not independent of each other, and the structure of source code resembles the structure of features. The distinguishing characteristics of the proposed approach are: (1) it takes into account the structural similarity between a feature and a program element to determine feature-element relevance and (2) it employs an iterative process to propagate the relevance of the established mappings between a feature and a program element to the neighbouring features and program elements. We evaluate our approach using two different systems, DirectBank, a small-scale industry financial system, and Linux kernel, a large-scale open-source operating system. Our evaluation suggests that the proposed approach is more robust and can significantly increase the recall of feature location with only a minor decrease of precision. },
  doi      = {https://doi.org/10.1016/j.jss.2012.10.270},
  keywords = {Feature location,Information retrieval,Structural similarity,Traceability recovery},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121212003007},
}

@Article{Beloglazov201575,
  author   = {Beloglazov, A and Banerjee, D and Hartman, A and Buyya, R},
  title    = {{Improving Productivity in Design and Development of Information Technology (IT) Service Delivery Simulation Models}},
  journal  = {Journal of Service Research},
  year     = {2015},
  volume   = {18},
  number   = {1},
  pages    = {75--89},
  abstract = {The unprecedented scale of Information Technology (IT) service delivery requires careful analysis and optimization of service systems. The simulation is an efficient way to handle the complexity of modeling and optimization of real-world service delivery systems. However, typically developed custom simulation models lack standard architectures and limit the reuse of design and implementation artifacts across multiple models. In this work, following the design science research methodology, based on a formal model of service delivery systems and applying an adapted software product line (SPL) approach, we create a design artifact for building product lines of IT service delivery simulation models, which vastly simplify and reduce the cost of simulation model design and development. We evaluate the design artifact by constructing a product line of simulation models for a set of IBM's IT service delivery systems. We validate the proposed approach by comparing the simulation results obtained using our models with the results from the corresponding custom simulation models. The case study demonstrates that the proposed approach leads to 5–8 times reductions in the time required to design and develop related simulation models. The potential implications of the application of the proposed approach within an organization are quicker responses to changes in the business environment, more information to assist in managerial decisions, and reduced workload on the process reengineering specialists. {\textcopyright} The Author(s) 2014.},
  annote   = {cited By 0},
  doi      = {10.1177/1094670514541002},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921056881{\&}doi=10.1177{\%}2F1094670514541002{\&}partnerID=40{\&}md5=f275d49946d43f6e2a61aadd9364b21b},
}

@Article{Lung2016311,
  author   = {Lung, Chung-Horng and Zhang, Xu and Rajeswaran, Pragash},
  title    = {{Improving software performance and reliability in a distributed and concurrent environment with an architecture-based self-adaptive framework}},
  journal  = {Journal of Systems and Software},
  year     = {2016},
  volume   = {121},
  pages    = {311--328},
  issn     = {0164-1212},
  abstract = {Abstract More and more, modern software systems in a distributed and parallel environment are becoming highly complex and difficult to manage. A self-adaptive approach that integrates monitoring, analyzing, and actuation functionalities has the potential to accommodate an ever dynamically changing environment. This paper proposes an architecture-level self-adaptive framework with the aim of improving performance and reliability. To meet such a goal, this paper presents a Self-Adaptive Framework for Concurrency Architectures (SAFCA) that consists of multiple well-documented architectural patterns in addition to monitoring and adaptive capabilities. With this framework, a system using an architectural alternative can activate another alternative at runtime to cope with increasing demands or to recover from failure. Five adaptation mechanisms have been developed for concept demonstration and evaluation; four focus on performance improvement and one deals with failover and reliability enhancement. We have performed a number of experiments with this framework. The experimental results demonstrate that the proposed adaptive framework can mitigate the over-provisioning method commonly used in practice. As a result, resource usage becomes more efficient for most normal conditions, while the system is still able to effectively handle bursty or growing demands using an adaptive mechanism. The performance of {\{}SAFCA{\}} is also better than systems using only standalone architectural alternatives without an adaptation scheme. Moreover, the experimental results show that a fast recovery can be realized in the case of failure by conducting an architecture switchover to maintain the desired service. },
  doi      = {https://doi.org/10.1016/j.jss.2016.06.102},
  keywords = {Autonomic computing,Distributed and concurrent architecture,Elastic computing,Patterns,Performance,Reliability,Software architecture},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121216300991},
}

@Article{GUANA2013541,
  author   = {Guana, Victor and Correal, Dario},
  title    = {{Improving software product line configuration: A quality attribute-driven approach}},
  journal  = {Information and Software Technology},
  year     = {2013},
  volume   = {55},
  number   = {3},
  pages    = {541--562},
  issn     = {0950-5849},
  abstract = {Context
During the definition of software product lines (SPLs) it is necessary to choose the components that appropriately fulfil a product's intended functionalities, including its quality requirements (i.e., security, performance, scalability). The selection of the appropriate set of assets from many possible combinations is usually done manually, turning this process into a complex, time-consuming, and error-prone task.
Objective
Our main objective is to determine whether, with the use of modeling tools, we can simplify and automate the definition process of a SPL, improving the selection process of reusable assets.
Method
We developed a model-driven strategy based on the identification of critical points (sensitivity points) inside the SPL architecture. This strategy automatically selects the components that appropriately match the product's functional and quality requirements. We validated our approach experimenting with different real configuration and derivation scenarios in a mobile healthcare SPL where we have worked during the last three years.
Results
Through our SPL experiment, we established that our approach improved in nearly 98{\%} the selection of reusable assets when compared with the unassisted analysis selection. However, using our approach there is an increment in the time required for the configuration corresponding to the learning curve of the proposed tools.
Conclusion
We can conclude that our domain-specific modeling approach significantly improves the software architect's decision making when selecting the most suitable combinations of reusable components in the context of a SPL.},
  annote   = {Special Issue on Software Reuse and Product Lines},
  doi      = {https://doi.org/10.1016/j.infsof.2012.09.007},
  keywords = {Domain specific modeling,Model driven – software product lines,Quality evaluation,Sensitivity points,Software architecture,Variability management},
  url      = {http://www.sciencedirect.com/science/article/pii/S0950584912002017},
}

@Article{Heradio20127919,
  author   = {Heradio, Ruben and Fernandez-Amoros, David and Torre-Cubillo, Luis and Garcia-Plaza, Alberto Perez},
  title    = {{Improving the accuracy of {\{}COPLIMO{\}} to estimate the payoff of a software product line}},
  journal  = {Expert Systems with Applications},
  year     = {2012},
  volume   = {39},
  number   = {9},
  pages    = {7919--7928},
  issn     = {0957-4174},
  abstract = {Software product line engineering pursues the efficient development of families of similar products. {\{}COPLIMO{\}} is an economic model that relies on {\{}COCOMO{\}} {\{}II{\}} to estimate the benefits of adopting a product line approach compared to developing the products one by one. Although {\{}COPLIMO{\}} is an ideal economic model to support decision making on the incremental development of a product line, it makes some simplifying assumptions that may produce high distortions in the estimates (e.g., {\{}COPLIMO{\}} takes for granted that all the products have the same size). This paper proposes a {\{}COPLIMO{\}} reformulation that avoids such assumptions and, consequently, improves the accuracy of the estimates. To support our proposal, we present an algorithm that infers the additional information that our {\{}COPLIMO{\}} reformulation requires from feature diagrams, which is a widespread notation to model the domain of a product line. },
  doi      = {https://doi.org/10.1016/j.eswa.2012.01.109},
  keywords = {Decision support,Economic model,Feature diagram,Product counting,Software product line},
  url      = {http://www.sciencedirect.com/science/article/pii/S0957417412001273},
}

@Article{REINHARTZBERGER2015191,
  author   = {Reinhartz-Berger, Iris and Wulf-Hadash, Ora},
  title    = {{Improving the management of product lines by performing domain knowledge extraction and cross product line analysis}},
  journal  = {Information and Software Technology},
  year     = {2015},
  volume   = {59},
  pages    = {191--204},
  issn     = {0950-5849},
  abstract = {Context
Increase in market competition is one of the main reasons for developing and maintaining families of systems, termed Product Lines (PLs). Managing those PLs is challenging, let alone the management of several related PLs. Currently, those PLs are managed separately or their relations are analyzed assuming explicit specification of dependencies or use of an underlying terminology. Such assumptions may not hold when developing the PLs in different departments or companies applying various engineering processes.
Objective
In this work we call for utilizing the knowledge gained from developing and maintaining different PLs in the same domain in order to recommend on improvements to the management of PLs.
Method
The suggested approach conducts domain knowledge extraction and cross PL analysis on feature diagrams – the main aid for modeling PL variability. The domain knowledge is extracted by applying similarity metrics, clustering, and mining techniques. Based on the created domain models, the approach performs cross PL analysis that examines relations in the domain models and generates improvement recommendations to existing PLs and overall management recommendations (e.g., merging or splitting PLs).
Results
The approach outcomes were evaluated by humans in a domain of mobile phones. The evaluation results may provide evidence that the outcomes of the approach in general and its recommendations in particular meet human perception of the given domain.
Conclusion
We conclude that through domain knowledge extraction and cross PL analysis the suggested approach may generate recommendations useful to the management of individual PLs, as well as to the overall management of different PLs in the same domain.},
  doi      = {https://doi.org/10.1016/j.infsof.2014.11.007},
  keywords = {Domain analysis,Feature modeling,Software product line engineering,Variability management},
  url      = {http://www.sciencedirect.com/science/article/pii/S0950584914002535},
}

@Article{Cabral2010241,
  author   = {Cabral, I and Cohen, M B and Rothermel, G},
  title    = {{Improving the testing and testability of software product lines}},
  journal  = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year     = {2010},
  volume   = {6287 LNCS},
  pages    = {241--255},
  abstract = {Software Product Line (SPL) engineering offers several advantages in the development of families of software products. There is still a need, however, for better understanding of testability issues and for testing techniques that can operate cost-effectively on SPLs. In this paper we consider these testability issues and highlight some differences between optional versus alternative features. We then provide a graph based testing approach called the FIG Basis Path method that selects products and features for testing based on a feature dependency graph. We conduct a case study on several non-trivial SPLs and show that for these subjects, the FIG Basis Path method is as effective as testing all products, but tests no more than 24{\%} of the products in the SPL. {\textcopyright} 2010 Springer-Verlag Berlin Heidelberg.},
  annote   = {cited By 9},
  doi      = {10.1007/978-3-642-15579-6_17},
  keywords = {Dependency graphs; Graph-based; Non-trivial; Path,Network architecture; Testing,Software design},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-78049355116{\&}doi=10.1007{\%}2F978-3-642-15579-6{\_}17{\&}partnerID=40{\&}md5=c5952bce2ebb6aa5b2e37ca2f83771da},
}

@Article{Al-Hajjaji:2016:IEP:3093335.2993253,
  author    = {Al-Hajjaji, Mustafa and Krieter, Sebastian and Th{\"{u}}m, Thomas and Lochau, Malte and Saake, Gunter},
  title     = {{IncLing: Efficient Product-line Testing Using Incremental Pairwise Sampling}},
  journal   = {SIGPLAN Not.},
  year      = {2016},
  volume    = {52},
  number    = {3},
  pages     = {144--155},
  issn      = {0362-1340},
  address   = {New York, NY, USA},
  doi       = {10.1145/3093335.2993253},
  keywords  = {Software product lines,combinatorial interaction testing,model-based testing,sampling},
  publisher = {ACM},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/3093335.2993253},
}

@Article{Lochau2016245,
  author   = {Lochau, Malte and Mennicke, Stephan and Baller, Hauke and Ribbeck, Lars},
  title    = {{Incremental model checking of delta-oriented software product lines}},
  journal  = {Journal of Logical and Algebraic Methods in Programming},
  year     = {2016},
  volume   = {85},
  number   = {1, Part 2},
  pages    = {245--267},
  issn     = {2352-2208},
  abstract = {Abstract We propose DeltaCCS, a delta-oriented extension to Milner's process calculus {\{}CCS{\}} to formalize behavioral variability in software product line specifications in a modular way. In DeltaCCS, predefined change directives are applied to core process semantics by overriding the {\{}CCS{\}} term rewriting rule in a determined way. On this basis, behavioral properties expressed in the Modal $\mu$-Calculus are verifiable for entire product-line specifications both product-by-product as well as in a family-based manner as usual. To overcome potential scalability limitations of those existing strategies, we propose a novel approach for incremental model checking of product lines. Therefore, variability-aware congruence notions and a respective normal form for DeltaCCS specifications allow for a rigorous local reasoning on the preservation of behavioral properties after varying {\{}CCS{\}} specifications. We present a prototypical DeltaCCS model checker implementation based on Maude and provide evaluation results obtained from various experiments concerning efficiency trade-offs compared to existing approaches. },
  annote   = {Formal Methods for Software Product Line Engineering},
  doi      = {https://doi.org/10.1016/j.jlamp.2015.09.004},
  keywords = {Model checking,Operational semantics,Variability modeling},
  url      = {http://www.sciencedirect.com/science/article/pii/S2352220815000863},
}

@Article{Lochau201267,
  author   = {Lochau, M and Schaefer, I and Kamischke, J and Lity, S},
  title    = {{Incremental model-based testing of delta-oriented software product lines}},
  journal  = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year     = {2012},
  volume   = {7305 LNCS},
  pages    = {67--82},
  abstract = {Software product line (SPL) engineering provides a promising approach for developing variant-rich software systems. But, testing of every product variant in isolation to ensure its correctness is in general not feasible due to the large number of product variants. Hence, a systematic approach that applies SPL reuse principles also to testing of SPLs in a safe and efficient way is essential. To address this issue, we propose a novel, model-based SPL testing framework that is based on a delta-oriented SPL test model and regression-based test artifact derivations. Test artifacts are incrementally constructed for every product variant by explicitly considering commonality and variability between two consecutive products under test. The resulting SPL testing process is proven to guarantee stable test coverage for every product variant and allows the derivation of redundancy-reduced, yet reliable retesting obligations. We compare our approach with an alternative SPL testing strategy by means of a case study from the automotive domain. {\textcopyright} 2012 Springer-Verlag.},
  annote   = {cited By 37},
  doi      = {10.1007/978-3-642-30473-6_7},
  keywords = {Automotive domains; Commonality and variability; M,Network architecture; Software design; Software t,Testing},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862204405{\&}doi=10.1007{\%}2F978-3-642-30473-6{\_}7{\&}partnerID=40{\&}md5=e736952c241a30cc6f92d8e3b797827c},
}

@Article{Sinnema2008584,
  author   = {Sinnema, Marco and Deelstra, Sybren},
  title    = {{Industrial validation of {\{}COVAMOF{\}}}},
  journal  = {Journal of Systems and Software},
  year     = {2008},
  volume   = {81},
  number   = {4},
  pages    = {584--600},
  issn     = {0164-1212},
  abstract = {{\{}COVAMOF{\}} is a variability management framework for product families that was developed to reduce the number of iterations required during product derivation and to reduce the dependency on experts. In this paper, we present the results of an experiment with {\{}COVAMOF{\}} in industry. The results show that with COVAMOF, engineers that are not involved in the product family were now capable of deriving the products in 100{\%} of the cases, compared to 29{\%} of the cases without COVAMOF. For experts, the use of {\{}COVAMOF{\}} reduced the number of iterations by 42{\%}, and the total derivation time by 38{\%}. },
  annote   = {Selected papers from the 10th Conference on Software Maintenance and Reengineering (CSMR 2006)},
  doi      = {https://doi.org/10.1016/j.jss.2007.06.002},
  file     = {:Users/mac/ownCloud/Anita/SLR/REVISION DE JOURNALS/PAPERS{\_}A/selected/checked/experimentos analizados/2 Industrial validation of COVAMOF.pdf:pdf},
  keywords = {Industrial validation,Product family engineering,Software Variability Management},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121207001422},
}

@Article{Riehle2016,
  author  = {Riehle, Dirk and Capraro, Maximilian and Kips, Detlef and Horn, Lars},
  title   = {{Inner Source in Platform-Based Product Engineering}},
  journal = {IEEE Transactions on Software Engineering},
  year    = {2016},
  volume  = {42},
  number  = {12},
  pages   = {1162--1177},
  month   = {dec},
  issn    = {0098-5589},
  doi     = {10.1109/TSE.2016.2554553},
  url     = {http://ieeexplore.ieee.org/document/7452676/},
}

@Article{Beohar20161131,
  author   = {Beohar, Harsh and Mousavi, Mohammad Reza},
  title    = {{Input–output conformance testing for software product lines}},
  journal  = {Journal of Logical and Algebraic Methods in Programming},
  year     = {2016},
  volume   = {85},
  number   = {6},
  pages    = {1131--1153},
  issn     = {2352-2208},
  abstract = {Abstract We extend the theory of input–output conformance (IOCO) testing to accommodate behavioral models of software product lines (SPLs). We present the notions of residual and spinal testing. These notions allow for structuring the test process for {\{}SPLs{\}} by taking variability into account and extracting separate test suites for common and specific features of an SPL. The introduced notions of residual and spinal test suites allow for focusing on the newly introduced behavior and avoiding unnecessary re-test of the old one. Residual test suites are very conservative in that they require retesting the old behavior that can reach to new behavior. However, spinal test suites more aggressively prune the old tests and only focus on those test sequences that are necessary in reaching the new behavior. We show that residual testing is complete but does not usually lead to much reduction in the test-suite. In contrast, spinal testing is not necessarily complete but does reduce the test-suite. We give sufficient conditions on the implementation to guarantee completeness of spinal testing. Finally, we specify and analyze an example regarding the Ceiling Speed Monitoring Function from the European Train Control System. },
  annote   = {{\{}NWPT{\}} 2013},
  doi      = {https://doi.org/10.1016/j.jlamp.2016.09.007},
  keywords = {Input–output conformance testing,Input–output featured transition systems,Model based testing,Software product lines},
  url      = {http://www.sciencedirect.com/science/article/pii/S2352220816301171},
}

@Article{Ahmed2007836,
  author   = {Ahmed, F and Capretz, L F and Sheikh, S A},
  title    = {{Institutionalization of software product line: An empirical investigation of key organizational factors}},
  journal  = {Journal of Systems and Software},
  year     = {2007},
  volume   = {80},
  number   = {6},
  pages    = {836--849},
  abstract = {A good fit between the person and the organization is essential in a better organizational performance. This is even more crucial in case of institutionalization of a software product line practice within an organization. Employees' participation, organizational behavior and management contemplation play a vital role in successfully institutionalizing software product lines in a company. Organizational dimension has been weighted as one of the critical dimensions in software product line theory and practice. A comprehensive empirical investigation to study the impact of some organizational factors on the performance of software product line practice is presented in this work. This is the first study to empirically investigate and demonstrate the relationships between some of the key organizational factors and software product line performance of an organization. The results of this investigation provide empirical evidence and further support the theoretical foundations that in order to institutionalize software product lines within an organization, organizational factors play an important role. {\textcopyright} 2006 Elsevier Inc. All rights reserved.},
  annote   = {cited By 16},
  doi      = {10.1016/j.jss.2006.09.010},
  file     = {:Users/mac/Library/Application Support/Mendeley Desktop/Downloaded/Ahmed, Capretz, Sheikh - 2007 - Institutionalization of software product line An empirical investigation of key organizational factors.pdf:pdf},
  keywords = {Computer software; Industrial management; Societie,Organizational behavior; Organizational theory; S,Software engineering},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-33947384858{\&}doi=10.1016{\%}2Fj.jss.2006.09.010{\&}partnerID=40{\&}md5=bbde19a109b7058e813ca72d3401c217},
}

@Article{TANG2013536,
  author   = {Tang, Jiafu and Zhiqiao, Wu and Kwong, C K and Luo, Xinggang},
  title    = {{Integrated production strategy and reuse scenario: A CoFAQ model and case study of mail server system development}},
  journal  = {Omega},
  year     = {2013},
  volume   = {41},
  number   = {3},
  pages    = {536--552},
  issn     = {0305-0483},
  abstract = {One of the core problems in software product family (SPF) is the coordination of product building and core asset development, specifically the integration of production strategy decision and core asset scenario selection. In the current paper, a model of Cost Optimization under Functional And Quality (CoFAQ) goal satisfaction constraints is developed. It provides a systematic mechanism for management to analyze all possible products and evaluate various reuse alternatives at the organizational level. The CoFAQ model facilitates decision-makers to optimize the SPF development process by determining which products are involved in the SPF (i.e. production strategy) and which reuse scenario for each module should be selected to implement the SPF toward minimum total developing cost under the constraints of satisfying functional and quality goals. A two-phase algorithm with heuristic (TPA) is developed to solve the model efficiently. Based on the TPA, the CoFAQ is reduced to a weighted set-covering problem for production strategy decision and a knapsack problem for the reuse scenario selection. An application of the model in mail server domain development is presented to illustrate how it has been used in practice.},
  doi      = {https://doi.org/10.1016/j.omega.2012.07.003},
  keywords = {Heuristic,Integrated decision,Optimization,Software reuse},
  url      = {http://www.sciencedirect.com/science/article/pii/S0305048312001053},
}

@Article{HUANG20081471,
  author   = {Huang, Hui-Wen and Shih, Chunkuan and Yih, Swu and Chen, Ming-Huei},
  title    = {{Integrated software safety analysis method for digital I{\&}C systems}},
  journal  = {Annals of Nuclear Energy},
  year     = {2008},
  volume   = {35},
  number   = {8},
  pages    = {1471--1483},
  issn     = {0306-4549},
  abstract = {The digitalized Instrumentation and Control (I{\&}C) system of Nuclear power plants can provide more powerful overall operation capability, and user friendly man-machine interface. The operator can obtain more information through digital I{\&}C system. However, while I{\&}C system being digitalized, three issues are encountered: (1) software common-cause failure, (2) the interaction failure between operator and digital instrumentation and control system interface, and (3) the non-detectability of software failure. These failures might defeat defense echelons, and make the Diversity and Defense-in-Depth (D3) analysis be more difficult. This work developed an integrated methodology to evaluate nuclear power plant safety effect by interactions between operator and digital I{\&}C system, and then propose improvement recommendations. This integrated methodology includes component-level software fault tree, system-level sequence-tree method and nuclear power plant computer simulation analysis. Software fault tree can clarify the software failure structure in digital I{\&}C systems. Sequence-tree method can identify the interaction process and relationship among operator and I{\&}C systems in each D3 echelon in a design basis event. Nuclear power plant computer simulation analysis method can further analyze the available backup facilities and allowable manual action duration for the operator when the digital I{\&}C fail to function. Applying this methodology to evaluate the performance of digital nuclear power plant D3 design, could promote the nuclear power plant operation safety. The operator can then trust the nuclear power plant than before, when operating the highly automatic digital I{\&}C facilities.},
  doi      = {https://doi.org/10.1016/j.anucene.2008.01.009},
  url      = {http://www.sciencedirect.com/science/article/pii/S0306454908000273},
}

@Article{Johnsen201567,
  author   = {Johnsen, Einar Broch and Schlatte, Rudolf and Tarifa, S Lizeth Tapia},
  title    = {{Integrating deployment architectures and resource consumption in timed object-oriented models}},
  journal  = {Journal of Logical and Algebraic Methods in Programming},
  year     = {2015},
  volume   = {84},
  number   = {1},
  pages    = {67--91},
  issn     = {2352-2208},
  abstract = {Abstract Software today is often developed for many deployment scenarios; the software may be adapted to sequential, concurrent, distributed, and even virtualized architectures. Since software performance can vary significantly depending on the target architecture, design decisions need to address which features to include and what performance to expect for different architectures. To make use of formal methods for these design decisions, system models need to range over deployment scenarios. For this purpose, it is desirable to lift aspects of low-level deployment to the abstraction level of the modeling language. This paper proposes an integration of deployment architectures in the Real-Time {\{}ABS{\}} language, with restrictions on processing resources. Real-Time {\{}ABS{\}} is a timed, abstract and behavioral specification language with a formal semantics and a Java-like syntax, that targets concurrent, distributed and object-oriented systems. A separation of concerns between execution cost at the object level and execution capacity at the deployment level makes it easy to compare the timing and performance of different deployment scenarios already during modeling. The language and associated simulation tool is demonstrated on examples and its semantics is formalized. },
  annote   = {Special Issue: The 23rd Nordic Workshop on Programming Theory (NWPT 2011)Special Issue: Domains X, International workshop on Domain Theory and applications, Swansea, 5-7 September, 2011},
  doi      = {https://doi.org/10.1016/j.jlamp.2014.07.001},
  keywords = {Deployment architecture,Formal methods,Object orientation,Performance,Real-Time {\{}ABS{\}},Resource management},
  url      = {http://www.sciencedirect.com/science/article/pii/S2352220814000479},
}

@Article{Tawhid2008490,
  author   = {Tawhid, R and Petriu, D},
  title    = {{Integrating performance analysis in the model driven development of software product lines}},
  journal  = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year     = {2008},
  volume   = {5301 LNCS},
  pages    = {490--504},
  abstract = {The paper proposes to integrate performance analysis in the early phases of the model-driven development process for Software Product Lines (SPL). We start by adding generic performance annotations to the UML model representing the set of core reusable SPL assets. The annotations are generic and use the MARTE Profile recently adopted by OMG. A first model transformation realized in the Atlas Transformation Language (ATL), which is the focus of this paper, derives the UML model of a specific product with concrete MARTE performance annotations from the SPL model. A second transformation generates a Layered Queueing Network performance model for the given product by applying an existing transformation approach named PUMA, developed in previous work. The proposed technique is illustrated with an e-commerce case study that models the commonality and variability in both structural and behavioural SPL views. A product is derived and the performance of two design alternatives is compared. {\textcopyright} 2008 Springer-Verlag Berlin Heidelberg.},
  annote   = {cited By 20},
  doi      = {10.1007/978-3-540-87875-9_35},
  keywords = {ATL; MARTE; Performance analysis; Software produc,Electronic commerce; Linguistics; Models; Network,Modal analysis},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-56649124490{\&}doi=10.1007{\%}2F978-3-540-87875-9{\_}35{\&}partnerID=40{\&}md5=509aa5e29e0be93164757d1736f3283a},
}

@Article{Ng201566,
  author        = {Ng, Pan-Wei},
  title         = {{Integrating software engineering theory and practice using essence: A case study}},
  journal       = {Science of Computer Programming},
  year          = {2015},
  volume        = {101},
  pages         = {66--78},
  issn          = {0167-6423},
  abstract      = {Abstract Software engineering is complex and success depends on many inter-related factors. Theory Based Software Engineering (TBSE) is about providing a practical way for software teams to understand the relationships and the influence of these factors to thereby adapt the way they work. This paper proposes an approach to {\{}TBSE{\}} based on Essence, a software engineering kernel distilled by the {\{}SEMAT{\}} (Software Engineering Method and Theory) initiative. Essence supports {\{}TBSE{\}} by providing a domain model that is useful for organizing and relating software engineering factors. Essence also helps make recommended practices precise and actionable to software teams. We provide a step-by-step application of our approach on an industrial software process improvement case study. The case study achieved 21{\%} productivity gains and 58{\%} decrease in defects. But more importantly than these results, it demonstrates the value of Essence in supporting TBSE.},
  annote        = {Towards general theories of software engineering},
  doi           = {https://doi.org/10.1016/j.scico.2014.11.009},
  keywords      = {Essence,Kernel,SEMAT,Software engineering,Theory,case study},
  mendeley-tags = {case study},
  url           = {http://www.sciencedirect.com/science/article/pii/S0167642314005413},
}

@Article{Vilela201768,
  author   = {Vilela, J{\'{e}}ssyka and Castro, Jaelson and Martins, Luiz Eduardo G and Gorschek, Tony},
  title    = {{Integration between requirements engineering and safety analysis: A systematic literature review}},
  journal  = {Journal of Systems and Software},
  year     = {2017},
  volume   = {125},
  pages    = {68--92},
  issn     = {0164-1212},
  abstract = {Abstract Context: Safety-Critical Systems (SCS) require more sophisticated requirements engineering (RE) approaches as inadequate, incomplete or misunderstood requirements have been recognized as a major cause in many accidents and safety-related catastrophes. Objective: In order to cope with the complexity of specifying {\{}SCS{\}} by RE, we investigate the approaches proposed to improve the communication or integration between {\{}RE{\}} and safety engineering in {\{}SCS{\}} development. We analyze the activities that should be performed by {\{}RE{\}} during safety analysis, the hazard/safety techniques it could use, the relationships between safety information that it should specify, the tools to support safety analysis as well as integration benefits between these areas. Method: We use a Systematic Literature Review (SLR) as the basis for our work. Results: We developed four taxonomies to help {\{}RE{\}} during specification of {\{}SCS{\}} that classify: techniques used in (1) hazard analysis; (2) safety analysis; (3) safety-related information and (4) a detailed set of information regarding hazards specification. Conclusions: This paper is a step towards developing a body of knowledge in safety concerns necessary to {\{}RE{\}} in the specification of {\{}SCS{\}} that is derived from a large-scale SLR. We believe the results will benefit both researchers and practitioners. },
  doi      = {https://doi.org/10.1016/j.jss.2016.11.031},
  keywords = {Communication,Integration,Requirements engineering,Safety analysis,Safety-critical systems,Systematic literature review},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121216302333},
}

@Article{Duque201229,
  author   = {Duque, Rafael and Rodr{\'{i}}guez, Mar{\'{i}}a Luisa and Hurtado, Mar{\'{i}}a Visitaci{\'{o}}n and Bravo, Crescencio and Rodr{\'{i}}guez-Dom{\'{i}}nguez, Carlos},
  title    = {{Integration of collaboration and interaction analysis mechanisms in a concern-based architecture for groupware systems}},
  journal  = {Science of Computer Programming},
  year     = {2012},
  volume   = {77},
  number   = {1},
  pages    = {29--45},
  issn     = {0167-6423},
  abstract = {Collaboration and interaction analysis allows for the characterization and study of the collaborative work performed by the users of a groupware system. The results of the analyzed processes allow problems in users' collaborative work and shortcomings in the functionalities of the groupware system to be identified. Therefore, automating collaboration and interaction analysis enables users' work to be assessed and groupware system support and behavior to be improved. This article proposes a concern-based architecture to be used by groupware developers as a guide to the integration of analysis subsystems into groupware systems. This architecture was followed to design the {\{}COLLECE{\}} groupware system, which supports collaborative programming practices and integrates an analysis subsystem that assesses different aspects of the work carried out by the programmers and adapts the functionality of the system under specific conditions. },
  annote   = {System and Software Solution Oriented Architectures},
  doi      = {https://doi.org/10.1016/j.scico.2010.05.003},
  keywords = {CSCW,Collaboration and interaction analysis,Groupware,Software architectures},
  url      = {http://www.sciencedirect.com/science/article/pii/S0167642310000936},
}

@Article{Nikolov201553,
  author        = {Nikolov, Nikolay and Rossini, Alessandro and Kritikos, Kyriakos},
  title         = {{Integration of {\{}DSLs{\}} and Migration of Models: A Case Study in the Cloud Computing Domain}},
  journal       = {Procedia Computer Science},
  year          = {2015},
  volume        = {68},
  pages         = {53--66},
  issn          = {1877-0509},
  abstract      = {Abstract Domain-specific languages (DSLs) are high-level software languages representing concepts in a particular domain. In real-world scenarios, it is common to adopt multiple {\{}DSLs{\}} to solve different aspects of a specific problem. As any other software artefact, {\{}DSLs{\}} evolve independently in response to changing requirements, which leads to two challenges. First, the concepts from the {\{}DSLs{\}} have to be integrated into a single language. Second, models that conform to an old version of the language have to be migrated to conform to its current version. In this paper, we discuss how we tackled the challenge of integrating the {\{}DSLs{\}} that comprise the Cloud Application Modelling and Execution Language (CAMEL) by leveraging upon Eclipse Modeling Framework (EMF) and Object Constraint Language (OCL). Moreover, we propose a solution to the challenge of persisting and automatically migrating {\{}CAMEL{\}} models based on Connected Data Objects (CDO) and Edapt.},
  annote        = {1st International Conference on Cloud Forward: From Distributed to Complete Computing},
  doi           = {https://doi.org/10.1016/j.procs.2015.09.223},
  keywords      = {CAMEL,CDO,EMF,Edapt,OCL,case study,cloud computing,domain-specific language,metamodel migration,model co-evolution,model-driven engineering},
  mendeley-tags = {case study},
  url           = {http://www.sciencedirect.com/science/article/pii/S1877050915030689},
}

@Article{Shi2012270,
  author   = {Shi, J and Cohen, M B and Dwyer, M B},
  title    = {{Integration testing of software product lines using compositional symbolic execution}},
  journal  = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year     = {2012},
  volume   = {7212 LNCS},
  pages    = {270--284},
  abstract = {Software product lines are families of products defined by feature commonality and variability, with a well-managed asset base. Recent work in testing of software product lines has exploited similarities across development phases to reuse shared assets and reduce test effort. The use of feature dependence graphs has also been employed to reduce testing effort, but little work has focused on code level analysis of dataflow between features. In this paper we present a compositional symbolic execution technique that works in concert with a feature dependence graph to extract the set of possible interaction trees in a product family. It composes these to incrementally and symbolically analyze feature interactions. We experiment with two product lines and determine that our technique can reduce the overall number of interactions that must be considered during testing, and requires less time to run than a traditional symbolic execution technique. {\textcopyright} 2012 Springer-Verlag Berlin Heidelberg.},
  annote   = {cited By 15},
  doi      = {10.1007/978-3-642-28872-2_19},
  keywords = {Code-level analysis; Commonality and variability;,Data flow analysis; Software testing; Trees (math,Software engineering},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859125355{\&}doi=10.1007{\%}2F978-3-642-28872-2{\_}19{\&}partnerID=40{\&}md5=bbbff7d1460a18a668bf5090f0f82715},
}

@Article{Afzal201630,
  author   = {Afzal, Uzma and Mahmood, Tariq and Shaikh, Zubair},
  title    = {{Intelligent software product line configurations: A literature review}},
  journal  = {Computer Standards {\&} Interfaces},
  year     = {2016},
  volume   = {48},
  pages    = {30--48},
  issn     = {0920-5489},
  abstract = {Abstract A software product line (SPL) is a set of industrial software-intensive systems for configuring similar software products in which personalized feature sets are configured by different business teams. The integration of these feature sets can generate inconsistencies that are typically resolved through manual deliberation. This is a time-consuming process and leads to a potential loss of business resources. Artificial intelligence (AI) techniques can provide the best solution to address this issue autonomously through more efficient configurations, lesser inconsistencies and optimized resources. This paper presents the first literature review of both research and industrial {\{}AI{\}} applications to {\{}SPL{\}} configuration issues. Our results reveal only 19 relevant research works which employ traditional {\{}AI{\}} techniques on small feature sets with no real-life testing or application in industry. We categorize these works in a typology by identifying 8 perspectives of SPL. We also show that only 2 standard industrial {\{}SPL{\}} tools employ {\{}AI{\}} in a limited way to resolve inconsistencies. To inject more interest and application in this domain, we motivate and present future research directions. Particularly, using real-world {\{}SPL{\}} data, we demonstrate how predictive analytics (a state of the art {\{}AI{\}} technique) can separately model inconsistent and consistent patterns, and then predict inconsistencies in advance to help {\{}SPL{\}} designers during the configuration of a product. },
  annote   = {Special Issue on Information System in Distributed Environment},
  doi      = {https://doi.org/10.1016/j.csi.2016.03.003},
  keywords = {Artificial intelligence,Automated feature selection,Inconsistencies,Industrial {\{}SPL{\}} tools,Literature review,Predictive analytics,Software product line},
  url      = {http://www.sciencedirect.com/science/article/pii/S0920548916300198},
}

@Article{Myllärniemi200673,
  author        = {Myll{\"{a}}rniemi, V and Raatikainen, M and M{\"{a}}nnist{\"{o}}, T},
  title         = {{Inter-organisational approach in rapid software product family development - A case study}},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year          = {2006},
  volume        = {4039 LNCS},
  pages         = {73--86},
  abstract      = {Software product families provide an efficient means of reuse between a set of related products. However, software product families are often solely associated with intra-organisational reuse. This paper presents a case study of Fathammer, a small company developing games for different mobile devices. Reuse at Fathammer takes place at multiple levels. The game framework and engine of Fathammer is reused by partner companies that in turn produce game assets to be reused by Fathammer while developing games for various devices. Very rapid development of games is a necessity for Fathammer, whereas maintainability of games is not important. The above characteristics in particular distinguish Fathammer from other case studies and practices usually presented in the product family literature. The results show the applicability and challenges of software product family practices in the context of multiple collaborating companies and a fast-changing domain. {\textcopyright} Springer-Verlag Berlin Heidelberg 2006.},
  annote        = {cited By 3},
  keywords      = {Animation,Computer,Computer software reusability,Inter-organisational approach,Product family dev,Software engineering,case study},
  mendeley-tags = {case study},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-33746216409{\&}partnerID=40{\&}md5=8bb8b10a47429fbecd1f5282101ed665},
}

@Article{deOMelo2013412,
  author        = {{de O. Melo}, Claudia and Cruzes, Daniela S and Kon, Fabio and Conradi, Reidar},
  title         = {{Interpretative case studies on agile team productivity and management}},
  journal       = {Information and Software Technology},
  year          = {2013},
  volume        = {55},
  number        = {2},
  pages         = {412--427},
  issn          = {0950-5849},
  abstract      = {Context The management of software development productivity is a key issue in software organizations, where the major drivers are lower cost and shorter time-to-market. Agile methods, including Extreme Programming and Scrum, have evolved as “light” approaches that simplify the software development process, potentially leading to increased team productivity. However, little empirical research has examined which factors do have an impact on productivity and in what way, when using agile methods. Objective Our objective is to provide a better understanding of the factors and mediators that impact agile team productivity. Method We have conducted a multiple-case study for 6 months in three large Brazilian companies that have been using agile methods for over 2 years. We have focused on the main productivity factors perceived by team members through interviews, documentation from retrospectives, and non-participant observation. Results We developed a novel conceptual framework, using thematic analysis to understand the possible mechanisms behind such productivity factors. Agile team management was found to be the most influential factor in achieving agile team productivity. At the intra-team level, the main productivity factors were team design (structure and work allocation) and member turnover. At the inter-team level, the main productivity factors were how well teams could be effectively coordinated by proper interfaces and other dependencies and avoiding delays in providing promised software to dependent teams. Conclusion Teams should be aware of the influence and magnitude of turnover, which has been shown negative for agile team productivity. Team design choices remain an important factor impacting team productivity, even more pronounced on agile teams that rely on teamwork and people factors. The intra-team coordination processes must be adjusted to enable productive work by considering priorities and pace between teams. Finally, the revised conceptual framework for agile team productivity supports further tests through confirmatory studies.},
  annote        = {Special Section: Component-Based Software Engineering (CBSE), 2011},
  doi           = {https://doi.org/10.1016/j.infsof.2012.09.004},
  keywords      = {Agile software development,Industrial case studies,Team management,Team productivity factors,Thematic analysis,case study},
  mendeley-tags = {case study},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584912001875},
}

@Article{Bosch2011871,
  author   = {Bosch, J and Bosch-Sijtsema, P M},
  title    = {{Introducing agile customer-centered development in a legacy software product line}},
  journal  = {Software - Practice and Experience},
  year     = {2011},
  volume   = {41},
  number   = {8},
  pages    = {871--882},
  abstract = {The ability to rapidly respond to customer interest and to effectively prioritize development effort has been a long-standing challenge for mass-market software intensive products. This problem is exacerbated in the context of software product lines as functionality may easily fall over software asset and organizational boundaries with consequent losses in efficiency and nimbleness. Some companies facing these problems in their product line respond with a new development process. In this paper we discuss the developments within a single case study, Intuit's Quickbooks product line that combined agile software development, design thinking and self-organizing teams in a successful approach, which provided a significant improvement in terms of responsiveness and accuracy of building customer value. Copyright {\textcopyright} 2011 John Wiley {\&} Sons, Ltd.},
  annote   = {cited By 14},
  doi      = {10.1002/spe.1063},
  keywords = {Agile software development; compositional software,Customer satisfaction; Network architecture; Prod,Software design},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-79958247118{\&}doi=10.1002{\%}2Fspe.1063{\&}partnerID=40{\&}md5=b2bfc7317e1d83ae546384531cf74963},
}

@Article{Martini2015237,
  author        = {Martini, Antonio and Bosch, Jan and Chaudron, Michel},
  title         = {{Investigating Architectural Technical Debt accumulation and refactoring over time: A multiple-case study}},
  journal       = {Information and Software Technology},
  year          = {2015},
  volume        = {67},
  pages         = {237--253},
  issn          = {0950-5849},
  abstract      = {AbstractContext A known problem in large software companies is to balance the prioritization of short-term with long-term feature delivery speed. Specifically, Architecture Technical Debt is regarded as sub-optimal architectural solutions taken to deliver fast that might hinder future feature development, which, in turn, would hinder agility. Objective This paper aims at improving software management by shedding light on the current factors responsible for the accumulation of Architectural Technical Debt and to understand how it evolves over time. Method We conducted an exploratory multiple-case embedded case study in 7 sites at 5 large companies. We evaluated the results with additional cross-company interviews and an in-depth, company-specific case study in which we initially evaluate factors and models. Results We compiled a taxonomy of the factors and their influence in the accumulation of Architectural Technical Debt, and we provide two qualitative models of how the debt is accumulated and refactored over time in the studied companies. We also list a set of exploratory propositions on possible refactoring strategies that can be useful as insights for practitioners and as hypotheses for further research. Conclusion Several factors cause constant and unavoidable accumulation of Architecture Technical Debt, which leads to development crises. Refactorings are often overlooked in prioritization and they are often triggered by development crises, in a reactive fashion. Some of the factors are manageable, while others are external to the companies. {\{}ATD{\}} needs to be made visible, in order to postpone the crises according to the strategic goals of the companies. There is a need for practices and automated tools to proactively manage ATD.},
  doi           = {https://doi.org/10.1016/j.infsof.2015.07.005},
  keywords      = {Agile software development,Architectural Technical Debt,Qualitative model,Software architecture,Software life-cycle,Software management,case study},
  mendeley-tags = {case study},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584915001287},
}

@Article{Lim201540,
  author   = {Lim, S L and Bentley, P J and Kanakam, N and Ishikawa, F and Honiden, S},
  title    = {{Investigating country differences in mobile app user behavior and challenges for software engineering}},
  journal  = {IEEE Transactions on Software Engineering},
  year     = {2015},
  volume   = {41},
  number   = {1},
  pages    = {40--64},
  abstract = {Mobile applications (apps) are software developed for use on mobile devices and made available through app stores. App stores are highly competitive markets where developers need to cater to a large number of users spanning multiple countries. This work hypothesizes that there exist country differences in mobile app user behavior and conducts one of the largest surveys to date of app users across the world, in order to identify the precise nature of those differences. The survey investigated user adoption of the app store concept, app needs, and rationale for selecting or abandoning an app. We collected data from more than 15 countries, including USA, China, Japan, Germany, France, Brazil, United Kingdom, Italy, Russia, India, Canada, Spain, Australia, Mexico, and South Korea. Analysis of data provided by 4,824 participants showed significant differences in app user behaviors across countries, for example users from USA are more likely to download medical apps, users from the United Kingdom and Canada are more likely to be influenced by price, users from Japan and Australia are less likely to rate apps. Analysis of the results revealed new challenges to market-driven software engineering related to packaging requirements, feature space, quality expectations, app store dependency, price sensitivity, and ecosystem effect. {\textcopyright} 1976-2012 IEEE.},
  annote   = {cited By 10},
  doi      = {10.1109/TSE.2014.2360674},
  file     = {:Users/mac/Library/Application Support/Mendeley Desktop/Downloaded/Lim et al. - 2015 - Investigating country differences in mobile app user behavior and challenges for software engineering(2).pdf:pdf},
  keywords = {Application programs,Behavioral research; Commerce; Ecology; Ecosystems,Market-driven softwares; Mobile application devel},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921368982{\&}doi=10.1109{\%}2FTSE.2014.2360674{\&}partnerID=40{\&}md5=f2277b95563f18dad360bb633d21ced4},
}

@Article{REINHARTZBERGER201781,
  author   = {Reinhartz-Berger, Iris and Figl, Kathrin and Haugen, {\O}ystein},
  title    = {{Investigating styles in variability modeling: Hierarchical vs. constrained styles}},
  journal  = {Information and Software Technology},
  year     = {2017},
  volume   = {87},
  pages    = {81--102},
  issn     = {0950-5849},
  abstract = {Context
A common way to represent product lines is with variability modeling. Yet, there are different ways to extract and organize relevant characteristics of variability. Comprehensibility of these models and the ease of creating models are important for the efficiency of any variability management approach.
Objective
The goal of this paper is to investigate the comprehensibility of two common styles to organize variability into models – hierarchical and constrained – where the dependencies between choices are specified either through the hierarchy of the model or as cross-cutting constraints, respectively.
Method
We conducted a controlled experiment with a sample of 90 participants who were students with prior training in modeling. Each participant was provided with two variability models specified in Common Variability Language (CVL) and was asked to answer questions requiring interpretation of provided models. The models included 9–20 nodes and 8–19 edges and used the main variability elements. After answering the questions, the participants were asked to create a model based on a textual description.
Results
The results indicate that the hierarchical modeling style was easier to comprehend from a subjective point of view, but there was also a significant interaction effect with the degree of dependency in the models, that influenced objective comprehension. With respect to model creation, we found that the use of a constrained modeling style resulted in higher correctness of variability models.
Conclusions
Prior exposure to modeling style and the degree of dependency among elements in the model determine what modeling style a participant chose when creating the model from natural language descriptions. Participants tended to choose a hierarchical style for modeling situations with high dependency and a constrained style for situations with low dependency. Furthermore, the degree of dependency also influences the comprehension of the variability model.},
  doi      = {https://doi.org/10.1016/j.infsof.2017.01.012},
  keywords = {Cognitive aspects,Comprehensibility,Empirical research,Feature modeling,Hierarchical modeling,Product line engineering,Textual constraints,Variability modeling},
  url      = {http://www.sciencedirect.com/science/article/pii/S0950584917300800},
}

@Article{Karhu2009663,
  author   = {Karhu, Katja and Taipale, Ossi and Smolander, Kari},
  title    = {{Investigating the relationship between schedules and knowledge transfer in software testing}},
  journal  = {Information and Software Technology},
  year     = {2009},
  volume   = {51},
  number   = {3},
  pages    = {663--677},
  issn     = {0950-5849},
  abstract = {This empirical study investigates the relationship between schedules and knowledge transfer in software testing. In our exploratory survey, statistical analysis indicated that increased knowledge transfer between testing and earlier phases of software development was associated with testing schedule over-runs. A qualitative case study was conducted to interpret this result. We found that this relationship can be explained with the size and complexity of software, knowledge management issues, and customer involvement. We also found that the primary strategies for avoiding testing schedule over-runs were reducing the scope of testing, leaving out features from the software, and allocating more resources to testing. },
  doi      = {https://doi.org/10.1016/j.infsof.2008.09.001},
  keywords = {Case study,Knowledge transfer,Schedule over-runs,Software testing,Survey},
  url      = {http://www.sciencedirect.com/science/article/pii/S0950584908001249},
}

@Article{Chien2007783,
  author        = {Chien, Shih-Wen and Tsaur, Shu-Ming},
  title         = {{Investigating the success of {\{}ERP{\}} systems: Case studies in three Taiwanese high-tech industries}},
  journal       = {Computers in Industry},
  year          = {2007},
  volume        = {58},
  number        = {8–9},
  pages         = {783--793},
  issn          = {0166-3615},
  abstract      = {The measurement of enterprise resource planning (ERP) systems success or effectiveness is critical to our understanding of the value and efficacy of {\{}ERP{\}} investment and managerial actions. Whether traditional information systems success models can be extended to investigating {\{}ERP{\}} systems success is yet to be investigated. This paper proposes a partial extension and respecification of the DeLone and MacLean model of {\{}IS{\}} success to {\{}ERP{\}} systems. The purpose of the present research is to re-examine the updated DeLone and McLean model [W. DeLone, E. McLean, The DeLone McLean model of information system success: a ten-year update, Journal of Management Information Systems 19 (4) (2003) 3–9] of {\{}ERP{\}} systems success. The updated DeLone and McLean model was applied to collect data from the questionnaires answered by 204 users of {\{}ERP{\}} systems at three high-tech firms in Taiwan. Finally, this study suggests that system quality, service quality, and information quality are most important successful factors.},
  doi           = {https://doi.org/10.1016/j.compind.2007.02.001},
  keywords      = {DeLone and McLean model,High-tech firms,case study,{\{}ERP{\}} success model},
  mendeley-tags = {case study},
  url           = {http://www.sciencedirect.com/science/article/pii/S0166361507000188},
}

@Article{DEMAGALHAES201576,
  author   = {de Magalh{\~{a}}es, Cleyton V C and da Silva, Fabio Q B and Santos, Ronnie E S and Suassuna, Marcos},
  title    = {{Investigations about replication of empirical studies in software engineering: A systematic mapping study}},
  journal  = {Information and Software Technology},
  year     = {2015},
  volume   = {64},
  pages    = {76--101},
  issn     = {0950-5849},
  abstract = {Context
Two recent mapping studies which were intended to verify the current state of replication of empirical studies in Software Engineering (SE) identified two sets of studies: empirical studies actually reporting replications (published between 1994 and 2012) and a second group of studies that are concerned with definitions, classifications, processes, guidelines, and other research topics or themes about replication work in empirical software engineering research (published between 1996 and 2012).
Objective
In this current article, our goal is to analyze and discuss the contents of the second set of studies about replications to increase our understanding of the current state of the work on replication in empirical software engineering research.
Method
We applied the systematic literature review method to build a systematic mapping study, in which the primary studies were collected by two previous mapping studies covering the period 1996–2012 complemented by manual and automatic search procedures that collected articles published in 2013.
Results
We analyzed 37 papers reporting studies about replication published in the last 17years. These papers explore different topics related to concepts and classifications, presented guidelines, and discuss theoretical issues that are relevant for our understanding of replication in our field. We also investigated how these 37 papers have been cited in the 135 replication papers published between 1994 and 2012.
Conclusions
Replication in SE still lacks a set of standardized concepts and terminology, which has a negative impact on the replication work in our field. To improve this situation, it is important that the SE research community engage on an effort to create and evaluate taxonomy, frameworks, guidelines, and methodologies to fully support the development of replications.},
  doi      = {https://doi.org/10.1016/j.infsof.2015.02.001},
  keywords = {Empirical studies,Experiments,Mapping study,Replications,Software engineering,Systematic literature review},
  url      = {http://www.sciencedirect.com/science/article/pii/S0950584915000300},
}

@Article{Thurimella2012933,
  author   = {Thurimella, A K and Bruegge, B},
  title    = {{Issue-based variability management}},
  journal  = {Information and Software Technology},
  year     = {2012},
  volume   = {54},
  number   = {9},
  pages    = {933--950},
  abstract = {Context: Variability management is a key activity in software product line engineering. This paper focuses on managing rationale information during the decision-making activities that arise during variability management. By decision-making we refer to systematic problem solving by considering and evaluating various alternatives. Rationale management is a branch of science that enables decision-making based on the argumentation of stakeholders while capturing the reasons and justifications behind these decisions. Objective: Decision-making should be supported to identify variability in domain engineering and to resolve variation points in application engineering. We capture the rationale behind variability management decisions. The captured rationale information is useful to evaluate future changes of variability models as well as to handle future instantiations of variation points. We claim that maintaining rationale will enhance the longevity of variability models. Furthermore, decisions should be performed using a formal communication between domain engineering and application engineering. Method: We initiate the novel area of issue-based variability management (IVM) by extending variability management with rationale management. The key contributions of this paper are: (i) an issue-based variability management methodology (IVMM), which combines questions, options and criteria (QOC) and a specific variability approach; (ii) a meta-model for IVMM and a process for variability management and (iii) a tool for the methodology, which was developed by extending an open source rationale management tool. Results: Rationale approaches (e.g. questions, options and criteria) guide distributed stakeholders when selecting choices for instantiating variation points. Similarly, rationale approaches also aid the elicitation of variability and the evaluation of changes. The rationale captured within the decision-making process can be reused to perform future decisions on variability. Conclusion: IVMM was evaluated comparatively based on an experimental survey, which provided evidence that IVMM is more effective than a variability modeling approach that does not use issues. {\textcopyright} 2012 Elsevier B.V. All rights reserved.},
  annote   = {cited By 17},
  doi      = {10.1016/j.infsof.2012.02.005},
  keywords = {Application engineering; Decision making process;,Decision making,Production engineering; Requirements engineering;},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861953812{\&}doi=10.1016{\%}2Fj.infsof.2012.02.005{\&}partnerID=40{\&}md5=3f2ec22a5e8fc4126819f77a5b6012e2},
}

@Article{Rabiser2011285,
  author   = {Rabiser, Rick and O'Leary, P{\'{a}}draig and Richardson, Ita},
  title    = {{Key activities for product derivation in software product lines}},
  journal  = {Journal of Systems and Software},
  year     = {2011},
  volume   = {84},
  number   = {2},
  pages    = {285--300},
  issn     = {0164-1212},
  abstract = {More and more organizations adopt software product lines to leverage extensive reuse and deliver a multitude of benefits such as increased quality and productivity and a decrease in cost and time-to-market of their software development. When compared to the vast amount of research on developing product lines, relatively little work has been dedicated to the actual use of product lines to derive individual products, i.e., the process of product derivation. Existing approaches to product derivation have been developed independently for different aims and purposes. While the definition of a general approach applicable to every domain may not be possible, it would be interesting for researchers and practitioners to know which activities are common in existing approaches, i.e., what are the key activities in product derivation. In this paper we report on how we compared two product derivation approaches developed by the authors in two different, independent research projects. Both approaches independently sought to identify product derivation activities, one through a process reference model and the other through a tool-supported derivation approach. Both approaches have been developed and validated in research industry collaborations with different companies. Through the comparison of the approaches we identify key product derivation activities. We illustrate the activities' importance with examples from industry collaborations. To further validate the activities, we analyze three existing product derivation approaches for their support for these activities. The validation provides evidence that the identified activities are relevant to product derivation and we thus conclude that they should be considered (e.g., as a checklist) when developing or evaluating a product derivation approach. },
  doi      = {https://doi.org/10.1016/j.jss.2010.09.042},
  keywords = {Process,Product derivation,Software product lines},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121210002700},
}

@Article{OVASKA2010577,
  author   = {Ovaska, Eila and Evesti, Antti and Henttonen, Katja and Palviainen, Marko and Aho, Pekka},
  title    = {{Knowledge based quality-driven architecture design and evaluation}},
  journal  = {Information and Software Technology},
  year     = {2010},
  volume   = {52},
  number   = {6},
  pages    = {577--601},
  issn     = {0950-5849},
  abstract = {Modelling and evaluating quality properties of software is of high importance, especially when our every day life depends on the quality of services produced by systems and devices embedded into our surroundings. This paper contributes to the body of research in quality and model driven software engineering. It does so by introducing; (1) a quality aware software architecting approach and (2) a supporting tool chain. The novel approach with supporting tools enables the systematic development of high quality software by merging benefits of knowledge modelling and management, and model driven architecture design enhanced with domain-specific quality attributes. The whole design flow of software engineering is semi-automatic; specifying quality requirements, transforming quality requirements to architecture design, representing quality properties in architectural models, predicting quality fulfilment from architectural models, and finally, measuring quality aspects from implemented source code. The semi-automatic design flow is exemplified by the ongoing development of a secure middleware for peer-to-peer embedded systems.},
  doi      = {https://doi.org/10.1016/j.infsof.2009.11.008},
  keywords = {Evaluation,Model-driven development,Ontology,Quality attribute,Software architecture,Tool},
  url      = {http://www.sciencedirect.com/science/article/pii/S0950584909002080},
}

@Article{deSouza2015378,
  author   = {de Souza, {\'{E}}rica Ferreira and {de Almeida Falbo}, Ricardo and Vijaykumar, Nandamudi L},
  title    = {{Knowledge management initiatives in software testing: A mapping study}},
  journal  = {Information and Software Technology},
  year     = {2015},
  volume   = {57},
  pages    = {378--391},
  issn     = {0950-5849},
  abstract = {AbstractContext Software testing is a knowledge intensive process, and, thus, Knowledge Management (KM) principles and techniques should be applied to manage software testing knowledge. Objective This study conducts a survey on existing research on {\{}KM{\}} initiatives in software testing, in order to identify the state of the art in the area as well as the future research. Aspects such as purposes, types of knowledge, technologies and research type are investigated. Method The mapping study was performed by searching seven electronic databases. We considered studies published until December 2013. The initial resulting set was comprised of 562 studies. From this set, a total of 13 studies were selected. For these 13, we performed snowballing and direct search to publications of researchers and research groups that accomplished these studies. Results From the mapping study, we identified 15 studies addressing {\{}KM{\}} initiatives in software testing that have been reviewed in order to extract relevant information on a set of research questions. Conclusions Although only a few studies were found that addressed {\{}KM{\}} initiatives in software testing, the mapping shows an increasing interest in the topic in the recent years. Reuse of test cases is the perspective that has received more attention. From the {\{}KM{\}} point of view, most of the studies discuss aspects related to providing automated support for managing testing knowledge by means of a {\{}KM{\}} system. Moreover, as a main conclusion, the results show that {\{}KM{\}} is pointed out as an important strategy for increasing test effectiveness, as well as for improving the selection and application of suited techniques, methods and test cases. On the other hand, inadequacy of existing {\{}KM{\}} systems appears as the most cited problem related to applying {\{}KM{\}} in software testing. },
  doi      = {https://doi.org/10.1016/j.infsof.2014.05.016},
  keywords = {Knowledge management,Mapping study,Software testing},
  url      = {http://www.sciencedirect.com/science/article/pii/S0950584914001335},
}

@Article{Mohan20071255,
  author   = {Mohan, Kannan and Jain, Radhika and Ramesh, Balasubramaniam},
  title    = {{Knowledge networking to support medical new product development}},
  journal  = {Decision Support Systems},
  year     = {2007},
  volume   = {43},
  number   = {4},
  pages    = {1255--1273},
  issn     = {0167-9236},
  abstract = {New product development (NPD) in the pharmaceutical industry is very knowledge intensive. Knowledge generated and used during medical {\{}NPD{\}} processes is fragmented and distributed across various phases and artifacts. Many challenges in medical {\{}NPD{\}} can be addressed by the integration of this fragmented knowledge. We propose the creation and use of knowledge networks to address these challenges. Based on a case study conducted in a leading pharmaceutical company, we have developed a knowledge framework that represents knowledge fragments that need to be integrated to support medical NPD. We have also developed a prototype system that supports knowledge integration using knowledge networks. We illustrate the capabilities of the system through scenarios drawn from the case study. Qualitative validation of our approach is also presented. },
  annote   = {Special Issue Clusters},
  doi      = {https://doi.org/10.1016/j.dss.2006.02.005},
  keywords = {Healthcare,Knowledge integration,Knowledge networks,New product development,Pharmaceutical knowledge management},
  url      = {http://www.sciencedirect.com/science/article/pii/S0167923606000273},
}

@Article{ASIKAINEN200723,
  author   = {Asikainen, Timo and M{\"{a}}nnist{\"{o}}, Tomi and Soininen, Timo},
  title    = {{Kumbang: A domain ontology for modelling variability in software product families}},
  journal  = {Advanced Engineering Informatics},
  year     = {2007},
  volume   = {21},
  number   = {1},
  pages    = {23--40},
  issn     = {1474-0346},
  abstract = {Variability is the ability of a system to be efficiently extended, changed, customised or configured for use in a particular context. There is an ever-growing demand for variability of software. Software product families are an important means for implementing software variability. We present a domain ontology called Kumbang for modelling the variability in software product families. Kumbang synthesises previous approaches to modelling variability in software product families. In addition, it incorporates modelling constructs developed in the product configuration domain for modelling the variability in non-software products. The modelling concepts include components and features with compositional structure and attributes, the interfaces of components and connections between them, and constraints. The semantics of Kumbang is rigorously described using natural language and a UML profile. We provide preliminary proof of concept for Kumbang: the domain ontology has been provided with a formal semantics by implementing a translation into a general-purpose knowledge representation language with formal semantics and inference support. A prototype tool for resolving variability has been implemented.},
  doi      = {https://doi.org/10.1016/j.aei.2006.11.007},
  keywords = {Feature modelling,Kumbang,Modelling,Software architecture,Software product family,Variability},
  url      = {http://www.sciencedirect.com/science/article/pii/S147403460600067X},
}

@Article{Dyer2013148,
  author   = {Dyer, R and Rajan, H and Cai, Y},
  title    = {{Language features for software evolution and aspect-oriented interfaces: An exploratory study}},
  journal  = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year     = {2013},
  volume   = {7800 LNCS},
  pages    = {148--183},
  abstract = {A variety of language features to modularize cross-cutting concerns have recently been discussed, e.g., open modules, annotation-based pointcuts, explicit join points, and quantified-typed events. All of these ideas are essentially a form of aspect-oriented interface between object-oriented and cross-cutting modules, but the representation of this interface differs. Previous works have studied maintenance benefits of AO programs compared to OO programs, by usually looking at a single AO interface. Other works have looked at several AO interfaces, but only on relatively small systems or systems with only one type of aspectual behavior. Thus, there is a need for a study that examines large, realistic systems for several AO interfaces to determine what problems arise and in which interface(s). The main contribution of this work is a rigorous empirical study that evaluates the effectiveness of these proposals for 4 different AO interfaces by applying them to 35 different releases of a software product line called MobileMedia and 50 different releases of a Web application called Health Watcher. In total, over 400k lines of code were studied across all releases. Our comparative analysis using quantitative metrics proposed by Chidamber and Kemerer shows the strengths and weaknesses of these AO interface proposals. Our change impact analysis shows the design stability provided by each of these recent proposals for AO interfaces. {\textcopyright} 2013 Springer-Verlag.},
  annote   = {cited By 1},
  doi      = {10.1007/978-3-642-36964-3_5},
  keywords = {Artificial intelligence,Change impact analysis; Comparative analysis; Cros,Computer systems programming},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875108456{\&}doi=10.1007{\%}2F978-3-642-36964-3{\_}5{\&}partnerID=40{\&}md5=ab6d868048fbc5f3580720c4f6f5d809},
}

@Article{Lohmann2006227,
  author   = {Lohmann, D and Spinczyk, O and Schr{\"{o}}der-Preikschat, W},
  title    = {{Lean and efficient system software product lines: Where aspects beat objects}},
  journal  = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year     = {2006},
  volume   = {4242 LNCS},
  pages    = {227--255},
  abstract = {Software development in the domain of embedded and deeply embedded systems is dominated by cost pressure and extremely limited hardware resources. As a result, modern concepts for separation of concerns and software reuse are widely ignored, as developers worry about the thereby induced memory and performance overhead. Especially object-oriented programming (OOP) is still little in demand. For the development of highly configurable fine-grained system software product lines, however, separation of concerns (SoC) is a crucial property. As the overhead of object-orientation is not acceptable in this domain, we propose aspect-oriented programming (AOP) as an alternative. Compared to OOP, AOP makes it possible to reach similar or even better separation of concerns with significantly smaller memory footprints. In a case study for an embedded system product line the memory costs for SoC could be reduced from 148-236{\%} to 2-10{\%} by using AOP instead of OOP. {\textcopyright} Springer-Verlag Berlin Heidelberg 2006.},
  annote   = {cited By 8},
  keywords = {Computer software,Computer software reusability; Embedded systems; O,Fine-grained system; Software product lines},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-38549155482{\&}partnerID=40{\&}md5=f4cc2c748e553e56181f5cfcf4e89349},
}

@Article{MéndezAcuña2016206,
  author   = {M{\'{e}}ndez-Acu{\~{n}}a, David and Galindo, Jos{\'{e}} A and Degueule, Thomas and Combemale, Beno{\^{i}}t and Baudry, Beno{\^{i}}t},
  title    = {{Leveraging Software Product Lines Engineering in the development of external DSLs: A systematic literature review}},
  journal  = {Computer Languages, Systems {\&} Structures},
  year     = {2016},
  volume   = {46},
  pages    = {206--235},
  issn     = {1477-8424},
  abstract = {Abstract The use of domain-specific languages (DSLs) has become a successful technique in the development of complex systems. Consequently, nowadays we can find a large variety of {\{}DSLs{\}} for diverse purposes. However, not all these {\{}DSLs{\}} are completely different; many of them share certain commonalities coming from similar modeling patterns – such as state machines or petri nets – used for several purposes. In this scenario, the challenge for language designers is to take advantage of the commonalities existing among similar {\{}DSLs{\}} by reusing, as much as possible, formerly defined language constructs. The objective is to leverage previous engineering efforts to minimize implementation from scratch. To this end, recent research in software language engineering proposes the use of product line engineering, thus introducing the notion of language product lines. Nowadays, there are several approaches that result useful in the construction of language product lines. In this article, we report on an effort for organizing the literature on language product line engineering. More precisely, we propose a definition for the life-cycle of language product lines, and we use it to analyze the capabilities of current approaches. In addition, we provide a mapping between each approach and the technological space it supports. },
  doi      = {https://doi.org/10.1016/j.cl.2016.09.004},
  keywords = {Domain-specific languages,Software Product Lines Engineering,Software language engineering,Variability management},
  url      = {http://www.sciencedirect.com/science/article/pii/S1477842416300768},
}

@Article{Font201720,
  author    = {Font, Jaime and Arcega, Lorena and Haugen, ??ystein and Cetina, Carlos and Haugen, {\O} and Cetina, Carlos},
  title     = {{Leveraging variability modeling to address metamodel revisions in Model-based Software Product Lines}},
  journal   = {Computer Languages, Systems and Structures},
  year      = {2017},
  volume    = {48},
  pages     = {20--38},
  issn      = {14778424},
  abstract  = {Metamodels evolve over time, which can break the conformance between the models and the metamodel. Model migration strategies aim to co-evolve models and metamodels together, but their application is currently not fully automatizable and is thus cumbersome and error prone. We introduce the Variable MetaModel (VMM) strategy to address the evolution of the reusable model assets of a model-based Software Product Line. The VMM strategy applies variability modeling ideas to express the evolution of the metamodel in terms of commonalities and variabilities. When the metamodel evolves, changes are automatically formalized into the VMM and models that conform to previous versions of the metamodel continue to conform to the VMM, thus eliminating the need for migration. We have applied both the traditional migration strategy and the VMM strategy to a retrospective case study that includes 13 years of evolution of our industrial partner, an induction hobs manufacturer. The comparison between the two strategies shows better results for the VMM strategy in terms of model indirection, automation, and trust leak. {\textcopyright} 2016 Elsevier Ltd},
  annote    = {From Duplicate 1 (Leveraging variability modeling to address metamodel revisions in Model-based Software Product Lines - Font, J; Arcega, L; Haugen, {\O}; Cetina, C) cited By 0},
  doi       = {10.1016/j.cl.2016.08.003},
  file      = {:Users/mac/Library/Application Support/Mendeley Desktop/Downloaded/Font et al. - 2017 - Leveraging variability modeling to address metamodel revisions in Model-based Software Product Lines.pdf:pdf},
  keywords  = {Computer software,Computer software reusability,Industrial partners,Meta model,Migration strate,Model and metamodel co-evolution,Model-based Software Product Lines,Software design,Variability Modeling},
  publisher = {Elsevier},
  url       = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994193695{\&}doi=10.1016{\%}2Fj.cl.2016.08.003{\&}partnerID=40{\&}md5=86f0c11fe9f791dbe01ed189f402cde1 http://dx.doi.org/10.1016/j.cl.2016.08.003},
}

@Article{Freeman200816,
  author        = {Freeman, G and Batory, D and Lavender, G},
  title         = {{Lifting transformational models of product lines: A case study}},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year          = {2008},
  volume        = {5063 LNCS},
  pages         = {16--30},
  abstract      = {Model driven development (MDD) of software product lines (SPLs) merges two increasing important paradigms that synthesize programs by transformation. MDD creates programs by transforming models, and SPLs elaborate programs by applying transformations called features. In this paper, we present the design and implementation of a transformational model of a product line of scalar vector graphics and JavaScript applications. We explain how we simplified our implementation by lifting selected features and their compositions from our original product line (whose implementations were complex) to features and their compositions of another product line (whose specifications were simple). We used operators to map higher-level features and their compositions to their lower-level counterparts. Doing so exposed commuting relationships among feature compositions in both product lines that helped validate our model and implementation. {\textcopyright} Springer-Verlag Berlin Heidelberg 2008.},
  annote        = {cited By 3},
  doi           = {10.1007/978-3-540-69927-9_2},
  keywords      = {Chemical analysis,Code generation,Features,Highlevel transformati,Software design,case study},
  mendeley-tags = {case study},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-54249099993{\&}doi=10.1007{\%}2F978-3-540-69927-9{\_}2{\&}partnerID=40{\&}md5=7fbb966fabdf9448fa7bca4939a832d9},
}

@Article{Breß201460,
  author   = {Bre{\ss}, Sebastian and Siegmund, Norbert and Heimel, Max and Saecker, Michael and Lauer, Tobias and Bellatreche, Ladjel and Saake, Gunter},
  title    = {{Load-aware inter-co-processor parallelism in database query processing}},
  journal  = {Data {\&} Knowledge Engineering},
  year     = {2014},
  volume   = {93},
  pages    = {60--79},
  issn     = {0169-023X},
  abstract = {Abstract For a decade, the database community has been exploring graphics processing units and other co-processors to accelerate query processing. While the developed algorithms often outperform their {\{}CPU{\}} counterparts, it is not beneficial to keep processing devices idle while overutilizing others. Therefore, an approach is needed that efficiently distributes a workload on available (co-)processors while providing accurate performance estimates for the query optimizer. In this paper, we contribute heuristics that optimize query processing for response time and throughput simultaneously via inter-device parallelism. Our empirical evaluation reveals that the new approach achieves speedups up to 1.85 compared to state-of-the-art approaches while preserving accurate performance estimations. In a further series of experiments, we evaluate our approach on two new use cases: joining and sorting. Furthermore, we use a simulation to assess the performance of our approach for systems with multiple co-processors and derive some general rules that impact performance in those systems. },
  annote   = {Selected Papers from the 17th East-¬-European Conference on Advances in Databases and Information Systems},
  doi      = {https://doi.org/10.1016/j.datak.2014.07.003},
  keywords = {Co-processing,Query optimization,Query processing},
  url      = {http://www.sciencedirect.com/science/article/pii/S0169023X14000627},
}

@Article{Kumar2008254,
  author   = {Kumar, Nanda and Mohan, Kannan and Holowczak, Richard},
  title    = {{Locking the door but leaving the computer vulnerable: Factors inhibiting home users' adoption of software firewalls}},
  journal  = {Decision Support Systems},
  year     = {2008},
  volume   = {46},
  number   = {1},
  pages    = {254--264},
  issn     = {0167-9236},
  abstract = {In the new era of a ubiquitously networked world, security measures are only as good as their weakest link. Home computers with access to the Internet are one of the weaker links as they are typically not as well protected as computers in the corporate world. Malicious actors can not only target such computers but also use them to launch attacks against other systems connected to the Internet, thus posing severe threats to data and infrastructure as well as disrupting electronic commerce. This paper investigates the factors that affect the use of security protection strategies by home computer users in relation to a specific, but crucial security technology for home – a software firewall. This paper proposes individuals' concern for privacy, awareness of common security measures, attitude towards security and privacy protection technologies, and computer anxiety as important antecedents that have an impact on the users' decision to adopt a software firewall. The results of our study suggest that attitude plays a more important role than perceived usefulness in shaping users' intention to use firewalls. We attribute this interesting finding to the non-functional nature of firewall systems that work best in the background with a complex relationship to users' productivity. Hence, the results add to our current understanding of Technology Acceptance Model vis-{\`{a}}-vis technologies that serve non-functional needs such as security. We then present a set of guidelines to home computer users, Internet Service Providers, e-commerce companies, and the government to increase home users' adoption rate of privacy and security protection technologies. },
  doi      = {https://doi.org/10.1016/j.dss.2008.06.010},
  keywords = {Firewalls,Privacy,Security protection technologies,e-Commerce,{\{}IS{\}} security},
  url      = {http://www.sciencedirect.com/science/article/pii/S0167923608001243},
}

@Article{Zheng:2018:MAC:3234930.3229048,
  author    = {Zheng, Yongjie and Cu, Cuong and Taylor, Richard N},
  title     = {{Maintaining Architecture-Implementation Conformance to Support Architecture Centrality: From Single System to Product Line Development}},
  journal   = {ACM Trans. Softw. Eng. Methodol.},
  year      = {2018},
  volume    = {27},
  number    = {2},
  pages     = {8:1----8:52},
  issn      = {1049-331X},
  address   = {New York, NY, USA},
  doi       = {10.1145/3229048},
  keywords  = {Architecture-implementation mapping,architectural evolution,architecture-centric development,architecture-centric feature traceability,variability conformance},
  publisher = {ACM},
  url       = {http://doi.acm.org/10.1145/3229048},
}

@Article{Ghanam2012968,
  author   = {Ghanam, Yaser and Maurer, Frank and Abrahamsson, Pekka},
  title    = {{Making the leap to a software platform strategy: Issues and challenges}},
  journal  = {Information and Software Technology},
  year     = {2012},
  volume   = {54},
  number   = {9},
  pages    = {968--984},
  issn     = {0950-5849},
  abstract = {Context While there are many success stories of achieving high reuse and improved quality using software platforms, there is a need to investigate the issues and challenges organizations face when transitioning to a software platform strategy. Objective This case study provides a comprehensive taxonomy of the challenges faced when a medium-scale organization decided to adopt software platforms. The study also reveals how new trends in software engineering (i.e. agile methods, distributed development, and flat management structures) interplayed with the chosen platform strategy. Method We used an ethnographic approach to collect data by spending time at a medium-scale company in Scandinavia. We conducted 16 in-depth interviews with representatives of eight different teams, three of which were working on three separate platforms. The collected data was analyzed using Grounded Theory. Results The findings identify four classes of challenges, namely: business challenges, organizational challenges, technical challenges, and people challenges. The article explains how these findings can be used to help researchers and practitioners identify practical solutions and required tool support. Conclusion The organization's decision to adopt a software platform strategy introduced a number of challenges. These challenges need to be understood and addressed in order to reap the benefits of reuse. Researchers need to further investigate issues such as supportive organizational structures for platform development, the role of agile methods in software platforms, tool support for testing and continuous integration in the platform context, and reuse recommendation systems. },
  doi      = {https://doi.org/10.1016/j.infsof.2012.03.005},
  keywords = {Ethnographic study,Grounded Theory,Platform challenges,Software platform,Software reuse},
  url      = {http://www.sciencedirect.com/science/article/pii/S0950584912000547},
}

@Article{Clemente20111032,
  author   = {Clemente, Pedro J and Hern{\'{a}}ndez, Juan and Conejero, Jos{\'{e}} M and Ortiz, Guadalupe},
  title    = {{Managing crosscutting concerns in component based systems using a model driven development approach}},
  journal  = {Journal of Systems and Software},
  year     = {2011},
  volume   = {84},
  number   = {6},
  pages    = {1032--1053},
  issn     = {0164-1212},
  abstract = {In the last few years, Model-Driven Development (MDD), Aspect-Oriented Software Development (AOSD), and Component-Based Software Development (CBSD) have become interesting alternatives for the design and construction of complex distributed applications. Although these methodological approaches share the principle of separation of concerns and their further integration as key factors to obtaining high-quality and evolvable large software systems, they usually each address this principle from their own particular perspective. In the present work, we combine Component-Based and Aspect-Oriented Software Developments in a Model Driven software process targeted at the development of complex systems. This process constitutes an enhancement of the separation of concerns by allowing the isolation of crosscutting concerns in both Platform Independent and Platform Specific models. Following a pure {\{}MDD{\}} philosophy, a set of model transformations are used to generate the system, from preliminary models to the final source code for the Corba Component Model platform. A twofold empirical analysis was used to evaluate the approach's benefits in terms of two internal quality attributes: modularity and complexity. Conclusions were drawn from this evaluation regarding other quality attributes correlated with these two – stability, changeability, error-proneness, and reusability. An Eclipse plug-in was developed to drive the development of the entire system from early modeling to late deployment stages. },
  doi      = {https://doi.org/10.1016/j.jss.2011.01.053},
  keywords = {Aspect Oriented Software Development (AOSD),CORBA Component Model (CCM),Component Based Software Development (CBSD),Crosscutting concerns,Model Driven Development (MDD),Transformation models},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121211000434},
}

@Article{Kim2007417,
  author   = {Kim, Minseong and Park, Sooyong and Sugumaran, Vijayan and Yang, Hwasil},
  title    = {{Managing requirements conflicts in software product lines: A goal and scenario based approach}},
  journal  = {Data {\&} Knowledge Engineering},
  year     = {2007},
  volume   = {61},
  number   = {3},
  pages    = {417--432},
  issn     = {0169-023X},
  abstract = {The product line approach is recognized as a successful approach to reuse in software development. However, in many cases, it has resulted in interactions between requirements and/or features. Interaction detection, especially conflict detection between requirements has become more challenging. Thus, detecting conflicts between requirements is essential for successful product line development. Formal methods have been proposed to address this problem, however, they are hard to understand by non-experts and are limited to restricted domains. In addition, there is no overall process that covers all the steps for managing conflicts. We propose an approach for systematically identifying and managing requirements conflicts, which is based on requirements partition in natural language and supported by a tool. To demonstrate its feasibility, the proposed approach has been applied to the home integration system (HIS) domain and the results are discussed. },
  annote   = {Advances on Natural Language ProcessingNLDB 05},
  doi      = {https://doi.org/10.1016/j.datak.2006.06.009},
  keywords = {Goal and scenario authoring,Requirements conflicts,Requirements partitioning,Software product line,Syntactic and semantic requirements conflict detec},
  url      = {http://www.sciencedirect.com/science/article/pii/S0169023X06001121},
}

@Article{Sellier2008299,
  author   = {Sellier, D and Mannion, M and Mansell, J X},
  title    = {{Managing requirements inter-dependency for software product line derivation}},
  journal  = {Requirements Engineering},
  year     = {2008},
  volume   = {13},
  number   = {4},
  pages    = {299--313},
  abstract = {Software Product Line Engineering (SPLE) can reduce software development costs, reduce time to market and improve product quality. A software product line is a set of software products sharing a set of common features but containing variation points. Successful SPLE requires making selection decisions at variation points effectively and efficiently. A significant challenge is how to identify, represent and manage the inter-dependency of selection decisions for requirements. We developed the concept of a meta-model for requirement decision models to bring formalism and consistency to the structure and to model inter-dependencies between requirement selection decisions. Here we present a meta-model for requirement selection decisions that includes inter-dependencies and we use a mobile phone worked example to illustrate our approach. To support our method, we developed two separate tools, V-Define (for domain decision model construction) and V-Resolve (for new product derivation). Finally the results of a metal processing product line case study using the tools are described. {\textcopyright} Springer-Verlag London Limited 2008.},
  annote   = {cited By 5},
  doi      = {10.1007/s00766-008-0066-4},
  keywords = {Case studies; Common features; Decision models; D,Computer software selection and evaluation,Concurrent engineering; Decision making; Productio},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-54249094126{\&}doi=10.1007{\%}2Fs00766-008-0066-4{\&}partnerID=40{\&}md5=32aa8c667a85021263ab3ed83b09389b},
}

@Article{ERIKSSON2009435,
  author   = {Eriksson, Magnus and B{\"{o}}rstler, J{\"{u}}rgen and Borg, Kjell},
  title    = {{Managing requirements specifications for product lines – An approach and industry case study}},
  journal  = {Journal of Systems and Software},
  year     = {2009},
  volume   = {82},
  number   = {3},
  pages    = {435--447},
  issn     = {0164-1212},
  abstract = {Software product line development has emerged as a leading approach for software reuse. This paper describes an approach to manage natural-language requirements specifications in a software product line context. Variability in such product line specifications is modeled and managed using a feature model. The proposed approach has been introduced in the Swedish defense industry. We present a multiple-case study covering two different product lines with in total eight product instances. These were compared to experiences from previous projects in the organization employing clone-and-own reuse. We conclude that the proposed product line approach performs better than clone-and-own reuse of requirements specifications in this particular industrial context.},
  doi      = {https://doi.org/10.1016/j.jss.2008.07.046},
  keywords = {Feature model,Natural-language requirements specification,Software product line,Variability management},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121208001830},
}

@Article{BAUMANN201520,
  author   = {Baumann, Thomas and Harfst, Sarah and Swanger, Alice and Bayer, Deborah and Cell, Amy and Boswell, William},
  title    = {{Managing Successful Project Teams in a Diverse Stakeholder Environment: Merging Industry Best Practices with an Education System to Address Critical Human Factors}},
  journal  = {Procedia - Social and Behavioral Sciences},
  year     = {2015},
  volume   = {194},
  pages    = {20--32},
  issn     = {1877-0428},
  abstract = {Across the United States, industry, education and government stakeholders are redefining their partnerships in order to address and reduce critical skill gaps in the current workforce. This represents a paradigm shift in technical and professional education, as well as the collaborative processes utilized in creating and maintaining complex multi-stakeholder talent development systems. In doing so, however, this paradigm shift also presents a huge challenge: education and public sectors are typically not familiar with matured product development and (project) management principles and often do not apply proven industry practices to the definition, design, delivery and improvement of their educational products. The evaluation, acceptance and ultimate implementation of those principles departs from the traditional (United States) culture of education, as it applies new instructional design processes, change implementation processes, and feedback/assessment models. Therefore, critical review of such methods, innovative development of transfer options and human factors management in successfully achieving the required systems change were identified, and include explanation, understanding, acceptance, personal development, and trust. Using a case study approach, this paper will analyze several highly visible and innovative adaptations of industry and educational standards which were accepted and released by all relevant stakeholders. By leveraging subject matter experts from both the industry and academic settings, Michigan Advanced Technician Training (MAT2) formed organizational and working teams comprised of the primary government, industry, and academic stakeholder groups and established a workable context for a knowledge transfer of best practice industry standards. Said standards were applied bi-directionally by academic providers and partnering manufacturing enterprises, and now serve as best practice examples for post-secondary systems of apprenticeship education.},
  annote   = {Proceedings of the 2014 IPMA World Congress (Sept 29-Oct 1 – Rotterdam, Netherlands)},
  doi      = {https://doi.org/10.1016/j.sbspro.2015.06.140},
  keywords = {Project management,educational projects,partnership management,public projects,quality and education,stakeholder management},
  url      = {http://www.sciencedirect.com/science/article/pii/S1877042815036198},
}

@Article{Ahmed2007194,
  author   = {Ahmed, F and Capretz, L F},
  title    = {{Managing the business of software product line: An empirical investigation of key business factors}},
  journal  = {Information and Software Technology},
  year     = {2007},
  volume   = {49},
  number   = {2},
  pages    = {194--208},
  abstract = {Business has been highlighted as a one of the critical dimensions of software product line engineering. This paper's main contribution is to increase the understanding of the influence of key business factors by showing empirically that they play an imperative role in managing a successful software product line. A quantitative survey of software organizations currently involved in the business of developing software product lines over a wide range of operations, including consumer electronics, telecommunications, avionics, and information technology, was designed to test the conceptual model and hypotheses of the study. This is the first study to demonstrate the relationships between the key business factors and software product lines. The results provide evidence that organizations in the business of software product line development have to cope with multiple key business factors to improve the overall performance of the business, in addition to their efforts in software development. The conclusions of this investigation reinforce current perceptions of the significance of key business factors in successful software product line business. {\textcopyright} 2006 Elsevier B.V. All rights reserved.},
  annote   = {cited By 15},
  doi      = {10.1016/j.infsof.2006.05.004},
  file     = {:Users/mac/Library/Application Support/Mendeley Desktop/Downloaded/Ahmed, Capretz - 2007 - Managing the business of software product line An empirical investigation of key business factors.pdf:pdf},
  keywords = {Avionics; Consumer electronics; Information techno,Key business factor; Marketing strategy; Software,Software engineering},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-33751118800{\&}doi=10.1016{\%}2Fj.infsof.2006.05.004{\&}partnerID=40{\&}md5=84a05c338e8155168576c110f8c208f8},
}

@Article{Dao2010377,
  author   = {Dao, T M and Kang, K C},
  title    = {{Mapping features to reusable components: A problem frames-based approach}},
  journal  = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year     = {2010},
  volume   = {6287 LNCS},
  pages    = {377--392},
  abstract = {In software product line engineering (SPLE), feature modeling has been extensively used to represent commonality and variability between the products of a domain in terms of features, based on which reusable components are developed. However, the link between a feature model and product requirements, that fundamentally decide how the features are developed into reusable components, has not been adequately addressed in SPLE methods. This paper introduces an approach to combining feature modeling and problem frames in an attempt to address this problem. First, features are mapped to problem frames using heuristics derived from feature modeling and feature mapping units. Requirements are then identified and analyzed to ensure that they are fully satisfied. Finally, a solution modeling method maps the problem frames to architectural components. A Home Integration System (HIS) case study is used to demonstrate the feasibility of the approach. {\textcopyright} 2010 Springer-Verlag Berlin Heidelberg.},
  annote   = {cited By 5},
  doi      = {10.1007/978-3-642-15579-6_26},
  keywords = {Architectural components; Commonality and variabil,Computer software reusability; Network architectu,Software design},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-78049354800{\&}doi=10.1007{\%}2F978-3-642-15579-6{\_}26{\&}partnerID=40{\&}md5=bc7e661954a4edda413019f40273c6bc},
}

@Article{Sanen2010167,
  author    = {Sanen, Frans and Truyen, Eddy and Joosen, Wouter},
  title     = {{Mapping problem-space to solution-space features: A feature interaction approach}},
  journal   = {ACM SIGPLAN Notices},
  year      = {2010},
  volume    = {45},
  number    = {2},
  pages     = {167--176},
  issn      = {0362-1340},
  abstract  = {Mapping problem-space features into solution-space features is a fundamental configuration problem in software product line engineering. A configuration problem is defined as generating the most optimal combination of software features given a requirements specification and given a set of configuration rules. Current approaches however provide little support for expressing complex configuration rules between problem and solution space that support incomplete requirements specifications. In this paper, we propose an approach to model complex configuration rules based on a generalization of the concept of problem-solution feature interactions. These are interactions between solution-space features that only arise in specific problem contexts. The use of an existing tool to support our approach is also discussed: we use the DLV answer set solver to express a particular configuration problem as a logic program whose answer set corresponds to the optimal combinations of solution-space features. We motivate and illustrate our approach with a case study in the field of managing dynamic adaptations in distributed software, where the goal is to generate an optimal protocol for accommodating a given adaptation. Copyright {\textcopyright} 2009 ACM.},
  address   = {New York, NY, USA},
  annote    = {From Duplicate 1 (Mapping Problem-space to Solution-space Features: A Feature Interaction Approach - Sanen, Frans; Truyen, Eddy; Joosen, Wouter) From Duplicate 2 (Mapping problem-space to solution-space features: A feature interaction approach - Sanen, Frans; Truyen, Eddy; Joosen, Wouter) From Duplicate 1 (Mapping problem-space to solution-space features: A feature interaction approach - Sanen, F; Truyen, E; Joosen, W) cited By 0 From Duplicate 2 (Mapping problem-space to solution-space features: A feature interaction approach - Sanen, F; Truyen, E; Joosen, W) cited By 0},
  doi       = {10.1145/1837852.1621633},
  keywords  = {Configuration knowledge,DLV,Default logic,Distribute,Logic programming,Optimization,Production engin,Software design,configuration knowledge,default logic,distributed runtime adaptation,problem-solution feature interactions,software product line engineering},
  publisher = {ACM},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/1837852.1621633 https://www.scopus.com/inward/record.uri?eid=2-s2.0-77957578691{\&}doi=10.1145{\%}2F1837852.1621633{\&}partnerID=40{\&}md5=cb98c2a724d5c725309d676a4c180ff3},
}

@Article{BUCCELLA2014108,
  author   = {Buccella, Agustina and Cechich, Alejandra and Pol׳la, Matias and Arias, Maximiliano and {del Socorro Doldan}, Maria and Morsan, Enrique},
  title    = {{Marine ecology service reuse through taxonomy-oriented SPL development}},
  journal  = {Computers {\&} Geosciences},
  year     = {2014},
  volume   = {73},
  pages    = {108--121},
  issn     = {0098-3004},
  abstract = {Nowadays, reusing software applications encourages researchers and industrials to collaborate in order to increase software quality and to reduce software development costs. However, effective reuse is not easy and only a limited portion of reusable models actually offers effective evidence regarding their appropriateness, usability and/or effectiveness. Focusing reuse on a particular domain, such as marine ecology, allows us to narrow the scope; and along with a systematic approach such as software product line development, helps us to potentially improving reuse. From our experiences developing a subdomain-oriented software product line (SPL for the marine ecology subdomain), in this paper we describe semantic resources created for assisting this development and thus promoting systematic software reuse. The main contributions of our work are focused on the definition of a standard conceptual model for marine ecology applications together with a set of services and guides which assist the process of product derivation. The services are structured in a service taxonomy (as a specialization of the ISO 19119 std) in which we create a new set of categories and services built over a conceptual model for marine ecology applications. We also define and exemplify a set of guides for composing the services of the taxonomy in order to fulfill different functionalities of particular systems in the subdomain.},
  doi      = {https://doi.org/10.1016/j.cageo.2014.09.004},
  keywords = {Domain engineering,Domain-specific taxonomies,Geographic information systems,ISO 19100 standards,Software reuse},
  url      = {http://www.sciencedirect.com/science/article/pii/S0098300414002155},
}

@Article{Hurtado20131153,
  author   = {Hurtado, J A and Bastarrica, M C and Ochoa, S F and Simmonds, J},
  title    = {{MDE software process lines in small companies}},
  journal  = {Journal of Systems and Software},
  year     = {2013},
  volume   = {86},
  number   = {5},
  pages    = {1153--1171},
  abstract = {Software organizations specify their software processes so that process knowledge can be systematically reused across projects. However, different projects may require different processes. Defining a separate process for each potential project context is expensive and error-prone, since these processes must simultaneously evolve in a consistent manner. Moreover, an organization cannot envision all possible project contexts in advance because several variables may be involved, and these may also be combined in different ways. This problem is even worse in small companies since they usually cannot afford to define more than one process. Software process lines are a specific type of software product lines, in the software process domain. A benefit of software process lines is that they allow software process customization with respect to a context. In this article we propose a model-driven approach for software process lines specification and configuration. The article also presents two industrial case studies carried out at two small Chilean software development companies. Both companies have benefited from applying our approach to their processes: new projects are now developed using custom processes, process knowledge is systematically reused, and the total time required to customize a process is much shorter than before.{\textcopyright} 2012 Elsevier Inc. All rights reserved.},
  annote   = {cited By 8},
  doi      = {10.1016/j.jss.2012.09.033},
  keywords = {Industrial case study; Model driven approach; Mode,Industry,Software design},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875270026{\&}doi=10.1016{\%}2Fj.jss.2012.09.033{\&}partnerID=40{\&}md5=b4bd0bb9378eaba09a42ff8306d01824},
}

@Article{Sillitti2004393,
  author   = {Sillitti, Alberto and Janes, Andrea and Succi, Giancarlo and Vernazza, Tullio},
  title    = {{Measures for mobile users: an architecture}},
  journal  = {Journal of Systems Architecture},
  year     = {2004},
  volume   = {50},
  number   = {7},
  pages    = {393--405},
  issn     = {1383-7621},
  abstract = {Software measures are important to evaluate software properties like complexity, reusability, maintainability, effort required, etc. Collecting such data is difficult because of the lack of tools that perform acquisition automatically. It is not possible to implement a manual data collection because it is error prone and very time expensive. Moreover, developers often work in teams and sometimes in different places using laptops. These conditions require tools that collect data automatically, can work offline and merge data from different developers working in the same project. This paper presents {\{}PROM{\}} (PRO Metrics), a distributed Java based tool designed to collect automatically software measures. This tool uses a distributed architecture based on plug-ins, integrated in most popular development tools, and the {\{}SOAP{\}} communication protocol. },
  annote   = {Adaptable System/Software Architectures},
  doi      = {https://doi.org/10.1016/j.sysarc.2003.09.005},
  keywords = {Development monitoring,Java,Process metrics,Product metrics},
  url      = {http://www.sciencedirect.com/science/article/pii/S1383762103001607},
}

@Article{Rincón201561,
  author   = {Rinc{\'{o}}n, L and Giraldo, G and Mazo, R and Salinesi, C and Diaz, D},
  title    = {{Method to Identify Corrections of Defects on Product Line Models}},
  journal  = {Electronic Notes in Theoretical Computer Science},
  year     = {2015},
  volume   = {314},
  pages    = {61--81},
  issn     = {1571-0661},
  abstract = {Abstract Software product line engineering is a promising paradigm for developing software intensive systems. Among their proven benefits are reduced time to market, better asset reuse and improved software quality. To achieve this, the collection of products of the product line are specified by means of product line models. Feature Models (FMs) are a common notation to represent product lines that express the set of feature combinations that software products can have. Experience shows that these models can have defects. Defects in {\{}FMs{\}} be inherited to the products configured from these models. Consequently, defects must be early identified and corrected. Several works reported in scientific literature, deal with identification of defects in FMs. However, only few of these proposals are able to explain how to fix defects, and only some corrections are suggested. This paper proposes a new method to detect all possible corrections from a defective product line model. The originality of the contribution is that corrections can be found when the method systematically eliminates dependencies from the FMs. The proposed method was applied on 78 distinct {\{}FMs{\}} with sizes up to 120 dependencies. Evaluation indicates that the method proposed in this paper scale up, is accurate, and sometimes useful in real scenarios. },
  annote   = {{\{}CLEI{\}} 2014, the {\{}XL{\}} Latin American Conference in Informatic},
  doi      = {https://doi.org/10.1016/j.entcs.2015.05.005},
  keywords = {Corrections,Defects,Features Models,Software Engineering,Software product lines},
  url      = {http://www.sciencedirect.com/science/article/pii/S1571066115000286},
}

@Article{Farahani2014545,
  author   = {Farahani, F F and Ramsin, R},
  title    = {{Methodologies for agile product line engineering: A survey and evaluation}},
  journal  = {Frontiers in Artificial Intelligence and Applications},
  year     = {2014},
  volume   = {265},
  pages    = {545--564},
  abstract = {Agile Product Line Engineering (APLE) is a relatively new approach which has emerged as the result of combining two successful approaches: Software Product Line Engineering and Agile Software Development. The goal of this combined approach is to cover the weaknesses of each of the two approaches while maximizing the advantages of both. Several methodologies exist which provide a practical process for applying APLE. In this paper, a select set of these methodologies are evaluated using a criteria-based approach, the results of which highlight each methodology's strengths and weaknesses. The evaluation framework and the results can be helpful in selecting, comparing, and modifying APLE methodologies; they can also be used for developing bespoke APLE methodologies, tailored to fit the specific needs of organizations and projects. {\textcopyright} 2014 The authors and IOS Press. All rights reserved.},
  annote   = {cited By 0},
  doi      = {10.3233/978-1-61499-434-3-545},
  keywords = {Agile software development; Criteria-based evalua,Software design,Software engineering},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84948778166{\&}doi=10.3233{\%}2F978-1-61499-434-3-545{\&}partnerID=40{\&}md5=92a9ac3a5646d4296c1c8c4d4631fbe1},
}

@Article{Fazal-E-Amin201166,
  author   = {Fazal-E-Amin and Mahmood, A K and Oxley, A},
  title    = {{Metrics based variability assessment of code assets}},
  journal  = {Communications in Computer and Information Science},
  year     = {2011},
  volume   = {181 CCIS},
  number   = {PART 3},
  pages    = {66--75},
  abstract = {The potential benefits of software reuse motivate the use of component based software development and software product lines. In these software development methodologies software assets are being reused. Variability management is a tenet of software reuse. Variability is the capacity of software to satisfy variant requirements. Variability, being the central player in reuse and an important characteristic of reusable components, needs to be measured. In this paper we acknowledge this need and identify measures of variability. Variability implementation mechanisms are analyzed followed by metrics. The metrics are applied on open source component code and the results are validated by an experiment carried out with human subjects. {\textcopyright} 2011 Springer-Verlag.},
  annote   = {cited By 0},
  doi      = {10.1007/978-3-642-22203-0_6},
  keywords = {Component-based software development; Human subjec,Computer software reusability; Software design,Open systems},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960364100{\&}doi=10.1007{\%}2F978-3-642-22203-0{\_}6{\&}partnerID=40{\&}md5=c113445b648d7d8aff676a5bf242dd11},
}

@Article{Famelis201582,
  author        = {Famelis, M and L{\'{u}}cio, L and Selim, G and {Di Sandro}, A and Salay, R and Chechik, M and Cordy, J R and Dingel, J and Vangheluwe, H and Ramesh, S},
  title         = {{Migrating automotive product lines: A case study}},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year          = {2015},
  volume        = {9152},
  pages         = {82--97},
  abstract      = {Software Product Lines (SPL) are widely used to manage variability in the automotive industry. In a rapidly changing industrial environment, model transformations are necessary to aid in automating the evolution of SPLs. However, existing transformation technologies are not well-suited to handling industrial-grade variability in software artifacts. We present a case study where we “lift” a previously developed migration transformation so that it becomes applicable to realistic industrial product lines. Our experience indicates that it is both feasible and scalable to lift transformations for industrial SPLs. {\textcopyright} Springer International Publishing Switzerland 2015.},
  annote        = {cited By 3},
  doi           = {10.1007/978-3-319-21155-8_7},
  keywords      = {Artificial intelligence,Automotive industry,Automotive products,Computers,Ind,Industrial environments,case study},
  mendeley-tags = {case study},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84952671657{\&}doi=10.1007{\%}2F978-3-319-21155-8{\_}7{\&}partnerID=40{\&}md5=b612409a8ad427664909fe1e65b1cd3d},
}

@Article{LIU2017126,
  author   = {Liu, Yuzhou and Liu, Lei and Liu, Huaxiao and Wang, Xiaoyu and Yang, Hongji},
  title    = {{Mining domain knowledge from app descriptions}},
  journal  = {Journal of Systems and Software},
  year     = {2017},
  volume   = {133},
  pages    = {126--144},
  issn     = {0164-1212},
  abstract = {Domain analysis aims at gaining knowledge to a particular domain in the early stage of software development. A key challenge in domain analysis is to extract features automatically from related product artifacts. Compared with other kinds of artifacts, high volume of descriptions can be collected from App marketplaces (such as Google Play and Apple Store) easily when developing a new mobile application (App), so it is essential for the success of domain analysis to gain features and relationships from them using data analysis techniques. In this paper, we propose an approach to mine domain knowledge from App descriptions automatically, where the information of features in a single App description is firstly extracted and formally described by a Concern-based Description Model (CDM), which is based on predefined rules of feature extraction and a modified topic modeling method; then the overall knowledge in the domain is identified by classifying, clustering and merging the knowledge in the set of CDMs and topics, and the results are formalized by a Data-based Raw Domain Model (DRDM). Furthermore, we propose a quantified evaluation method for prioritizing the knowledge in DRDM. The proposed approach is validated by a series of experiments.},
  doi      = {https://doi.org/10.1016/j.jss.2017.08.024},
  file     = {:Users/mac/Downloads/los nuevos/Mining domain knowledge from app descriptions.pdf:pdf},
  keywords = {App descriptions,Data analysis,Domain analysis,Feature extraction},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121217301784},
}

@Article{Marinho2013,
  author    = {Marinho, Fabiana G. and Andrade, Rossana M C and Werner, Cl??udia and Viana, Windson and Maia, Marcio E F and Rocha, Lincoln S. and Teixeira, Eld??nae and Filho, Jo??o B Ferreira and Dantas, Val??ria L L and Lima, Fabr??cio and Aguiar, Saulo},
  title     = {{MobiLine: A Nested Software Product Line for the domain of mobile and context-aware applications}},
  journal   = {Science of Computer Programming},
  year      = {2013},
  volume    = {78},
  number    = {12},
  pages     = {2381--2398},
  issn      = {01676423},
  abstract  = {Mobile devices are multipurpose and multi-sensor equipments supporting applications able to adapt their behavior according to changes in the user's context (device, location, time, etc.). Meanwhile, the development of mobile and context-aware software is not a simple task, mostly due to the peculiar characteristics of these devices. Although several solutions have been proposed to facilitate their development, reuse is not systematically used throughout the software development life-cycle. In this paper, we discuss an approach for the development of mobile and context-aware software using the Software Product Line (SPL) paradigm. Furthermore, a Nested SPL for the domain of mobile and context-aware applications is presented, lessons learned in the SPL development are discussed and a product for a context-aware visit guide is shown. ?? 2012 Elsevier B.V. All rights reserved.},
  doi       = {10.1016/j.scico.2012.04.009},
  file      = {:Users/mac/Library/Application Support/Mendeley Desktop/Downloaded/Marinho et al. - 2013 - MobiLine A Nested Software Product Line for the domain of mobile and context-aware applications.pdf:pdf},
  isbn      = {0167-6423},
  keywords  = {Context-awareness,Mobility,Software product line},
  publisher = {Elsevier B.V.},
  url       = {http://dx.doi.org/10.1016/j.scico.2012.04.009},
}

@Article{Buchmann2013630,
  author   = {Buchmann, Thomas and Dotor, Alexander and Westfechtel, Bernhard},
  title    = {{MOD2-SCM: A model-driven product line for software configuration management systems}},
  journal  = {Information and Software Technology},
  year     = {2013},
  volume   = {55},
  number   = {3},
  pages    = {630--650},
  issn     = {0950-5849},
  abstract = {Context Software Configuration Management (SCM) is the discipline of controlling the evolution of large and complex software systems. Over the years many different {\{}SCM{\}} systems sharing similar concepts have been implemented from scratch. Since these concepts usually are hard-wired into the respective program code, reuse is hardly possible. Objective Our objective is to create a model-driven product line for {\{}SCM{\}} systems. By explicitly describing the different concepts using models, reuse can be performed on the modeling level. Since models are executable, the need for manual programming is eliminated. Furthermore, by providing a library of loosely coupled modules, we intend to support flexible composition of {\{}SCM{\}} systems. Method We developed a method and a tool set for model-driven software product line engineering which we applied to the {\{}SCM{\}} domain. For domain analysis, we applied the {\{}FORM{\}} method, resulting in a layered feature model for {\{}SCM{\}} systems. Furthermore, we developed an executable object-oriented domain model which was annotated with features from the feature model. A specific {\{}SCM{\}} system is configured by selecting features from the feature model and elements of the domain model realizing these features. Results Due to the orthogonality of both feature model and domain model, a very large number of {\{}SCM{\}} systems may be configured. We tested our approach by creating instances of the product line which mimic wide-spread systems such as CVS, GIT, Mercurial, and Subversion. Conclusion The experiences gained from this project demonstrate the feasibility of our approach to model-driven software product line engineering. Furthermore, our work advances the state of the art in the domain of {\{}SCM{\}} systems since it support the modular composition of {\{}SCM{\}} systems at the model rather than the code level. },
  annote   = {Special Issue on Software Reuse and Product LinesSpecial Issue on Software Reuse and Product Lines},
  doi      = {https://doi.org/10.1016/j.infsof.2012.07.010},
  keywords = {Code generation,Executable models,Feature models,Model transformation,Model-driven software engineering,Software configuration management,Software product line engineering},
  url      = {http://www.sciencedirect.com/science/article/pii/S095058491200136X},
}

@Article{Apel20094,
  author   = {Apel, S and Janda, F and Trujillo, S and K{\"{a}}stner, C},
  title    = {{Model superimposition in software product lines}},
  journal  = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year     = {2009},
  volume   = {5563 LNCS},
  pages    = {4--19},
  abstract = {In software product line engineering, feature composition generates software tailored to specific requirements from a common set of artifacts. Superimposition is a technique to merge code pieces belonging to different features. The advent of model-driven development raises the question of how to support the variability of software product lines in modeling techniques. We propose to use superimposition as a model composition technique in order to support variability. We analyze the feasibility of superimposition for model composition, offer corresponding tool support, and discuss our experiences with three case studies (including an industrial case study). {\textcopyright} 2009 Springer Berlin Heidelberg.},
  annote   = {cited By 38},
  doi      = {10.1007/978-3-642-02408-5_2},
  keywords = {Computer software,Industrial case study; Model composition; Model dr,Network architecture; Production engineering; Res},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349538082{\&}doi=10.1007{\%}2F978-3-642-02408-5{\_}2{\&}partnerID=40{\&}md5=62082203ab17f8f5a582a05aaab53238},
}

@Article{Belli201625,
  author        = {Belli, Fevzi and Budnik, Christof J and Hollmann, Axel and Tuglular, Tugkan and Wong, W Eric},
  title         = {{Model-based mutation testing—Approach and case studies}},
  journal       = {Science of Computer Programming},
  year          = {2016},
  volume        = {120},
  pages         = {25--48},
  issn          = {0167-6423},
  abstract      = {Abstract This paper rigorously introduces the concept of model-based mutation testing (MBMT) and positions it in the landscape of mutation testing. Two elementary mutation operators, insertion and omission, are exemplarily applied to a hierarchy of graph-based models of increasing expressive power including directed graphs, event sequence graphs, finite-state machines and statecharts. Test cases generated based on the mutated models (mutants) are used to determine not only whether each mutant can be killed but also whether there are any faults in the corresponding system under consideration (SUC) developed based on the original model. Novelties of our approach are: (1) evaluation of the fault detection capability (in terms of revealing faults in the SUC) of test sets generated based on the mutated models, and (2) superseding of the great variety of existing mutation operators by iterations and combinations of the two proposed elementary operators. Three case studies were conducted on industrial and commercial real-life systems to demonstrate the feasibility of using the proposed {\{}MBMT{\}} approach in detecting faults in SUC, and to analyze its characteristic features. Our experimental data suggest that test sets generated based on the mutated models created by insertion operators are more effective in revealing faults in {\{}SUC{\}} than those generated by omission operators. Worth noting is that test sets following the {\{}MBMT{\}} approach were able to detect faults in the systems that were tested by manufacturers and independent testing organizations before they were released.},
  doi           = {https://doi.org/10.1016/j.scico.2016.01.003},
  keywords      = {Fault detection capability,Model-based mutation testing,Model-based testing,Mutation operator,Mutation testing,case study},
  mendeley-tags = {case study},
  url           = {http://www.sciencedirect.com/science/article/pii/S0167642316000137},
}

@Article{Lochau2012567,
  author   = {Lochau, M and Oster, S and Goltz, U and Sch{\"{u}}rr, A},
  title    = {{Model-based pairwise testing for feature interaction coverage in software product line engineering}},
  journal  = {Software Quality Journal},
  year     = {2012},
  volume   = {20},
  number   = {3-4},
  pages    = {567--604},
  abstract = {Testing software product lines (SPLs) is very challenging due to a high degree of variability leading to an enormous number of possible products. The vast majority of today's testing approaches for SPLs validate products individually using different kinds of reuse techniques for testing. Because of their reusability and adaptability capabilities, model-based approaches are suitable to describe variability and are therefore frequently used for implementation and testing purposes of SPLs. Due to the enormous number of possible products, individual product testing becomes more and more infeasible. Pairwise testing offers one possibility to test a subset of all possible products. However, according to the best of our knowledge, there is no contribution discussing and rating this approach in the SPL context. In this contribution, we provide a mapping between feature models describing the common and variable parts of an SPL and a reusable test model in the form of statecharts. Thereby, we interrelate feature model-based coverage criteria and test model-based coverage criteria such as control and data flow coverage and are therefore able to discuss the potentials and limitations of pairwise testing. We pay particular attention to test requirements for feature interactions constituting a major challenge in SPL engineering. We give a concise definition of feature dependencies and feature interactions from a testing point of view, and we discuss adequacy criteria for SPL coverage under pairwise feature interaction testing and give a generalization to the T-wise case. The concept and implementation of our approach are evaluated by means of a case study from the automotive domain. {\textcopyright} 2011 Springer Science+Business Media, LLC.},
  annote   = {cited By 18},
  doi      = {10.1007/s11219-011-9165-4},
  keywords = {Combinatorial testing; Feature interactions; Mode,Computer software; Reusability,Software design},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865660809{\&}doi=10.1007{\%}2Fs11219-011-9165-4{\&}partnerID=40{\&}md5=ba9f3f3ec3827bc0c43273d7364916b1},
}

@Article{Ghezzi2013508,
  author   = {Ghezzi, Carlo and Sharifloo, Amir Molzam},
  title    = {{Model-based verification of quantitative non-functional properties for software product lines}},
  journal  = {Information and Software Technology},
  year     = {2013},
  volume   = {55},
  number   = {3},
  pages    = {508--524},
  issn     = {0950-5849},
  abstract = {Evaluating quality attributes of a design model in the early stages of development can significantly reduce the cost and risks of developing a low quality product. To make this possible, software designers should be able to predict quality attributes by reasoning on a model of the system under development. Although there exists a variety of quality-driven analysis techniques for software systems, only a few work address software product lines. This paper describes how probabilistic model checking techniques and tools can be used to verify non-functional properties of different configurations of a software product line. We propose a model-based approach that enables software engineers to assess their design solutions for software product lines in the early stages of development. Furthermore, we discuss how the analysis time can be surprisingly reduced by applying parametric model checking instead of classic model checking. The results show that the parametric approach is able to substantially alleviate the verification time and effort required to analyze non-functional properties of software product lines. },
  annote   = {Special Issue on Software Reuse and Product LinesSpecial Issue on Software Reuse and Product Lines},
  doi      = {https://doi.org/10.1016/j.infsof.2012.07.017},
  keywords = {Non-functional requirements,Parametric verification,Probabilistic model checking,Quality analysis,Software product lines},
  url      = {http://www.sciencedirect.com/science/article/pii/S0950584912001516},
}

@Article{Yu2015533,
  author   = {Yu, Jian and Sheng, Quan Z and Swee, Joshua K Y and Han, Jun and Liu, Chengfei and Noor, Talal H},
  title    = {{Model-driven development of adaptive web service processes with aspects and rules}},
  journal  = {Journal of Computer and System Sciences},
  year     = {2015},
  volume   = {81},
  number   = {3},
  pages    = {533--552},
  issn     = {0022-0000},
  abstract = {Abstract Modern software systems are frequently required to be adaptive in order to cope with constant changes. Unfortunately, service-oriented systems built with WS-BPEL are still too rigid. In this paper, we propose a novel model-driven approach to supporting the development of dynamically adaptive WS-BPEL based systems. We model the system functionality with two distinct but highly correlated parts: a stable part called the base model describing the flow logic aspect and a volatile part called the variable model describing the decision logic aspect. We develop an aspect-oriented method to weave the base model and the variable model together so that runtime changes can be applied to the variable model without affecting the base model. A model-driven platform has been implemented to support the development of adaptive WS-BPEL processes. In-lab experiments show that our approach has low performance overhead. A real-life case study also validates the applicability of our approach. },
  annote   = {Special Issue on selected papers from the 4th International Conference on Ambient Systems, Networks and Technologies (ANT 2013)},
  doi      = {https://doi.org/10.1016/j.jcss.2014.11.008},
  keywords = {Adaptive systems,Aspect-oriented methodology,Design tools and techniques,Model-driven development,Web services},
  url      = {http://www.sciencedirect.com/science/article/pii/S0022000014001494},
}

@Article{Santiago20121340,
  author   = {Santiago, Iv{\'{a}}n and Jim{\'{e}}nez, {\'{A}}lvaro and Vara, Juan Manuel and Castro, Valeria De and Bollati, Ver{\'{o}}nica A and Marcos, Esperanza},
  title    = {{Model-Driven Engineering as a new landscape for traceability management: A systematic literature review}},
  journal  = {Information and Software Technology},
  year     = {2012},
  volume   = {54},
  number   = {12},
  pages    = {1340--1356},
  issn     = {0950-5849},
  abstract = {Context Model-Driven Engineering provides a new landscape for dealing with traceability in software development. Objective Our goal is to analyze the current state of the art in traceability management in the context of Model-Driven Engineering. Method We use the systematic literature review based on the guidelines proposed by Kitchenham. We propose five research questions and six quality assessments. Results Of the 157 relevant studies identified, 29 have been considered primary studies. These studies have resulted in 17 proposals. Conclusion The evaluation shows that the most addressed operations are storage, {\{}CRUD{\}} and visualization, while the most immature operations are exchange and analysis traceability information. },
  annote   = {Special Section on Software Reliability and Security},
  doi      = {https://doi.org/10.1016/j.infsof.2012.07.008},
  keywords = {Model-Driven Engineering,Systematic literature review,Traceability},
  url      = {http://www.sciencedirect.com/science/article/pii/S0950584912001346},
}

@Article{Hutchinson2014144,
  author   = {Hutchinson, John and Whittle, Jon and Rouncefield, Mark},
  title    = {{Model-driven engineering practices in industry: Social, organizational and managerial factors that lead to success or failure}},
  journal  = {Science of Computer Programming},
  year     = {2014},
  volume   = {89, Part B},
  pages    = {144--161},
  issn     = {0167-6423},
  abstract = {Abstract In this article, we attempt to address the relative absence of empirical studies of model driven engineering (MDE) in two different but complementary ways. First, we present an analysis of a large online survey of {\{}MDE{\}} deployment and experience that provides some rough quantitative measures of {\{}MDE{\}} practices in industry. Second, we supplement these figures with qualitative data obtained from some semi-structured, in-depth interviews with {\{}MDE{\}} practitioners, and, in particular, through describing the practices of four commercial organizations as they adopted a model driven engineering approach to their software development practices. Using in-depth semi-structured interviewing, we invited practitioners to reflect on their experiences and selected four to use as exemplars or case studies. In documenting some details of their attempts to deploy model driven practices, we identify a number of factors, in particular the importance of complex organizational, managerial and social factors–as opposed to simple technical factors–that appear to influence the relative success, or failure, of the endeavor. Three of the case study companies describe genuine success in their use of model driven development, but explain that as examples of organizational change management, the successful deployment of model driven engineering appears to require: a progressive and iterative approach; transparent organizational commitment and motivation; integration with existing organizational processes and a clear business focus. },
  annote   = {Special issue on Success Stories in Model Driven Engineering},
  doi      = {https://doi.org/10.1016/j.scico.2013.03.017},
  keywords = {Empirical software engineering,Industry practice,Model driven engineering},
  url      = {http://www.sciencedirect.com/science/article/pii/S0167642313000786},
}

@Article{RodriguesdaSilva2015139,
  author   = {da Silva, Alberto Rodrigues},
  title    = {{Model-driven engineering: A survey supported by the unified conceptual model}},
  journal  = {Computer Languages, Systems {\&} Structures},
  year     = {2015},
  volume   = {43},
  pages    = {139--155},
  issn     = {1477-8424},
  abstract = {Abstract During the last decade a new trend of approaches has emerged, which considers models not just documentation artefacts, but also central artefacts in the software engineering field, allowing the creation or automatic execution of software systems starting from those models. These proposals have been classified generically as Model-Driven Engineering (MDE) and share common concepts and terms that need to be abstracted, discussed and understood. This paper presents a survey on {\{}MDE{\}} based on a unified conceptual model that clearly identifies and relates these essential concepts, namely the concepts of system, model, metamodel, modeling language, transformations, software platform, and software product. In addition, this paper discusses the terminologies relating MDE, MDD, {\{}MDA{\}} and others. This survey is based on earlier work, however, contrary to those, it intends to give a simple, broader and integrated view of the essential concepts and respective terminology commonly involved in the MDE, answering to key questions such as: What is a model? What is the relation between a model and a metamodel? What are the key facets of a modeling language? How can I use models in the context of a software development process? What are the relations between models and source code artefacts and software platforms? and What are the relations between MDE, MDD, {\{}MDA{\}} and other {\{}MD{\}} approaches? },
  doi      = {https://doi.org/10.1016/j.cl.2015.06.001},
  keywords = {Metamodel,Model,Model-driven approaches,Model-driven engineering,Modeling language,Software system},
  url      = {http://www.sciencedirect.com/science/article/pii/S1477842415000408},
}

@Article{Štuikys201648,
  author   = {{\v{S}}tuikys, Vytautas and Burbaitė, Renata and Bespalova, Kristina and Ziberkas, Giedrius},
  title    = {{Model-driven processes and tools to design robot-based generative learning objects for computer science education}},
  journal  = {Science of Computer Programming},
  year     = {2016},
  volume   = {129},
  pages    = {48--71},
  issn     = {0167-6423},
  abstract = {Abstract In this paper, we introduce a methodology to design robot-oriented generative learning objects (GLOs) that are, in fact, heterogeneous meta-programs to teach computer science (CS) topics such as programming. The methodology includes {\{}CS{\}} learning variability modelling using the feature-based approaches borrowed from the {\{}SW{\}} engineering domain. Firstly, we define the {\{}CS{\}} learning domain using the known educational framework {\{}TPACK{\}} (Technology, Pedagogy And Content Knowledge). By learning variability we mean the attributes of the framework extracted and represented as feature models with multiple values. Therefore, the {\{}CS{\}} learning variability represents the problem domain. Meta-programming is considered as a solution domain. Both are represented by feature models. The {\{}GLO{\}} design task is formulated as mapping the problem domain model on the solution domain model. Next, we present the design framework to design {\{}GLOs{\}} manually or semi-automatically. The multi-level separation of concepts, model representation and transformation forms the conceptual background. Its theoretical background includes: (a) a formal definition of feature-based models; (b) a graph-based and set-based definition of meta-programming concepts; (c) transformation rules to support the model mapping; (d) a computational Abstract State Machine model to define the processes and design tool for developing GLOs. We present the architecture and some characteristics of the tool. The tool enables to improve the {\{}GLO{\}} design process significantly (in terms of time and quality) and to achieve a higher quality and functionality of {\{}GLOs{\}} themselves (in terms of the parameter space enlargement for reuse and adaptation). We demonstrate the appropriateness of the methodology in the real teaching setting. In this paper, we present the case study that analyses three robot-oriented {\{}GLOs{\}} as the higher-level specifications. Then, using the meta-language processor, we are able to produce, from the specifications, the concrete robot control programs on demand automatically and to demonstrate teaching algorithms visually by robot's actions. We evaluate the approach from technological and pedagogical perspectives using the known structural metrics. Also, we indicate the merits and demerits of the approach. The main contribution and originality of the paper is the seamless integration of two known technologies (feature modelling and meta-programming) in designing robot-oriented {\{}GLOs{\}} and their supporting tools. },
  annote   = {Special issue on eLearning Software Architectures},
  doi      = {https://doi.org/10.1016/j.scico.2016.03.009},
  keywords = {Educational robots,Feature models,GLO design tool,Generative learning objects (GLOs),Model transformation},
  url      = {http://www.sciencedirect.com/science/article/pii/S0167642316300247},
}

@Article{Anjorin2013441,
  author   = {Anjorin, A and Saller, K and Reimund, I and Oster, S and Zorcic, I and Sch{\"{u}}rr, A},
  title    = {{Model-driven rapid prototyping with programmed graph transformations}},
  journal  = {Journal of Visual Languages and Computing},
  year     = {2013},
  volume   = {24},
  number   = {6},
  pages    = {441--462},
  abstract = {Modern software systems are constantly increasing in complexity and supporting the rapid prototyping of such systems has become crucial to check the feasibility of extensions and optimizations, thereby reducing risks and, consequently, the cost of development. As modern software systems are also expected to be reused, extended, and adapted over a much longer lifetime than ever before, ensuring the maintainability of such systems is equally gaining relevance. In this paper, we present the development, optimization and maintenance of MoSo-PoLiTe, a framework for Software Product Line (SPL) testing, as a novel case study for rapid prototyping via metamodelling and programmed graph transformations. The first part of the case study evaluates the use of programmed graph transformations for optimizing an existing, hand-written system (MoSo-PoLiTe) via rapid prototyping of various strategies. In the second part, we present a complete re-engineering of the hand-written system with programmed graph transformations and provide a critical comparison of both implementations.Our results and conclusions indicate that metamodelling and programmed graph transformation are not only suitable techniques for rapid prototyping, but also lead to more maintainable systems. {\textcopyright} 2013 Elsevier Ltd.},
  annote   = {cited By 1},
  doi      = {10.1016/j.jvlc.2013.08.001},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84888819910{\&}doi=10.1016{\%}2Fj.jvlc.2013.08.001{\&}partnerID=40{\&}md5=67b44619de5e40fa303fed2e7e82fe10},
}

@Article{Pleuss20122261,
  author   = {Pleuss, Andreas and Botterweck, Goetz and Dhungana, Deepak and Polzer, Andreas and Kowalewski, Stefan},
  title    = {{Model-driven support for product line evolution on feature level}},
  journal  = {Journal of Systems and Software},
  year     = {2012},
  volume   = {85},
  number   = {10},
  pages    = {2261--2274},
  issn     = {0164-1212},
  abstract = {Software Product Lines (SPL) are an engineering technique to efficiently derive a set of similar products from a set of shared assets. In particular in conjunction with model-driven engineering, {\{}SPL{\}} engineering promises high productivity benefits. There is however, a lack of support for systematic management of {\{}SPL{\}} evolution, which is an important success factor as a product line often represents a long term investment. In this article, we present a model-driven approach for managing {\{}SPL{\}} evolution on feature level. To reduce complexity we use model fragments to cluster related elements. The relationships between these fragments are specified using feature model concepts itself leading to a specific kind of feature model called EvoFM. A configuration of EvoFM represents an evolution step and can be transformed to a concrete instance of the product line (i.e., a feature model for the corresponding point in time). Similarly, automatic transformations allow the derivation of an EvoFM from a given set of feature models. This enables retrospective analysis of historic evolution and serves as a starting point for introduction of EvoFM, e.g., to plan future evolution steps. },
  annote   = {Automated Software Evolution},
  doi      = {https://doi.org/10.1016/j.jss.2011.08.008},
  keywords = {Evolving systems,Feature modeling,Model-driven engineering,Software Product Lines},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121211002093},
}

@Article{Gröner2013709,
  author   = {Gr{\"{o}}ner, Gerd and Bo{\v{s}}kovi{\'{c}}, Marko and Parreiras, Fernando Silva and Ga{\v{s}}evi{\'{c}}, Dragan},
  title    = {{Modeling and validation of business process families}},
  journal  = {Information Systems},
  year     = {2013},
  volume   = {38},
  number   = {5},
  pages    = {709--726},
  issn     = {0306-4379},
  abstract = {Process modeling is an expensive task that needs to encompass requirements of different stakeholders, assure compliance with different standards, and enable the flexible adaptivity to newly emerging requirements in today's dynamic global market. Identifying reusability of process models is a promising direction towards reducing the costs of process modeling. Recent research has offered several solutions. Such solutions promote effective and formally sound methods for variability modeling and configuration management. However, ensuring behavioral validity of reused process models with respect to the original process models (often referred to as reference process models) is still an open research challenge. To address this challenge, in this paper, we propose the notion of business process families by building upon the well-known software engineering discipline—software product line engineering. Business process families comprise (i) a variability modeling perspective, (ii) a process model template (or reference model), and (iii) mappings between (i) and (ii). For business process families, we propose a correct validation algorithm ensuring that each member of a business process family adheres to the core intended behavior that is specified in the process model template. The proposed validation approach is based on the use of Description Logics, variability is represented by using the well-known Feature Models and behavior of process models is considered in terms of control flow patterns. The paper also reports on the experience gained in two external trial cases and results obtained by measuring the tractability of the implementation of the proposed validation approach. },
  doi      = {https://doi.org/10.1016/j.is.2012.11.010},
  keywords = {Business process families,Control flow relations,Process model configuration,Process model variability,Validation},
  url      = {http://www.sciencedirect.com/science/article/pii/S0306437912001524},
}

@Article{Lee20061,
  author   = {Lee, S C},
  title    = {{Modeling Variant User Interfaces for Web-Based Software Product Lines}},
  journal  = {International Journal of Information Technology and Web Engineering (IJITWE)},
  year     = {2006},
  volume   = {1},
  number   = {1},
  pages    = {1--34},
  abstract = {Software product line (SPL) is a software engineering paradigm for software development. SPL is important in promoting software reuse, leading to higher productivity and quality. A software product within a product line often has specific functionalities that are not common to all other products within the product line. Those specific functionalities are termed “variant features” in a product line. SPL paradigm involves the modeling of variant features. However, little work in SPL investigates and addresses the modeling of variant features specific to UI. UML is the de facto modeling language for object-oriented software systems. It is known that UML needs better support in modeling UIs. Thus, much research developed UML extensions to improve UML support in modeling UIs. Yet little of this work is related to developing such extensions for modeling UIs for SPLs in which variant features specific to user interfaces (UI) modeling must be addressed. This research develops a UML extension, WUIML, to address these problems. WUIML defines elements for modeling variant features specific to UIs for Web-based SPLs. The model elements in WUIML extend from the metaclass and of the UML2.0 metamodel. WUIML integrates the modeling of variant features specific to UIs to UML. For example, in a Web-based patient registration SPL, member products targeting British users may use British date format in the user interface, while member products targeting United States users may use United States date format in the user interface. Thus, this is a variant feature for this product line. WUIML defines a model element, XOR, to represent such exclusive or conditions in a product line user interface model. WUIML would reduce SPL engineers' efforts needed in UI development. To validate the WUIML research outcome, a case study was conducted. The results of this empirical study indicate that modeling UIs for Web-based SPLs using WUIML is more effective and efficient than using standard UML. {\textcopyright} 2006, IGI Global. All rights reserved.},
  annote   = {cited By 0},
  doi      = {10.4018/jitwe.2006010101},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85001610191{\&}doi=10.4018{\%}2Fjitwe.2006010101{\&}partnerID=40{\&}md5=48768aae9dbeec09b817e0f8045f0510},
}

@Article{Wong201249,
  author        = {Wong, P Y H and Diakov, N and Schaefer, I},
  title         = {{Modelling adaptable distributed object oriented systems using the HATS approach: A fredhopper case study}},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year          = {2012},
  volume        = {7421 LNCS},
  pages         = {49--66},
  abstract      = {The HATS project aims at developing a model-centric engineering methodology for the design, implementation and verification of distributed, concurrent and highly configurable systems. Such systems also have high demands on their dependability and trustworthiness. The HATS approach is centered around the Abstract Behavioural Specification modelling language (ABS) and its accompanying tools suite. The HATS approach allows the precise specification and analysis of the abstract behaviour of distributed software systems and their variability. The HATS project measures its success by applying its framework not only to toy examples, but to real industrial scenarios. In this paper, we evaluate the HATS approach for modelling an industrial scale case study provided by the eCommerce company Fredhopper. In this case study we consider Fredhopper Access Server (FAS). We model the commonality and variability of FAS's replication system using the ABS language and provide an evaluation based on our experience. {\textcopyright} 2012 Springer-Verlag Berlin Heidelberg.},
  annote        = {cited By 3},
  doi           = {10.1007/978-3-642-31762-0_5},
  keywords      = {Commonality and variability,Configurable systems,Industrial applications,Research,Specifications,Verification,case study},
  mendeley-tags = {case study},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864860105{\&}doi=10.1007{\%}2F978-3-642-31762-0{\_}5{\&}partnerID=40{\&}md5=eb2461c92499b0e064e3a2d26fba502b},
}

@Article{TERBEEK2016287,
  author   = {ter Beek, Maurice H and Fantechi, Alessandro and Gnesi, Stefania and Mazzanti, Franco},
  title    = {{Modelling and analysing variability in product families: Model checking of modal transition systems with variability constraints}},
  journal  = {Journal of Logical and Algebraic Methods in Programming},
  year     = {2016},
  volume   = {85},
  number   = {2},
  pages    = {287--315},
  issn     = {2352-2208},
  abstract = {We present the formal underpinnings of a modelling and analysis framework for the specification and verification of variability in product families. We address variability at the behavioural level by modelling the family behaviour by means of a Modal Transition System (MTS) with an associated set of variability constraints expressed over action labels. An MTS is a Labelled Transition System (LTS) which distinguishes between optional and mandatory transitions. Steered by the variability constraints, the inclusion or exclusion of labelled transitions in an LTS refining the MTS determines the family's possible product behaviour. We formalise this as a special-purpose refinement relation for MTSs, which differs fundamentally from the classical one, and show how to use it for the definition and derivation of valid product behaviour starting from product family behaviour. We also present a variability-aware action-based branching-time modal temporal logic to express properties over MTSs, and demonstrate a number of results regarding the preservation of logical properties from family to product behaviour. These results pave the way for the more efficient family-based analyses of MTSs, limiting the need for product-by-product analyses of LTSs. Finally, we define a high-level modal process algebra for the specification of MTSs. The complete framework is implemented in a model-checking tool: given the behaviour of a product family modelled as an MTS with an additional set of variability constraints, it allows the explicit generation of valid product behaviour as well as the efficient on-the-fly verification of logical properties over family and product behaviour alike.},
  doi      = {https://doi.org/10.1016/j.jlamp.2015.11.006},
  keywords = {Modal transition systems,Model checking,Product families,Temporal logic,Variability},
  url      = {http://www.sciencedirect.com/science/article/pii/S2352220815001431},
}

@Article{Milani201655,
  author   = {Milani, Fredrik and Dumas, Marlon and Ahmed, Naved and Matulevi{\v{c}}ius, Raimundas},
  title    = {{Modelling families of business process variants: A decomposition driven method}},
  journal  = {Information Systems},
  year     = {2016},
  volume   = {56},
  pages    = {55--72},
  issn     = {0306-4379},
  abstract = {Abstract Business processes usually do not exist as singular entities that can be managed in isolation, but rather as families of business process variants. When modelling such families of variants, analysts are confronted with the choice between modelling each variant separately, or modelling multiple or all variants in a single model. Modelling each variant separately leads to a proliferation of models that share common parts, resulting in redundancies and inconsistencies. Meanwhile, modelling all variants together leads to less but more complex models, thus hindering on comprehensibility. This paper introduces a method for modelling families of process variants that addresses this trade-off. The key tenet of the method is to alternate between steps of decomposition (breaking down processes into sub-processes) and deciding which parts should be modelled together and which ones should be modelled separately. We have applied the method to two case studies: one concerning the consolidation of existing process models, and another dealing with green-field process discovery. In both cases, the method produced fewer models with respect to the baseline and reduced duplicity by up to 50{\%} without significant impact on complexity. },
  doi      = {https://doi.org/10.1016/j.is.2015.09.003},
  keywords = {Business process model consolidation,Business process modelling,Business process variant},
  url      = {http://www.sciencedirect.com/science/article/pii/S0306437915001684},
}

@Article{SCHMOLZER2008551,
  author   = {Schm{\"{o}}lzer, Gernot and Teiniker, Egon and Kreiner, Christian},
  title    = {{Model-typed component interfaces}},
  journal  = {Journal of Systems Architecture},
  year     = {2008},
  volume   = {54},
  number   = {6},
  pages    = {551--561},
  issn     = {1383-7621},
  abstract = {Component based software engineering (CBSE) allows to design and develop reusable software components that can be assembled to construct software systems via well defined interfaces. However, designing such reusable components for data intensive business logic often requires heavy data transfer between components over interfaces. Static interface definitions using basic data types or structures of such lead to large interfaces susceptible to modifications. The goal of this paper is to present model-typed interfaces based on generic interface parameters, which allows to transfer complex structured data between components. Providing such generic, model-defined types (MDT) with data models specifying the parameter structure supports compatibility checks of model-typed interfaces at platform independent system design time. The methodology is described platform independently and the coherency with our system development process is discussed. Moreover, a technology mapping to IDL and the CORBA component model (CCM) is illustrated.},
  annote   = {Selection of best papers from the 32nd EUROMICRO Conference on ‘Software Engineering and Advanced Applications' (SEAA 2006)},
  doi      = {https://doi.org/10.1016/j.sysarc.2008.01.006},
  keywords = {Data modeling,Interfaces definition,Software engineering},
  url      = {http://www.sciencedirect.com/science/article/pii/S1383762108000246},
}

@Article{Kofroň200931,
  author   = {Kofroň, J and Pl{\'{a}}{\v{s}}il, F and {\v{S}}er{\'{y}}, O},
  title    = {{Modes in component behavior specification via EBP and their application in product lines}},
  journal  = {Information and Software Technology},
  year     = {2009},
  volume   = {51},
  number   = {1},
  pages    = {31--41},
  abstract = {The concept of software product lines (SPL) is a modern approach to software development simplifying construction of related variants of a product thus lowering development costs and shortening time-to-market. In SPL, software components play an important role. In this paper, we show how the original idea of component mode can be captured and further developed in behavior specification via the formalism of extended behavior protocols (EBP). Moreover, we demonstrate how the modes in behavior specification can be used for modeling behavior of an entire product line. The main benefits include (i) the existence of a single behavior specification capturing the behavior of all product variants, and (ii) automatic verification of absence of communication errors among the cooperating components taking the variability into account. These benefits are demonstrated on a part of a non-trivial case study. {\textcopyright} 2008 Elsevier B.V. All rights reserved.},
  annote   = {cited By 5},
  doi      = {10.1016/j.infsof.2008.09.011},
  keywords = {Automatic verifications; Behavior specification;,Communication,Specifications},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-56649114354{\&}doi=10.1016{\%}2Fj.infsof.2008.09.011{\&}partnerID=40{\&}md5=985fa698b6816ece343555191796cd75},
}

@Article{Bagheri2011109,
  author   = {Bagheri, E and Ensan, F and Ga{\v{s}}evic, D and Bo{\v{s}}kovic, M},
  title    = {{Modular feature models: Representation and configuration}},
  journal  = {Journal of Research and Practice in Information Technology},
  year     = {2011},
  volume   = {43},
  number   = {2},
  pages    = {109--140},
  abstract = {Within the realm of software product line engineering, feature modeling is one of the widely used techniques for modeling commonality as well as variability. Feature models incorporate the entire domain application configuration space, and are therefore developed collectively by teams of domain experts. In large scale industrial domains, feature models become too complex both in terms of maintenance and configuration. In order to make the maintenance and configuration of feature models feasible, we propose to modularize feature models based on the well-established Distributed Description Logics formalism. Modular feature models provide for an enhanced collaborative/ distributed feature model design, more efficient feature model evolution and better reusability of feature model structure. We also develop methods for the configuration and configuration verification of a modular feature model based on standard inference mechanisms. We describe and evaluate our proposed approach through a case study on an online electronic store application domain. {\textcopyright} 2011, Australian Computer Society Inc.},
  annote   = {cited By 4},
  keywords = {Application domains; Configuration space; Configur,Data description; Maintenance; Reusability,Software design},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860433186{\&}partnerID=40{\&}md5=05447b5ad7d0b50d1200456b1f58c390},
}

@Article{d’Amorim20121012,
  author   = {D'Amorim, Fernanda and Borba, Paulo},
  title    = {{Modularity analysis of use case implementations}},
  journal  = {Journal of Systems and Software},
  year     = {2012},
  volume   = {85},
  number   = {4},
  pages    = {1012--1027},
  issn     = {0164-1212},
  abstract = {A component-based decomposition can result in implementations having use cases code tangled with other concerns and scattered across components. Modularity mechanisms such as aspects, mixins, and virtual classes have been proposed to address this kind of problem. One can use such mechanisms to group together code related to a single use case. This paper quantitatively analyzes the impact of this kind of use case modularization. We apply one specific technique, aspect oriented programming, to modularize the use case implementations of two information systems that conform to the layered architecture pattern. We extract traditional and contemporary metrics – including cohesion, coupling, and separation of concerns – to analyze modularity in terms of quality attributes such as changeability, support for independent development, and pluggability. Our findings indicate that the results of a given modularity analysis depend on other factors beyond the chosen system, metrics, and the applied modularity technique. },
  doi      = {https://doi.org/10.1016/j.jss.2011.11.1025},
  keywords = {Aspect-oriented programming,Empirical software engineering,Modularity,Use cases},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121211002950},
}

@Article{VogelHeuser201735,
  author   = {Vogel-Heuser, Birgit and Fischer, Juliane and Feldmann, Stefan and Ulewicz, Sebastian and R{\"{o}}sch, Susanne},
  title    = {{Modularity and architecture of PLC-based software for automated production Systems: An analysis in industrial companies}},
  journal  = {Journal of Systems and Software},
  year     = {2017},
  volume   = {131},
  pages    = {35--62},
  issn     = {0164-1212},
  abstract = {Abstract Adaptive and flexible production systems require modular and reusable software especially considering their long-term life cycle of up to 50 years. SWMAT4aPS, an approach to measure Software Maturity for automated Production Systems is introduced. The approach identifies weaknesses and strengths of various companies' solutions for modularity of software in the design of automated Production Systems (aPS). At first, a self-assessed questionnaire is used to evaluate a large number of companies concerning their software maturity. Secondly, we analyze {\{}PLC{\}} code, architectural levels, workflows and abilities to configure code automatically out of engineering information in four selected companies. In this paper, the questionnaire results from 16 German world-leading companies in machine and plant manufacturing and four case studies validating the results from the detailed analyses are introduced to prove the applicability of the approach and give a survey of the state of the art in industry. },
  doi      = {https://doi.org/10.1016/j.jss.2017.05.051},
  keywords = {Automated production systems,Control software,Factory automation,Maturity,Modularity,Programmable logic controller},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121217300985},
}

@Article{Batory20082059,
  author        = {Batory, D and B{\"{o}}rger, E},
  title         = {{Modularizing theorems for software product lines: The jbook case study}},
  journal       = {Journal of Universal Computer Science},
  year          = {2008},
  volume        = {14},
  number        = {12},
  pages         = {2059--2082},
  abstract      = {A gvvoal of software product lines is the economical assembly of programs in a family of programs. In this paper, we explore how theorems about program properties may be integrated into feature-based development of software product lines. As a case study, we analyze an existing Java/JVM compilation correctness proof for defining, interpreting, compiling, and executing bytecode for the Java language. We show how features modularize program source, theorem statements and their proofs. By composing features, the source code, theorem statements and proofs for a program are assembled. The investigation in this paper reveals a striking similarity of the refinement concepts used in Abstract State Machines (ASM) based system development and Feature-Oriented Programming (FOP) of software product lines. We suggest to exploit this observation for a fruitful interaction of researchers in the two communities. {\textcopyright} J.UCS.},
  annote        = {cited By 22},
  keywords      = {case study},
  mendeley-tags = {case study},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-55249084004{\&}partnerID=40{\&}md5=05f8438cdd2224729c25937b88e8b009},
}

@Article{Rosenmüller20123,
  author   = {Rosenm{\"{u}}ller, M and Siegmund, N and Pukall, M and Apel, S},
  title    = {{Multilingual component programming in racket}},
  journal  = {ACM SIGPLAN Notices},
  year     = {2012},
  volume   = {47},
  number   = {3},
  pages    = {3--12},
  abstract = {Software product lines (SPLs) and adaptive systems aim at variability to cope with changing requirements. Variability can be described in terms of features, which are central for development and configuration of SPLs. In traditional SPLs, features are bound statically before runtime. By contrast, adaptive systems support feature binding at runtime and are sometimes called dynamic SPLs (DSPLs). DSPLs are usually built from coarse-grained components, which reduces the number of possible application scenarios. To overcome this limitation, we closely integrate static binding of traditional SPLs and runtime adaptation of DSPLs. We achieve this integration by statically generating a tailor-made DSPL from a highly customizable SPL. The generated DSPL provides only the runtime variability required by a particular application scenario and the execution environment. The DSPL supports self-configuration based on coarse-grained modules. We provide a feature-based adaptation mechanism that reduces the effort of computing an optimal configuration at runtime. In a case study, we demonstrate the practicability of our approach and show that a seamless integration of static binding and runtime adaptation reduces the complexity of the adaptation process. {\textcopyright} 2011 ACM.},
  annote   = {cited By 0},
  doi      = {10.1145/2189751.2047864},
  keywords = {Adaptation mechanism; Adaptation process; Applicat,Adaptive systems,Computer programming; Computer science},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867552032{\&}doi=10.1145{\%}2F2189751.2047864{\&}partnerID=40{\&}md5=ef3e6bbbdf9ec375dcbc008e3e2e2345},
}

@Article{Assunção20171763,
  author   = {Assun{\c{c}}{\~{a}}o, W K G and Lopez-Herrejon, R E and Linsbauer, L and Vergilio, S R and Egyed, A},
  title    = {{Multi-objective reverse engineering of variability-safe feature models based on code dependencies of system variants}},
  journal  = {Empirical Software Engineering},
  year     = {2017},
  volume   = {22},
  number   = {4},
  pages    = {1763--1794},
  abstract = {Maintenance of many variants of a software system, developed to supply a wide range of customer-specific demands, is a complex endeavour. The consolidation of such variants into a Software Product Line is a way to effectively cope with this problem. A crucial step for this consolidation is to reverse engineer feature models that represent the desired combinations of features of all the available variants. Many approaches have been proposed for this reverse engineering task but they present two shortcomings. First, they use a single-objective perspective that does not allow software engineers to consider design trade-offs. Second, they do not exploit knowledge from implementation artifacts. To address these limitations, our work takes a multi-objective perspective and uses knowledge from source code dependencies to obtain feature models that not only represent the desired feature combinations but that also check that those combinations are indeed well-formed, i.e. variability safe. We performed an evaluation of our approach with twelve case studies using NSGA-II and SPEA2, and a single-objective algorithm. Our results indicate that the performance of the multi-objective algorithms is similar in most cases and that both clearly outperform the single-objective algorithm. Our work also unveils several avenues for further research. {\textcopyright} 2016, Springer Science+Business Media New York.},
  annote   = {cited By 5},
  doi      = {10.1007/s10664-016-9462-4},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991821880{\&}doi=10.1007{\%}2Fs10664-016-9462-4{\&}partnerID=40{\&}md5=8e45fa95048b0edd14d9f3e88ad41f1a},
}

@Article{Parejo2016287,
  author        = {Parejo, Jos{\'{e}} A and S{\'{a}}nchez, Ana B and Segura, Sergio and Ruiz-Cort{\'{e}}s, Antonio and Lopez-Herrejon, Roberto E and Egyed, Alexander},
  title         = {{Multi-objective test case prioritization in highly configurable systems: A case study}},
  journal       = {Journal of Systems and Software},
  year          = {2016},
  volume        = {122},
  pages         = {287--310},
  issn          = {0164-1212},
  abstract      = {Abstract Test case prioritization schedules test cases for execution in an order that attempts to accelerate the detection of faults. The order of test cases is determined by prioritization objectives such as covering code or critical components as rapidly as possible. The importance of this technique has been recognized in the context of Highly-Configurable Systems (HCSs), where the potentially huge number of configurations makes testing extremely challenging. However, current approaches for test case prioritization in {\{}HCSs{\}} suffer from two main limitations. First, the prioritization is usually driven by a single objective which neglects the potential benefits of combining multiple criteria to guide the detection of faults. Second, instead of using industry-strength case studies, evaluations are conducted using synthetic data, which provides no information about the effectiveness of different prioritization objectives. In this paper, we address both limitations by studying 63 combinations of up to three prioritization objectives in accelerating the detection of faults in the Drupal framework. Results show that non–functional properties such as the number of changes in the features are more effective than functional metrics extracted from the configuration model. Results also suggest that multi-objective prioritization typically results in faster fault detection than mono-objective prioritization.},
  doi           = {https://doi.org/10.1016/j.jss.2016.09.045},
  keywords      = {Automated software testing,Highly-configurable systems,Test case prioritization,Variability,case study},
  mendeley-tags = {case study},
  url           = {http://www.sciencedirect.com/science/article/pii/S0164121216301935},
}

@Article{Alférez2010103,
  author   = {Alf{\'{e}}rez, M and Santos, J and Moreira, A and Garcia, A and Kulesza, U and Ara{\'{u}}jo, J and Amaral, V},
  title    = {{Multi-view composition language for software product line requirements}},
  journal  = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year     = {2010},
  volume   = {5969 LNCS},
  pages    = {103--122},
  abstract = {Composition of requirements models in Software Product Line (SPL) development enables stakeholders to derive the requirements of target software products and, very important, to reason about them. Given the growing complexity of SPL development and the various stakeholders involved, their requirements are often specified from heterogeneous, partial views. However, existing requirements composition languages are very limited to generate specific requirements views for SPL products. They do not provide specialized composition rules for referencing and composing elements in recurring requirements models, such as use cases and activity models. This paper presents a multi-view composition language for SPL requirements, the Variability Modeling Language for Requirements (VML4RE). This language describes how requirements elements expressed in different models should be composed to generate a specific SPL product. The use of VML4RE is illustrated with UML-based requirements models defined for a home automation SPL case study. The language is evaluated with additional case studies from different application domains, such as mobile phones and sales management. {\textcopyright} 2010 Springer-Verlag.},
  annote   = {cited By 19},
  doi      = {10.1007/978-3-642-12107-4_8},
  keywords = {Activity models; Application domains; Composition,Computer software reusability; Network architectu,Linguistics},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-77951642464{\&}doi=10.1007{\%}2F978-3-642-12107-4{\_}8{\&}partnerID=40{\&}md5=bc504d6b8baecd0db67ea5835a2541ea},
}

@Article{Segura20111124,
  author   = {Segura, Sergio and Hierons, Robert M and Benavides, David and Ruiz-Cort{\'{e}}s, Antonio},
  title    = {{Mutation testing on an object-oriented framework: An experience report}},
  journal  = {Information and Software Technology},
  year     = {2011},
  volume   = {53},
  number   = {10},
  pages    = {1124--1136},
  issn     = {0950-5849},
  abstract = {Context The increasing presence of Object-Oriented (OO) programs in industrial systems is progressively drawing the attention of mutation researchers toward this paradigm. However, while the number of research contributions in this topic is plentiful, the number of empirical results is still marginal and mostly provided by researchers rather than practitioners. Objective This article reports our experience using mutation testing to measure the effectiveness of an automated test data generator from a user perspective. Method In our study, we applied both traditional and class-level mutation operators to FaMa, an open source Java framework currently being used for research and commercial purposes. We also compared and contrasted our results with the data obtained from some motivating faults found in the literature and two real tools for the analysis of feature models, FaMa and SPLOT. Results Our results are summarized in a number of lessons learned supporting previous isolated results as well as new findings that hopefully will motivate further research in the field. Conclusion We conclude that mutation testing is an effective and affordable technique to measure the effectiveness of test mechanisms in {\{}OO{\}} systems. We found, however, several practical limitations in current tool support that should be addressed to facilitate the work of testers. We also missed specific techniques and tools to apply mutation testing at the system level. },
  annote   = {Special Section on Mutation Testing},
  doi      = {https://doi.org/10.1016/j.infsof.2011.03.006},
  keywords = {Automated analysis,Feature models,Mutation testing,Test adequacy,Test data generation},
  url      = {http://www.sciencedirect.com/science/article/pii/S0950584911000826},
}

@Article{Pedrycz2003383,
  author   = {Pedrycz, W and Chun, M G and Succi, G},
  title    = {{N4: computing with neural receptive fields}},
  journal  = {Neurocomputing},
  year     = {2003},
  volume   = {55},
  number   = {1–2},
  pages    = {383--401},
  issn     = {0925-2312},
  abstract = {In this study, we introduce a new neural architecture called {\{}N4{\}} that is based on a collection of local receptive fields realized in the form of referential neural networks. While the network exhibits some similarities to other structures of modular neural networks (such as expert networks), it comes with a number of unique features. Especially, its receptive fields exhibit high flexibility by being formed by neural networks. Subsequently, the processing therein is of referential nature. A ”skeleton” (structure) of the network is completed through unsupervised learning that is aimed at “discovering” and structuring the main dependencies in data. More specifically, the design of the network consists of two phases. First, a blueprint of the network is formed and this involves the prototypes obtained through clustering of training data. This structural development of the network is followed by further refinement in a form of parametric training of the individual neural receptive fields. The study provides a detailed analysis and learning of the network and includes experimental investigations. },
  annote   = {Support Vector Machines},
  doi      = {https://doi.org/10.1016/S0925-2312(02)00630-6},
  keywords = {Fuzzy clustering,Local neural networks,Modular neural networks,Nearest neighbor classification rule,Neural receptive fields,Prototypes,RBF neural networks},
  url      = {http://www.sciencedirect.com/science/article/pii/S0925231202006306},
}

@Article{Lago2004214,
  author   = {Lago, P and {Van Vliet}, H},
  title    = {{Observations from the recovery of a software product family}},
  journal  = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year     = {2004},
  volume   = {3154},
  pages    = {214--227},
  abstract = {The problem of managing the evolution of complex and large software systems is well known. Evolution implies the reuse and modification of existing software artifacts, and this means that the related knowledge must be documented and maintained. This paper focuses on the evolution of software product families, although the same principles apply in other software development environments as well. We describe our experience gained in a case study recovering a family of six software products. We give an overview of the case study, and provide lessons learned, implicit assumptions reconstructed during the case study, and some rules we think are generally applicable. Our experience indicates that organizing architectural knowledge is a difficult task. To properly serve the various uses of this knowledge, it needs to be organized along different dimensions and tools are required. Our experience also indicates that, next to variability explicitly designed into the product family, a "variation creep" is caused by different, and evolving, technical and organizational environments of the products. We propose explicitly modeling invariabilities, next to variabilities, in software product lines to get a better grip on this variation creep. {\textcopyright} Springer-Verlag 2004.},
  annote   = {cited By 4},
  keywords = {Architectural knowledge; Large software systems;,Computer software; Computer software reusability;,Software design},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-35048869103{\&}partnerID=40{\&}md5=269509eb0f763140ade98854c78a3159},
}

@Article{Wnuk2013921,
  author   = {Wnuk, Krzysztof and Gorschek, Tony and Zahda, Showayb},
  title    = {{Obsolete software requirements}},
  journal  = {Information and Software Technology},
  year     = {2013},
  volume   = {55},
  number   = {6},
  pages    = {921--940},
  issn     = {0950-5849},
  abstract = {AbstractContext Coping with rapid requirements change is crucial for staying competitive in the software business. Frequently changing customer needs and fierce competition are typical drivers of rapid requirements evolution resulting in requirements obsolescence even before project completion. Objective Although the obsolete requirements phenomenon and the implications of not addressing them are known, there is a lack of empirical research dedicated to understanding the nature of obsolete software requirements and their role in requirements management. Method In this paper, we report results from an empirical investigation with 219 respondents aimed at investigating the phenomenon of obsolete software requirements. Results Our results contain, but are not limited to, defining the phenomenon of obsolete software requirements, investigating how they are handled in industry today and their potential impact. Conclusion We conclude that obsolete software requirements constitute a significant challenge for companies developing software intensive products, in particular in large projects, and that companies rarely have processes for handling obsolete software requirements. Further, our results call for future research in creating automated methods for obsolete software requirements identification and management, methods that could enable efficient obsolete software requirements management in large projects. },
  doi      = {https://doi.org/10.1016/j.infsof.2012.12.001},
  keywords = {Change impact analysis,Empirical study,Market driven requirements engineering,Obsolete requirements,Requirements management,Survey},
  url      = {http://www.sciencedirect.com/science/article/pii/S0950584912002364},
}

@Article{Köksal2017191,
  author   = {K{\"{o}}ksal, {\"{O}}mer and Tekinerdogan, Bedir},
  title    = {{Obstacles in Data Distribution Service Middleware: A Systematic Review}},
  journal  = {Future Generation Computer Systems},
  year     = {2017},
  volume   = {68},
  pages    = {191--210},
  issn     = {0167-739X},
  abstract = {Abstract Context: Data Distribution Service (DDS) is a standard data-centric publish–subscribe programming model and specification for distributed systems. {\{}DDS{\}} has been applied for the development of high performance distributed systems such as in the defense, finance, automotive, and simulation domains. Various papers have been written on the application of DDS, however, there has been no attempt to systematically review and categorize the identified obstacles. Objective: The overall objective of this paper is to identify the state of the art of DDS, and describe the main lessons learned and obstacles in applying DDS. In addition, we aim to identify the important open research issues. Method: A systematic literature review (SLR) is conducted by a multiphase study selection process using the published literature since the introduction of {\{}DDS{\}} in 2003. Results: We reviewed 468 papers that are discovered using a well-planned review protocol, and 34 of them were assessed as primary studies related to our research questions. Conclusions: We have identified 11 basic categories for describing the identified obstacles and the corresponding research challenges that can be used to depict the state-of-the-art in {\{}DDS{\}} and provide a vision for further research. },
  doi      = {https://doi.org/10.1016/j.future.2016.09.020},
  keywords = {Data Distribution Service (DDS),Middleware,Systematic literature review},
  url      = {http://www.sciencedirect.com/science/article/pii/S0167739X1630351X},
}

@Article{Lung2015301,
  author   = {Lung, C.-H. and Balasubramaniam, B and Selvarajah, K and Elankeswaran, P and Gopalasundaram, U},
  title    = {{On building architecture-centric product line architecture}},
  journal  = {Requirements Engineering},
  year     = {2015},
  volume   = {20},
  number   = {3},
  pages    = {301--321},
  abstract = {Software architects typically spend a great deal of time and effort exploring uncertainties, evaluating alternatives, and balancing the concerns of stakeholders. Selecting the best architecture to meet both the functional and non-functional requirements is a critical but difficult task, especially at the early stage of software development when there may be many uncertainties. For example, how will a technology match the operational or performance expectations in reality? This paper presents an approach to building architecture-centric product line. The main objective of the proposed approach is to support effective requirements validation and architectural prototyping for the application-level software. Architectural prototyping is practically essential to architecture design and evaluation. However, architectural prototyping practiced in the field mostly is not used to explore alternatives. Effective construction and evaluation of multiple architecture alternatives is one of the critically challenging tasks. The product line architecture advocated in this paper consists of multiple software architecture alternatives, from which the architect can select and rapidly generate a working application prototype. The paper presents a case study of developing a framework that is primarily built with robust architecture patterns in distributed and concurrent computing and includes variation mechanisms to support various applications even in different domains. The development process of the framework is an application of software product line engineering with an aim to effectively facilitate upfront requirements analysis for an application and rapid architectural prototyping to explore and evaluate architecture alternatives. {\textcopyright} Springer-Verlag London 2014.},
  annote   = {cited By 2},
  doi      = {10.1007/s00766-014-0201-3},
  keywords = {Application programs; Computer software; Requireme,Architecture evaluation; Patterns; Requirements v,Software prototyping},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84943348957{\&}doi=10.1007{\%}2Fs00766-014-0201-3{\&}partnerID=40{\&}md5=5934c044b3b58dcc93f783735a84538c},
}

@Article{Lienhardt20183,
  author   = {Lienhardt, M and Damiani, F and Testa, L and Turin, G},
  title    = {{On checking delta-oriented product lines of statecharts}},
  journal  = {Science of Computer Programming},
  year     = {2018},
  volume   = {166},
  pages    = {3--34},
  abstract = {A Software Product Line (SPL) is a set of programs, called variants, which are generated from a common artifact base. Delta-Oriented Programming (DOP) is a flexible approach to implement SPLs. In this article, we provide a foundation for rigorous development of delta-oriented product lines of statecharts. We introduce a core language for statecharts, we define DOP on top of it, we present an analysis ensuring that a product line is well-formed (i.e., all variants can be generated and are well-formed statecharts), and we illustrate how an implementation of the analysis has been applied to an industrial case study. {\textcopyright} 2018 Elsevier B.V.},
  annote   = {cited By 0},
  doi      = {10.1016/j.scico.2018.05.007},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047936031{\&}doi=10.1016{\%}2Fj.scico.2018.05.007{\&}partnerID=40{\&}md5=71a5ae273d36841b172898f5b5f457c8},
}

@Article{Malhotra201785,
  author   = {Malhotra, Ruchika and Khanna, Megha and Raje, Rajeev R},
  title    = {{On the application of search-based techniques for software engineering predictive modeling: A systematic review and future directions}},
  journal  = {Swarm and Evolutionary Computation},
  year     = {2017},
  volume   = {32},
  pages    = {85--109},
  issn     = {2210-6502},
  abstract = {Abstract Software engineering predictive modeling involves construction of models, with the help of software metrics, for estimating quality attributes. Recently, the use of search-based techniques have gained importance as they help the developers and project-managers in the identification of optimal solutions for developing effective prediction models. In this paper, we perform a systematic review of 78 primary studies from January 1992 to December 2015 which analyze the predictive capability of search-based techniques for ascertaining four predominant software quality attributes, i.e., effort, defect proneness, maintainability and change proneness. The review analyses the effective use and application of search-based techniques by evaluating appropriate specifications of fitness functions, parameter settings, validation methods, accounting for their stochastic natures and the evaluation of developmental models with the use of well-known statistical tests. Furthermore, we compare the effectiveness of different models, developed using the various search-based techniques amongst themselves, and also with the prevalent machine learning techniques used in literature. Although there are very few studies which use search-based techniques for predicting maintainability and change proneness, we found that the results of the application of search-based techniques for effort estimation and defect prediction are encouraging. Hence, this comprehensive study and the associated results will provide guidelines to practitioners and researchers and will enable them to make proper choices for applying the search-based techniques to their specific situations. },
  doi      = {https://doi.org/10.1016/j.swevo.2016.10.002},
  keywords = {Change prediction,Defect prediction,Effort estimation,Maintainability prediction,Search-based techniques,Software quality},
  url      = {http://www.sciencedirect.com/science/article/pii/S2210650216303418},
}

@Article{Frantz201689,
  author   = {Frantz, Rafael Z and Corchuelo, Rafael and Roos-Frantz, Fabricia},
  title    = {{On the design of a maintainable software development kit to implement integration solutions}},
  journal  = {Journal of Systems and Software},
  year     = {2016},
  volume   = {111},
  pages    = {89--104},
  issn     = {0164-1212},
  abstract = {Abstract Companies typically rely on applications purchased from third parties or developed at home to support their business activities. It is not uncommon that these applications were not designed taking integration into account. Enterprise Application Integration provides methodologies and tools to design and implement integration solutions. Camel, Spring Integration, and Mule range amongst the most popular open-source tools that provide support to implement integration solutions. The adaptive maintenance of a software tool is very important for companies that need to reuse existing tools to build their own. We have analysed 25 maintainability measures on Camel, Spring Integration, and Mule. We have conducted a statistical analysis to confirm the results obtained with the maintainability measures, and it follows that these tools may have problems regarding maintenance. These problems increase the costs of the adaptation process. This motivated us to work on a new proposal that has been carefully designed in order to reduce maintainability efforts. Guaran{\'{a}} SDK is the software tool that we provide to implement integration solutions. We have also computed the maintainability measures regarding Guaran{\'{a}} SDK and the results suggest that maintaining it is easier than maintaining the others. Furthermore, we have conducted an industrial experience to demonstrate the application of our proposal in industry. },
  doi      = {https://doi.org/10.1016/j.jss.2015.08.044},
  keywords = {Enterprise Application Integration,Integration framework},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121215001880},
}

@Article{Chimalakonda:2016:ESS:2934240.2934248,
  author    = {Chimalakonda, Sridhar and Lee, Dan Hyung},
  title     = {{On the Evolution of Software and Systems Product Line Standards}},
  journal   = {SIGSOFT Softw. Eng. Notes},
  year      = {2016},
  volume    = {41},
  number    = {3},
  pages     = {27--30},
  issn      = {0163-5948},
  address   = {New York, NY, USA},
  doi       = {10.1145/2934240.2934248},
  keywords  = {Methods,Patterns,Software Product Lines,Standards,Tools},
  publisher = {ACM},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/2934240.2934248},
}

@Article{Ribeiro:2011:IFD:2189751.2047868,
  author    = {Ribeiro, M{\'{a}}rcio and Queiroz, Felipe and Borba, Paulo and Tol{\^{e}}do, T{\'{a}}rsis and Brabrand, Claus and Soares, S{\'{e}}rgio},
  title     = {{On the Impact of Feature Dependencies when Maintaining Preprocessor-based Software Product Lines}},
  journal   = {SIGPLAN Not.},
  year      = {2011},
  volume    = {47},
  number    = {3},
  pages     = {23--32},
  issn      = {0362-1340},
  address   = {New York, NY, USA},
  doi       = {10.1145/2189751.2047868},
  keywords  = {modularity,preprocessors,software product lines},
  publisher = {ACM},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/2189751.2047868},
}

@Article{Maia20131023,
  author   = {{de Almeida Maia}, Marcelo and Lafet{\'{a}}, Raquel Fialho},
  title    = {{On the impact of trace-based feature location in the performance of software maintainers}},
  journal  = {Journal of Systems and Software},
  year     = {2013},
  volume   = {86},
  number   = {4},
  pages    = {1023--1037},
  issn     = {0164-1212},
  abstract = {Software maintainers frequently strive to locate source code related to specific software features. This situation is mostly observable when features are scattered in the code. Considering this problem, several approaches for feature location using execution traces have been developed. Nonetheless, the practice of post-mortem analysis based on execution traces is not fully incorporated in the daily practice of software maintainers. Empirical studies that reveal strengths and weaknesses on the use of execution traces in maintenance activities could better explain the role of execution traces in software maintenance. This study reports on a controlled experiment conducted with maintainers performing actual maintenance activities on systems of different sizes unknown to them. There are benefits from systematic use of execution traces: the reduction of the maintenance activity time and greater accuracy of the activity outcome. Other qualitative observations were the lower level of activity difficulty perceived by the participants that used execution trace information and that this kind of information seems to be less useful in maintenance activities where the problem of feature scattering does not occur clearly. },
  annote   = {{\{}SI{\}} : Software Engineering in Brazil: Retrospective and Prospective Views},
  doi      = {https://doi.org/10.1016/j.jss.2012.12.032},
  keywords = {Empirical assessment,Execution traces,Feature location,Software maintenance},
  url      = {http://www.sciencedirect.com/science/article/pii/S016412121200341X},
}

@Article{Conejero2012212,
  author   = {Conejero, J M and Figueiredo, E and Garcia, A and Hern{\'{a}}ndez, J and Jurado, E},
  title    = {{On the relationship of concern metrics and requirements maintainability}},
  journal  = {Information and Software Technology},
  year     = {2012},
  volume   = {54},
  number   = {2},
  pages    = {212--238},
  abstract = {Context: Maintainability has become one of the most essential attributes of software quality, as software maintenance has shown to be one of the most costly and time-consuming tasks of software development. Many studies reveal that maintainability is not often a major consideration in requirements and design stages, and software maintenance costs may be reduced by a more controlled design early in the software life cycle. Several problem factors have been identified as harmful for software maintainability, such as lack of upfront consideration of proper modularity choices. In that sense, the presence of crosscutting concerns is one of such modularity anomalies that possibly exert negative effects on software maintainability. However, to the date there is little or no knowledge about how characteristics of crosscutting concerns, observable in early artefacts, are correlated with maintainability. Objective: In this setting, this paper introduces an empirical analysis where the correlation between crosscutting properties and two ISO/IEC 9126 maintainability attributes, namely changeability and stability, is presented. Method: This correlation is based on the utilization of a set of concern metrics that allows the quantification of crosscutting, scattering and tangling. Results: Our study confirms that a change in a crosscutting concern is more difficult to be accomplished and that artefacts addressing crosscutting concerns are found to be less stable later as the system evolves. Moreover, our empirical analysis reveals that crosscutting properties introduce non-syntactic dependencies between software artefacts, thereby decreasing the quality of software in terms of changeability and stability as well. These subtle dependencies cannot be easily detected without the use of concern metrics. Conclusion: The correlation provides evidence that the presence of certain crosscutting properties negatively affects to changeability and stability. The whole analysis is performed using as target cases three software product lines, where maintainability properties are of upmost importance not only for individual products but also for the core architecture of the product line. {\textcopyright} 2011 Elsevier B.V. All rights reserved.},
  annote   = {cited By 9},
  doi      = {10.1016/j.infsof.2011.09.003},
  keywords = {Computer software maintenance; Computer software,Concern metrics; Crosscutting; Crosscutting concer,Maintainability},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-81055140252{\&}doi=10.1016{\%}2Fj.infsof.2011.09.003{\&}partnerID=40{\&}md5=2eb396552b5d1d2a125d858d5e0c5f44},
}

@Article{Beg2015674,
  author   = {Beg, Azam and Awwad, Falah and Ibrahim, Walid and Ahmed, Faheem},
  title    = {{On the reliability estimation of nano-circuits using neural networks}},
  journal  = {Microprocessors and Microsystems},
  year     = {2015},
  volume   = {39},
  number   = {8},
  pages    = {674--685},
  issn     = {0141-9331},
  abstract = {Abstract As the integrated circuit geometries shrink, it becomes important for the designers to take into consideration the reliability of the circuits. Different techniques can be used for reliability calculation or estimation. Some of these techniques are accurate but time-consuming while others are quick but not accurate. For example, using a set of mathematical equations for reliability estimation is very fast but not precise enough for large systems. Alternatively, Monte Carlo simulations are highly accurate, but very time-intensive. This work presents three different neural network models for estimating circuit reliability. The models provide better prediction accuracies than the mathematical technique. A reasonably large number of combinational circuits were simulated over a wide range of device reliabilities to collect the training data for the models. Multiple slices of an ISCAS-85 benchmark circuit were used to validate the models' prediction results. },
  doi      = {https://doi.org/10.1016/j.micpro.2015.09.008},
  keywords = {Digital circuit,Modeling,Nano-electronics,Neural network,Reliability},
  url      = {http://www.sciencedirect.com/science/article/pii/S0141933115001507},
}

@Article{Wohlin20132594,
  author   = {Wohlin, C and Runeson, P and {Da Mota Silveira Neto}, P A and Engstr{\"{o}}m, E and {Do Carmo Machado}, I and {De Almeida}, E S},
  title    = {{On the reliability of mapping studies in software engineering}},
  journal  = {Journal of Systems and Software},
  year     = {2013},
  volume   = {86},
  number   = {10},
  pages    = {2594--2610},
  abstract = {Background Systematic literature reviews and systematic mapping studies are becoming increasingly common in software engineering, and hence it becomes even more important to better understand the reliability of such studies. Objective This paper presents a study of two systematic mapping studies to evaluate the reliability of mapping studies and point out some challenges related to this type of study in software engineering. Method The research is based on an in-depth case study of two published mapping studies on software product line testing. Results We found that despite the fact that the two studies are addressing the same topic, there are quite a number of differences when it comes to papers included and in terms of classification of the papers included in the two mapping studies. Conclusions From this we conclude that although mapping studies are important, their reliability cannot simply be taken for granted. Based on the findings we also provide four conjectures that further research has to address to make secondary studies (systematic mapping studies and systematic literature reviews) even more valuable to both researchers and practitioners. {\textcopyright} 2013 Elsevier Inc.},
  annote   = {cited By 31},
  doi      = {10.1016/j.jss.2013.04.076},
  keywords = {Mapping,Mapping studies; Software Product Line; Software p,Reliability; Research; Software design; Software},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84882695716{\&}doi=10.1016{\%}2Fj.jss.2013.04.076{\&}partnerID=40{\&}md5=097cdd6ea83b702f2c96adbfb2e4edda},
}

@Article{SousaFerreira201465,
  author   = {{Sousa Ferreira}, G C and Gaia, F N and Figueiredo, E and {De Almeida Maia}, M},
  title    = {{On the use of feature-oriented programming for evolving software product lines - A comparative study}},
  journal  = {Science of Computer Programming},
  year     = {2014},
  volume   = {93},
  number   = {PART A},
  pages    = {65--85},
  abstract = {Feature-oriented programming (FOP) is a programming technique based on composition mechanisms, called refinements. It is often assumed that feature-oriented programming is more suitable than other variability mechanisms for implementing Software Product Lines (SPLs). However, there is no empirical evidence to support this claim. In fact, recent research work found out that some composition mechanisms might degenerate the SPL modularity and stability. However, there is no study investigating these properties focusing on the FOP composition mechanisms. This paper presents quantitative and qualitative analysis of how feature modularity and change propagation behave in the context of two evolving SPLs, namely WebStore and MobileMedia. Quantitative data have been collected from the SPLs developed in three different variability mechanisms: FOP refinements, conditional compilation, and object-oriented design patterns. Our results suggest that FOP requires few changes in source code and a balanced number of added modules, providing better support than other techniques for non-intrusive insertions. Therefore, it adheres closer to the Open-Closed principle. Additionally, FOP seems to be more effective tackling modularity degeneration, by avoiding feature tangling and scattering in source code, than conditional compilation and design patterns. These results are based not only on the variability mechanism itself, but also on careful SPL design. However, the aforementioned results are weaker when the design needs to cope with crosscutting and fine-grained features. {\textcopyright} 2013 Elsevier B.V. All rights reserved.},
  annote   = {cited By 12},
  doi      = {10.1016/j.scico.2013.10.010},
  keywords = {Computer programming; Software engineering,Computer software,Conditional compilations; Design Patterns; Featur},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905459866{\&}doi=10.1016{\%}2Fj.scico.2013.10.010{\&}partnerID=40{\&}md5=b909f275cd0c20a8c0c799592c8c29de},
}

@Article{Ferreira2014,
  author   = {Ferreira, Gabriel Coutinho Sousa and Gaia, Felipe Nunes and Figueiredo, Eduardo and {de Almeida Maia}, Marcelo},
  title    = {{On the use of feature-oriented programming for evolving software product lines — A comparative study}},
  journal  = {Science of Computer Programming},
  year     = {2014},
  volume   = {93, Part A},
  pages    = {65--85},
  issn     = {0167-6423},
  abstract = {Abstract Feature-oriented programming (FOP) is a programming technique based on composition mechanisms, called refinements. It is often assumed that feature-oriented programming is more suitable than other variability mechanisms for implementing Software Product Lines (SPLs). However, there is no empirical evidence to support this claim. In fact, recent research work found out that some composition mechanisms might degenerate the {\{}SPL{\}} modularity and stability. However, there is no study investigating these properties focusing on the {\{}FOP{\}} composition mechanisms. This paper presents quantitative and qualitative analysis of how feature modularity and change propagation behave in the context of two evolving SPLs, namely WebStore and MobileMedia. Quantitative data have been collected from the {\{}SPLs{\}} developed in three different variability mechanisms: {\{}FOP{\}} refinements, conditional compilation, and object-oriented design patterns. Our results suggest that {\{}FOP{\}} requires few changes in source code and a balanced number of added modules, providing better support than other techniques for non-intrusive insertions. Therefore, it adheres closer to the Open–Closed principle. Additionally, {\{}FOP{\}} seems to be more effective tackling modularity degeneration, by avoiding feature tangling and scattering in source code, than conditional compilation and design patterns. These results are based not only on the variability mechanism itself, but also on careful {\{}SPL{\}} design. However, the aforementioned results are weaker when the design needs to cope with crosscutting and fine-grained features. },
  annote   = {Special Issue with Selected Papers from the Brazilian Symposium on Programming Languages (SBLP 2011)},
  doi      = {https://doi.org/10.1016/j.scico.2013.10.010},
  keywords = {Conditional compilation,Design patterns,Feature-oriented programming,Software product lines,Variability management},
  url      = {http://www.sciencedirect.com/science/article/pii/S0167642313002815},
}

@Article{Walther20162,
  author   = {Walther, Sven and Wehrheim, Heike},
  title    = {{On-the-fly construction of provably correct service compositions – templates and proofs}},
  journal  = {Science of Computer Programming},
  year     = {2016},
  volume   = {127},
  pages    = {2--23},
  issn     = {0167-6423},
  abstract = {Abstract Today, service compositions often need to be assembled or changed on-the-fly, which leaves only little time for quality assurance. Moreover, quality assurance is complicated by service providers only giving information on their services in terms of domain specific concepts with only limited semantic meaning. In this paper, we propose a method for constructing service compositions based on pre-verified templates. Templates, given as workflow descriptions, are typed over a (domain-independent) template ontology defining concepts and predicates. Their meaning is defined by an abstract semantics, leaving the specific meaning of ontology concepts open, however, only up to given ontology rules. Templates are proven correct using a Hoare-style proof calculus, extended by a specific rule for service calls. Construction of service compositions amounts to instantiation of templates with domain-specific services. Correctness of an instantiation can then simply be checked by verifying that the domain ontology (a) adheres to the rules of the template ontology, and (b) fulfills the constraints of the employed template. },
  annote   = {Special issue of the 11th International Symposium on Formal Aspects of Component Software},
  doi      = {https://doi.org/10.1016/j.scico.2016.04.002},
  keywords = {Correctness by construction,Hoare-calculus,Service compositions,Templates,Verification},
  url      = {http://www.sciencedirect.com/science/article/pii/S0167642316300028},
}

@Article{Soujanya20182361,
  author   = {Soujanya, K L S},
  title    = {{Ontology based variability management for dynamic reconfiguration of software product lines}},
  journal  = {Journal of Advanced Research in Dynamical and Control Systems},
  year     = {2018},
  volume   = {9},
  number   = {Special Issue 18},
  pages    = {2361--2375},
  abstract = {Software Product Line (SPL) a collection of software products that have certain common features and variable features for customization to satisfy the needs of target customers. Usually core assets and custom assets required by SPLS are built at the development time. However there are some software systems that need automatic and dynamic reconfiguration. To accommodate this, the SPL should have mechanisms to deal with it. Feature model is an important constituent of SPL. The feature model can represent similarities and variability's of software besides supporting quality product derivation. However, this model has drawbacks in usage for dynamic reconfiguration. Therefore it is essential to represent features using a different model for monitoring, retrieving and modifying automatically. Ontology is one such proven model that can formally represent well the features and their relationships in machine processable fashion. In the existing work, ontology was not implemented in the proposed frame work. The present work aims at implementing ontology based solution for automatic reconfiguration of SPL and product derivation. The empirical study with the improved prototype reveals that there is significant performance advantage when ontology is used for SPL configuration management. {\textcopyright} 2018, Institute of Advanced Scientific Research, Inc. All rights reserved.},
  annote   = {cited By 0},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055963348{\&}partnerID=40{\&}md5=8b1a3c6bb9e6fe4cb9359f45e22219bc},
}

@Article{Dermeval20154950,
  author   = {Dermeval, D and Ten{\'{o}}rio, T and Bittencourt, I I and Silva, A and Isotani, S and Ribeiro, M},
  title    = {{Ontology-based feature modeling: An empirical study in changing scenarios}},
  journal  = {Expert Systems with Applications},
  year     = {2015},
  volume   = {42},
  number   = {11},
  pages    = {4950--4964},
  abstract = {A software product line (SPL) is a set of software systems that have a particular set of common features and that satisfy the needs of a particular market segment or mission. Feature modeling is one of the key activities involved in the design of SPLs. The feature diagram produced in this activity captures the commonalities and variabilities of SPLs. In some complex domains (e.g.; ubiquitous computing, autonomic systems and context-aware computing), it is difficult to foresee all functionalities and variabilities a specific SPL may require. Thus, Dynamic Software Product Lines (DSPLs) bind variation points at runtime to adapt to fluctuations in user needs as well as to adapt to changes in the environment. In this context, relying on formal representations of feature models is important to allow them to be automatically analyzed during system execution. Among the mechanisms used for representing and analyzing feature models, description logic (DL) based approaches demand to be better investigated in DSPLs since it provides capabilities, such as automated inconsistency detection, reasoning efficiency, scalability and expressivity. Ontology is the most common way to represent feature models knowledge based on DL reasoners. Previous works conceived ontologies for feature modeling either based on OWL classes and properties or based on OWL individuals. However, considering change or evolution scenarios of feature models, we need to compare whether a class-based or an individual-based feature modeling style is recommended to describe feature models to support SPLs, and especially its capabilities to deal with changes in feature models, as required by DSPLs. In this paper, we conduct a controlled experiment to empirically compare two approaches based on each one of these modeling styles in several changing scenarios (e.g.; add/remove mandatory feature, add/remove optional feature and so on). We measure time to perform changes, structural impact of changes (flexibility) and correctness for performing changes in our experiment. Our results indicate that using OWL individuals requires less time to change and is more flexible than using OWL classes and properties. These results provide insightful assumptions towards the definition of an approach relying on reasoning capabilities of ontologies that can effectively support products reconfiguration in the context of DSPL. {\textcopyright} 2015 Elsevier Ltd. All rights reserved.},
  annote   = {cited By 6},
  doi      = {10.1016/j.eswa.2015.02.020},
  keywords = {Birds; Computation theory; Computer software; Data,Context-aware computing; Dynamic software product,Software design},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924777581{\&}doi=10.1016{\%}2Fj.eswa.2015.02.020{\&}partnerID=40{\&}md5=2fefeaf0ff5c921b30d1c9a4d9f75aa9},
}

@Article{COLOMBO2014891,
  author   = {Colombo, Massimo G and Piva, Evila and Rossi-Lamastra, Cristina},
  title    = {{Open innovation and within-industry diversification in small and medium enterprises: The case of open source software firms}},
  journal  = {Research Policy},
  year     = {2014},
  volume   = {43},
  number   = {5},
  pages    = {891--902},
  issn     = {0048-7333},
  abstract = {This paper examines the within-industry diversification of software small and medium enterprises that collaborate with the open source software community (OSS SMEs). In doing so, it offers new insights into the association between open innovation and diversification. We rely on arguments inspired by the literature and evidence collected through interviews with OSS SMEs' top managers to investigate factors that favor or hinder within-industry diversification. First, in line with the mainstream diversification literature, we focus attention on the role of firm size. Second, in the spirit of the open innovation research, we concentrate on the mechanisms that OSS SMEs put in place to get access to the external resources of the OSS community. Econometric evidence on 100 European OSS SMEs shows that firm size is negatively associated to within-industry diversification, while OSS SMEs that have contributed to a larger number of OSS projects have a more diversified portfolio of software products. Furthermore, we provide preliminary evidence that the practice of authorizing firm programmers to contribute autonomously to OSS projects of their own choice during working hours may be positively associated to within-industry diversification only if OSS SMEs possess adequate internal technological resources.},
  annote   = {Open Innovation: New Insights and Evidence},
  doi      = {https://doi.org/10.1016/j.respol.2013.08.015},
  keywords = {Open Source community,Open innovation,Small and medium enterprises,Within-industry diversification},
  url      = {http://www.sciencedirect.com/science/article/pii/S0048733313001601},
}

@Article{FRANCOBEDOYA2017160,
  author   = {Franco-Bedoya, Oscar and Ameller, David and Costal, Dolors and Franch, Xavier},
  title    = {{Open source software ecosystems: A Systematic mapping}},
  journal  = {Information and Software Technology},
  year     = {2017},
  volume   = {91},
  pages    = {160--185},
  issn     = {0950-5849},
  abstract = {Context: Open source software (OSS) and software ecosystems (SECOs) are two consolidated research areas in software engineering. OSS influences the way organizations develop, acquire, use and commercialize software. SECOs have emerged as a paradigm to understand dynamics and heterogeneity in collaborative software development. For this reason, SECOs appear as a valid instrument to analyze OSS systems. However, there are few studies that blend both topics together. Objective: The purpose of this study is to evaluate the current state of the art in OSS ecosystems (OSSECOs) research, specifically: (a) what the most relevant definitions related to OSSECOs are; (b) what the particularities of this type of SECO are; and (c) how the knowledge about OSSECO is represented. Method: We conducted a systematic mapping following recommended practices. We applied automatic and manual searches on different sources and used a rigorous method to elicit the keywords from the research questions and selection criteria to retrieve the final papers. As a result, 82 papers were selected and evaluated. Threats to validity were identified and mitigated whenever possible. Results: The analysis allowed us to answer the research questions. Most notably, we did the following: (a) identified 64 terms related to the OSSECO and arranged them into a taxonomy; (b) built a genealogical tree to understand the genesis of the OSSECO term from related definitions; (c) analyzed the available definitions of SECO in the context of OSS; and (d) classified the existing modelling and analysis techniques of OSSECOs. Conclusion: As a summary of the systematic mapping, we conclude that existing research on several topics related to OSSECOs is still scarce (e.g., modelling and analysis techniques, quality models, standard definitions, etc.). This situation calls for further investigation efforts on how organizations and OSS communities actually understand OSSECOs.},
  doi      = {https://doi.org/10.1016/j.infsof.2017.07.007},
  keywords = {Literature review,OSS,OSSECO,Open source software,SECO,Software ecosystem,Systematic mapping},
  url      = {http://www.sciencedirect.com/science/article/pii/S0950584917304512},
}

@Article{Mariani2015173,
  author   = {Mariani, T and Vergilio, S R and Colanzi, T E},
  title    = {{Optimizing aspect-oriented product line architectures with search-based algorithms}},
  journal  = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year     = {2015},
  volume   = {9275},
  pages    = {173--187},
  abstract = {The adoption of Aspect-Oriented Product Line Architectures (AOPLA) brings many benefits to the software product line design. It contributes to improve modularity, stability and to reduce feature tangling and scattering. Improvements can also be obtained with a search based and multi-objective approach, such as MOA4PLA, which generates PLAs with the best trade-off between different measures, such as cohesion, coupling and feature modularization. However, MOA4PLA operators may violate the aspect-oriented modeling (AOM) rules, impacting negatively on the architecture understanding. In order to solve this problem, this paper introduces a more adequate representation for AOPLAs and a set of search operators, called SO4ASPAR (Search Operators for Aspect-Oriented Architectures). Results from an empirical evaluation show that the proposed operators yield better solutions regarding the fitness values, besides preserving the AOM rules. {\textcopyright} Springer International Publishing Switzerland 2015.},
  annote   = {cited By 0},
  doi      = {10.1007/978-3-319-22183-0_12},
  keywords = {Algorithms; Economic and social effects; Modular c,Aspect-oriented architecture; Aspect-Oriented Mod,Product design},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84951263106{\&}doi=10.1007{\%}2F978-3-319-22183-0{\_}12{\&}partnerID=40{\&}md5=6a4fb2f275935485b04ca1ca42fe14b3},
}

@Article{Rebêlo20131137,
  author   = {Reb{\^{e}}lo, Henrique and Lima, Ricardo and Leavens, Gary T and Corn{\'{e}}lio, M{\'{a}}rcio and Mota, Alexandre and Oliveira, C{\'{e}}sar},
  title    = {{Optimizing generated aspect-oriented assertion checking code for {\{}JML{\}} using program transformations: An empirical study}},
  journal  = {Science of Computer Programming},
  year     = {2013},
  volume   = {78},
  number   = {8},
  pages    = {1137--1156},
  issn     = {0167-6423},
  abstract = {The AspectJ {\{}JML{\}} compiler (ajmlc) explores aspect-oriented programming (AOP) mechanisms to implement {\{}JML{\}} specifications, such as pre- and postconditions, and enforce them during runtime. This compiler was created to improve source-code modularity. Some experiments were conducted to evaluate the performance of the code generated through ajmlc. Results demonstrated that the strategy of adopting {\{}AOP{\}} to implement {\{}JML{\}} specifications is very promising. However, there is still a need for optimization of the generated code's bytecode size and running time. This paper presents a catalog of transformations which represent the optimizations implemented in the new optimized version of the ajmlc compiler. We employ such transformations to reduce the bytecode size and running time of the code generated through the ajmlc compiler. Aiming at demonstrating the impact of such transformation on the code quality, we conduct an empirical study using four applications in optimized and non-optimized versions generated by ajmlc. We show that our {\{}AOP{\}} transformations provide a significant improvement, regarding bytecode size and running time. },
  annote   = {Special section on software evolution, adaptability, and maintenance {\&} Special section on the Brazilian Symposium on Programming Languages},
  doi      = {https://doi.org/10.1016/j.scico.2012.09.003},
  keywords = {Aspect-oriented programming,JML,Program transformation},
  url      = {http://www.sciencedirect.com/science/article/pii/S0167642312001682},
}

@Article{Jamshidi2015249,
  author   = {Jamshidi, P and Pahl, C},
  title    = {{Orthogonal to support multi-cloud application configuration}},
  journal  = {Communications in Computer and Information Science},
  year     = {2015},
  volume   = {508},
  pages    = {249--261},
  abstract = {Cloud service providers benefit from a vast majority of customers due to variability and making profit from commonalities between the cloud services that they provide. Recently, application configuration dimensions has been increased dramatically due to multi-tenant, multi-device and multi-cloud paradigm. This challenges the configuration and customization of cloud-based software that are typically offered as a service due to the intrinsic variability. In this paper, we present a model-driven approach based on variability models originating from the software product line community to handle such multi-dimensional variability in the cloud. We exploit orthogonal variability models to systematically manage and create tenant-specific configuration and customizations. We also demonstrate how such variability models can be utilized to take into account the already deployed application parts to enable harmonized deployments for new tenants in a multi-cloud setting. The approach considers application functional and non-functional requirements to provide a set of valid multi-cloud configurations. We illustrate our approach through a case study. {\textcopyright} Springer International Publishing Switzerland 2015.},
  annote   = {cited By 0},
  doi      = {10.1007/978-3-319-14886-1_23},
  keywords = {Cloud architectures; Cloud service providers; Int,Cloud computing,Distributed database systems},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924168003{\&}doi=10.1007{\%}2F978-3-319-14886-1{\_}23{\&}partnerID=40{\&}md5=a1e8c513d863386eefdc8b6c17e84678},
}

@Article{Barve201820,
  author   = {Barve, Y D and Patil, P and Bhattacharjee, A and Gokhale, A},
  title    = {{PADS: Design and Implementation of a Cloud-Based, Immersive Learning Environment for Distributed Systems Algorithms}},
  journal  = {IEEE Transactions on Emerging Topics in Computing},
  year     = {2018},
  volume   = {6},
  number   = {1},
  pages    = {20--31},
  abstract = {As distributed systems become more complex, understanding the underlying algorithms that make these systems work becomes even harder. Traditional learning modalities based on didactic teaching and theoretical proofs alone are no longer sufficient for a holistic understanding of these algorithms. Instead, an environment that promotes an immersive, hands-on learning of distributed systems algorithms is needed to complement existing teaching modalities. Such an environment must be flexible to support the learning of a variety of algorithms. The environment should also support extensibility and reuse since many of these algorithms share several common traits with each other while differing only in some aspects. Finally, it must also allow students to experiment with large-scale deployments in a variety of operating environments. To address these concerns, we use the principles of software product lines and model-driven engineering, and adopt the cloud platform to design an immersive learning environment called the Playground of Algorithms for Distributed Systems (PADS). A prototype implementation of PADS is described to showcase use cases involving BitTorrent Peer-to-Peer file sharing, ZooKeeper-based coordination, and Paxos-based consensus, which show the benefits of rapid deployment of the distributed systems algorithms. Results from a preliminary user study are also presented. {\textcopyright} 2013 IEEE.},
  annote   = {cited By 0},
  doi      = {10.1109/TETC.2017.2731984},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029188711{\&}doi=10.1109{\%}2FTETC.2017.2731984{\&}partnerID=40{\&}md5=cac99429f55ae0fd634190e6dc7b3ab7},
}

@Article{Qu2014544,
  author   = {Qu, Wei and Jia, Yuanyuan and Jiang, Michael},
  title    = {{Pattern mining of cloned codes in software systems}},
  journal  = {Information Sciences},
  year     = {2014},
  volume   = {259},
  pages    = {544--554},
  issn     = {0020-0255},
  abstract = {Pattern mining of cloned codes in software systems is a challenging task due to various modifications and the large size of software codes. Most existing approaches adopt a token-based software representation and use sequential analysis for pattern mining of cloned codes. Due to the intrinsic limitations of such spatial space analysis, these methods have difficulties handling statement reordering, insertion and control replacement. Recently, graph-based models such as program dependent graph have been exploited to solve these issues. Although they can improve the performance in terms of accuracy, they introduce additional problems. Their computational complexity is very high and dramatically increases with the software size, thus limiting their applications in practice. In this paper, we propose a novel pattern mining framework for cloned codes in software systems. It efficiently exploits software's spatial space information as well as graph space information and thus can mine accurate patterns of cloned codes for software systems. Preliminary experimental results have demonstrated the superior performance of the proposed approach compared with other methods. },
  doi      = {https://doi.org/10.1016/j.ins.2010.04.022},
  keywords = {Pattern mining,Software clone detection,Software engineering,Software reuse detection},
  url      = {http://www.sciencedirect.com/science/article/pii/S0020025510001787},
}

@Article{Coarfa:2006:PAT:1124153.1124155,
  author    = {Coarfa, Cristian and Druschel, Peter and Wallach, Dan S},
  title     = {{Performance Analysis of TLS Web Servers}},
  journal   = {ACM Trans. Comput. Syst.},
  year      = {2006},
  volume    = {24},
  number    = {1},
  pages     = {39--69},
  issn      = {0734-2071},
  address   = {New York, NY, USA},
  doi       = {10.1145/1124153.1124155},
  keywords  = {Internet,RSA accelerator,TLS,e-commerce,secure Web servers},
  publisher = {ACM},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/1124153.1124155},
}

@Article{Koziolek2010634,
  author   = {Koziolek, Heiko},
  title    = {{Performance evaluation of component-based software systems: A survey}},
  journal  = {Performance Evaluation},
  year     = {2010},
  volume   = {67},
  number   = {8},
  pages    = {634--658},
  issn     = {0166-5316},
  abstract = {Performance prediction and measurement approaches for component-based software systems help software architects to evaluate their systems based on component performance specifications created by component developers. Integrating classical performance models such as queueing networks, stochastic Petri nets, or stochastic process algebras, these approaches additionally exploit the benefits of component-based software engineering, such as reuse and division of work. Although researchers have proposed many approaches in this direction during the last decade, none of them has attained widespread industrial use. On this basis, we have conducted a comprehensive state-of-the-art survey of more than 20 of these approaches assessing their applicability. We classified the approaches according to the expressiveness of their component performance modelling languages. Our survey helps practitioners to select an appropriate approach and scientists to identify interesting topics for future research. },
  annote   = {Special Issue on Software and Performance},
  doi      = {https://doi.org/10.1016/j.peva.2009.07.007},
  keywords = {CBSE,Classification,Measurement,Modelling,Performance,Prediction,Software component,Survey},
  url      = {http://www.sciencedirect.com/science/article/pii/S016653160900100X},
}

@Article{Myllärniemi20161623,
  author        = {Myll{\"{a}}rniemi, V and Savolainen, J and Raatikainen, M and M{\"{a}}nnist{\"{o}}, T},
  title         = {{Performance variability in software product lines: proposing theories from a case study}},
  journal       = {Empirical Software Engineering},
  year          = {2016},
  volume        = {21},
  number        = {4},
  pages         = {1623--1669},
  abstract      = {In the software product line research, product variants typically differ by their functionality and quality attributes are not purposefully varied. The goal is to study purposeful performance variability in software product lines, in particular, the motivation to vary performance, and the strategy for realizing performance variability in the product line architecture. The research method was a theory-building case study that was augmented with a systematic literature review. The case was a mobile network base station product line with capacity variability. The data collection, analysis and theorizing were conducted in several stages: the initial case study results were augmented with accounts from the literature. We constructed three theoretical models to explain and characterize performance variability in software product lines: the models aim to be generalizable beyond the single case. The results describe capacity variability in a base station product line. Thereafter, theoretical models of performance variability in software product lines in general are proposed. Performance variability is motivated by customer needs and characteristics, by trade-offs and by varying operating environment constraints. Performance variability can be realized by hardware or software means; moreover, the software can either realize performance differences in an emergent way through impacts from other variability or by utilizing purposeful varying design tactics. The results point out two differences compared with the prevailing literature. Firstly, when the customer needs and characteristics enable price differentiation, performance may be varied even with no trade-offs or production cost differences involved. Secondly, due to the dominance of feature modeling, the literature focuses on the impact management realization. However, performance variability can be realized through purposeful design tactics to downgrade the available software resources and by having more efficient hardware. {\textcopyright} 2015, Springer Science+Business Media New York.},
  annote        = {cited By 0},
  doi           = {10.1007/s10664-014-9359-z},
  keywords      = {Base stations,Commerce,Computer software,Econom,Operating environment,P,Performance variability,Software design,case study},
  mendeley-tags = {case study},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923360642{\&}doi=10.1007{\%}2Fs10664-014-9359-z{\&}partnerID=40{\&}md5=f7cd2d20470c075b0a65d80d8de67744},
}

@Article{Feitelson2012859,
  author   = {Feitelson, Dror G},
  title    = {{Perpetual development: A model of the Linux kernel life cycle}},
  journal  = {Journal of Systems and Software},
  year     = {2012},
  volume   = {85},
  number   = {4},
  pages    = {859--875},
  issn     = {0164-1212},
  abstract = {Software evolution is widely recognized as an important and common phenomenon, whereby the system follows an ever-extending development trajectory with intermittent releases. Nevertheless there have been only few lifecycle models that attempt to portray such evolution. We use the evolution of the Linux kernel as the basis for the formulation of such a model, integrating the progress in time with growth of the codebase, and differentiating between development of new functionality and maintenance of production versions. A unique element of the model is the sequence of activities involved in releasing new production versions, and how this has changed with the growth of Linux. In particular, the release follow-up phase before the forking of a new development version, which was prominent in early releases of production versions, has been eliminated in favor of a concurrent merge window in the release of 2.6.x versions. We also show that a piecewise linear model with increasing slopes provides the best description of the growth of Linux. The perpetual development model is used as a framework in which commonly recognized benefits of incremental and evolutionary development may be demonstrated, and to comment on issues such as architecture, conservation of familiarity, and failed projects. We suggest that this model and variants thereof may apply to many other projects in addition to Linux. },
  doi      = {https://doi.org/10.1016/j.jss.2011.10.050},
  keywords = {Linux kernel,Maintenance,Software evolution,Software release},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121211002822},
}

@Article{Amálio20113,
  author   = {Am{\'{a}}lio, N and Glodt, C and Pinto, F and Kelsen, P},
  title    = {{Platform-variant applications from platform-independent models via templates}},
  journal  = {Electronic Notes in Theoretical Computer Science},
  year     = {2011},
  volume   = {279},
  number   = {3},
  pages    = {3--25},
  abstract = {By raising the level of abstraction from code to models, model-driven development (MDD) emphasises design rather than implementation and platform-specificity. This paper presents an experiment with a MDD approach, which takes platform-independent models and generates code for various platforms from them. The platform code is generated from templates. Our approach is based on EP, a formal executable modelling language, supplemented with OCL, and FTL, a formal language of templates. The paper's experiment generates code for the mobile platforms Android and iPhone from the same abstract functional model of a case study. The experiment shows the feasibility of MDD to tackle present day problems, highlighting many benefits of the MDD approach and opportunities for improvement. {\textcopyright} 2011 Elsevier B.V. All rights reserved.},
  annote   = {cited By 0},
  doi      = {10.1016/j.entcs.2011.11.035},
  keywords = {Abstracting; Experiments; Formal languages,Codes (symbols),Executable model; Functional model; Level of abstr},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855696791{\&}doi=10.1016{\%}2Fj.entcs.2011.11.035{\&}partnerID=40{\&}md5=0f6226b96c53c65c7ed20c03c28ac926},
}

@Article{Jalote:2008:PRG:13487689.13487690,
  author    = {Jalote, Pankaj and Murphy, Brendan and Sharma, Vibhu Saujanya},
  title     = {{Post-release Reliability Growth in Software Products}},
  journal   = {ACM Trans. Softw. Eng. Methodol.},
  year      = {2008},
  volume    = {17},
  number    = {4},
  pages     = {17:1----17:20},
  issn      = {1049-331X},
  address   = {New York, NY, USA},
  doi       = {10.1145/13487689.13487690},
  keywords  = {Post-release reliability growth,product stabilization time},
  publisher = {ACM},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/13487689.13487690},
}

@Article{Succi20031,
  author   = {Succi, Giancarlo and Pedrycz, Witold and Stefanovic, Milorad and Miller, James},
  title    = {{Practical assessment of the models for identification of defect-prone classes in object-oriented commercial systems using design metrics}},
  journal  = {Journal of Systems and Software},
  year     = {2003},
  volume   = {65},
  number   = {1},
  pages    = {1--12},
  issn     = {0164-1212},
  abstract = {The goal of this paper is to investigate and assess the ability of explanatory models based on design metrics to describe and predict defect counts in an object-oriented software system. Specifically, we empirically evaluate the influence of design decisions to defect behavior of the classes in two products from the commercial software domain. Information provided by these models can help in resource allocation and serve as a base for assessment and future improvements. We use innovative statistical methods to deal with the peculiarities of the software engineering data, such as non-normally distributed count data. To deal with overdispersed data and excess of zeroes in the dependent variable, we use negative binomial (NB) and zero-inflated {\{}NB{\}} regression in addition to Poisson regression. Furthermore, we form a framework for comparison of models' descriptive and predictive ability. Predictive capability of the models to identify most critical classes in the system early in the software development process can help in allocation of resources and foster software quality improvement. In addition to the correlation coefficients, we use additional statistics to assess a models' ability to explain high variability in the data and Pareto analysis to assess a models' ability to identify the most critical classes in the system. Results indicate that design aspects related to communication between classes and inheritance can be used as indicators of the most defect-prone classes, which require the majority of resources in development and testing phases. The zero-inflated negative binomial regression model, designed to explicitly model the occurrence of zero counts in the dataset, provides the best results for this purpose. },
  doi      = {https://doi.org/10.1016/S0164-1212(02)00024-9},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121202000249},
}

@Article{Hervieu2016129,
  author   = {Hervieu, Aymeric and Marijan, Dusica and Gotlieb, Arnaud and Baudry, Benoit},
  title    = {{Practical minimization of pairwise-covering test configurations using constraint programming}},
  journal  = {Information and Software Technology},
  year     = {2016},
  volume   = {71},
  pages    = {129--146},
  issn     = {0950-5849},
  abstract = {Abstract Context: Testing highly-configurable software systems is challenging due to a large number of test configurations that have to be carefully selected in order to reduce the testing effort as much as possible, while maintaining high software quality. Finding the smallest set of valid test configurations that ensure sufficient coverage of the system's feature interactions is thus the objective of validation engineers, especially when the execution of test configurations is costly or time-consuming. However, this problem is NP-hard in general and approximation algorithms have often been used to address it in practice. Objective: In this paper, we explore an alternative exact approach based on constraint programming that will allow engineers to increase the effectiveness of configuration testing while keeping the number of configurations as low as possible. Method: Our approach consists in using a (time-aware) minimization algorithm based on constraint programming. Given the amount of time, our solution generates a minimized set of valid test configurations that ensure coverage of all pairs of feature values (a.k.a. pairwise coverage). The approach has been implemented in a tool called PACOGEN. Results: {\{}PACOGEN{\}} was evaluated on 224 feature models in comparison with the two existing tools that are based on a greedy algorithm. For 79{\%} of 224 feature models, {\{}PACOGEN{\}} generated up to 60{\%} fewer test configurations than the competitor tools. We further evaluated {\{}PACOGEN{\}} in the case study of an industrial video conferencing product line with a feature model of 169 features, and found 60{\%} fewer configurations compared with the manual approach followed by test engineers. The set of test configurations generated by {\{}PACOGEN{\}} decreased the time required by test engineers in manual test configuration by 85{\%}, increasing the feature-pairs coverage at the same time. Conclusion: Our experimental evaluation concluded that optimal time-aware minimization of pairwise-covering test configurations is efficiently addressed using constraint programming techniques. },
  doi      = {https://doi.org/10.1016/j.infsof.2015.11.007},
  keywords = {Constraint programming,Highly-configurable software systems,Variability testing},
  url      = {http://www.sciencedirect.com/science/article/pii/S0950584915002013},
}

@Article{Krishnan20131479,
  author   = {Krishnan, Sandeep and Strasburg, Chris and Lutz, Robyn R and Goseva-Popstojanova, Katerina and Dorman, Karin S},
  title    = {{Predicting failure-proneness in an evolving software product line}},
  journal  = {Information and Software Technology},
  year     = {2013},
  volume   = {55},
  number   = {8},
  pages    = {1479--1495},
  issn     = {0950-5849},
  abstract = {AbstractContext Previous work by researchers on 3 years of early data for an Eclipse product has identified some predictors of failure-prone files that work well. Eclipse has also been used previously by researchers to study characteristics of product line software. Objective The work reported here investigates whether classification-based prediction of failure-prone files improves as the product line evolves. Method This investigation first repeats, to the extent possible, the previous study and then extends it by including four more recent years of data, comparing the prominent predictors with the previous results. The research then looks at the data for three additional Eclipse products as they evolve over time. The analysis compares results from three different types of datasets with alternative data collection and prediction periods. Results Our experiments with a variety of learners show that the difference between the performance of J48, used in this work, and the other top learners is not statistically significant. Furthermore, new results show that the effectiveness of classification significantly depends on the data collection period and prediction period. The study identifies change metrics that are prominent predictors across all four releases of all four products in the product line for the three different types of datasets. From the product line perspective, prediction of failure-prone files for the four products studied in the Eclipse product line shows statistically significant improvement in accuracy but not in recall across releases. Conclusion As the product line matures, the learner performance improves significantly for two of the three datasets, but not for prediction of post-release failure-prone files using only pre-release change data. This suggests that it may be difficult to detect failure-prone files in the evolving product line. At least in part, this may be due to the continuous change, even for commonalities and high-reuse variation components, which we previously have shown to exist. },
  doi      = {https://doi.org/10.1016/j.infsof.2012.11.008},
  keywords = {Change metrics,Failure-prone files,Post-release defects,Prediction,Reuse,Software product lines},
  url      = {http://www.sciencedirect.com/science/article/pii/S0950584912002340},
}

@Article{Karus20111161,
  author   = {Karus, Siim and Dumas, Marlon},
  title    = {{Predicting the maintainability of {\{}XSL{\}} transformations}},
  journal  = {Science of Computer Programming},
  year     = {2011},
  volume   = {76},
  number   = {12},
  pages    = {1161--1176},
  issn     = {0167-6423},
  abstract = {{\{}XSLT{\}} is a popular language for implementing both presentation templates in Web applications as well as document and message converters in enterprise applications. The widespread adoption and popularity of {\{}XSLT{\}} raises the challenge of efficiently managing the evolution of significant amounts of {\{}XSLT{\}} code. This challenge calls for guidelines and tool support for developing maintainable {\{}XSLT{\}} code. In this setting, this paper addresses the following question: Can the maintainability of {\{}XSL{\}} transformations, measured in terms of code churn in the next revision of a transformation, be predicted using a combination of simple metrics? This question is studied using a dataset extracted from open-source software project repositories. An outcome of this empirical study is a set of statistical models for predicting the maintainability of {\{}XSL{\}} transformations with relatively high accuracy. In addition, by analyzing the major influencers of code churn in these models, the paper identifies guidelines for designing {\{}XSL{\}} transformations with reduced future churn. },
  annote   = {Special Issue on Software Evolution, Adaptability and Variability},
  doi      = {https://doi.org/10.1016/j.scico.2010.12.006},
  keywords = {Software maintenance,Software metrics,XML,XSLT},
  url      = {http://www.sciencedirect.com/science/article/pii/S0167642310002315},
}

@Article{KAIYA20171141,
  author   = {Kaiya, Haruhiko and Sato, Ryohei and Hazeyama, Atsuo and Ogata, Shinpei and Okubo, Takao and Tanaka, Takafumi and Yoshioka, Nobukazu and Washizaki, Hironori},
  title    = {{Preliminary Systematic Literature Review of Software and Systems Traceability}},
  journal  = {Procedia Computer Science},
  year     = {2017},
  volume   = {112},
  pages    = {1141--1150},
  issn     = {1877-0509},
  abstract = {Traceability is important knowledge for improving the artifacts of software and systems and processes related to them. Even in a single system, various kinds of artifacts exist. Various kinds of processes also exist, and each of them relates to different kinds of artifacts. Traceability over them has thus large diversity. In addition, developers in each process have different types of purposes to improve their artifacts and process. Research results in traceability have to be categorized and analyzed so that such a developer can choose one of them to achieve his/her purposes. In this paper, we report on the results of Systematic Literature Review (SLR) related to software and systems traceability. Our SLR is preliminary one because we only analyzed articles in ACM digital library and IEEE computer society digital library. We found several interesting trends in traceability research. For example, researches related to creating or maintaining traceability are larger than those related to using it or thinking its strategy. Various kinds of traceability purposes are addressed or assumed in many researches, but some researches do not specify purposes. Purposes related to changes and updates are dominant.},
  annote   = {Knowledge-Based and Intelligent Information {\&} Engineering Systems: Proceedings of the 21st International Conference, KES-20176-8 September 2017, Marseille, France},
  doi      = {https://doi.org/10.1016/j.procs.2017.08.152},
  keywords = {ACM,IEEE CPS,Software,Systematic Literature Review,Systems Traceability},
  url      = {http://www.sciencedirect.com/science/article/pii/S1877050917315090},
}

@Article{Hunsen2016449,
  author   = {Hunsen, C and Zhang, B and Siegmund, J and K{\"{a}}stner, C and Le{\ss}enich, O and Becker, M and Apel, S},
  title    = {{Preprocessor-based variability in open-source and industrial software systems: An empirical study}},
  journal  = {Empirical Software Engineering},
  year     = {2016},
  volume   = {21},
  number   = {2},
  pages    = {449--482},
  abstract = {Almost every sufficiently complex software system today is configurable. Conditional compilation is a simple variability-implementation mechanism that is widely used in open-source projects and industry. Especially, the C preprocessor (CPP) is very popular in practice, but it is also gaining (again) interest in academia. Although there have been several attempts to understand and improve CPP, there is a lack of understanding of how it is used in open-source and industrial systems and whether different usage patterns have emerged. The background is that much research on configurable systems and product lines concentrates on open-source systems, simply because they are available for study in the first place. This leads to the potentially problematic situation that it is unclear whether the results obtained from these studies are transferable to industrial systems. We aim at lowering this gap by comparing the use of CPP in open-source projects and industry—especially from the embedded-systems domain—based on a substantial set of subject systems and well-known variability metrics, including size, scattering, and tangling metrics. A key result of our empirical study is that, regarding almost all aspects we studied, the analyzed open-source systems and the considered embedded systems from industry are similar regarding most metrics, including systems that have been developed in industry and made open source at some point. So, our study indicates that, regarding CPP as variability-implementation mechanism, insights, methods, and tools developed based on studies of open-source systems are transferable to industrial systems—at least, with respect to the metrics we considered. {\textcopyright} 2015, Springer Science+Business Media New York.},
  annote   = {cited By 1},
  doi      = {10.1007/s10664-015-9360-1},
  keywords = {C (programming language); Computer software; Embed,C preprocessor; Configurable systems; cppstats; I,Open systems},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84927927173{\&}doi=10.1007{\%}2Fs10664-015-9360-1{\&}partnerID=40{\&}md5=ccef9e8933e9d1d78ec680cab9cf1b1c},
}

@Article{Mariani2016157,
  author   = {Mariani, T and {Elita Colanzi}, T and {Regina Vergilio}, S},
  title    = {{Preserving architectural styles in the search based design of software product line architectures}},
  journal  = {Journal of Systems and Software},
  year     = {2016},
  volume   = {115},
  pages    = {157--173},
  abstract = {Architectural styles help to improve the Product Line Architecture (PLA) design by providing a better organization of its elements, which results in some benefits, like flexibility, extensibility and maintainability. The PLA design can also be improved by using a search based optimization approach, taking into account different metrics, such as cohesion, coupling and feature modularization. However, the application of search operators changes the PLA organization, and consequently may violate the architectural styles rules, impacting negatively in the architecture understanding. To overcome such limitation, this work introduces a set of search operators to be used in the search based design with the goal of preserving the architectural styles during the optimization process. Such operators consider rules of the layered and client/server architectural styles, generally used in the search based design of conventional architectures and PLAs. The operators are implemented and evaluated in the context of MOA4PLA, a Multi-objective Optimization Approach for PLA Design. Results from an empirical evaluation show that the proposed operators contribute to obtain better solutions, preserving the adopted style and also improving some software metric values. {\textcopyright} 2016 Elsevier Inc. All rights reserved.},
  annote   = {cited By 0},
  doi      = {10.1016/j.jss.2016.01.039},
  keywords = {Architectural design,Architectural style; Design of softwares; Empiric,Architecture; Computer software; Design; Modular c},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959319935{\&}doi=10.1016{\%}2Fj.jss.2016.01.039{\&}partnerID=40{\&}md5=26a38167189dc84b18847bdde4d16b12},
}

@Article{Wallin2012686,
  author   = {Wallin, Peter and Larsson, Stig and Fr{\"{o}}berg, Joakim and Axelsson, Jakob},
  title    = {{Problems and their mitigation in system and software architecting}},
  journal  = {Information and Software Technology},
  year     = {2012},
  volume   = {54},
  number   = {7},
  pages    = {686--700},
  issn     = {0950-5849},
  abstract = {Context Today, software and embedded systems act as enablers for developing new functionality in traditional industries such as the automotive, process automation, and manufacturing automation domains. This differs from 25–30 years ago when these systems where based on electronics and electro-mechanical solutions. The architecture of the embedded system and of the software is important to ensure the qualities of these applications. However, the effort of designing and evolving the architecture is in practice often neglected during system development, whilst development efforts are centered on implementing new functionality. Objective We present problems and success factors that are central to the architectural development of software intensive systems in the domain of automotive and automation products as judged by practitioners. Method The method consisted of three steps. First, we used semi-structured interviews to collect data in an exploratory manner. As a second step, a survey based on problems extracted from the interview data was used to investigate the occurrence of these problems at a wider range of organizations. In order to identify and suggest how to mitigate the problems that were considered important, we finally performed root cause analysis workshops, and from these a number of success factors were elicited. Results A total of 21 problems have been identified based on the interview data, and these are related to the technical, organizational, project, and agreement processes. Based on the survey results, the following four problems were selected for a root cause analysis: (1) there is a lack of process for architecture development, (2) there is a lack of method or model to evaluate the business value when choosing the architecture, (3) there is a lack of clear long-term architectural strategy, and (4) processes and methods are less valued than knowledge and competence of individuals. Conclusion In conclusion, the following identified success factors are crucial components to be successful in developing software intensive systems: (1) define an architectural strategy, (2) implement a process for architectural work, (3) ensure authority for architects, (4) clarify the business impact of the architecture, and (5) optimize on the project portfolio level instead of optimizing each project. },
  doi      = {https://doi.org/10.1016/j.infsof.2012.01.004},
  keywords = {Embedded systems,Experience from practice,Success factors,System and software architecture},
  url      = {http://www.sciencedirect.com/science/article/pii/S0950584912000158},
}

@Article{Hanssen2008843,
  author        = {Hanssen, G K and F{\ae}gri, T E},
  title         = {{Process fusion: An industrial case study on agile software product line engineering}},
  journal       = {Journal of Systems and Software},
  year          = {2008},
  volume        = {81},
  number        = {6},
  pages         = {843--854},
  abstract      = {This paper presents a case study of a software product company that has successfully integrated practices from software product line engineering and agile software development. We show how practices from the two fields support the company's strategic and tactical ambitions, respectively. We also discuss how the company integrates strategic, tactical and operational processes to optimize collaboration and consequently improve its ability to meet market needs, opportunities and challenges. The findings from this study are relevant to software product companies seeking ways to balance agility and product management. The findings also contribute to research on industrializing software engineering. {\textcopyright} 2007 Elsevier Inc. All rights reserved.},
  annote        = {cited By 45},
  doi           = {10.1016/j.jss.2007.10.025},
  file          = {:Users/mac/ownCloud/Anita/SLR/REVISION DE JOURNALS/PAPERS{\_}D/SELECT DAVID/checked/analizados/casos de estudios analizados/3Process fusion- An industrial case study on agile software product line engineering.pdf:pdf},
  keywords      = {Decision theory,Operational processes,Optimization,Product design,Product management,Software engineering,Str,case study},
  mendeley-tags = {case study},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-42049088680{\&}doi=10.1016{\%}2Fj.jss.2007.10.025{\&}partnerID=40{\&}md5=70bc5b3b44f009b164aebff9608f2e82},
}

@Article{Gao2014405,
  author   = {Gao, X and Chen, Y and Ding, Z and Wang, M and Zhang, X and Yan, Z and Wen, L and Guo, Q and Chen, R},
  title    = {{Process model fragmentization, clustering and merging: An empirical study}},
  journal  = {Lecture Notes in Business Information Processing},
  year     = {2014},
  volume   = {171 LNBIP},
  pages    = {405--416},
  abstract = {Nowadays, it is common for an organization tomaintain thousands of business processes. Technologies that provide automatic management for such amount of models are required. The objective of this paper is to deal with the problem of process model fragmentization, clustering and merging for the consolidation of Office Automation (OA) systems in ChinaMobile Communications Corporation (CMCC). After investigating the structural statistics of real-life process model samples, we propose an approach, based on the refined process structure tree (RPST) and software product line (SPL), to automatically identify reusable process fragments and merge similar ones into master fragments. These fragments can, for example, be used to facilitate the (re)design of numerous process models. Special attention is paid to the empirical study and statistics from the experiment on a sample set of 37 real-life OA processes. Lesson learned and problems to be further considered are also proposed. {\textcopyright} Springer International Publishing Switzerland 2014.},
  annote   = {cited By 1},
  doi      = {10.1007/978-3-319-06257-0},
  keywords = {Automatic management; Clustering; Empirical studi,Enterprise resource management; Office automation,Merging},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904538233{\&}doi=10.1007{\%}2F978-3-319-06257-0{\&}partnerID=40{\&}md5=c3ba390ac1f0cde60cc030b31e040df4},
}

@Article{Fontana2014140,
  author   = {Fontana, Rafaela Mantovani and Fontana, Isabela Mantovani and {da Rosa Garbuio}, Paula Andrea and Reinehr, Sheila and Malucelli, Andreia},
  title    = {{Processes versus people: How should agile software development maturity be defined?}},
  journal  = {Journal of Systems and Software},
  year     = {2014},
  volume   = {97},
  pages    = {140--155},
  issn     = {0164-1212},
  abstract = {Abstract Maturity in software development is currently defined by models such as CMMI-DEV and ISO/IEC 15504, which emphasize the need to manage, establish, measure and optimize processes. Teams that develop software using these models are guided by defined, detailed processes. However, an increasing number of teams have been implementing agile software development methods that focus on people rather than processes. What, then, is maturity for these agile teams that focus less on detailed, defined processes? This is the question we sought to answer in this study. To this end, we asked agile practitioners about their perception of the maturity level of a number of practices and how they defined maturity in agile software development. We used cluster analysis to analyze quantitative data and triangulated the results with content analysis of the qualitative data. We then proposed a new definition for agile software development maturity. The findings show that practitioners do not see maturity in agile software development as process definition or quantitative management capabilities. Rather, agile maturity means fostering more subjective capabilities, such as collaboration, communication, commitment, care, sharing and self-organization. },
  doi      = {https://doi.org/10.1016/j.jss.2014.07.030},
  keywords = {Agile software development,Maturity,Software process improvement},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121214001587},
}

@Article{DeSouza2015,
  author    = {{De Souza}, Leandro Oliveira and O'Leary, P{\'{a}}draig and {De Almeida}, Eduardo Santana and {De Lemos Meira}, S{\'{i}}lvio Romero},
  title     = {{Product derivation in practice}},
  journal   = {Information and Software Technology},
  year      = {2015},
  volume    = {58},
  pages     = {319--337},
  issn      = {09505849},
  abstract  = {Context: The process of constructing a product from a product line of software assets is known product derivation. An effective product derivation process is important in order to ensure that the efforts required to develop these shared assets is lower than the benefits achieved through their use. Despite its importance, relatively little work has been dedicated to the product derivation process and the strategies applied in practice. Additionally, there is a lack of empirical reports describing product derivation in industrial settings, and, in general, where these reports are available, they have been conducted as informal studies. Objective: Our aim is to investigate how product derivation is performed in practice. Method: We apply a multi-case study design to two different industrial software product line projects with the goal of investigating how they derive their products in practice. The findings from our studies were individually analyzed using the Constant Comparison technique. In order to identify patterns across these studies, the findings were compared using a Cross-case analysis approach. Results: The research approach allowed us to examine the case study outcomes from different perspectives, capturing similarities and differences. From the cases, we identified context specific strategies for product derivation which are easier for practitioners to contextualise and implement. Conclusions: The case studies provide method-in-action insights into concepts explored in the literature, such as: iterative and incremental product derivation, instantiation and integration of platform components and derivation of product databases. Practitioners can use this work as a basis for defining, adapting or evaluating their own product derivation approaches. While researchers can use this work as a starting point for new industrial reports, presenting their experiences with product derivation.},
  doi       = {10.1016/j.infsof.2014.07.004},
  file      = {:Users/mac/ownCloud/Anita/SLR/REVISION DE JOURNALS/PAPERS{\_}D/SELECT DAVID/checked/analizados/casos de estudios analizados/22 Product derivation in practice.pdf:pdf},
  keywords  = {Case study,Constant comparison analysis,Cross-case analysis,Multiple case study,Product derivation,Software product lines},
  publisher = {Elsevier B.V.},
  url       = {http://dx.doi.org/10.1016/j.infsof.2014.07.004},
}

@Article{Deelstra2005173,
  author   = {Deelstra, S and Sinnema, M and Bosch, J},
  title    = {{Product derivation in software product families: A case study}},
  journal  = {Journal of Systems and Software},
  year     = {2005},
  volume   = {74},
  number   = {2 SPEC. ISS.},
  pages    = {173--194},
  abstract = {From our experience with several organizations that employ software product families, we have learned that, contrary to popular belief, deriving individual products from shared software assets is a time-consuming and expensive activity. In this paper we therefore present a study that investigated the source of those problems. We provide the reader with a framework of terminology and concepts regarding product derivation. In addition, we present several problems and issues we identified during a case study at two large industrial organizations that are relevant to other, for example, comparable or less mature organizations. {\textcopyright} 2003 Elsevier Inc. All rights reserved.},
  annote   = {cited By 135},
  doi      = {10.1016/j.jss.2003.11.012},
  file     = {:Users/mac/Library/Application Support/Mendeley Desktop/Downloaded/Deelstra, Sinnema, Bosch - 2005 - Product derivation in software product families A case study.pdf:pdf},
  keywords = {Case study; Product derivation; Software product,Costs; Interfaces (computer); Investments; Problem,Software engineering},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-6444234242{\&}doi=10.1016{\%}2Fj.jss.2003.11.012{\&}partnerID=40{\&}md5=31f3032b01263f5e37e93d65ed64b827},
}

@Article{Bosch2001147,
  author        = {Bosch, J and H{\"{o}}gstr{\"{o}}m, M},
  title         = {{Product instantiation in software product lines: A case study}},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year          = {2001},
  volume        = {2177},
  pages         = {147--161},
  abstract      = {Product instantiation is one of the less frequently studied activities in the domain of software product lines. In this paper, we present the results of a case study at Axis Communication AB on product instantiation in an industrial product line, i.e. five problems and three issues. The problems are concerned the insufficiency of functional commonality, features spanning multiple components, the exclusion of unwanted features, the evolution of product line components and the handling of initialization code. The issues discuss architectural compliance versus product instantiation effort, quick-fixes versus properly engineered extensions and component instantiation support versus product instantiation effort. The identified problems and issues are based on the case study, but have been generalized to apply to a wider context. {\textcopyright} Springer-Verlag Berlin Heidelberg 2001.},
  annote        = {cited By 9},
  keywords      = {Architectural compliances,Computer software,Industrial product,Mu,Software engineering,case study},
  mendeley-tags = {case study},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-33646842621{\&}partnerID=40{\&}md5=d15c360028691650e4973473f5a8fd3f},
}

@Article{Oberweis2007909,
  author   = {Oberweis, Andreas and Pankratius, Victor and Stucky, Wolffried},
  title    = {{Product lines for digital information products}},
  journal  = {Information Systems},
  year     = {2007},
  volume   = {32},
  number   = {6},
  pages    = {909--939},
  issn     = {0306-4379},
  abstract = {The growth of the Web has fueled the creation, storage, and exchange of digital information products (DIPs), whose main purpose is the delivery of information, entertainment, education, or training. Very often, after their initial creation, the growing amount of content also leads to a more complicated maintenance, since updates typically occur rather often and the potential variability of modifications is not limited in advance. Moreover, commonalities between different parts of similar information products are not exploited, which often leads to redundancy. At the moment, there is hardly any attempt to compose information products from different sources and to produce more complex information products in a coordinated way. To help remedy this situation, this paper introduces the Product Lines for digitAl iNformation producTs (PLANT) approach, which applies the concept of software product lines to DIPs. The {\{}PLANT{\}} approach explicitly manages the commonalities of similar {\{}DIPs{\}} by defining common requirements, limiting variability in advance, as well as planning and coordinating reuse. This article focuses on the modeling of such product lines. In particular, the developed general concepts will be exemplified throughout the paper in the area of e-learning, in which {\{}DIPs{\}} play an important role. The application of {\{}PLANT{\}} in other areas and an implemented tool that supports the creation of information products in a product line are outlined as well. },
  doi      = {https://doi.org/10.1016/j.is.2006.09.003},
  keywords = {Digital information products,Digital products,Feature models,Software product lines,Workflow management,e-learning},
  url      = {http://www.sciencedirect.com/science/article/pii/S0306437906000809},
}

@Article{Purao:2003:PMO:857076.857090,
  author    = {Purao, Sandeep and Vaishnavi, Vijay},
  title     = {{Product Metrics for Object-oriented Systems}},
  journal   = {ACM Comput. Surv.},
  year      = {2003},
  volume    = {35},
  number    = {2},
  pages     = {191--221},
  issn      = {0360-0300},
  address   = {New York, NY, USA},
  doi       = {10.1145/857076.857090},
  keywords  = {Software metrics,measurement theory,object-oriented metrics,object-oriented product metrics,object-oriented systems},
  publisher = {ACM},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/857076.857090},
}

@Article{Fontana201588,
  author   = {Fontana, Rafaela Mantovani and Jr., Victor Meyer and Reinehr, Sheila and Malucelli, Andreia},
  title    = {{Progressive Outcomes: A framework for maturing in agile software development}},
  journal  = {Journal of Systems and Software},
  year     = {2015},
  volume   = {102},
  pages    = {88--108},
  issn     = {0164-1212},
  abstract = {Abstract Maturity models are used to guide improvements in the software engineering field and a number of maturity models for agile methods have been proposed in the last years. These models differ in their underlying structure prescribing different possible paths to maturity in agile software development, neglecting the fact that agile teams struggle to follow prescribed processes and practices. Our objective, therefore, was to empirically investigate how agile teams evolve to maturity, as a means to conceive a theory for agile software development evolvement that considers agile teams nature. The complex adaptive systems theory was used as a lens for analysis and four case studies were conducted to collect qualitative and quantitative data. As a result, we propose the Progressive Outcomes framework to describe the agile software development maturing process. It is a framework in which people have the central role, ambidexterity is a key ability to maturity, and improvement is guided by outcomes agile teams pursue, instead of prescribed practices. },
  doi      = {https://doi.org/10.1016/j.jss.2014.12.032},
  keywords = {Agile software development,Ambidexterity,Complex adaptive systems,Maturity,Software process improvement},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121214002908},
}

@Article{Walkingshaw:2014:PEV:2775053.2658766,
  author    = {Walkingshaw, Eric and Ostermann, Klaus},
  title     = {{Projectional Editing of Variational Software}},
  journal   = {SIGPLAN Not.},
  year      = {2014},
  volume    = {50},
  number    = {3},
  pages     = {29--38},
  issn      = {0362-1340},
  address   = {New York, NY, USA},
  doi       = {10.1145/2775053.2658766},
  keywords  = {bidirectional transformations,projectional editing,software product lines,variation,view-update problem},
  publisher = {ACM},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/2775053.2658766},
}

@Article{Weidlich20121885,
  author   = {Weidlich, Matthias and Mendling, Jan and Weske, Mathias},
  title    = {{Propagating changes between aligned process models}},
  journal  = {Journal of Systems and Software},
  year     = {2012},
  volume   = {85},
  number   = {8},
  pages    = {1885--1898},
  issn     = {0164-1212},
  abstract = {There is a wide variety of drivers for business process modelling initiatives, reaching from organisational redesign to the development of information systems. Consequently, a common business process is often captured in multiple models that overlap in content due to serving different purposes. Business process management aims at flexible adaptation to changing business needs. Hence, changes of business processes occur frequently and have to be incorporated in the respective process models. Once a process model is changed, related process models have to be updated accordingly, despite the fact that those process models may only be loosely coupled. In this article, we introduce an approach that supports change propagation between related process models. Given a change in one process model, we leverage the behavioural abstraction of behavioural profiles for corresponding activities in order to determine a change region in another model. Our approach is able to cope with changes in pairs of models that are not related by hierarchical refinement and show behavioural inconsistencies. We evaluate the applicability of our approach with two real-world process model collections. To this end, we either deduce change operations from different model revisions or rely on synthetic change operations. },
  doi      = {https://doi.org/10.1016/j.jss.2012.02.044},
  keywords = {Behavioural analysis,Change propagation,Model synchronisation,Process model alignment},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121212000672},
}

@Article{CETINA20132399,
  author   = {Cetina, Carlos and Giner, Pau and Fons, Joan and Pelechano, Vicente},
  title    = {{Prototyping Dynamic Software Product Lines to evaluate run-time reconfigurations}},
  journal  = {Science of Computer Programming},
  year     = {2013},
  volume   = {78},
  number   = {12},
  pages    = {2399--2413},
  issn     = {0167-6423},
  abstract = {Dynamic Software Product Lines (DSPL) encompass systems that are capable of modifying their own behavior with respect to changes in their operating environment by using run-time reconfigurations. A failure in these reconfigurations can directly impact the user experience since the reconfigurations are performed when the system is already under the users control. In this work, we prototype a Smart Hotel DSPL to evaluate the reliability-based risk of the DSPL reconfigurations, specifically, the probability of malfunctioning (Availability) and the consequences of malfunctioning (Severity). This DSPL prototype was performed with the participation of human subjects by means of a Smart Hotel case study which was deployed with real devices. Moreover, we successfully identified and addressed two challenges associated with the involvement of human subjects in DSPL prototyping: enabling participants to (1) trigger the run-time reconfigurations and to (2) understand the effects of the reconfigurations. The evaluation of the case study reveals positive results regarding both Availability and Severity. However, the participant feedback highlights issues with recovering from a failed reconfiguration or a reconfiguration triggered by mistake. To address these issues, we discuss some guidelines learned in the case study. Finally, although the results achieved by the DSPL may be considered satisfactory for its particular domain, DSPL engineers must provide users with more control over the reconfigurations or the users will not be comfortable with DSPLs.},
  annote   = {Special Section on International Software Product Line Conference 2010 and Fundamentals of Software Engineering (selected papers of FSEN 2011)},
  doi      = {https://doi.org/10.1016/j.scico.2012.06.007},
  keywords = {Dynamic Software Product Line,Smart Hotel,Variability modeling},
  url      = {http://www.sciencedirect.com/science/article/pii/S0167642312001189},
}

@Article{PérezLamancha20151,
  author   = {Lamancha, Beatriz P{\'{e}}rez and Polo, Macario and Piattini, Mario},
  title    = {{PROW: A Pairwise algorithm with constRaints, Order and Weight}},
  journal  = {Journal of Systems and Software},
  year     = {2015},
  volume   = {99},
  pages    = {1--19},
  issn     = {0164-1212},
  abstract = {Abstract Testing systems with many variables and/or values is often quite expensive due to the huge number of possible combinations to be tested. There are several criteria available to combine test data and produce scalable test suites. One of them is pairwise. With the pairwise criterion, each pair of values of any two parameters is included in at least one test case. Although this is a widely-used coverage criterion, two main characteristics improve considerably pairwise: constraints handling and prioritisation. This paper presents an algorithm and a tool. The algorithm (called PROW: Pairwise with constRaints, Order and Weight) handles constraints and prioritisation for pairwise coverage. The tool called {\{}CTWeb{\}} adds functionalities to execute {\{}PROW{\}} in different contexts, one of them is product sampling in Software Product Lines via importing feature models. Software Product Line (SPL) development is a recent paradigm, where a family of software systems is constructed by means of the reuse of a set of common functionalities and some variable functionalities. An essential artefact of a {\{}SPL{\}} is the feature model, which shows the features offered by the product line, jointly with the relationships (includes and excludes) among them. Pairwise testing could be used to obtain the product sampling to test in a SPL, using features as pairwise parameters. In this context, the constraint handling becomes essential. As a difference with respect to other tools, {\{}CTWeb{\}} does not require {\{}SAT{\}} solvers. This paper describes the {\{}PROW{\}} algorithm, also analysing its complexity and efficiency. The {\{}CTWeb{\}} tool is presented, including two examples of the {\{}PROW{\}} application to two real environments: the first corresponds to the migration of the subsystem of transactions processing of a credit card management system from {\{}AS400{\}} to Oracle with .NET; the second applies both the algorithm and the tool to a {\{}SPL{\}} that monitors and controls some parameters of the load in trucks. },
  doi      = {https://doi.org/10.1016/j.jss.2014.08.005},
  keywords = {Combinatorial testing,Software testing},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121214001733},
}

@Article{Menascé2007646,
  author   = {Menasc{\'{e}}, Daniel A and Ruan, Honglei and Gomaa, Hassan},
  title    = {{QoS management in service-oriented architectures}},
  journal  = {Performance Evaluation},
  year     = {2007},
  volume   = {64},
  number   = {7–8},
  pages    = {646--663},
  issn     = {0166-5316},
  abstract = {The next generation of software systems will be highly distributed, component-based and service-oriented. They will need to operate in unattended mode and possibly in hostile environments, will be composed of a large number of ‘replaceable' components discoverable at run-time, and will have to run on a multitude of unknown and heterogeneous hardware and network platforms. This paper focuses on QoS management in service-oriented architectures in which service providers (SP) provide a set of interrelated services to service consumers, and a QoS broker mediates QoS negotiations between {\{}SPs{\}} and consumers. The main contributions of this paper are: (i) the description of an architecture that includes a QoS broker and service provider software components, (ii) the specification of a secure protocol for QoS negotiation with the support of a QoS broker, (iii) the specification of an admission control mechanism used by SPs, (iv) a report on the implementation of the QoS broker and SPs, and (v) the experimental validation of the ideas presented in the paper. },
  doi      = {https://doi.org/10.1016/j.peva.2006.10.001},
  keywords = {Performance,QoS,QoS broker,Service oriented architectures},
  url      = {http://www.sciencedirect.com/science/article/pii/S0166531606000940},
}

@Article{Axelsson201669,
  author   = {Axelsson, Jakob and Skoglund, Mats},
  title    = {{Quality assurance in software ecosystems: A systematic literature mapping and research agenda}},
  journal  = {Journal of Systems and Software},
  year     = {2016},
  volume   = {114},
  pages    = {69--81},
  issn     = {0164-1212},
  abstract = {Abstract Software ecosystems are becoming a common model for software development in which different actors cooperate around a shared platform. However, it is not clear what the implications are on software quality when moving from a traditional approach to an ecosystem, and this is becoming increasingly important as ecosystems emerge in critical domains such as embedded applications. Therefore, this paper investigates the challenges related to quality assurance in software ecosystems, and identifies what approaches have been proposed in the literature. The research method used is a systematic literature mapping, which however only resulted in a small set of six papers. The literature findings are complemented with a constructive approach where areas are identified that merit further research, resulting in a set of research topics that form a research agenda for quality assurance in software ecosystems. The agenda spans the entire system life-cycle, and focuses on challenges particular to an ecosystem setting, which are mainly the results of the interactions across organizational borders, and the dynamic system integration being controlled by the users. },
  doi      = {https://doi.org/10.1016/j.jss.2015.12.020},
  keywords = {Quality,Software ecosystems,Testing,Verification},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121215002861},
}

@Article{Zhang2014365,
  author   = {Zhang, G and Ye, H and Lin, Y},
  title    = {{Quality attribute modeling and quality aware product configuration in software product lines}},
  journal  = {Software Quality Journal},
  year     = {2014},
  volume   = {22},
  number   = {3},
  pages    = {365--401},
  abstract = {In software product line engineering, the customers mostly concentrate on the functionalities of the target product during product configuration. The quality attributes of a target product, such as security and performance, are often assessed until the final product is generated. However, it might be very costly to fix the problem if it is found that the generated product cannot satisfy the customers' quality requirements. Although the quality of a generated product will be affected by all the life cycles of product development, feature-based product configuration is the first stage where the estimation or prediction of the quality attributes should be considered. As we know, the key issue of predicting the quality attributes for a product configured from feature models is to measure the interdependencies between functional features and quality attributes. The current existing approaches have several limitations on this issue, such as requiring real products for the measurement or involving domain experts' efforts. To overcome these limitations, we propose a systematic approach of modeling quality attributes in feature models based on domain experts' judgments using the analytic hierarchical process (AHP) and conducting quality aware product configuration based on the captured quality knowledge. Domain experts' judgments are adapted to avoid generating the real products for quality evaluation, and AHP is used to reduce domain experts' efforts involved in the judgments. A prototype tool is developed to implement the concepts of the proposed approach, and a formal evaluation is carried out based on a large-scale case study. {\textcopyright} 2013, Springer Science+Business Media New York.},
  annote   = {cited By 14},
  doi      = {10.1007/s11219-013-9197-z},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875049200{\&}doi=10.1007{\%}2Fs11219-013-9197-z{\&}partnerID=40{\&}md5=ad822ef7f6b285eb153f76658a503708},
}

@Article{ZelaRuiz2016113,
  author        = {Ruiz, Jael Zela and Rubira, Cec{\'{i}}lia M},
  title         = {{Quality of Service Conflict During Web Service Monitoring: A Case Study}},
  journal       = {Electronic Notes in Theoretical Computer Science},
  year          = {2016},
  volume        = {321},
  pages         = {113--127},
  issn          = {1571-0661},
  abstract      = {Abstract Web services have become one of the most used technologies in service-oriented systems. Its popularity is due to its property to adapt to any context. As a consequence of the increasing number of Web services on the Internet and its important role in many applications today, Web service quality has become a crucial requirement and demanded by service consumers. Terms of quality levels are written between service providers and service consumers to ensure a degree of quality. The use of monitoring tools to control service quality levels is very important. Quality attributes suffer variations in their values during runtime, this is produced by many factors such as a memory leak, deadlock, race data, inconsistent data, etc. However, sometimes monitoring tools can impact negatively affecting the quality of service when they are not properly used and configured, producing possible conflicts between quality attributes. This paper aims to show the impact of monitoring tools over service quality, two of the most important quality attributes – performance and accuracy – were chosen to be monitored. A case study is conducted to present and evaluate the relationship between performance and accuracy over a Web service. As a result, conflict is found between performance and accuracy, where performance was the most affected, because it presented a degradation in its quality level during monitoring.},
  annote        = {{\{}CLEI{\}} 2015, the {\{}XLI{\}} Latin American Computing Conference},
  doi           = {https://doi.org/10.1016/j.entcs.2016.02.007},
  keywords      = {Accuracy,Conflict,Monitoring Tools,Performance,Quality Attributes,Quality of Service,SOA,Web Services,case study},
  mendeley-tags = {case study},
  url           = {http://www.sciencedirect.com/science/article/pii/S1571066116300081},
}

@Article{Kulk2008136,
  author   = {Kulk, G P and Verhoef, C},
  title    = {{Quantifying requirements volatility effects}},
  journal  = {Science of Computer Programming},
  year     = {2008},
  volume   = {72},
  number   = {3},
  pages    = {136--175},
  issn     = {0167-6423},
  abstract = {In an organization operating in the bancassurance sector we identified a low-risk {\{}IT{\}} subportfolio of 84 {\{}IT{\}} projects comprising together 16,500 function points, each project varying in size and duration, for which we were able to quantify its requirements volatility. This representative portfolio stems from a much larger portfolio of {\{}IT{\}} projects. We calculated the volatility from the function point countings that were available to us. These figures were aggregated into a requirements volatility benchmark. We found that maximum requirements volatility rates depend on size and duration, which refutes currently known industrial averages. For instance, a monthly growth rate of 5{\%} is considered a critical failure factor, but in our low-risk portfolio we found more than 21{\%} of successful projects with a volatility larger than 5{\%}. We proposed a mathematical model taking size and duration into account that provides a maximum healthy volatility rate that is more in line with the reality of low-risk {\{}IT{\}} portfolios. Based on the model, we proposed a tolerance factor expressing the maximal volatility tolerance for a project or portfolio. For a low-risk portfolio its empirically found tolerance is apparently acceptable, and values exceeding this tolerance are used to trigger {\{}IT{\}} decision makers. We derived two volatility ratios from this model, the $\pi$ -ratio and the $\rho$ -ratio. These ratios express how close the volatility of a project has approached the danger zone when requirements volatility reaches a critical failure rate. The volatility data of a governmental {\{}IT{\}} portfolio were juxtaposed to our bancassurance benchmark, immediately exposing a problematic project, which was corroborated by its actual failure. When function points are less common, e.g. in the embedded industry, we used daily source code size measures and illustrated how to govern the volatility of a software product line of a hardware manufacturer. With the three real-world portfolios we illustrated that our results serve the purpose of an early warning system for projects that are bound to fail due to excessive volatility. Moreover, we developed essential requirements volatility metrics that belong on an {\{}IT{\}} governance dashboard and presented such a volatility dashboard. },
  doi      = {https://doi.org/10.1016/j.scico.2008.04.003},
  keywords = {Compound monthly growth rate,IT dashboard,IT portfolio management,Quantitative {\{}IT{\}} portfolio management,Requirements churn,Requirements creep,Requirements metric,Requirements scrap,Requirements volatility,Requirements volatility dashboard,Scope creep,Volatility benchmark,Volatility tolerance factor,$\pi$ -ratio,$\rho$ -ratio},
  url      = {http://www.sciencedirect.com/science/article/pii/S0167642308000464},
}

@Article{Sobernig20161670,
  author   = {Sobernig, S and Apel, S and Kolesnikov, S and Siegmund, N},
  title    = {{Quantifying structural attributes of system decompositions in 28 feature-oriented software product lines: An exploratory study}},
  journal  = {Empirical Software Engineering},
  year     = {2016},
  volume   = {21},
  number   = {4},
  pages    = {1670--1705},
  abstract = {A key idea of feature orientation is to decompose a software product line along the features it provides. Feature decomposition is orthogonal to object-oriented decomposition—it crosscuts the underlying package and class structure. It has been argued often that feature decomposition improves system structure by reducing coupling and by increasing cohesion. However, recent empirical findings suggest that this is not necessarily the case. In this exploratory, observational study, we investigate the decompositions of 28 feature-oriented software product lines into classes, features, and feature-specific class fragments. The product lines under investigation are implemented using the feature-oriented programming language Fuji. In particular, we quantify and compare the internal attributes import coupling and cohesion of the different product-line decompositions in a systematic, reproducible manner. For this purpose, we adopt three established software measures (e.g., coupling between units, CBU; internal-ratio unit dependency, IUD) as well as standard concentration statistics (e.g., Gini coefficient). In our study, we found that feature decomposition can be associated with higher levels of structural coupling in a product line than a decomposition into classes. Although coupling can be concentrated in very few features in most feature decompositions, there are not necessarily hot-spot features in all product lines. Interestingly, feature cohesion is not necessarily higher than class cohesion, whereas features are more equal in serving dependencies internally than classes of a product line. Our empirical study raises critical questions about alleged advantages of feature decomposition. At the same time, we demonstrate how our measurement approach of coupling and cohesion has potential to support static and dynamic analyses of software product lines (i.e., type checking and feature-interaction detection) by facilitating product sampling. {\textcopyright} 2014, Springer Science+Business Media New York.},
  annote   = {cited By 0},
  doi      = {10.1007/s10664-014-9336-6},
  keywords = {Adhesion; Computer programming languages; Computer,Feature-oriented programming; Fuji; Software Meas,Object oriented programming},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908355432{\&}doi=10.1007{\%}2Fs10664-014-9336-6{\&}partnerID=40{\&}md5=cc1eb006ed351a5be2ec4619fb3e256f},
}

@Article{Verhoef2007247,
  author   = {Verhoef, C},
  title    = {{Quantifying the effects of IT-governance rules}},
  journal  = {Science of Computer Programming},
  year     = {2007},
  volume   = {67},
  number   = {2–3},
  pages    = {247--277},
  issn     = {0167-6423},
  abstract = {Via quantitative analyses of large IT-portfolio databases, we detected unique data patterns pointing to certain IT-governance rules and styles, plus their sometimes nonintuitive and negative side-effects. We grouped the most important patterns in seven categories and highlighted them separately. These patterns relate to the five fundamental parameters for IT-governance: data, control, time, cost and functionality. We revealed patterns of overperfect and heterogeneous data signifying reporting anomalies or ambiguous IT-governance rules, respectively. We also detected patterns of overregulation and underregulation, portending bloated control or no IT-control at all, both with negative side-effects: productivity loss, and too costly IT-development. Uniform management on time, cost or functionality showed clear patterns in the time and cost case, and more diffuse combined patterns for functionality. For these in total seven types of patterns, it was possible to take corrective measures to reduce unwanted side-effects, and/or amplify the intended purpose of the underlying IT-governance rules. These modifications ranged from refinements and additions, to eradications of IT-governance rules. For each of the seven patterns we provided lessons learned and recommendations on how to recognize and remove unwanted effects. Some effects were dangerous, and addressing them led to significant risk reduction and cost savings. },
  doi      = {https://doi.org/10.1016/j.scico.2007.01.010},
  keywords = {Heterogeneous data,IT-governance,IT-governance rules,IT-portfolio analysis,Managing on budget,Managing on functionality,Managing on time,Overperfect data,Overregulation,Quantitative IT-governance,Seasonality effects,Time compression,Time decompression,Underregulation},
  url      = {http://www.sciencedirect.com/science/article/pii/S0167642307000780},
}

@Article{Albuquerque2015245,
  author   = {Albuquerque, Diego and Cafeo, Bruno and Garcia, Alessandro and Barbosa, Simone and Abrah{\~{a}}o, Silvia and Ribeiro, Ant{\'{o}}nio},
  title    = {{Quantifying usability of domain-specific languages: An empirical study on software maintenance}},
  journal  = {Journal of Systems and Software},
  year     = {2015},
  volume   = {101},
  pages    = {245--259},
  issn     = {0164-1212},
  abstract = {Abstract A domain-specific language (DSL) aims to support software development by offering abstractions to a particular domain. It is expected that {\{}DSLs{\}} improve the maintainability of artifacts otherwise produced with general-purpose languages. However, the maintainability of the {\{}DSL{\}} artifacts and, hence, their adoption in mainstream development, is largely dependent on the usability of the language itself. Unfortunately, it is often hard to identify their usability strengths and weaknesses early, as there is no guidance on how to objectively reveal them. Usability is a multi-faceted quality characteristic, which is challenging to quantify beforehand by {\{}DSL{\}} stakeholders. There is even less support on how to quantitatively evaluate the usability of {\{}DSLs{\}} used in maintenance tasks. In this context, this paper reports a study to compare the usability of textual {\{}DSLs{\}} under the perspective of software maintenance. A usability measurement framework was developed based on the cognitive dimensions of notations. The framework was evaluated both qualitatively and quantitatively using two {\{}DSLs{\}} in the context of two evolving object-oriented systems. The results suggested that the proposed metrics were useful: (1) to early identify {\{}DSL{\}} usability limitations, (2) to reveal specific {\{}DSL{\}} features favoring maintenance tasks, and (3) to successfully analyze eight critical {\{}DSL{\}} usability dimensions. },
  doi      = {https://doi.org/10.1016/j.jss.2014.11.051},
  keywords = {DSL,Metrics,Usability},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121214002799},
}

@Article{Pereira201666,
  author   = {Pereira, J A and Constantino, K and Figueiredo, E and Saake, G},
  title    = {{Quantitative and qualitative empirical analysis of three feature modeling tools}},
  journal  = {Communications in Computer and Information Science},
  year     = {2016},
  volume   = {703},
  pages    = {66--88},
  abstract = {During the last couple of decades, feature modeling tools have played a significant role in the improvement of software productivity and quality by assisting tasks in software product line (SPL). SPL decomposes a large-scale software system in terms of their functionalities. The goal of the decomposition is to create well-structured individual software systems that can meet different users' requirements. Thus, feature modeling tools provides means to manage the inter-dependencies among reusable common and variable functionalities, called features. There are several tools to support variability management by modeling features in SPL. The variety of tools in the current literature makes it difficult to understand what kinds of tasks are supported and how much effort can be reduced by using these tools. In this paper, we present the results of an empirical study aiming to support SPL engineers choosing the feature modeling tool that best fits their needs. This empirical study compares and analyzes three tools, namely SPLOT, FeatureIDE, and pure::variants. These tools are analyzed based on data from 119 participants. Each participant used one tool for typical feature modeling tasks, such as create a model, update a model, automated analysis of the model, and product configuration. Finally, analysis concerning the perceived ease of use, usefulness, effectiveness, and efficiency are presented. {\textcopyright} Springer International Publishing AG 2016.},
  annote   = {cited By 0},
  doi      = {10.1007/978-3-319-56390-9_4},
  keywords = {Computer software; Software design,Feature models; Featureide; Pure::variants; Softw,Software engineering},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019123685{\&}doi=10.1007{\%}2F978-3-319-56390-9{\_}4{\&}partnerID=40{\&}md5=4640e9e6abae4822bfe3de6c9a50a71d},
}

@Article{Caporuscio200618,
  author   = {Caporuscio, M and Muccini, H and Pelliccione, P and Nisio, E D},
  title    = {{Rapid system development via product line architecture implementation}},
  journal  = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year     = {2006},
  volume   = {3943 LNCS},
  pages    = {18--33},
  abstract = {Software Product Line (SPL) engineering allows designers to reason about an entire family of software applications, instead of a single product, with a strategic importance for the rapid development of new applications. While much effort has been spent so far in understanding and modeling SPLs and their architectures, very little attention has been given on how to systematically enforce SPL architectural decisions into the implementation step. In this paper we propose a methodological approach and an implementation framework, based on a plugin component-based development, which allows us to move from an architectural specification of the SPL to its implementation in a systematic way. We show the suitability of this framework through its application to the TOOL•one case study SPL. {\textcopyright} Springer-Verlag Berlin Heidelberg 2006.},
  annote   = {cited By 1},
  doi      = {10.1007/11751113_3},
  keywords = {Architectural decisions; Methodological approach;,Computer applications; Computer architecture; Comp,Rapid prototyping},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745842249{\&}doi=10.1007{\%}2F11751113{\_}3{\&}partnerID=40{\&}md5=ea3cd525b8637603f29f49e1046dd6ca},
}

@Article{Günther2012152,
  author   = {G{\"{u}}nther, Sebastian and Sunkle, Sagar},
  title    = {{rbFeatures: Feature-oriented programming with Ruby}},
  journal  = {Science of Computer Programming},
  year     = {2012},
  volume   = {77},
  number   = {3},
  pages    = {152--173},
  issn     = {0167-6423},
  abstract = {Features are pieces of core functionality of a program that is relevant to particular stakeholders. Features pose dependencies and constraints among each other. These dependencies and constraints describe the possible number of variants of the program: A valid feature configuration generates a specific variant with unique behavior. Feature-Oriented Programming is used to implement features as program units. This paper introduces rbFeatures, a feature-oriented programming language implemented on top of the dynamic programming language Ruby. With rbFeatures, programmers use software product lines, variants, and features as first-class entities. This allows several runtime reflection and modification capabilities, including the extension of the product line with new features and the provision of multiple variants. The paper gives a broad overview to the implementation and application of rbFeatures. We explain how features as first-class entities are designed and implemented, and discuss how the semantics of features are carefully added to Ruby programs. We show two case studies: The expression product line, a common example in feature-oriented programming, and a web application. },
  annote   = {Feature-Oriented Software Development (FOSD 2009)},
  doi      = {https://doi.org/10.1016/j.scico.2010.12.007},
  keywords = {Domain-specific languages,Dynamic programming languages,Feature-oriented programming},
  url      = {http://www.sciencedirect.com/science/article/pii/S0167642311000025},
}

@Article{Poort20121995,
  author   = {Poort, Eltjo R and van Vliet, Hans},
  title    = {{RCDA: Architecting as a risk- and cost management discipline}},
  journal  = {Journal of Systems and Software},
  year     = {2012},
  volume   = {85},
  number   = {9},
  pages    = {1995--2013},
  issn     = {0164-1212},
  abstract = {We propose to view architecting as a risk- and cost management discipline. This point of view helps architects identify the key concerns to address in their decision making, by providing a simple, relatively objective way to assess architectural significance. It also helps business stakeholders to align the architect's activities and results with their own goals. We examine the consequences of this point of view on the architecture process. The point of view is the basis of RCDA, the Risk- and Cost Driven Architecture approach. So far, more than 150 architects have received {\{}RCDA{\}} training. For a majority of the trainees, {\{}RCDA{\}} has a significant positive impact on their architecting work. },
  annote   = {Selected papers from the 2011 Joint Working IEEE/IFIP Conference on Software Architecture (WICSA 2011)},
  doi      = {https://doi.org/10.1016/j.jss.2012.03.071},
  keywords = {Cost management,Risk Management,Software architecture},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121212000994},
}

@Article{OLIVEIRA20071902,
  author   = {Oliveira, Toacy C and Alencar, Paulo S C and de Lucena, Carlos J P and Cowan, Donald D},
  title    = {{RDL: A language for framework instantiation representation}},
  journal  = {Journal of Systems and Software},
  year     = {2007},
  volume   = {80},
  number   = {11},
  pages    = {1902--1929},
  issn     = {0164-1212},
  abstract = {Reusing software artifacts for system development is showing increasing promise as an approach to reducing the time and effort involved in building new systems, and to improving the software development process and the quality of its outcome. However, software reuse has an associated steep learning curve, since practitioners must become familiar with a third party rationale for representing and implementing reusable assets. For this reason, enabling a systematic approach to the reuse process by making software reuse tasks explicit, allowing software frameworks to be instantiated using pre-defined primitive and complex reuse operations, and supporting the reuse process in a (semi-)automated way become crucial goals. In this paper, we present a systematic reuse approach and the Reuse Description Language (RDL), a language designed to specify object-oriented framework instantiation processes, and an RDL execution environment, which is the tool support for definition and execution of reuse processes and framework instantiations that lead to domain-specific applications. We illustrate our approach using DTFrame, a framework for creating drawing editors.},
  doi      = {https://doi.org/10.1016/j.jss.2007.01.005},
  keywords = {Model driven architecture,Object oriented framework,Product line architecture,Software reuse},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121207000106},
}

@Article{Bürdek2016687,
  author   = {B{\"{u}}rdek, J and Kehrer, T and Lochau, M and Reuling, D and Kelter, U and Sch{\"{u}}rr, A},
  title    = {{Reasoning about product-line evolution using complex feature model differences}},
  journal  = {Automated Software Engineering},
  year     = {2016},
  volume   = {23},
  number   = {4},
  pages    = {687--733},
  abstract = {Features define common and variable parts of the members of a (software) product line. Feature models are used to specify the set of all valid feature combinations. Feature models not only enjoy an intuitive tree-like graphical syntax, but also a precise formal semantics, which can be denoted as propositional formulae over Boolean feature variables. A product line usually constitutes a long-term investment and, therefore, has to undergo continuous evolution to meet ever-changing requirements. First of all, product-line evolution leads to changes of the feature model due to its central role in the product-line paradigm. As a result, product-line engineers are often faced with the problems that (1) feature models are changed in an ad-hoc manner without proper documentation, and (2) the semantic impact of feature diagram changes is unclear. In this article, we propose a comprehensive approach to tackle both challenges. For (1), our approach compares the old and new version of the diagram representation of a feature model and specifies the changes using complex edit operations on feature diagrams. In this way, feature model changes are automatically detected and formally documented. For (2), we propose an approach for reasoning about the semantic impact of diagram changes. We present a set of edit operations on feature diagrams, where complex operations are primarily derived from evolution scenarios observed in a real-world case study, i.e., a product line from the automation engineering domain. We evaluated our approach to demonstrate its applicability with respect to the case study, as well as its scalability concerning experimental data sets. {\textcopyright} 2015, Springer Science+Business Media New York.},
  annote   = {cited By 0},
  doi      = {10.1007/s10515-015-0185-3},
  keywords = {Automation engineering; Feature combination; Feat,Boolean algebra; Boolean functions; Formal methods,Software design},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944615031{\&}doi=10.1007{\%}2Fs10515-015-0185-3{\&}partnerID=40{\&}md5=dedba32d55a76719cbfda76e77866829},
}

@Article{Dey2017160,
  author   = {Dey, Sangeeta and Lee, Seok-Won},
  title    = {{REASSURE: Requirements elicitation for adaptive socio-technical systems using repertory grid}},
  journal  = {Information and Software Technology},
  year     = {2017},
  volume   = {87},
  pages    = {160--179},
  issn     = {0950-5849},
  abstract = {AbstractContext Socio-technical systems are expected to understand the dynamics of the execution environment and behave accordingly. Significant work has been done on formalizing and modeling requirements of such adaptive systems. However, not enough attention is paid on eliciting requirements from users and introducing flexibility in the system behavior at an early phase of requirements engineering. Most of the work is based on an assumption that general users' cognitive level would be able to support the inherent complexity of variability acquisition. Objective Our main focus is on providing help to the users with ordinary cognitive level to express their expectations from the complex system considering various contexts. This work also helps the designers to explore the design variability based on the general users' preferences. Method We explore the idea of using a cognitive technique Repertory Grid (RG) to acquire knowledge from users and experts along multiple dimensions of problem and design space. We propose {\{}REASSURE{\}} methodology which guides requirements engineers to explore the intentional and design variability in an organized way. We also provide a tool support to analyze the knowledge captured in multiple repertory grid files and detect potential conflicts in the intentional variability. Finally, we evaluate the proposed idea by performing an empirical study using smart home system domain. Results The result of our study shows that a greater number of requirements can be elicited after applying our approach. With the help of the provided tool support, it is even possible to detect a greater number of conflicts in user's requirements than the traditional practices. Conclusion We envision {\{}RG{\}} as a technique to filter design options based on the intentional variability in various contexts. The promising results of empirical study open up new research questions: “how to elicit requirements from multiple stakeholders and reach consensus for multi-dimensional problem domain”. },
  doi      = {https://doi.org/10.1016/j.infsof.2017.03.004},
  file     = {:Users/mac/ownCloud/Anita/SLR/REVISION DE JOURNALS/PAPERS{\_}D/SELECT DAVID/checked/analizados/casos de estudios analizados/14 REASSURE- Requirements elicitation for adaptive socio-technical systems using repertory grid.pdf:pdf},
  keywords = {Adaptive systems,Repertory grid,Requirements elicitation,Socio-technical systems},
  url      = {http://www.sciencedirect.com/science/article/pii/S095058491730229X},
}

@Article{Navas20131073,
  author   = {Navas, Juan F and Babau, Jean-Philippe and Pulou, Jacques},
  title    = {{Reconciling run-time evolution and resource-constrained embedded systems through a component-based development framework}},
  journal  = {Science of Computer Programming},
  year     = {2013},
  volume   = {78},
  number   = {8},
  pages    = {1073--1098},
  issn     = {0167-6423},
  abstract = {This paper deals with the evolution of embedded systems software at run-time. To accomplish such software evolution activities in resource-constrained embedded systems, we propose a component-based, execution time evolution infrastructure, that reconciles richness of evolution alternatives and performance requirements. The proposition is based on fine-grained optimization of embedded components, and on off-site component reifications called mirrors, which are representations of components that allow us to treat evolution concerns remotely and hence to reduce the memory footprint. An evaluation on a real-world evolution scenario shows the efficiency and relevance of our approach. },
  annote   = {Special section on software evolution, adaptability, and maintenance {\&} Special section on the Brazilian Symposium on Programming Languages},
  doi      = {https://doi.org/10.1016/j.scico.2012.08.004},
  keywords = {Architecture,Components,Embedded software,Evolution,Optimization,Reifications},
  url      = {http://www.sciencedirect.com/science/article/pii/S0167642312001633},
}

@Article{Magdaleno2012351,
  author   = {Magdaleno, Andr{\'{e}}a Magalh{\~{a}}es and Werner, Cl{\'{a}}udia Maria Lima and de Araujo, Renata Mendes},
  title    = {{Reconciling software development models: A quasi-systematic review}},
  journal  = {Journal of Systems and Software},
  year     = {2012},
  volume   = {85},
  number   = {2},
  pages    = {351--369},
  issn     = {0164-1212},
  abstract = {Purpose The purpose of this paper is to characterize reconciliation among the plan-driven, agile, and free/open source software models of software development. Design/methodology/approach An automated quasi-systematic review identified 42 papers, which were then analyzed. Findings The main findings are: there exist distinct – organization, group and process – levels of reconciliation; few studies deal with reconciliation among the three models of development; a significant amount of work addresses reconciliation between plan-driven and agile development; several large organizations (such as Microsoft, Motorola, and Philips) are interested in trying to combine these models; and reconciliation among software development models is still an open issue, since it is an emerging area and research on most proposals is at an early stage. Research limitations Automated searches may not capture relevant papers in publications that are not indexed. Other data sources not amenable to execution of the protocol were not used. Data extraction was performed by only one researcher, which may increase the risk of threats to internal validity. Implications This characterization is important for practitioners wanting to be current with the state of research. This review will also assist the scientific community working with software development processes to build a common understanding of the challenges that must be faced, and to identify areas where research is lacking. Finally, the results will be useful to software industry that is calling for solutions in this area. Originality/value There is no other systematic review on this subject, and reconciliation among software development models is an emerging area. This study helps to identify and consolidate the work done so far and to guide future research. The conclusions are an important step towards expanding the body of knowledge in the field. },
  annote   = {Special issue with selected papers from the 23rd Brazilian Symposium on Software Engineering},
  doi      = {https://doi.org/10.1016/j.jss.2011.08.028},
  keywords = {Agile,Free/open source software,Plan-driven,Reconciliation among development models,Software process,Systematic review},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121211002287},
}

@Article{Kwiatkowski20131368,
  author   = {Kwiatkowski, {\L} M and Verhoef, C},
  title    = {{Recovering management information from source code}},
  journal  = {Science of Computer Programming},
  year     = {2013},
  volume   = {78},
  number   = {9},
  pages    = {1368--1406},
  issn     = {0167-6423},
  abstract = {{\{}IT{\}} has become a production means for many organizations and an important element of business strategy. Even though its effective management is a must, reality shows that this area still remains in its infancy. {\{}IT{\}} management relies profoundly on relevant information which enables risk mitigation or cost control. However, the needed information is either missing or its gathering boils down to daunting tasks. We propose an approach to recovery of management information from the essence of IT; the software's source code. In this paper we show how to employ source code analysis techniques and recover management information. In our approach we exploit the potential of the concealed data which resides in the source code statements, source comments, and also compiler listings. We show how to depart from the raw sources, extract data, organize it, and eventually utilize so that the bit level data provides {\{}IT{\}} executives with support at the portfolio level. Our approach is pragmatic as we rely on real management questions, best practices in software engineering, and also {\{}IT{\}} market specifics. We enable, for instance, an assessment of the IT-portfolio market value, support for carrying out what-if scenarios, or identification and evaluation of the hidden risks for IT-portfolio maintainability. The study is based on a real-life IT-portfolio which supports business functions of an organization operating in the financial sector. The IT-portfolio comprises Cobol applications run on a mainframe with the total number of lines of code amounting to over 18 million. The approach we propose is suited for facilitation within a large organization. It provides for a fact-based support for strategic decision making at the portfolio level. },
  doi      = {https://doi.org/10.1016/j.scico.2012.07.016},
  keywords = {Automated data extraction,Case study,Cobol,Compilers,Cost control,IT assets,IT metrics,IT-portfolio management,Information retrieval,LSI,Latent Semantic Indexing,Legacy systems,Lexical analysis,Management information,Market value,Obsolete language constructs,Operational risk,Risk mitigation,Scenario analysis,Source code analysis,Source code comments,Technology risk,Vendor locks,Volatility},
  url      = {http://www.sciencedirect.com/science/article/pii/S016764231200144X},
}

@Article{SHATNAWI2017325,
  author   = {Shatnawi, Anas and Seriai, Abdelhak-Djamel and Sahraoui, Houari},
  title    = {{Recovering software product line architecture of a family of object-oriented product variants}},
  journal  = {Journal of Systems and Software},
  year     = {2017},
  volume   = {131},
  pages    = {325--346},
  issn     = {0164-1212},
  abstract = {Software Product Line Engineering (SPLE) aims at applying a pre-planned systematic reuse of large-grained software artifacts to increase the software productivity and reduce the development cost. The idea of SPLE is to analyze the business domain of a family of products to identify the common and the variable parts between the products. However, it is common for companies to develop, in an ad-hoc manner (e.g. clone and own), a set of products that share common services and differ in terms of others. Thus, many recent research contributions are proposed to re-engineer existing product variants to a software product line. These contributions are mostly focused on managing the variability at the requirement level. Very few contributions address the variability at the architectural level despite its major importance. Starting from this observation, we propose an approach to reverse engineer the architecture of a set of product variants. Our goal is to identify the variability and dependencies among architectural-element variants. Our work relies on formal concept analysis to analyze the variability. To validate the proposed approach, we evaluated on two families of open-source product variants; Mobile Media and Health Watcher. The results of precision and recall metrics of the recovered architectural variability and dependencies are 81{\%}, 91{\%}, 67{\%} and 100{\%}, respectively.},
  doi      = {https://doi.org/10.1016/j.jss.2016.07.039},
  keywords = {Formal concept analysis,Object-oriented product variants,Software architecture recovery,Software component,Software product line,Software reuse},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121216301327},
}

@Article{Gurov201369,
  author   = {Gurov, Dilian and Huisman, Marieke},
  title    = {{Reducing behavioural to structural properties of programs with procedures}},
  journal  = {Theoretical Computer Science},
  year     = {2013},
  volume   = {480},
  pages    = {69--103},
  issn     = {0304-3975},
  abstract = {There is an intimate link between program structure and behaviour. Exploiting this link to phrase program correctness problems in terms of the structural properties of a program graph rather than in terms of its unfoldings is a useful strategy for making analyses more tractable. The present paper presents a characterisation of behavioural program properties through sets of structural properties by means of a translation. The characterisation is given in the context of a program model based on control flow graphs of sequential programs with procedures, abstracting away completely from program data, and properties expressed in a fragment of the modal $\mu$ -calculus with boxes and greatest fixed-points only. The property translation is based on a tableau construction that conceptually amounts to symbolic execution of the behavioural formula, collecting structural constraints along the way. By keeping track of the subformulae that have been examined, recursion in the structural constraints can be identified and captured by fixed-point formulae. The tableau construction terminates, and the characterisation is exact, i.e., the translation is sound and complete. A prototype implementation has been developed. In addition, we show how the translation can be extended beyond the basic flow graph model and safety logic to richer behavioural models (such as open programs) and richer program models (including Boolean programs), and discuss possible extensions for more complex logics. We present several applications of the characterisation, in particular sound and complete compositional verification for behavioural properties based on maximal models. },
  doi      = {https://doi.org/10.1016/j.tcs.2013.02.006},
  keywords = {Compositional reasoning,Control-flow behaviour,Control-flow structure,Modal $\mu$ -calculus,Program verification,Safety properties},
  url      = {http://www.sciencedirect.com/science/article/pii/S0304397513001151},
}

@Article{Sabouri2012296,
  author   = {Sabouri, H and Khosravi, R},
  title    = {{Reducing the model checking cost of product lines using static analysis techniques}},
  journal  = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year     = {2012},
  volume   = {7253 LNCS},
  pages    = {296--312},
  abstract = {Software product line engineering is a paradigm to develop software applications using platforms and mass customization. Component based approaches play an important role in development of product lines: Components represent features, and different component combinations lead to different products. The number of combinations is exponential in the number of features, which makes the cost of product line model checking high. In this paper, we propose two techniques to reduce the number of component combinations that have to be verified. The first technique is using the static slicing approach to eliminate the features that do not affect the property. The second technique is analyzing the property and extracting sufficient conditions of property satisfaction/violation, to identify products that satisfy or violate the property without model checking. We apply these techniques on a vending machine case study to show the applicability and effectiveness of our approach. The results show that the number of generated states and time of model checking is reduced significantly using the proposed reduction techniques. {\textcopyright} 2012 Springer-Verlag.},
  annote   = {cited By 0},
  doi      = {10.1007/978-3-642-35743-5_18},
  keywords = {Analysis techniques; Component based approach; Mas,Computer software,Model checking},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871569046{\&}doi=10.1007{\%}2F978-3-642-35743-5{\_}18{\&}partnerID=40{\&}md5=6d26c0ec246632a3495074aec70d8c8b},
}

@Article{SABOURI201435,
  author   = {Sabouri, Hamideh and Khosravi, Ramtin},
  title    = {{Reducing the verification cost of evolving product families using static analysis techniques}},
  journal  = {Science of Computer Programming},
  year     = {2014},
  volume   = {83},
  pages    = {35--55},
  issn     = {0167-6423},
  abstract = {Software product line engineering enables proactive reuse among a set of related products through explicit modeling of commonalities and differences among them. Software product lines are intended to be used in a long period of time. As a result, they evolve over time, due to the changes in the requirements. Having several individual products in a software family, verification of the entire family may take a considerable effort. In this paper we aim to decrease this cost by reducing the number of verified products using static analysis techniques. Furthermore, to reduce model checking costs after product line evolution, we restrict the number of products that should be re-verified by reusing the previous verification result. All proposed techniques are based on static analysis of the product family model with respect to the property and can be automated. To show the effectiveness of these techniques we apply them on a set of case studies and present the results.},
  annote   = {Formal Aspects of Component Software (FACS 2011 selected {\&} extended papers)},
  doi      = {https://doi.org/10.1016/j.scico.2013.06.009},
  keywords = {Model checking,Program slicing,Reduction techniques,Software product lines,Static analysis},
  url      = {http://www.sciencedirect.com/science/article/pii/S0167642313001585},
}

@Article{Assunção20172972,
  author   = {Assun{\c{c}}{\~{a}}o, W K G and Lopez-Herrejon, R E and Linsbauer, L and Vergilio, S R and Egyed, A},
  title    = {{Reengineering legacy applications into software product lines: a systematic mapping}},
  journal  = {Empirical Software Engineering},
  year     = {2017},
  volume   = {22},
  number   = {6},
  pages    = {2972--3016},
  abstract = {Software Product Lines (SPLs) are families of systems that share common assets allowing a disciplined reuse. Rarely SPLs start from scratch, instead they usually start from a set of existing systems that undergo a reengineering process. Many approaches to conduct the reengineering process have been proposed and documented in research literature. This scenario is a clear testament to the interest in this research area. We conducted a systematic mapping study to provide an overview of the current research on reengineering of existing systems to SPLs, identify the community activity in regarding of venues and frequency of publications in this field, and point out trends and open issues that could serve as references for future research. This study identified 119 relevant publications. These primary sources were classified in six different dimensions related to reengineering phases, strategies applied, types of systems used in the evaluation, input artefacts, output artefacts, and tool support. The analysis of the results points out the existence of a consolidate community on this topic and a wide range of strategies to deal with different phases and tasks of the reengineering process, besides the availability of some tools. We identify some open issues and areas for future research such as the implementation of automation and tool support, the use of different sources of information, need for improvements in the feature management, the definition of ways to combine different strategies and methods, lack of sophisticated refactoring, need for new metrics and measures and more robust empirical evaluation. Reengineering of existing systems into SPLs is an active research topic with real benefits in practice. This mapping study motivates new research in this field as well as the adoption of systematic reuse in software companies. {\textcopyright} 2017, Springer Science+Business Media New York.},
  annote   = {cited By 10},
  doi      = {10.1007/s10664-017-9499-z},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011923741{\&}doi=10.1007{\%}2Fs10664-017-9499-z{\&}partnerID=40{\&}md5=cdb40d653362e54b58f0cb7eb7331cbe},
}

@Article{Kolb2006109,
  author        = {Kolb, R and Muthig, D and Patzke, T and Yamauchi, K},
  title         = {{Refactoring a legacy component for reuse in a software product line: A case study}},
  journal       = {Journal of Software Maintenance and Evolution},
  year          = {2006},
  volume        = {18},
  number        = {2},
  pages         = {109--132},
  abstract      = {Product lines are a promising approach to improve conceptually the productivity of the software development process and thus to reduce both the cost and time of developing and maintaining increasingly complex systems. An important issue in the adoption of the product-line approach is the migration of legacy software components, which have not been designed for reuse, systematically into reusable product-line components. This article describes activities performed to improve systematically the design and implementation of an existing software component in order to reuse it in a software product line. The activities are embedded in the application of Fraunhofer PuLSE™-DSSA - an approach for defining domain-specific software architectures (DSSA) and product-line architectures. The component under investigation is the so-called Image Memory Handler (IMH), which is used in Ricoh's current products of office appliances such as copier machines, printers, and multi-functional peripherals. It is responsible for controlling memory usage and compressing and decompressing image data. Improvement of both the component's design and implementation are based on a systematic analysis and focused on increasing maintainability and reusability and hence suitability for use in a product line. As a result of the analysis and refactoring activities, the documentation and implementation of the component has been considerably improved as shown by quantitative data collected at the end of the activities. Despite a number of changes to the code, the external behavior of the component has been preserved without significantly affecting the performance. Copyright {\textcopyright} 2006 John Wiley {\&} Sons, Ltd.},
  annote        = {cited By 31},
  doi           = {10.1002/smr.329},
  keywords      = {Code analysis,Codes (standards),Components,Computer architect,Computer software maintenance,Refactoring,Software product lines,case study},
  mendeley-tags = {case study},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-33646408526{\&}doi=10.1002{\%}2Fsmr.329{\&}partnerID=40{\&}md5=501ee942227e40e04ac86fcd865112dd},
}

@Article{daSilva2009105,
  author   = {da Silva, Bruno Carreiro and Figueiredo, Eduardo and Garcia, Alessandro and Nunes, Daltro},
  title    = {{Refactoring of Crosscutting Concerns with Metaphor-Based Heuristics}},
  journal  = {Electronic Notes in Theoretical Computer Science},
  year     = {2009},
  volume   = {233},
  pages    = {105--125},
  issn     = {1571-0661},
  abstract = {It has been advocated that Aspect-Oriented Programming (AOP) is an effective technique to improve software maintainability through explicit support for modularising crosscutting concerns. However, in order to take the advantages of AOP, there is a need for supporting the systematic refactoring of crosscutting concerns to aspects. Existing techniques for aspect-oriented refactoring are too fine-grained and do not take the concern structure into consideration. This paper presents two categories towards a metaphor-based classification of crosscutting concerns driven by their manifested shapes through a system's modular structure. The proposed categories provide an intuitive and fundamental terminology for detecting concern-oriented design flaws and identifying refactorings in terms of recurring crosscutting structures. On top of this classification, we define a suite of metaphor-based refactorings to guide the “aspectisation” of each concern category. We evaluate our technique by classifying concerns of 23 design patterns and by proposing refactorings to aspectise them according to observations made in previous empirical studies. Based on our experience, we also determine a catalogue of potential additional categories and heuristics for refactoring of crosscutting concerns. },
  annote   = {Proceedings of the International Workshop on Software Quality and Maintainability (SQM 2008)},
  doi      = {https://doi.org/10.1016/j.entcs.2009.02.064},
  keywords = {Aspect-oriented programming,Crosscutting concerns,Design heuristics,Metaphor-based classification,Refactoring},
  url      = {http://www.sciencedirect.com/science/article/pii/S1571066109000693},
}

@Article{Romanovsky2011158,
  author   = {Romanovsky, K and Koznov, D and Minchin, L},
  title    = {{Refactoring the documentation of software product lines}},
  journal  = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year     = {2011},
  volume   = {4980 LNCS},
  pages    = {158--170},
  abstract = {One of the most vital techniques in the context of software product line (SPL) evolution is refactoring - extracting and refining reusable assets and improving SPL architecture in such a way that the behavior of existing products remains unchanged. We extend the idea of SPL refactoring to technical documentation because reuse techniques could effectively be applied to this area and reusable assets evolve and should be maintained. Various XML-based technologies for documentation development are widely spread today, and XML-specifications appear to be a good field for formal transformations. We base our research on the DocLine technology; the main goal of which is to introduce adaptive reuse into documentation development. We define a model of refactoring-based documentation development process, a set of refactoring operations, and describe their implementation in the DocLine toolset. Also, we present an experiment in which we applied the proposed approach to the documentation of a telecommunication systems SPL. {\textcopyright} 2011 IFIP International Federation for Information Processing.},
  annote   = {cited By 3},
  doi      = {10.1007/978-3-642-22386-0_12},
  keywords = {Computer software reusability; Computer software,Development process; Documentation of software; Fo,Formal methods; Network architecture; Software de},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053157402{\&}doi=10.1007{\%}2F978-3-642-22386-0{\_}12{\&}partnerID=40{\&}md5=7f8bfaa737b162c6996848360257c634},
}

@Article{Azevedo2009411,
  author   = {Azevedo, S and Machado, R J and Muthig, D and Ribeiro, H},
  title    = {{Refinement of software product line architectures through recursive modeling techniques}},
  journal  = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year     = {2009},
  volume   = {5872 LNCS},
  pages    = {411--422},
  abstract = {Currently, modeling methods applicable to software product line architectures do not explicitly comprise refinement, which implies dealing with a lot of complexity during their application to a high number of requirements. This paper suggests the extension of a modeling method applicable to product line architectural modeling, the 4SRS (Four Step Rule Set), to support the refinement of product lines. We have used the GoPhone case study to illustrate the approach and the recursion capability of the method as a solution to the challenges of modeling product line architectures. The strength of our approach resides in its stepwise nature and in allowing the modeler to work at the user requirements level without delving into lower abstraction concerns. {\textcopyright} Springer-Verlag 2009.},
  annote   = {cited By 7},
  doi      = {10.1007/978-3-642-05290-3_53},
  keywords = {Architectural modeling; Logical architecture; Mode,Cams,Internet},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650724841{\&}doi=10.1007{\%}2F978-3-642-05290-3{\_}53{\&}partnerID=40{\&}md5=25a169ce1bda25c517b599c500478ac1},
}

@Article{Heidenreich201069,
  author   = {Heidenreich, F and S{\'{a}}nchez, P and Santos, J and Zschaler, S and Alf{\'{e}}rez, M and Ara{\'{u}}jo, J and Fuentes, L and Kulesza, U and Moreira, A and Rashid, A},
  title    = {{Relating feature models to other models of a software product line: A comparative study of FeatureMapper and VML}},
  journal  = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year     = {2010},
  volume   = {6210 LNCS},
  pages    = {69--114},
  abstract = {Software product lines using feature models often require the relation between feature models in problem space and the models used to describe the details of the product line to be expressed explicitly. This is particularly important, where automatic product derivation is required. Different approaches for modelling this mapping have been proposed in the literature. However, a discussion of their relative benefits and drawbacks is currently missing. As a first step towards a better understanding of this field, this paper applies two of these approaches-FeatureMapper as a representative of declarative approaches and VML* as a representative of operational approaches-to the case study. We show in detail how the case study can be expressed using these approaches and discuss strengths and weaknesses of the two approaches with regard to the case study. {\textcopyright} 2010 Springer-Verlag.},
  annote   = {cited By 20},
  doi      = {10.1007/978-3-642-16086-8_3},
  keywords = {Comparative studies; Feature models; Problem space,Computer systems programming; Network architectur,Software design},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-78649878322{\&}doi=10.1007{\%}2F978-3-642-16086-8{\_}3{\&}partnerID=40{\&}md5=a4aa098925031225dfa6e2c3468ffc6b},
}

@Article{Nakagawa2013985,
  author   = {Nakagawa, Elisa Y and Antonino, Pablo O and Becker, Martin and Maldonado, Jos{\'{e}} C and Storf, Holger and Villela, Karina B and Rombach, Dieter},
  title    = {{Relevance and perspectives of {\{}AAL{\}} in Brazil}},
  journal  = {Journal of Systems and Software},
  year     = {2013},
  volume   = {86},
  number   = {4},
  pages    = {985--996},
  issn     = {0164-1212},
  abstract = {Population aging has been taking place in many countries across the globe and more recently in emerging countries. In this context, Ambient Assisted Living (AAL) has become one focus of attention, including methods, products, services, and {\{}AAL{\}} software systems that support the everyday lives of elderly people, promoting mainly their independence and dignity. From the perspective of computer science, efforts are already being dedicated to adequately developing {\{}AAL{\}} systems. However, in spite of its relevance, {\{}AAL{\}} has not been properly investigated in emerging countries, including Brazil. Thus, the contribution of this paper is to present the main perspectives of research in AAL, in particular in the area of software engineering, considering that the Brazilian population is also subject to the aging process. The main intention of this paper is to raise the interest of Brazilian researchers, as well as government and industry, for this important area. },
  annote   = {{\{}SI{\}} : Software Engineering in Brazil: Retrospective and Prospective Views},
  doi      = {https://doi.org/10.1016/j.jss.2012.10.013},
  keywords = {AAL platform,Ambient Assisted Living (AAL),Population aging,Reference architecture},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121212002841},
}

@Article{Vierhauser2016123,
  author   = {Vierhauser, Michael and Rabiser, Rick and Gr{\"{u}}nbacher, Paul and Seyerlehner, Klaus and Wallner, Stefan and Zeisel, Helmut},
  title    = {{ReMinds : A flexible runtime monitoring framework for systems of systems}},
  journal  = {Journal of Systems and Software},
  year     = {2016},
  volume   = {112},
  pages    = {123--136},
  issn     = {0164-1212},
  abstract = {Abstract Many software-intensive systems today can be characterized as systems of systems (SoS) comprising complex, interrelated, and heterogeneous systems. The behavior of SoS often only emerges at runtime due to complex interactions between the involved systems and their environment. It is thus necessary to determine unexpected behavior by monitoring SoS at runtime, i.e., collecting and analyzing events and data at different layers and levels of granularity. Existing monitoring approaches are often limited to individual systems, particular architectural styles, or technologies. In this paper we thus derive challenges for monitoring SoS based on an industrial case. We present a flexible framework adaptable to different system architectures and technologies. We discuss its capabilities for instrumenting systems, collecting and persisting events and data, checking constraints on events and data, and visualizing the systems' behavior to users. We demonstrate the framework's flexibility by tailoring and applying it to an industrial SoS and assessing its performance and scalability. Our results show that the framework is flexible and scalable for monitoring an industrial SoS with realistic event loads. },
  doi      = {https://doi.org/10.1016/j.jss.2015.07.008},
  keywords = {Framework,Runtime monitoring,System-of-systems architectures},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121215001478},
}

@Article{Olszak2012131,
  author   = {Olszak, Andrzej and J{\o}rgensen, Bo N{\o}rregaard},
  title    = {{Remodularizing Java programs for improved locality of feature implementations in source code}},
  journal  = {Science of Computer Programming},
  year     = {2012},
  volume   = {77},
  number   = {3},
  pages    = {131--151},
  issn     = {0167-6423},
  abstract = {Explicit traceability between features and source code is known to help programmers to understand and modify programs during maintenance tasks. However, the complex relations between features and their implementations are not evident from the source code of object-oriented Java programs. Consequently, the implementations of individual features are difficult to locate, comprehend, and modify in isolation. In this paper, we present a novel remodularization approach that improves the representation of features in the source code of Java programs. Both forward and reverse restructurings are supported through on-demand bidirectional restructuring between feature-oriented and object-oriented decompositions. The approach includes a feature location phase based on tracing of program execution, a feature representation phase that reallocates classes into a new package structure based on single-feature and multi-feature packages, and an annotation-based reverse transformation of code. Case studies performed on two open-source projects indicate that our approach requires relatively little manual effort and reduces tangling and scattering of feature implementations in the source code. },
  annote   = {Feature-Oriented Software Development (FOSD 2009)},
  doi      = {https://doi.org/10.1016/j.scico.2010.10.007},
  keywords = {Feature location,Features,Fragile decomposition problem,Remodularization},
  url      = {http://www.sciencedirect.com/science/article/pii/S0167642310001917},
}

@Article{Simidchieva2007109,
  author   = {Simidchieva, B I and Clarke, L A and Osterweil, L J},
  title    = {{Representing process variation with a process family}},
  journal  = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year     = {2007},
  volume   = {4470 LNCS},
  pages    = {109--120},
  abstract = {The formalization of process definitions has been an invaluable aid in many domains. However, noticeable variations in processes start to emerge as precise details are added to process definitions. While each such variation gives rise to a different process, these processes might more usefully be considered as variants of each other, rather than completely different processes. This paper proposes that it is beneficial to regard such an appropriately close set of process variants as a process family. The paper suggests a characterization of what might comprise a process family and introduces a formal approach to defining families based upon this characterization. To illustrate this approach, we describe a case study that demonstrates the different variations we observed in processes that define how dispute resolution is performed at the U.S. National Mediation Board. We demonstrate how our approach supports the definition of this set of process variants as a process family. {\textcopyright} Springer-Verlag Berlin Heidelberg 2007.},
  annote   = {cited By 21},
  keywords = {Formal methods; Set theory,Process engineering,Process family; Process instance generation; Proce},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-37149016114{\&}partnerID=40{\&}md5=819724924b0a96138361ad0c3279ccb0},
}

@Article{SPE:SPE558,
  author    = {Jaring, M and Krikhaar, R L and Bosch, J},
  title     = {{Representing variability in a family of MRI scanners}},
  journal   = {Software: Practice and Experience},
  year      = {2004},
  volume    = {34},
  number    = {1},
  pages     = {69--100},
  issn      = {1097-024X},
  doi       = {10.1002/spe.558},
  file      = {:Users/mac/Library/Application Support/Mendeley Desktop/Downloaded/Jaring, Krikhaar, Bosch - 2004 - Representing variability in a family of MRI scanners.pdf:pdf},
  keywords  = {dependencies,product derivation,software product families,variability modeling},
  publisher = {John Wiley {\&} Sons, Ltd.},
  url       = {http://dx.doi.org/10.1002/spe.558},
}

@Article{Jaring200215,
  author        = {Jaring, M and Bosch, J},
  title         = {{Representing variability in software product lines: A case study}},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year          = {2002},
  volume        = {2379},
  pages         = {15--36},
  abstract      = {Variability is the ability to change or customize a software system (i.e., software architects anticipate change and design architectures that support those changes). If the architecture is used for different product versions (e.g., in a software product line context, it becomes important to understand where change has to be planned and the possible options in particular situations. Three variability issues have been identified in a case study involving a software company specializing in product and system development for a professional mobile communication infrastructure. These issues are discussed and analyzed and illustrate the need for handling variability in a more explicit manner. To address this need, this paper suggests a method to represent and normalize variability in industrial software systems. The method is exemplified by applying it to the software product line of the aforementioned company. {\textcopyright} Springer-Verlag Berlin Heidelberg 2002.},
  annote        = {cited By 39},
  keywords      = {Computer software,Design architecture,Industrial software,Mobile,Software architecture,Software design,case study},
  mendeley-tags = {case study},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84974661208{\&}partnerID=40{\&}md5=860864deb3816bcf9b81bf59258e5a5e},
}

@Article{Montalvillo2016110,
  author   = {Montalvillo, Leticia and D{\'{i}}az, Oscar},
  title    = {{Requirement-driven evolution in software product lines: A systematic mapping study}},
  journal  = {Journal of Systems and Software},
  year     = {2016},
  volume   = {122},
  pages    = {110--143},
  issn     = {0164-1212},
  abstract = {Abstract CONTEXT. Software Product Lines (SPLs) aim to support the development of a whole family of software products through systematic reuse of shared assets. As {\{}SPLs{\}} exhibit a long life-span, evolution is an even greater concern than for single-systems. For the purpose of this work, evolution refers to the adaptation of the {\{}SPL{\}} as a result of changing requirements. Hence, evolution is triggered by requirement changes, and not by bug fixing or refactoring. OBJECTIVE. Research on {\{}SPL{\}} evolution has not been previously mapped. This work provides a mapping study along Petersen's and Kichenham's guidelines, to identify strong areas of knowledge, trends and gaps. RESULTS. We identified 107 relevant contributions. They were classified according to four facets: evolution activity (e.g., identify, analyze and plan, implement), product-derivation approach (e.g., annotation-based, composition-based), research type (e.g., solution, experience, evaluation), and asset type (i.e., variability model, {\{}SPL{\}} architecture, code assets and products). CONCLUSION. Analyses of the results indicate that “Solution proposals” are the most common type of contribution (31{\%}). Regarding the evolution activity, “Implement change” (43{\%}) and “Analyze and plan change” (37{\%}) are the most covered ones. A finer-grained analysis uncovered some tasks as being underexposed. A detailed description of the 107 papers is also included. },
  doi      = {https://doi.org/10.1016/j.jss.2016.08.053},
  keywords = {Evolution,Software product lines,Systematic mapping study},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121216301510},
}

@Article{Rabiser2010324,
  author   = {Rabiser, R and Gr{\"{u}}nbacher, P and Dhungana, D},
  title    = {{Requirements for product derivation support: Results from a systematic literature review and an expert survey}},
  journal  = {Information and Software Technology},
  year     = {2010},
  volume   = {52},
  number   = {3},
  pages    = {324--346},
  abstract = {Context: An increasing number of publications in product line engineering address product derivation, i.e., the process of building products from reusable assets. Despite its importance, there is still no consensus regarding the requirements for product derivation support. Objective: Our aim is to identify and validate requirements for tool-supported product derivation. Method: We identify the requirements through a systematic literature review and validate them with an expert survey. Results: We discuss the resulting requirements and provide implementation examples from existing product derivation approaches. Conclusions: We conclude that key requirements are emerging in the research literature and are also considered relevant by experts in the field. {\textcopyright} 2009 Elsevier B.V. All rights reserved.},
  annote   = {cited By 59},
  doi      = {10.1016/j.infsof.2009.11.001},
  keywords = {Building products; Expert survey; Product derivati,Computer software; Surveys,Production engineering},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-75449091148{\&}doi=10.1016{\%}2Fj.infsof.2009.11.001{\&}partnerID=40{\&}md5=dd02663c17d0c088444aea71f6824956},
}

@Article{Huysegoms2011238,
  author   = {Huysegoms, T and Snoeck, M and Dedene, G and Goderis, A},
  title    = {{Requirements for successful software development with variability: A case study}},
  journal  = {Communications in Computer and Information Science},
  year     = {2011},
  volume   = {219 CCIS},
  number   = {PART 1},
  pages    = {238--247},
  abstract = {According to state of the art literature, software product lines are an effective way to achieve economies of scale through reusability while coping with the problem of variability in related software systems. Fundamentals of variability management and product lines have been available in the software engineering research field for several decades. Nevertheless, projects to cope with variability in practice tend to fall short of target. The reason for this gap between sound theories and poor practice, common in multiple software engineering subfields, remains unclear. Therefore, an empirical study was conducted in a large-scale software dependent multinational. The results of this case study show a number of factors that impact successful variability practice. These factors can be abstracted into general hypotheses useful for bridging the gap between theory and practice. Based on the sources of discrepancy, this research suggests a practical way to overcome the obstacles on the road towards successful variability management. {\textcopyright} 2011 Springer-Verlag.},
  annote   = {cited By 1},
  doi      = {10.1007/978-3-642-24358-5_24},
  keywords = {Case study research; Economies of scale; Empirical,Computer software reusability; Economics; Informa,Software design},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-80054063237{\&}doi=10.1007{\%}2F978-3-642-24358-5{\_}24{\&}partnerID=40{\&}md5=041136ec5e283f157e5b134bff7c0340},
}

@Article{Sepúlveda201616,
  author   = {Sep{\'{u}}lveda, S and Cravero, A and Cachero, C},
  title    = {{Requirements modeling languages for software product lines: A systematic literature review}},
  journal  = {Information and Software Technology},
  year     = {2016},
  volume   = {69},
  pages    = {16--36},
  abstract = {Context: Software product lines (SPLs) have reached a considerable level of adoption in the software industry, having demonstrated their cost-effectiveness for developing higher quality products with lower costs. For this reason, in the last years the requirements engineering community has devoted much effort to the development of a myriad of requirements modelling languages for SPLs. Objective: In this paper, we review and synthesize the current state of research of requirements modelling languages used in SPLs with respect to their degree of empirical validation, origin and context of use, level of expressiveness, maturity, and industry adoption. Method: We have conducted a systematic literature review with six research questions that cover the main objective. It includes 54 studies, published from 2000 to 2013. Results: The mean level of maturity of the modelling languages is 2.59 over 5, with 46{\%} of them falling within level 2 or below -no implemented abstract syntax reported-. They show a level of expressiveness of 0.7 over 1.0. Some constructs (feature, mandatory, optional, alternative, exclude and require) are present in all the languages, while others (cardinality, attribute, constraint and label) are less common. Only 6{\%} of the languages have been empirically validated, 41{\%} report some kind of industry adoption and 71{\%} of the languages are independent from any development process. Last but not least, 57{\%} of the languages have been proposed by the academia, while 43{\%} have been the result of a joint effort between academia and industry. Conclusions: Research on requirements modeling languages for SPLs has generated a myriad of languages that differ in the set of constructs provided to express SPL requirements. Their general lack of empirical validation and adoption in industry, together with their differences in maturity, draws the picture of a discipline that still needs to evolve. {\textcopyright} 2015 Elsevier B.V. All rights reserved.},
  annote   = {cited By 5},
  doi      = {10.1016/j.infsof.2015.08.007},
  keywords = {Computational linguistics; Computer software; Cost,Development process; Empirical validation; Requir,Modeling languages},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946595413{\&}doi=10.1016{\%}2Fj.infsof.2015.08.007{\&}partnerID=40{\&}md5=718ae3d53a66e2e0acba045e425380dd},
}

@Article{Derakhshanmanesh2014333,
  author   = {Derakhshanmanesh, M and Fox, J and Ebert, J},
  title    = {{Requirements-driven incremental adoption of variability management techniques and tools: an industrial experience report}},
  journal  = {Requirements Engineering},
  year     = {2014},
  volume   = {19},
  number   = {4},
  pages    = {333--354},
  abstract = {In theory, software product line engineering has reached a mature state. In practice though, implementing a variability management approach remains a tough case-by-case challenge for any organization. To tame the complexity of this undertaking, it is inevitable to handle variability from multiple perspectives and to manage variability consistently across artifacts, tools, and workflows. Especially, a solid understanding and management of the requirements to be met by the products is an inevitable prerequisite. In this article, we share experiences from the ongoing incremental adoption of explicit variability management at TRW Automotive's department for automotive slip control systems—located in Koblenz, Germany. On the technical side, the three key drivers of this adoption effort are (a) domain modeling and scoping, (b) handling of variability in requirements and (c) tighter integration of software engineering focus areas (e.g., domain modeling, requirements engineering, architectural modeling) to make use of variability-related data. In addition to implementation challenges with using and integrating concrete third-party tools, social and workflow-related issues are covered as well. The lessons learned are presented, discussed, and thoroughly compared with the state of the art in research. {\textcopyright} 2013, Springer-Verlag London.},
  annote   = {cited By 4},
  doi      = {10.1007/s00766-013-0185-4},
  keywords = {C (programming language),Computer software reusability; Software design,Features; Incremental adoption; Requirements; Reu},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84920258824{\&}doi=10.1007{\%}2Fs00766-013-0185-4{\&}partnerID=40{\&}md5=87e9e513d18e8be3cc6ae95b7eccfec8},
}

@Article{Zanardini2016173,
  author   = {Zanardini, Damiano and Albert, Elvira and Villela, Karina},
  title    = {{Resource–usage–aware configuration in software product lines}},
  journal  = {Journal of Logical and Algebraic Methods in Programming},
  year     = {2016},
  volume   = {85},
  number   = {1, Part 2},
  pages    = {173--199},
  issn     = {2352-2208},
  abstract = {Abstract Deriving concrete products from a product-line infrastructure requires resolving the variability captured in the product line, based on the company market strategy or requirements from specific customers. Selecting the most appropriate set of features for a product is a complex task, especially if quality requirements have to be considered. Resource–usage–aware configuration aims at providing awareness of resource–usage properties of artifacts throughout the configuration process. This article envisages several strategies for resource–usage–aware configuration which feature different performance and efficiency trade-offs. The common idea in all strategies is the use of resource–usage estimates obtained by an off-the-shelf static resource–usage analyzer as a heuristic for choosing among different candidate configurations. We report on a prototype implementation of the most practical strategies for resource–usage–aware configuration and apply it on an industrial case study. },
  annote   = {Formal Methods for Software Product Line Engineering},
  doi      = {https://doi.org/10.1016/j.jlamp.2015.08.003},
  url      = {http://www.sciencedirect.com/science/article/pii/S2352220815000814},
}

@Article{Tiwari:2013:RRT:2439976.2439982,
  author    = {Tiwari, Rajeev and Goel, Noopur},
  title     = {{Reuse: Reducing Test Effort}},
  journal   = {SIGSOFT Softw. Eng. Notes},
  year      = {2013},
  volume    = {38},
  number    = {2},
  pages     = {1--11},
  issn      = {0163-5948},
  address   = {New York, NY, USA},
  doi       = {10.1145/2439976.2439982},
  keywords  = {reusable test cases,reuse-oriented test approaches,software product lines,test effort},
  publisher = {ACM},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/2439976.2439982},
}

@Article{Oliveira20112234,
  author   = {Oliveira, Toacy C and Alencar, Paulo and Cowan, Don},
  title    = {{ReuseTool—An extensible tool support for object-oriented framework reuse}},
  journal  = {Journal of Systems and Software},
  year     = {2011},
  volume   = {84},
  number   = {12},
  pages    = {2234--2252},
  issn     = {0164-1212},
  abstract = {Object-oriented frameworks have become a popular paradigm used to improve the software development lifecycle. They promote reuse by providing a semi-complete architecture that can be extended through an instantiation process to integrate the needs of the new software application. Instantiation processes are typically enacted in an ad-hoc manner, which may lead to tedious and error-prone procedures. This work leverages our previous work on the definition of RDL, a language to facilitate the description of instantiation process, and describe the ReuseTool, which is an extensible tool to execute {\{}RDL{\}} programs and assist framework reuse by manipulating {\{}UML{\}} Diagrams. The ReuseTool integrates a {\{}RDL{\}} Compiler and a Workflow Engine to control most of the activities required to extend a framework design and, therefore, incorporates application-specific needs. This work also describes how the tool can be extended to incorporate new reuse activities and provides information of its use based on an exploratory Case Study. },
  doi      = {https://doi.org/10.1016/j.jss.2011.06.030},
  keywords = {Object-oriented framework,Software process,Software reuse,UML},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121211001531},
}

@Article{Pesce2018145,
  author   = {Pesce, F and Caballero, S and Buccella, A and Cechich, A},
  title    = {{Reusing a geographic software product line platform: A case study in the paleontological sub-domain}},
  journal  = {Communications in Computer and Information Science},
  year     = {2018},
  volume   = {790},
  pages    = {145--154},
  abstract = {Developing Software Product Lines (SPLs) is a paradigm oriented to reusing software within particular domains. Key aspects within this paradigm are the inherent particularities of these domains and the techniques applied to systematize the ways to maximize reuse. In this article, we describe a process for creating SPLs by reusing through domain hierarchies, starting from the geographical domain and going deeper into the paleontological sub-domain. In particular, our process is based on standardizations and previous techniques already applied to another geographical sub-domain, which is marine ecology. Here we show how these techniques are applied in the paleontological sub-domain, improving the systematic reuse of software artifacts. {\textcopyright} Springer International Publishing AG, part of Springer Nature 2018.},
  annote   = {cited By 0; Conference of 23rd Argentine Congress of Computer Science, CACIC 2017 ; Conference Date: 9 October 2017 Through 13 October 2017; Conference Code:210229},
  doi      = {10.1007/978-3-319-75214-3_14},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041843530{\&}doi=10.1007{\%}2F978-3-319-75214-3{\_}14{\&}partnerID=40{\&}md5=24673d7c3f1e6f7248334eb6ffd27af5},
}

@Article{DELGADO201648,
  author   = {Delgado, A and Estepa, A and Troyano, J A and Estepa, R},
  title    = {{Reusing UI elements with Model-Based User Interface Development}},
  journal  = {International Journal of Human-Computer Studies},
  year     = {2016},
  volume   = {86},
  pages    = {48--62},
  issn     = {1071-5819},
  abstract = {This paper introduces the potential for reusing UI elements in the context of Model-Based UI Development (MBUID) and provides guidance for future MBUID systems with enhanced reutilization capabilities. Our study is based upon the development of six inter-related projects with a specific MBUID environment which supports standard techniques for reuse such as parametrization and sub-specification, inclusion or shared repositories. We analyze our experience and discuss the benefits and limitations of each technique supported by our MBUID environment. The system architecture, the structure and composition of UI elements and the models specification languages have a decisive impact on reusability. In our case, more than 40{\%} of the elements defined in the UI specifications were reused, resulting in a reduction of 55{\%} of the specification size. Inclusion, parametrization and sub-specification have facilitated modularity and internal reuse of UI specifications at development time, whereas the reuse of UI elements between applications has greatly benefited from sharing repositories of UI elements at run time.},
  doi      = {https://doi.org/10.1016/j.ijhcs.2015.09.003},
  keywords = {MBUID,Reuse,Software engineering,User Interface},
  url      = {http://www.sciencedirect.com/science/article/pii/S1071581915001470},
}

@Article{SHATNAWI2017442,
  author   = {Shatnawi, Anas and Seriai, Abdelhak-Djamel and Sahraoui, Houari and Alshara, Zakarea},
  title    = {{Reverse engineering reusable software components from object-oriented APIs}},
  journal  = {Journal of Systems and Software},
  year     = {2017},
  volume   = {131},
  pages    = {442--460},
  issn     = {0164-1212},
  abstract = {Object-oriented Application Programing Interfaces (APIs) support software reuse by providing pre-implemented functionalities. Due to the huge number of included classes, reusing and understanding large APIs is a complex task. Otherwise, software components are accepted to be more reusable and understandable entities than object-oriented ones. Thus, in this paper, we propose an approach for reengineering object-oriented APIs into component-based ones. We mine components as a group of classes based on the frequency they are used together and their ability to form a quality-centric component. To validate our approach, we experimented on 100 Java applications that used four APIs.},
  doi      = {https://doi.org/10.1016/j.jss.2016.06.101},
  keywords = {API,Frequent usage pattern,Object-oriented,Reverse engineering,Software component,Software reuse},
  url      = {http://www.sciencedirect.com/science/article/pii/S016412121630098X},
}

@Article{Ferber2002364,
  author   = {Ferber, S and Heidi, P and Lutz, P},
  title    = {{Reviewing product line architectures: Experience report of ATAM in an automotive context}},
  journal  = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year     = {2002},
  volume   = {2290},
  pages    = {364--382},
  abstract = {Product lines are an important system development paradigm in the automotive industry to amortize costs beyond a single product. The paradigm is well established in the mechanical and electrical engineering practice in automotive companies like Bosch. As software is covering more and more functionality in cars, software product lines are getting more attention. The architecture of a software-intensive system is a key asset in developing a software product line. The Architecture Trade-off Analysis Method (ATAM) developed by the SEI assesses the quality of software architecture early in the development process. ATAM is therefore a useful review technique to guarantee important quality attributes of every single product created with the product line architecture later on. This article reports about the experience Bosch made in using ATAM in two cases. Benefits in using ATAM are not only the review results itself but a better documented and better understood architecture. We experienced the most important benefit of ATAM is the rising stakeholders' awareness of architectural decisions, tradeoffs, and risks. It illuminates the software architecture better than any written documentation. Bosch employees are trained in the evaluation roles in order to transition ATAM to Bosch. The reports conclude with some suggestions for improving the ATAM itself and the training of ATAM roles. {\textcopyright} Springer-Verlag Berlin Heidelberg 2002.},
  annote   = {cited By 8},
  keywords = {Architectural decision; Automotive companies; Dev,Automotive industry; Computer software; Economic a,Quality control},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944052218{\&}partnerID=40{\&}md5=34bea22dd9d1e0b027ae7749212fe967},
}

@Article{Manikas201684,
  author   = {Manikas, Konstantinos},
  title    = {{Revisiting software ecosystems Research: A longitudinal literature study}},
  journal  = {Journal of Systems and Software},
  year     = {2016},
  volume   = {117},
  pages    = {84--103},
  issn     = {0164-1212},
  abstract = {Abstract ‘Software ecosystems' is argued to first appear as a concept more than 10 years ago and software ecosystem research started to take off in 2010. We conduct a systematic literature study, based on the most extensive literature review in the field up to date, with two primarily aims: (a) to provide an updated overview of the field and (b) to document evolution in the field. In total, we analyze 231 papers from 2007 until 2014 and provide an overview of the research in software ecosystems. Our analysis reveals a field that is rapidly growing, both in volume and empirical focus, while becoming more mature. We identify signs of field maturity from the increase in: (i) the number of journal articles, (ii) the empirical models within the last two years, and (iii) the number of ecosystems studied. However, we note that the field is far from mature and identify a set of challenges that are preventing the field from evolving. We propose means for future research and the community to address them. Finally, our analysis shapes the view of the field having evolved outside the existing definitions of software ecosystems and thus propose the update of the definition of software ecosystems. },
  doi      = {https://doi.org/10.1016/j.jss.2016.02.003},
  keywords = {Longitudinal literature study,Software ecosystem maturity,Software ecosystems},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121216000406},
}

@Article{Snook2008112,
  author        = {Snook, Colin and Poppleton, Michael and Johnson, Ian},
  title         = {{Rigorous engineering of product-line requirements: A case study in failure management}},
  journal       = {Information and Software Technology},
  year          = {2008},
  volume        = {50},
  number        = {1–2},
  pages         = {112--129},
  issn          = {0950-5849},
  abstract      = {We consider the failure detection and management function for engine control systems as an application domain where product line engineering is indicated. The need to develop a generic requirement set – for subsequent system instantiation – is complicated by the addition of the high levels of verification demanded by this safety-critical domain, subject to avionics industry standards. We present our case study experience in this area as a candidate method for the engineering, validation and verification of generic requirements using domain engineering and Formal Methods techniques and tools. For a defined class of systems, the case study produces a generic requirement set in {\{}UML{\}} and an example system instance. Domain analysis and engineering produce a validated model which is integrated with the formal specification/verification method B by the use of our UML-B profile. The formal verification both of the generic requirement set, and of a simple system instance, is demonstrated using our U2B, ProB and prototype Requirements Manager tools. This work is a demonstrator for a tool-supported method which will be an output of {\{}EU{\}} project {\{}RODIN{\}} (This work is conducted in the setting of the {\{}EU{\}} funded Research Project: {\{}IST{\}} 511599 {\{}RODIN{\}} (Rigorous Open Development Environment for Complex Systems) http://rodin.cs.ncl.ac.uk/). The use of existing and prototype formal verification and support tools is discussed. The method, developed in application to this novel combination of product line, failure management and safety-critical engineering, is evaluated and considered to be applicable to a wide range of domains.},
  annote        = {Special issue with two special sections. Section 1: Most-cited software engineering articles in 2001. Section 2: Requirement engineering: Foundation for software quality},
  doi           = {https://doi.org/10.1016/j.infsof.2007.10.010},
  keywords      = {Formal specification,Generic requirements,Product line,Refinement,Tools,UML-B,Verification,case study},
  mendeley-tags = {case study},
  url           = {http://www.sciencedirect.com/science/article/pii/S0950584907001176},
}

@Article{Lobato2013523,
  author   = {Lobato, L L and Bittar, T J and Neto, P.A.D.M.S. and MacHado, I D C and {De Almeida}, E S and Meira, S.R.D.L.},
  title    = {{Risk management in software product line engineering: A mapping study}},
  journal  = {International Journal of Software Engineering and Knowledge Engineering},
  year     = {2013},
  volume   = {23},
  number   = {4},
  pages    = {523--558},
  abstract = {Software Product Line (SPL) Engineering focuses on systematic software reuse, which has benefits such as reductions in time-to-market and effort, and improvements in the quality of products. However, establishing a SPL is not a simple matter, and can affect all aspects of the organization, since the approach is complex and involves major investment and considerable risk. These risks can have a negative impact on the expected ROI for an organization, if SPL is not sufficiently managed. This paper presents a mapping study of Risk Management (RM) in SPL Engineering. We analyzed a set of thirty studies in the field. The results points out the need for risk management practices in SPL, due to the little research on RM practices in SPL and the importance of identifying insight on RM in SPL. Most studies simply mention the importance of RM, however the steps for managing risk are not clearly specified. Our findings suggest that greater attention should be given, through the use of industrial case studies and experiments, to improve SPL productivity and ensure its success. This research is a first attempt within the SPL community to identify, classify, and manage risks, and establish mitigation strategies. {\textcopyright} 2013 World Scientific Publishing Company.},
  annote   = {cited By 2},
  doi      = {10.1142/S0218194013500150},
  keywords = {Computer software; Mapping; Research; Software de,Industrial case study; Mapping studies; Mitigation,Risk management},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84881014807{\&}doi=10.1142{\%}2FS0218194013500150{\&}partnerID=40{\&}md5=c6c53641edd6d02b759a1afa12f08755},
}

@Article{Bu2019130,
  author   = {Bu, C and Wang, X and Cheng, H and Huang, M and Li, K},
  title    = {{Routing as a service (RaaS): An open framework for customizing routing services}},
  journal  = {Journal of Network and Computer Applications},
  year     = {2019},
  volume   = {125},
  pages    = {130--145},
  abstract = {With the emergence of various types of network applications, the user communication requirements for them are becoming more and more diversified and personalized. In order to accommodate the user frequently changing demands for different network applications, the Internet Service Provider (ISP) traditionally purchases and operates new dedicated network equipment, which always incurs high capital expense (CAPEX) and operating expense (OPEX) from the economic viewpoint and also burdens network management. Inspired by the ideas of Software Defined Networking (SDN) and Network Function Virtualization (NFV), we consider dealing with the above challenge by reusing virtualized network functions and selecting appropriate ones to compose the customized routing services on the routing paths for different applications. In this paper, based on SDN and NFV, we propose Routing as a Service (RaaS) as an open framework to customize the specific routing services for applications. Then, we present Routing Service Product Line (RSPL) by introducing Dynamic Software Product Line (DSPL) into the proposed RaaS, so as to rapidly customize a large number of routing services with different characteristics. In addition, according to the proposed framework, we also carry out a case study to customizing routing services with benefits of both the user and the ISP considered. Simulation results show that the proposed RaaS is feasible and efficient. {\textcopyright} 2018 Elsevier Ltd},
  annote   = {cited By 0},
  doi      = {10.1016/j.jnca.2018.10.010},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056187673{\&}doi=10.1016{\%}2Fj.jnca.2018.10.010{\&}partnerID=40{\&}md5=461981270098fdfa938721ecd3ca9471},
}

@Article{Kumara2013192,
  author   = {Kumara, I and Han, J and Colman, A and Kapuruge, M},
  title    = {{Runtime evolution of service-based multi-tenant SaaS applications}},
  journal  = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year     = {2013},
  volume   = {8274 LNCS},
  pages    = {192--206},
  abstract = {The Single-Instance Multi-Tenancy (SIMT) model for service delivery enables a SaaS provider to achieve economies of scale via the reuse and runtime sharing of software assets between tenants. However, evolving such an application at runtime to cope with the changing requirements from its different stakeholders is challenging. In this paper, we propose an approach to evolving service-based SIMT SaaS applications that are developed based on Dynamic Software Product Lines (DSPL) with runtime sharing and variation among tenants. We first identify the different kinds of changes to a service-based SaaS application, and the consequential impacts of those changes. We then discuss how to realize and manage each change and its resultant impacts in the DSPL. A software engineer declaratively specifies changes in a script, and realizes the changes to the runtime model of the DSPL using the script. We demonstrate the feasibility of our approach with a case study. {\textcopyright} 2013 Springer-Verlag.},
  annote   = {cited By 2},
  doi      = {10.1007/978-3-642-45005-1_14},
  keywords = {Compositional; Evolution; Feature; Multi tenancies,Computer software reusability,Software as a service (SaaS)},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84892386621{\&}doi=10.1007{\%}2F978-3-642-45005-1{\_}14{\&}partnerID=40{\&}md5=6cebcfa8d05eed97e93b2fa28623bbc5},
}

@Article{Teixeira2013,
  author    = {Teixeira, Leopoldo and Borba, Paulo and Gheyi, Rohit},
  title     = {{Safe composition of configuration knowledge-based software product lines}},
  journal   = {Journal of Systems and Software},
  year      = {2013},
  volume    = {86},
  number    = {4},
  pages     = {1038--1053},
  issn      = {01641212},
  abstract  = {Mistakes made when implementing or specifying the models of a Software Product Line (SPL) can result in ill-formed products - the safe composition problem. Such problem can hinder productivity and it might be hard to detect, since SPLs can have thousands of products. In this article, we propose a language independent approach for verifying safe composition of SPLs with dedicated Configuration Knowledge models. We translate feature model and Configuration Knowledge into propositional logic and use the Alloy Analyzer to perform the verification. To provide evidence for the generality of our approach, we instantiate this approach in different compositional settings. We deal with different kinds of assets such as use case scenarios and Eclipse RCP components. We analyze both the code and the requirements for a larger scale SPL, finding problems that affect thousands of products in minutes. Moreover, our evaluation suggests that the analysis time grows linearly with respect to the number of products in the analyzed SPLs. ?? 2012 Elsevier Inc.},
  doi       = {10.1016/j.jss.2012.11.006},
  file      = {:Users/mac/Library/Application Support/Mendeley Desktop/Downloaded/Teixeira, Borba, Gheyi - 2013 - Safe composition of configuration knowledge-based software product lines.pdf:pdf},
  isbn      = {9780769546032},
  keywords  = {Configuration Knowledge,Safe composition,Software Product Lines},
  publisher = {Elsevier Inc.},
  url       = {http://dx.doi.org/10.1016/j.jss.2012.11.006},
}

@Article{Neves201542,
  author   = {Neves, L and Borba, P and Alves, V and Turnes, L and Teixeira, L and Sena, D and Kulesza, U},
  title    = {{Safe evolution templates for software product lines}},
  journal  = {Journal of Systems and Software},
  year     = {2015},
  volume   = {106},
  pages    = {42--58},
  issn     = {0164-1212},
  abstract = {Abstract Software product lines enable generating related software products from reusable assets. Adopting a product line strategy can bring significant quality and productivity improvements. However, evolving a product line can be risky, since it might impact many products. When introducing new features or improving its design, it is important to make sure that the behavior of existing products is not affected. To ensure that, one usually has to analyze different types of artifacts, an activity that can lead to errors. To address this issue, in this work we discover and analyze concrete evolution scenarios from five different product lines. We discover a total of 13 safe evolution templates, which are generic transformations that developers can apply when evolving compositional and annotative product lines, with the goal of preserving the behavior of existing products. We also evaluate the templates by analyzing the evolution history of these product lines. In this evaluation, we observe that the templates can address the modifications that developers performed in the analyzed scenarios, which corroborates the expressiveness of our template set. We also observe that the templates could also have helped to avoid the errors that we identified during our analysis. },
  doi      = {https://doi.org/10.1016/j.jss.2015.04.024},
  keywords = {Evolution,Refinement,Software product lines},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121215000801},
}

@Article{SIERLA2014231,
  author   = {Sierla, Seppo and O'Halloran, Bryan M and Nikula, Heikki and Papakonstantinou, Nikolaos and Tumer, Irem Y},
  title    = {{Safety analysis of mechatronic product lines}},
  journal  = {Mechatronics},
  year     = {2014},
  volume   = {24},
  number   = {3},
  pages    = {231--240},
  issn     = {0957-4158},
  abstract = {Most methodologies for the design and analysis of mechatronic systems target a single product. From a business perspective, successful product development requires shortening development times, reducing engineering costs and offering a greater variety of product options for customers. In software engineering, the software product line (SPL) technology has been developed to meet these conflicting goals, and several major companies have reported success stories resulting from SPL adoption. In mechanical engineering, similar methodologies have been developed under the name of product platforms. Methodologies for analyzing product qualities such as safety or reliability have been introduced for both SPL and product platforms. The problem with these methodologies is that they consider either software or mechanical product design, so they do not guide developers to find the best balance between the controller and the equipment to be controlled. Several system properties of a mechatronic product line should be investigated with mechatronic analysis methodologies before the development process branches to software, electronic and mechanical design. In particular, safety is one system property that can only be analyzed by considering both the equipment and its controller, so mechatronic methodologies early in the design are advantageous for discovering safety-related design constraints before costly design commitments are made. This paper extends the Functional Failure Identification and Propagation (FFIP) framework to the safety analysis of a mechatronic product line with options in software signal connections and equipment. The result of applying FFIP is that unsafe combinations of options are removed from the product line.},
  doi      = {https://doi.org/10.1016/j.mechatronics.2014.02.003},
  keywords = {Functional Failure Identification and Propagation,Product line,Product platform,Risk analysis,Safety},
  url      = {http://www.sciencedirect.com/science/article/pii/S0957415814000312},
}

@Article{Liu20071879,
  author   = {Liu, Jing and Dehlinger, Josh and Lutz, Robyn},
  title    = {{Safety analysis of software product lines using state-based modeling}},
  journal  = {Journal of Systems and Software},
  year     = {2007},
  volume   = {80},
  number   = {11},
  pages    = {1879--1892},
  issn     = {0164-1212},
  abstract = {The difficulty of managing variations and their potential interactions across an entire product line currently hinders safety analysis in safety-critical, software product lines. The work described here contributes to a solution by integrating product-line safety analysis with model-based development. This approach provides a structured way to construct state-based models of a product line having significant, safety-related variations and to systematically explore the relationships between behavioral variations and potential hazardous states through scenario-guided executions of the state model over the variations. The paper uses a product line of safety-critical medical devices to demonstrate and evaluate the technique and results. },
  doi      = {https://doi.org/10.1016/j.jss.2007.01.047},
  keywords = {Model-based development,Product lines,Safety-critical systems,State-based modeling},
  url      = {http://www.sciencedirect.com/science/article/pii/S016412120700057X},
}

@Article{Quinton201655,
  author   = {Quinton, C and Romero, D and Duchien, L},
  title    = {{SALOON: A platform for selecting and configuring cloud environments}},
  journal  = {Software - Practice and Experience},
  year     = {2016},
  volume   = {46},
  number   = {1},
  pages    = {55--78},
  abstract = {Migrating legacy systems or deploying a new application to a cloud environment has recently become very trendy, because the number of cloud providers available is still increasing. These cloud environments provide a wide range of resources at different levels of functionality, which must be appropriately configured by stakeholders for the application to run properly. Handling this variability during the configuration and deployment stages is known as a complex and error-prone process, usually made in an ad hoc manner. In this paper, we propose SALOON, a software product lines-based platform to face these issues. We describe the architecture of the SALOON platform, which relies on feature models combined with a domain model used to select among cloud environments a well-suited one. SALOON supports stakeholders while configuring the selected cloud environment in a consistent way and automates the deployment of such configurations through the generation of executable configuration scripts. This paper also reports on some experiments, showing that using SALOON significantly reduces time to configure a cloud environment compared with a manual approach and provides a reliable way to find a correct and suitable configuration. Moreover, our empirical evaluation shows that our approach is effective and scalable to properly deal with a significant number of cloud environments. {\textcopyright} 2015 John Wiley {\&} Sons, Ltd.},
  annote   = {cited By 5},
  doi      = {10.1002/spe.2311},
  keywords = {Cloud computing; Computer software; Legacy systems,Cloud environments; Cloud providers; Empirical ev,Software design},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84955684730{\&}doi=10.1002{\%}2Fspe.2311{\&}partnerID=40{\&}md5=89bb5c31cca858f908e56667d4988d6a},
}

@Article{Siegmund2013491,
  author   = {Siegmund, N and Rosenm{\"{u}}ller, M and K{\"{a}}stner, C and Giarrusso, P G and Apel, S and Kolesnikov, S S},
  title    = {{Scalable prediction of non-functional properties in software product lines: Footprint and memory consumption}},
  journal  = {Information and Software Technology},
  year     = {2013},
  volume   = {55},
  number   = {3},
  pages    = {491--507},
  abstract = {Context: A software product line is a family of related software products, typically created from a set of common assets. Users select features to derive a product that fulfills their needs. Users often expect a product to have specific non-functional properties, such as a small footprint or a bounded response time. Because a product line may have an exponential number of products with respect to its features, it is usually not feasible to generate and measure non-functional properties for each possible product. Objective: Our overall goal is to derive optimal products with respect to non-functional requirements by showing customers which features must be selected. Method: We propose an approach to predict a product's non-functional properties based on the product's feature selection. We aggregate the influence of each selected feature on a non-functional property to predict a product's properties. We generate and measure a small set of products and, by comparing measurements, we approximate each feature's influence on the non-functional property in question. As a research method, we conducted controlled experiments and evaluated prediction accuracy for the non-functional properties footprint and main-memory consumption. But, in principle, our approach is applicable for all quantifiable non-functional properties. Results: With nine software product lines, we demonstrate that our approach predicts the footprint with an average accuracy of 94{\%}, and an accuracy of over 99{\%} on average if feature interactions are known. In a further series of experiments, we predicted main memory consumption of six customizable programs and achieved an accuracy of 89{\%} on average. Conclusion: Our experiments suggest that, with only few measurements, it is possible to accurately predict non-functional properties of products of a product line. Furthermore, we show how already little domain knowledge can improve predictions and discuss trade-offs between accuracy and required number of measurements. With this technique, we provide a basis for many reasoning and product-derivation approaches. {\textcopyright} 2012 Elsevier B.V. All rights reserved.},
  annote   = {cited By 28},
  doi      = {10.1016/j.infsof.2012.07.020},
  keywords = {Controlled experiment; Customizable; Domain knowle,Experiments; Measurements; Network architecture;,Forecasting},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872961131{\&}doi=10.1016{\%}2Fj.infsof.2012.07.020{\&}partnerID=40{\&}md5=6b156f0c57fa225ac4233c5604b091d6},
}

@Article{Thianniwet2016128,
  author   = {Thianniwet, T and Cohen, M B},
  title    = {{Scaling up the fitness function for reverse engineering feature models}},
  journal  = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year     = {2016},
  volume   = {9962 LNCS},
  pages    = {128--142},
  abstract = {Recent research on software product line engineering has led to several search-based frameworks for reverse engineering feature models. The most common fitness function utilized maximizes the number of matched products with an oracle set of products. However, to calculate this fitness each product defined by the chromosome has to be enumerated using a SAT solver and this limits scalability to product lines with fewer than 30 features. In this paper we propose SATff, a fitness function that simulates validity by computing the difference between constraints in the chromosome and oracle. In an empirical study on 101 feature models comparing SATff with two existing fitness functions that use the enumeration technique we find that SATff shows a significant improvement over one, and no significant difference with the other one. We also find that SATff requires only 7{\%} of the runtime on average scaling to feature models with as many as 97 features. {\textcopyright} Springer International Publishing AG 2016.},
  annote   = {cited By 0},
  doi      = {10.1007/978-3-319-47106-8_9},
  keywords = {Chromosomes; Computer software; Genetic algorithms,Empirical studies; Enumeration techniques; Featur,Software design},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84989920884{\&}doi=10.1007{\%}2F978-3-319-47106-8{\_}9{\&}partnerID=40{\&}md5=17b955197bdc3d3d0c031db010197984},
}

@Article{GonzalezHerrera2016398,
  author   = {Gonzalez-Herrera, I and Bourcier, J and Daubert, E and Rudametkin, W and Barais, O and Fouquet, F and J{\'{e}}z{\'{e}}quel, J M and Baudry, B},
  title    = {{ScapeGoat: Spotting abnormal resource usage in component-based reconfigurable software systems}},
  journal  = {Journal of Systems and Software},
  year     = {2016},
  volume   = {122},
  pages    = {398--415},
  issn     = {0164-1212},
  abstract = {Abstract Modern component frameworks support continuous deployment and simultaneous execution of multiple software components on top of the same virtual machine. However, isolation between the various components is limited. A faulty version of any one of the software components can compromise the whole system by consuming all available resources. In this paper, we address the problem of efficiently identifying faulty software components running simultaneously in a single virtual machine. Current solutions that perform permanent and extensive monitoring to detect anomalies induce high overhead on the system, and can, by themselves, make the system unstable. In this paper we present an optimistic adaptive monitoring system to determine the faulty components of an application. Suspected components are finely analyzed by the monitoring system, but only when required. Unsuspected components are left untouched and execute normally. Thus, we perform localized just-in-time monitoring that decreases the accumulated overhead of the monitoring system. We evaluate our approach on two case studies against a state-of-the-art monitoring system and show that our technique correctly detects faulty components, while reducing overhead by an average of 93{\%}. },
  doi      = {https://doi.org/10.1016/j.jss.2016.02.027},
  keywords = {Component,Models@Run.Time,Resource monitoring},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121216000595},
}

@Article{Colanzi2013970,
  author   = {Colanzi, Thelma Elita and Vergilio, Silvia Regina and Assun{\c{c}}{\~{a}}o, Wesley Klewerton Guez and Pozo, Aurora},
  title    = {{Search Based Software Engineering: Review and analysis of the field in Brazil}},
  journal  = {Journal of Systems and Software},
  year     = {2013},
  volume   = {86},
  number   = {4},
  pages    = {970--984},
  issn     = {0164-1212},
  abstract = {Search Based Software Engineering (SBSE) is the field of software engineering research and practice that applies search based techniques to solve different optimization problems from diverse software engineering areas. {\{}SBSE{\}} approaches allow software engineers to automatically obtain solutions for complex and labor-intensive tasks, contributing to reduce efforts and costs associated to the software development. The {\{}SBSE{\}} field is growing rapidly in Brazil. The number of published works and research groups has significantly increased in the last three years and a Brazilian {\{}SBSE{\}} community is emerging. This is mainly due to the Brazilian Workshop on Search Based Software Engineering (WOES), co-located with the Brazilian Symposium on Software Engineering (SBES). Considering these facts, this paper presents results of a mapping we have performed in order to provide an overview of the {\{}SBSE{\}} field in Brazil. The main goal is to map the Brazilian {\{}SBSE{\}} community on {\{}SBES{\}} by identifying the main researchers, focus of the published works, fora and frequency of publications. The paper also introduces {\{}SBSE{\}} concerns and discusses trends, challenges, and open research problems to this emergent area. We hope the work serves as a reference to this novel field, contributing to disseminate {\{}SBSE{\}} and to its consolidation in Brazil. },
  annote   = {{\{}SI{\}} : Software Engineering in Brazil: Retrospective and Prospective Views},
  doi      = {https://doi.org/10.1016/j.jss.2012.07.041},
  keywords = {Metaheuristics,Search based algorithms,Software engineering},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121212002166},
}

@Article{Ouni201755,
  author   = {Ouni, Ali and Kula, Raula Gaikovina and Kessentini, Marouane and Ishio, Takashi and German, Daniel M and Inoue, Katsuro},
  title    = {{Search-based software library recommendation using multi-objective optimization}},
  journal  = {Information and Software Technology},
  year     = {2017},
  volume   = {83},
  pages    = {55--75},
  issn     = {0950-5849},
  abstract = {Abstract Context: Software library reuse has significantly increased the productivity of software developers, reduced time-to-market and improved software quality and reusability. However, with the growing number of reusable software libraries in code repositories, finding and adopting a relevant software library becomes a fastidious and complex task for developers. Objective: In this paper, we propose a novel approach called LibFinder to prevent missed reuse opportunities during software maintenance and evolution. The goal is to provide a decision support for developers to easily find “useful” third-party libraries to the implementation of their software systems. Method: To this end, we used the non-dominated sorting genetic algorithm (NSGA-II), a multi-objective search-based algorithm, to find a trade-off between three objectives : 1) maximizing co-usage between a candidate library and the actual libraries used by a given system, 2) maximizing the semantic similarity between a candidate library and the source code of the system, and 3) minimizing the number of recommended libraries. Results: We evaluated our approach on 6083 different libraries from Maven Central super repository that were used by 32,760 client systems obtained from Github super repository. Our results show that our approach outperforms three other existing search techniques and a state-of-the art approach, not based on heuristic search, and succeeds in recommending useful libraries at an accuracy score of 92{\%}, precision of 51{\%} and recall of 68{\%}, while finding the best trade-off between the three considered objectives. Furthermore, we evaluate the usefulness of our approach in practice through an empirical study on two industrial Java systems with developers. Results show that the top 10 recommended libraries was rated by the original developers with an average of 3.25 out of 5. Conclusion: This study suggests that (1) library usage history collected from different client systems and (2) library semantics/content embodied in library identifiers should be balanced together for an efficient library recommendation technique. },
  doi      = {https://doi.org/10.1016/j.infsof.2016.11.007},
  keywords = {Multi-objective optimization,Search-based software engineering,Software library,Software reuse},
  url      = {http://www.sciencedirect.com/science/article/pii/S0950584916303652},
}

@Article{Mellado2014,
  author    = {Mellado, Daniel and Mouratidis, Haralambos and Fern{\'{a}}ndez-Medina, Eduardo},
  title     = {{Secure Tropos framework for software product lines requirements engineering}},
  journal   = {Computer Standards {\&} Interfaces},
  year      = {2014},
  volume    = {36},
  number    = {4},
  pages     = {711--722},
  issn      = {09205489},
  abstract  = {Security and requirements engineering are two of the most important factors of success in the development of a software product line (SPL). Goal-driven security requirements engineering approaches, such as Secure Tropos, have been proposed as a suitable paradigm for elicitation of security requirements and their analysis on both a social and a technical dimension. Nevertheless, goal-driven security requirements engineering methodologies are not appropriately tailored to the specific demands of SPL, while on the other hand specific proposals of SPL engineering have traditionally ignored security requirements. This paper presents work that fills this gap by proposing “SecureTropos-SPL” framework.},
  doi       = {10.1016/j.csi.2013.12.006},
  file      = {:Users/mac/Library/Application Support/Mendeley Desktop/Downloaded/Mellado, Mouratidis, Fern{\'{a}}ndez-Medina - 2014 - Secure Tropos framework for software product lines requirements engineering.pdf:pdf},
  keywords  = {Product lines,Requirements engineering,Secure Tropos,Security requirement engineering,Security requirements,security requirements},
  publisher = {Elsevier B.V.},
  url       = {http://linkinghub.elsevier.com/retrieve/pii/S0920548913001803},
}

@Article{Mellado20101094,
  author   = {Mellado, Daniel and Fern{\'{a}}ndez-Medina, Eduardo and Piattini, Mario},
  title    = {{Security requirements engineering framework for software product lines}},
  journal  = {Information and Software Technology},
  year     = {2010},
  volume   = {52},
  number   = {10},
  pages    = {1094--1117},
  issn     = {0950-5849},
  abstract = {Context The correct analysis and understanding of security requirements are important because they assist in the discovery of any security or requirement defects or mistakes during the early stages of development. Security requirements engineering is therefore both a central task and a critical success factor in product line development owing to the complexity and extensive nature of software product lines (SPL). However, most of the current {\{}SPL{\}} practices in requirements engineering do not adequately address security requirements engineering. Objective The aim of this approach is to describe a holistic security requirements engineering framework with which to facilitate the development of secure {\{}SPLs{\}} and their derived products. It will conform with the most relevant security standards with regard to the management of security requirements, such as ISO/IEC 27001 and ISO/IEC 15408. Results This framework is composed of: a security requirements engineering process for {\{}SPL{\}} (SREPPLine) driven by security standards; a Security Reference Meta Model to manage the variability of those {\{}SPL{\}} artefacts related to security requirements; and a tool (SREPPLineTool) which implements the meta-model and supports the process. Method A complete explanation of the framework will be provided. The process will be formally specified with {\{}SPEM{\}} 2.0 and the repository will be formally specified with an {\{}XML{\}} grammar. The application of {\{}SREPPLine{\}} and {\{}SREPPLineTool{\}} will be illustrated through a description of a simple example as a preliminary validation. Conclusion Although there have been several attempts to fill the gap between requirements engineering and {\{}SPL{\}} requirements engineering, no systematic approach with which to define security quality requirements and to manage their variability and their related security artefacts in {\{}SPL{\}} models is, as yet, available. The contribution of this work is that of providing a systematic approach for the management of the security requirements and their variability from the early stages of product line development in order to facilitate the conformance of {\{}SPL{\}} products with the most relevant security standards. },
  doi      = {https://doi.org/10.1016/j.infsof.2010.05.007},
  keywords = {ISO 27001,Product lines,Requirements engineering,Security requirement,Security requirements engineering,Security software engineering},
  url      = {http://www.sciencedirect.com/science/article/pii/S0950584910000960},
}

@Article{Gerostathopoulos2016378,
  author   = {Gerostathopoulos, Ilias and Bures, Tomas and Hnetynka, Petr and Keznikl, Jaroslav and Kit, Michal and Plasil, Frantisek and Plouzeau, No{\"{e}}l},
  title    = {{Self-adaptation in software-intensive cyber–physical systems: From system goals to architecture configurations}},
  journal  = {Journal of Systems and Software},
  year     = {2016},
  volume   = {122},
  pages    = {378--397},
  issn     = {0164-1212},
  abstract = {Abstract Design of self-adaptive software-intensive cyber–physical systems (siCPS) operating in dynamic environments is a significant challenge when a sufficient level of dependability is required. This stems partly from the fact that the concerns of self-adaptivity and dependability are to an extent contradictory. In this paper, we introduce IRM-SA (Invariant Refinement Method for Self-Adaptation)—a design method and associated formally grounded model targeting siCPS—that addresses self-adaptivity and supports dependability by providing traceability between system requirements, distinct situations in the environment, and predefined configurations of system architecture. Additionally, IRM-SA allows for architecture self-adaptation at runtime and integrates the mechanism of predictive monitoring that deals with operational uncertainty. As a proof of concept, it was implemented in DEECo, a component framework that is based on dynamic ensembles of components. Furthermore, its feasibility was evaluated in experimental settings assuming decentralized system operation. },
  doi      = {https://doi.org/10.1016/j.jss.2016.02.028},
  keywords = {Cyber–physical systems,Dependability,Self-adaptivity},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121216000601},
}

@Article{Pascual2015127,
  author   = {Pascual, Gustavo G and Pinto, M{\'{o}}nica and Fuentes, Lidia},
  title    = {{Self-adaptation of mobile systems driven by the Common Variability Language}},
  journal  = {Future Generation Computer Systems},
  year     = {2015},
  volume   = {47},
  pages    = {127--144},
  issn     = {0167-739X},
  abstract = {Abstract The execution context in which pervasive systems or mobile computing run changes continually. Hence, applications for these systems require support for self-adaptation to the continual context changes. Most of the approaches for self-adaptive systems implement a reconfiguration service that receives as input the list of all possible configurations and the plans to switch between them. In this paper we present an alternative approach for the automatic generation of application configurations and the reconfiguration plans at runtime. With our approach, the generated configurations are optimal as regards different criteria, such as functionality or resource consumption (e.g. battery or memory). This is achieved by: (1) modelling architectural variability at design-time using the Common Variability Language (CVL), and (2) using a genetic algorithm that finds nearly-optimal configurations at run-time using the information provided by the variability model. We also specify a case study and we use it to evaluate our approach, showing that it is efficient and suitable for devices with scarce resources. },
  annote   = {Special Section: Advanced Architectures for the Future Generation of Software-Intensive Systems},
  doi      = {https://doi.org/10.1016/j.future.2014.08.015},
  keywords = {Architectural variability,CVL,Context,Dynamic reconfiguration,Genetic algorithm,Pervasive systems},
  url      = {http://www.sciencedirect.com/science/article/pii/S0167739X14001630},
}

@Article{Peng20122707,
  author   = {Peng, Xin and Chen, Bihuan and Yu, Yijun and Zhao, Wenyun},
  title    = {{Self-tuning of software systems through dynamic quality tradeoff and value-based feedback control loop}},
  journal  = {Journal of Systems and Software},
  year     = {2012},
  volume   = {85},
  number   = {12},
  pages    = {2707--2719},
  issn     = {0164-1212},
  abstract = {Quality requirements of a software system cannot be optimally met, especially when it is running in an uncertain and changing environment. In principle, a controller at runtime can monitor the change impact on quality requirements of the system, update the expectations and priorities from the environment, and take reasonable actions to improve the overall satisfaction. In practice, however, existing controllers are mostly designed for tuning low-level performance indicators instead of high-level requirements. By maintaining a live goal model to represent runtime requirements and linking the overall satisfaction of quality requirements to an indicator of earned business value, we propose a control-theoretic self-tuning method that can dynamically tune the preferences of different quality requirements, and can autonomously make tradeoff decisions through our Preference-Based Goal Reasoning procedure. The reasoning procedure results in an optimal configuration of the variation points by selecting the right alternative of OR-decomposed goals and such a configuration is mapped onto corresponding system architecture reconfigurations. The effectiveness of our self-tuning method is evaluated by earned business value, comparing our results with those obtained using static and ad hoc methods. },
  annote   = {Self-Adaptive Systems},
  doi      = {https://doi.org/10.1016/j.jss.2012.04.079},
  keywords = {Earned business value,Feedback control theory,Goal-oriented reasoning,Preference,Self-tuning},
  url      = {http://www.sciencedirect.com/science/article/pii/S016412121200132X},
}

@Article{Cengarle2006141,
  author   = {Cengarle, Mar{\'{i}}a Victoria and Graubmann, Peter and Wagner, Stefan},
  title    = {{Semantics of {\{}UML{\}} 2.0 Interactions with Variabilities}},
  journal  = {Electronic Notes in Theoretical Computer Science},
  year     = {2006},
  volume   = {160},
  pages    = {141--155},
  issn     = {1571-0661},
  abstract = {Means for the representation of variability in {\{}UML{\}} 2.0 interactions, as presented in a previous work, are further formalised and given a mathematically formal semantics. In this way, {\{}UML{\}} 2.0 interactions can be used in the conception and development of system families within domain and application engineering tasks. Following the transition from domain to application engineering as a configuration endeavour, resolution of the variability according to a given configuration is captured by a denotational semantics for plain interactions extended to the features for the specification of variability. An example based on a previous case study explicates the semantics hereby defined. },
  annote   = {Proceedings of the International Workshop on Formal Aspects of Component Software (FACS 2005)Proceedings of the International Workshop on Formal Aspects of Component Software (FACS 2005)},
  doi      = {https://doi.org/10.1016/j.entcs.2006.05.020},
  keywords = {formal semantics,product lines,system families,variability,{\{}UML{\}} interactions},
  url      = {http://www.sciencedirect.com/science/article/pii/S1571066106003823},
}

@Article{Haitzer2014135,
  author   = {Haitzer, Thomas and Zdun, Uwe},
  title    = {{Semi-automated architectural abstraction specifications for supporting software evolution}},
  journal  = {Science of Computer Programming},
  year     = {2014},
  volume   = {90, Part B},
  pages    = {135--160},
  issn     = {0167-6423},
  abstract = {Abstract In this paper we present an approach for supporting the semi-automated architectural abstraction of architectural models throughout the software life-cycle. It addresses the problem that the design and implementation of a software system often drift apart as software systems evolve, leading to architectural knowledge evaporation. Our approach provides concepts and tool support for the semi-automatic abstraction of architecture component and connector views from implemented systems and keeping the abstracted architecture models up-to-date during software evolution. In particular, we propose architecture abstraction concepts that are supported through a domain-specific language (DSL). Our main focus is on providing architectural abstraction specifications in the {\{}DSL{\}} that only need to be changed, if the architecture changes, but can tolerate non-architectural changes in the underlying source code. Once the software architect has defined an architectural abstraction in the DSL, we can automatically generate architectural component views from the source code using model-driven development (MDD) techniques and check whether architectural design constraints are fulfilled by these models. Our approach supports the automatic generation of traceability links between source code elements and architectural abstractions using {\{}MDD{\}} techniques to enable software architects to easily link between components and the source code elements that realize them. It enables software architects to compare different versions of the generated architectural component view with each other. We evaluate our research results by studying the evolution of architectural abstractions in different consecutive versions of five open source systems and by analyzing the performance of our approach in these cases. },
  annote   = {Special Issue on Component-Based Software Engineering and Software Architecture},
  doi      = {https://doi.org/10.1016/j.scico.2013.10.004},
  keywords = {Architectural abstraction,Architectural component and connector views,Model transformation,Software evolution,UML},
  url      = {http://www.sciencedirect.com/science/article/pii/S0167642313002542},
}

@Article{Hubaux:2013:SCF:2501654.2501665,
  author    = {Hubaux, Arnaud and Tun, Thein Than and Heymans, Patrick},
  title     = {{Separation of Concerns in Feature Diagram Languages: A Systematic Survey}},
  journal   = {ACM Comput. Surv.},
  year      = {2013},
  volume    = {45},
  number    = {4},
  pages     = {51:1----51:23},
  issn      = {0360-0300},
  address   = {New York, NY, USA},
  doi       = {10.1145/2501654.2501665},
  keywords  = {Software product line,feature diagram,separation of concerns,variability},
  publisher = {ACM},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/2501654.2501665},
}

@Article{Çelik20132520,
  author   = {{\c{C}}elik, Turgay and Tekinerdogan, Bedir},
  title    = {{S-IDE: A tool framework for optimizing deployment architecture of High Level Architecture based simulation systems}},
  journal  = {Journal of Systems and Software},
  year     = {2013},
  volume   = {86},
  number   = {10},
  pages    = {2520--2541},
  issn     = {0164-1212},
  abstract = {Abstract One of the important problems in High Level Architecture (HLA) based distributed simulation systems is the allocation of the different simulation modules to the available physical resources. Usually, the deployment of the simulation modules to the physical resources can be done in many different ways, and each deployment alternative will have a different impact on the performance. Although different algorithmic solutions have been provided to optimize the allocation with respect to the performance, the problem has not been explicitly tackled from an architecture design perspective. Moreover, for optimizing the deployment of the simulation system, tool support is largely missing. In this paper we propose a method for automatically deriving deployment alternatives for {\{}HLA{\}} based distributed simulation systems. The method extends the {\{}IEEE{\}} Recommended Practice for High Level Architecture Federation Development and Execution Process by providing an approach for optimizing the allocation at the design level. The method is realized by the tool framework, S-IDE (Simulation-IDE) that we have developed to provide an integrated development environment for deriving a feasible deployment alternative based on the simulation system and the available physical resources at the design phase. The method and the tool support have been validated using a case study for the development of a traffic simulation system. },
  doi      = {https://doi.org/10.1016/j.jss.2013.03.013},
  keywords = {Deployment model optimization,Distributed simulation,FEDEP,High Level Architecture (HLA),Metamodel based tool development,Metamodeling,Model transformations,Software architecture},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121213000599},
}

@Article{Bertolino2015355,
  author   = {Bertolino, Antonia and Daoudagh, Said and Kateb, Donia El and Henard, Christopher and Traon, Yves Le and Lonetti, Francesca and Marchetti, Eda and Mouelhi, Tejeddine and Papadakis, Mike},
  title    = {{Similarity testing for access control}},
  journal  = {Information and Software Technology},
  year     = {2015},
  volume   = {58},
  pages    = {355--372},
  issn     = {0950-5849},
  abstract = {AbstractContext Access control is among the most important security mechanisms, and {\{}XACML{\}} is the de facto standard for specifying, storing and deploying access control policies. Since it is critical that enforced policies are correct, policy testing must be performed in an effective way to identify potential security flaws and bugs. In practice, exhaustive testing is impossible due to budget constraints. Therefore the tests need to be prioritized so that resources are focused on their most relevant subset. Objective This paper tackles the issue of access control test prioritization. It proposes a new approach for access control test prioritization that relies on similarity. Method The approach has been applied to several policies and the results have been compared to random prioritization (as a baseline). To assess the different prioritization criteria, we use mutation analysis and compute the mutation scores reached by each criterion. This helps assessing the rate of fault detection. Results The empirical results indicate that our proposed approach is effective and its rate of fault detection is higher than that of random prioritization. Conclusion We conclude that prioritization of access control test cases can be usefully based on similarity criteria. },
  doi      = {https://doi.org/10.1016/j.infsof.2014.07.003},
  keywords = {Security policies,Similarity,Test prioritization},
  url      = {http://www.sciencedirect.com/science/article/pii/S0950584914001578},
}

@Article{BEHJATI2013607,
  author   = {Behjati, Razieh and Yue, Tao and Briand, Lionel and Selic, Bran},
  title    = {{SimPL: A product-line modeling methodology for families of integrated control systems}},
  journal  = {Information and Software Technology},
  year     = {2013},
  volume   = {55},
  number   = {3},
  pages    = {607--629},
  issn     = {0950-5849},
  abstract = {Context
Integrated control systems (ICSs) are heterogeneous systems where software and hardware components are integrated to control and monitor physical devices and processes. A family of ICSs share the same software code base, which is configured differently for each product to form a unique installation. Due to the complexity of ICSs and inadequate automation support, product configuration in this context is typically error-prone and costly.
Objective
As a first step to overcome these challenges, we propose a UML-based product-line modeling methodology that provides a foundation for semi-automated product configuration in the specific context of ICSs.
Method
We performed a comprehensive domain analysis to identify characteristics of ICS families, and their configuration challenges. Based on this, we formulated the characteristics of an adequate configuration solution, and derived from them a set of modeling requirements for a model-based solution to configuration. The SimPL methodology is proposed to fulfill these requirements.
Results
To evaluate the ability of SimPL to fulfill the modeling requirements, we applied it to a large-scale industrial case study. Our experience with the case study shows that SimPL is adequate to provide a model of the product family that meets the modeling requirements. Further evaluation is still required to assess the applicability and scalability of SimPL in practice. Doing this requires conducting field studies with human subjects and is left for future work.
Conclusion
We conclude that configuration in ICSs requires better automation support, and UML-based approaches to product family modeling can be tailored to provide the required foundation.},
  annote   = {Special Issue on Software Reuse and Product Lines},
  doi      = {https://doi.org/10.1016/j.infsof.2012.09.006},
  keywords = {Integrated control systems,MARTE,Product-line engineering,UML,Variability modeling},
  url      = {http://www.sciencedirect.com/science/article/pii/S0950584912002005},
}

@Article{Heider2010758,
  author   = {Heider, Wolfgang and Froschauer, Roman and Gr{\"{u}}nbacher, Paul and Rabiser, Rick and Dhungana, Deepak},
  title    = {{Simulating evolution in model-based product line engineering}},
  journal  = {Information and Software Technology},
  year     = {2010},
  volume   = {52},
  number   = {7},
  pages    = {758--769},
  issn     = {0950-5849},
  abstract = {Context Numerous approaches are available for modeling product lines and their variability. However, the long-term impacts of model-based development on maintenance effort and model complexity can hardly be investigated due to a lack of empirical data. Conducting empirical research in product line engineering is difficult as companies are typically reluctant to provide access to data from their product lines. Also, many benefits of product lines can be measured only in longitudinal studies, which are difficult to perform in most environments. Objective In this paper, we thus aim to explore the benefit of simulation to investigate the evolution of model-based product lines. Method We present a simulation approach for exploring the effects of product line evolution on model complexity and maintenance effort. Our simulation considers characteristics of product lines (e.g., size, dependencies in models) and we experiment with different evolution profiles (e.g., technical refactoring vs. placement of new products). Results We apply the approach in a simulation experiment that uses data from real-world product lines from the domain of industrial automation systems to demonstrate its feasibility. Conclusion Our results demonstrate that simulation contributes to understanding the effects of maintenance and evolution in model-based product lines. },
  doi      = {https://doi.org/10.1016/j.infsof.2010.03.007},
  keywords = {Industrial automation systems,Maintenance and evolution,Model-based development,Product line engineering,Simulation},
  url      = {http://www.sciencedirect.com/science/article/pii/S0950584910000479},
}

@Article{Hierons:2016:SOP:2913009.2897760,
  author    = {Hierons, Robert M and Li, Miqing and Liu, Xiaohui and Segura, Sergio and Zheng, Wei},
  title     = {{SIP: Optimal Product Selection from Feature Models Using Many-Objective Evolutionary Optimization}},
  journal   = {ACM Trans. Softw. Eng. Methodol.},
  year      = {2016},
  volume    = {25},
  number    = {2},
  pages     = {17:1----17:39},
  issn      = {1049-331X},
  address   = {New York, NY, USA},
  doi       = {10.1145/2897760},
  keywords  = {Product selection},
  publisher = {ACM},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/2897760},
}

@Article{BerntssonSvensson2012175,
  author   = {{Berntsson Svensson}, R and Aurum, A and Paech, B and Gorschek, T and Sharma, D},
  title    = {{Software architecture as a means of communication in a globally distributed software development context}},
  journal  = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year     = {2012},
  volume   = {7343 LNCS},
  pages    = {175--189},
  abstract = {The management and coordination of globally distributed development poses many new challenges, including compensating for informal implicit communication, which is aggravated by heterogeneous social and engineering traditions between development sites. Although much research has gone into identifying challenges and working with practical solutions, such as tools for communication, little research has focused on comparing communication mechanisms in terms of their ability to provide large volumes of rich information in a timely manner. Data was collected through in-depth interviews with eleven practitioners and twenty-eight responses through a web-based questionnaire from three product lines at an international software development organization. This paper assesses the relative importance of ten commonly used communication mechanisms and practices across local and global development sites. The results clearly indicate that some communication mechanisms are more important than others in providing large volumes of rich information in a timely manner. The prevalence of architecture in providing rich information in large volumes for both local and global communication can be clearly observed. {\textcopyright} 2012 Springer-Verlag.},
  annote   = {cited By 3},
  doi      = {10.1007/978-3-642-31063-8_14},
  keywords = {Communication mechanisms; Distributed development;,Communication; Research; Software architecture,Software design},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862196503{\&}doi=10.1007{\%}2F978-3-642-31063-8{\_}14{\&}partnerID=40{\&}md5=1c6a9d0ce3ffb7ffdd95be625b0461ae},
}

@Article{Unphon20102211,
  author   = {Unphon, Hataichanok and Dittrich, Yvonne},
  title    = {{Software architecture awareness in long-term software product evolution}},
  journal  = {Journal of Systems and Software},
  year     = {2010},
  volume   = {83},
  number   = {11},
  pages    = {2211--2226},
  issn     = {0164-1212},
  abstract = {Software architecture has been established in software engineering for almost 40 years. When developing and evolving software products, architecture is expected to be even more relevant compared to contract development. However, the research results seem not to have influenced the development practice around software products very much. The architecture often only exists implicitly in discussions that accompany the development. Nonetheless many of the software products have been used for over 10, or even 20 years. How do development teams manage to accommodate changing needs and at the same time maintain the quality of the product? In order to answer this question, grounded theory study based on 15 semi-structured interviews was conducted in order to find out about the wide spectrum of architecture practices in software product developing organisations. Our results indicate that a chief architect or central developer acts as a ‘walking architecture' devising changes and discussing local designs while at the same time updating his own knowledge about problematic aspects that need to be addressed. Architecture documentation and representations might not be used, especially if they replace the feedback from on-going developments into the ‘architecturing' practices. Referring to results from Computer Supported Cooperative Work, we discuss how explicating the existing architecture needs to be complemented by social protocols to support the communication and knowledge sharing processes of the ‘walking architecture'. },
  annote   = {Interplay between Usability Evaluation and Software Development},
  doi      = {https://doi.org/10.1016/j.jss.2010.06.043},
  keywords = {Architecture knowledge management,Cooperative and human aspects,Long-term evolution,Qualitative empirical studies,Software architecture,Software products},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121210001743},
}

@Article{Tibermacine201637,
  author   = {Tibermacine, Chouki and Sadou, Salah and That, Minh Tu Ton and Dony, Christophe},
  title    = {{Software architecture constraint reuse-by-composition}},
  journal  = {Future Generation Computer Systems},
  year     = {2016},
  volume   = {61},
  pages    = {37--53},
  issn     = {0167-739X},
  abstract = {Abstract Architecture constraints are specifications which enable developers to formalize design rules that architectures should respect, like the topological conditions of a given architecture pattern or style. These constraints can serve as a documentation to better understand an existing architecture description, or can serve as invariants that can be checked after the application of an architecture change to see whether design rules still hold. Like any specifications, architecture constraints are frequently subject to reuse. Besides, these constraints are specified and checked during architecture design time, when component descriptions are specified or selected from repositories, then instantiated and connected together to define architecture descriptions. These two facts (being subject to reuse and instantiation/connection) make architecture constraints good candidates for component-based design within a unified environment. In this paper, we propose a component model for specifying architecture constraints. This model has been implemented as an extension to an {\{}ADL{\}} that we have developed, which is called CLACS. The obtained process advocates the idea of specifying architecture constraints using the same paradigm of component-based development as for architecture description. To evaluate the component model, we conducted an experiment with a catalog of constraints formalizing the topological conditions of architecture patterns. The results of this experiment showed that constraint specification is improved by this reuse-by-composition model. },
  doi      = {https://doi.org/10.1016/j.future.2016.02.006},
  keywords = {Architecture constraint,Architecture description,OCL,Software component},
  url      = {http://www.sciencedirect.com/science/article/pii/S0167739X1630019X},
}

@Article{Tekinerdogan2008558,
  author   = {Tekinerdogan, Bedir and Sozer, Hasan and Aksit, Mehmet},
  title    = {{Software architecture reliability analysis using failure scenarios}},
  journal  = {Journal of Systems and Software},
  year     = {2008},
  volume   = {81},
  number   = {4},
  pages    = {558--575},
  issn     = {0164-1212},
  abstract = {With the increasing size and complexity of software in embedded systems, software has now become a primary threat for the reliability. Several mature conventional reliability engineering techniques exist in literature but traditionally these have primarily addressed failures in hardware components and usually assume the availability of a running system. Software architecture analysis methods aim to analyze the quality of software-intensive system early at the software architecture design level and before a system is implemented. We propose a Software Architecture Reliability Analysis Approach (SARAH) that benefits from mature reliability engineering techniques and scenario-based software architecture analysis to provide an early software reliability analysis at the architecture design level. {\{}SARAH{\}} defines the notion of failure scenario model that is based on the Failure Modes and Effects Analysis method (FMEA) in the reliability engineering domain. The failure scenario model is applied to represent so-called failure scenarios that are utilized to derive fault tree sets (FTS). Fault tree sets are utilized to provide a severity analysis for the overall software architecture and the individual architectural elements. Despite conventional reliability analysis techniques which prioritize failures based on criteria such as safety concerns, in {\{}SARAH{\}} failure scenarios are prioritized based on severity from the end-user perspective. {\{}SARAH{\}} results in a failure analysis report that can be utilized to identify architectural tactics for improving the reliability of the software architecture. The approach is illustrated using an industrial case for analyzing reliability of the software architecture of the next release of a Digital TV. },
  annote   = {Selected papers from the 10th Conference on Software Maintenance and Reengineering (CSMR 2006)},
  doi      = {https://doi.org/10.1016/j.jss.2007.10.029},
  keywords = {FMEA,Fault trees,Reliability analysis,Scenario-based architectural evaluation},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121207003032},
}

@Article{Kaur2017152,
  author   = {Kaur, Loveleen and Mishra, Ashutosh},
  title    = {{Software component and the semantic Web: An in-depth content analysis and integration history}},
  journal  = {Journal of Systems and Software},
  year     = {2017},
  volume   = {125},
  pages    = {152--169},
  issn     = {0164-1212},
  abstract = {Abstract With the advent of Component-based software engineering (CBSE), large software systems are being built by integrating pre-built software components. The Semantic Web in association with {\{}CBSE{\}} has shown to offer powerful representation facilities and reasoning techniques to enhance and support querying, reasoning, discovery, etc. of software components. The goal of this paper is to research the applicability of Semantic Web technologies in performing the various tasks of {\{}CBSE{\}} and review the experimental results of the same in an easy and effective manner. To the best of our knowledge, this is the first study which provides an extensive review of the application of Semantic Web in {\{}CBSE{\}} from different perspectives. A systematic literature review of the Semantic Web approaches, employed for use in CBSE, reported from 2001 until 2015, is conducted in this research article. Empirical results have been drawn through the question-answer based analysis of the research, which clearly tells the year wise trend of the research articles, with the possible justification of the usage of Semantic Web technology and tools for a particular phase of CBSE. To conclude, gaps in the current research and potential future prospects have been discussed. },
  doi      = {https://doi.org/10.1016/j.jss.2016.11.028},
  keywords = {Component-based software engineering,Linked Data,Ontology,Reasoners,Semantic Web,Web services},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121216302308},
}

@Article{DITTRICH20141436,
  author   = {Dittrich, Yvonne},
  title    = {{Software engineering beyond the project – Sustaining software ecosystems}},
  journal  = {Information and Software Technology},
  year     = {2014},
  volume   = {56},
  number   = {11},
  pages    = {1436--1456},
  issn     = {0950-5849},
  abstract = {Context
The main part of software engineering methods, tools and technologies has developed around projects as the central organisational form of software development. A project organisation depends on clear bounds regarding scope, participants, development effort and lead-time. What happens when these conditions are not given? The article claims that this is the case for software product specific ecosystems. As software is increasingly developed, adopted and deployed in the form of customisable and configurable products, software engineering as a discipline needs to take on the challenge to support software ecosystems.
Objective
The article provides a holistic understanding of the observed and reported practices as a starting point to device specific support for the development in software ecosystems.
Method
A qualitative interview study was designed based on previous long-term ethnographical inspired research.
Results
The analysis results in a set of common features of product development and evolution despite differences in size, kind of software and business models. Design is distributed and needs to be coordinated across heterogeneous design constituencies that, together with the software, build a product specific socio-technical ecosystem. The technical design has to support the deference of part of the development not only to 3rd-party developers but also to local designers tailoring the software in the use organisation. The technical interfaces that separate the work of different design constituencies are contested and need to be maintained permanently. Development takes place as cycles within cycles – overlaying development cycles with different rhythms to accommodate different evolution drivers.
Conclusion
The reported practices challenge some of the very core assumptions of traditional software engineering, but makes perfect sense, considering that the frame of reference for product development is not a project but continuous innovation across the respective ecosystem. The article provides a number of concrete points for further research.},
  annote   = {Special issue on Software Ecosystems},
  doi      = {https://doi.org/10.1016/j.infsof.2014.02.012},
  keywords = {Qualitative empirical research,Software ecosystems,Software product development},
  url      = {http://www.sciencedirect.com/science/article/pii/S0950584914000652},
}

@Article{DelRosso20081,
  author   = {Rosso, Christian Del},
  title    = {{Software performance tuning of software product family architectures: Two case studies in the real-time embedded systems domain}},
  journal  = {Journal of Systems and Software},
  year     = {2008},
  volume   = {81},
  number   = {1},
  pages    = {1--19},
  issn     = {0164-1212},
  abstract = {Software performance is an important non-functional quality attribute and software performance evaluation is an essential activity in the software development process. Especially in embedded real-time systems, software design and evaluation are driven by the needs to optimize the limited resources, to respect time deadlines and, at the same time, to produce the best experience for end-users. Software product family architectures add additional requirements to the evaluation process. In this case, the evaluation includes the analysis of the optimizations and tradeoffs for the whole products in the family. Performance evaluation of software product family architectures requires knowledge and a clear understanding of different domains: software architecture assessments, software performance and software product family architecture. We have used a scenario-driven approach to evaluate performance and dynamic memory management efficiency in one Nokia software product family architecture. In this paper we present two case studies. Furthermore, we discuss the implications and tradeoffs of software performance against evolvability and maintenability in software product family architectures. },
  doi      = {https://doi.org/10.1016/j.jss.2007.07.006},
  keywords = {Dynamic memory management,Embedded real-time systems,Software architecture assessments,Software performance,Software product family},
  url      = {http://www.sciencedirect.com/science/article/pii/S016412120700180X},
}

@Article{Ianzen201354,
  author   = {Ianzen, Andressa and Mauda, Everson Carlos and Paludo, Marco Ant{\^{o}}nio and Reinehr, Sheila and Malucelli, Andreia},
  title    = {{Software process improvement in a financial organization: An action research approach}},
  journal  = {Computer Standards {\&} Interfaces},
  year     = {2013},
  volume   = {36},
  number   = {1},
  pages    = {54--65},
  issn     = {0920-5489},
  abstract = {Abstract In order to increase the quality of systems of a financial company, the process of a software development team has changed some times to get stabilized. This paper presents the action research steps that were conducted, the perceptions of the team about the process evolution and the solved problems. Also, a software process improvement assessment has been conducted in order to identify the success factors on this implementation and the result is analyzed and discussed through the Servqual method. Among other conclusions, the involvement of the team during the improvement process and future perspectives are crucial to achieve success. },
  doi      = {https://doi.org/10.1016/j.csi.2013.07.002},
  keywords = {Software engineering,Software process evaluation,Software process improvement,Software quality improvement,Systems development},
  url      = {http://www.sciencedirect.com/science/article/pii/S0920548913000676},
}

@Article{PETERSEN20101275,
  author   = {Petersen, Kai and Wohlin, Claes},
  title    = {{Software process improvement through the Lean Measurement (SPI-LEAM) method}},
  journal  = {Journal of Systems and Software},
  year     = {2010},
  volume   = {83},
  number   = {7},
  pages    = {1275--1287},
  issn     = {0164-1212},
  abstract = {Software process improvement methods help to continuously refine and adjust the software process to improve its performance (e.g., in terms of lead-time, quality of the software product, reduction of change requests, and so forth). Lean software development propagates two important principles that help process improvement, namely identification of waste in the process and considering interactions between the individual parts of the software process from an end-to-end perspective. A large shift of thinking about the own way of working is often required to adopt lean. One of the potential main sources of failure is to try to make a too large shift about the ways of working at once. Therefore, the change to lean has to be done in a continuous and incremental way. In response to this we propose a novel approach to bring together the quality improvement paradigm and lean software development practices, the approach being called Software Process Improvement through the Lean Measurement (SPI-LEAM) Method. The method allows to assess the performance of the development process and take continuous actions to arrive at a more lean software process over time. The method is under implementation in industry and an initial evaluation of the method has been performed.},
  annote   = {SPLC 2008},
  doi      = {https://doi.org/10.1016/j.jss.2010.02.005},
  keywords = {Lean software development,Quality improvement paradigm,Software process improvement},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121210000403},
}

@Article{Kuvaja2011143,
  author        = {Kuvaja, P and Simil{\"{a}}, J and Hanhela, H},
  title         = {{Software product line adoption - Guidelines from a case study}},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year          = {2011},
  volume        = {4980 LNCS},
  pages         = {143--157},
  abstract      = {It is possible to proceed with software product line adoption only once without major reinvestments and loss of time and money. In the literature, reported experiences of using the adoption models are not to be found, and especially the suitability of the models has not been reported. The purpose of this research is to compare known adoption models by formulating general evaluation criteria for the selection of an adoption model. Next an adoption model is selected for empirical research based on the context of a multimedia unit of a global telecommunication company. The empirical part consists of a case study analyzing the present state of adoption and producing plans for proceeding with the adoption. The research results can be utilized when selecting an adoption model for an empirical case and adopting a software product line in a software intensive organization. {\textcopyright} 2011 IFIP International Federation for Information Processing.},
  annote        = {cited By 4},
  doi           = {10.1007/978-3-642-22386-0_11},
  keywords      = {Adoption model,Computer software,Investm,Investments,Research,Software design,adoption,adoption strategy,case study,guide},
  mendeley-tags = {case study},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053153703{\&}doi=10.1007{\%}2F978-3-642-22386-0{\_}11{\&}partnerID=40{\&}md5=94cb976461fc93d4a97d1b1adf462476},
}

@Article{Brugali201889,
  author   = {Brugali, D and Hochgeschwender, N},
  title    = {{Software product line engineering for robotic perception systems}},
  journal  = {International Journal of Semantic Computing},
  year     = {2018},
  volume   = {12},
  number   = {1},
  pages    = {89--107},
  abstract = {Control systems for autonomous robots are concurrent, distributed, embedded, real-time and data intensive software systems. A real-world robot control system is composed of tens of software components. For each component providing robotic functionality, tens of different implementations may be available. The difficult challenge in robotic system engineering consists in selecting a coherent set of components, which provide the functionality required by the application requirements, taking into account their mutual dependencies. This challenge is exacerbated by the fact that robotics system integrators and application developers are usually not specifically trained in software engineering. In various application domains, software product line (SPL) development has proven to be the most effective approach to face this kind of challenges. In a previous paper [D. Brugali and N. Hochgeschwender, Managing the functional variability of robotic perception systems, in First IEEE Int. Conf. Robotic Computing, 2017, pp. 277-283.] we have presented a model-based approach to the development of SPL for robotic perception systems, which integrates two modeling technologies developed by the authors: The HyperFlex toolkit [L. Gherardi and D. Brugali, Modeling and reusing robotic software architectures: The HyperFlex toolchain, in IEEE Int. Conf. Robotics and Automation, 2014, pp. 6414-6420.] and the Robot Perception Specification Language (RPSL) [N. Hochgeschwender, S. Schneider, H. Voos and G. K. Kraetzschmar, Declarative specification of robot perception architectures, in 4th Int. Conf. Simulation, Modeling, and Programming for Autonomous Robots, 2014, pp. 291-302.]. This paper extends our previous work by illustrating the entire development process of an SPL for robot perception systems with a real case study. {\textcopyright} 2018 World Scientific Publishing Company.},
  annote   = {cited By 0},
  doi      = {10.1142/S1793351X18400056},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051567683{\&}doi=10.1142{\%}2FS1793351X18400056{\&}partnerID=40{\&}md5=3f5695b83cad5bd31b567996e47a51b2},
}

@Article{DaSilva2014,
  author    = {{Da Silva}, Ivonei Freitas and {Da Mota Silveira Neto}, Paulo Anselmo and O'Leary, P{\'{a}}draig and {De Almeida}, Eduardo Santana and Meira, Silvio Romero De Lemos},
  title     = {{Software product line scoping and requirements engineering in a small and medium-sized enterprise: An industrial case study}},
  journal   = {Journal of Systems and Software},
  year      = {2014},
  volume    = {88},
  number    = {1},
  pages     = {189--206},
  issn      = {01641212},
  abstract  = {Software product line (SPL) engineering has been applied in several domains, especially in large-scale software development. Given the benefits experienced and reported, SPL engineering has increasingly garnered interest from small to medium-sized companies. It is possible to find a wide range of studies reporting on the challenges of running a SPL project in large companies. However, very little reports exist that consider the situation for small to medium-sized enterprises and these studies try develop universal truths for SPL without lessons learned from empirical evidence need to be contextualized. This study is a step towards bridging this gap in contextual evidence by characterizing the weaknesses discovered in the scoping (SC) and requirements (RE) disciplines of SPL. Moreover, in this study we conducted a case study in a small to medium sized enterprises (SMEs) to justify the use of agile methods when introducing the SPL SC and RE disciplines through the characterization of their bottlenecks. The results of the characterization indicated that ineffective communication and collaboration, long iteration cycles, and the absence of adaptability and flexibility can increase the effort and reduce motivation during project development. These issues can be mitigated by agile methods. {\textcopyright} 2013 Elsevier Inc. All rights reserved.},
  doi       = {10.1016/j.jss.2013.10.040},
  file      = {:Users/mac/ownCloud/Anita/SLR/REVISION DE JOURNALS/PAPERS{\_}A/selected/checked/casos de estudios analizados/1 Software product line scoping and requirements engineering in a small and medium-sized enterprise- An industrial case stud.pdf:pdf},
  isbn      = {0164-1212},
  keywords  = {Agile methods,Requirements engineering,Software product line scoping},
  publisher = {Elsevier Inc.},
}

@Article{Vale20171,
  author   = {Vale, Tassio and de Almeida, Eduardo Santana and Alves, Vander and Kulesza, Uir{\'{a}} and Niu, Nan and de Lima, Ricardo},
  title    = {{Software product lines traceability: A systematic mapping study}},
  journal  = {Information and Software Technology},
  year     = {2017},
  volume   = {84},
  pages    = {1--18},
  issn     = {0950-5849},
  abstract = {Abstract Context: Traceability in Software Product Lines (SPL) is the ability to interrelate software engineering artifacts through required links to answer specific questions related to the families of products and underlying development processes. Despite the existence of studies to map out available evidence on traceability for single systems development, there is a lack of understanding on common strategies, activities, artifacts, and research gaps for {\{}SPL{\}} traceability. Objective: This paper analyzes 62 studies dating from 2001 to 2015 and discusses seven aspects of {\{}SPL{\}} traceability: main goals, strategies, application domains, research intensity, research challenges, rigor, and industrial relevance. In addition to the analysis, this paper also synthesizes the available evidence, identifies open issues and points out areas calling for further research. Method: To gather evidence, we defined a mapping study process adapted from existing guidelines. Driven by a set of research questions, this process comprises three major phases: planning, conducting, and documenting the review. Results: This work provides a structured understanding of {\{}SPL{\}} traceability, indicating areas for further research. The lack of evidence regarding the application of research methods indicates the need for more rigorous {\{}SPL{\}} traceability studies with better description of context, study design, and limitations. For practitioners, although most identified studies have low industrial relevance, a few of them have high relevance and thus could provide some decision making support for application of {\{}SPL{\}} traceability in practice. Conclusions: This work concludes that {\{}SPL{\}} traceability is maturing and pinpoints areas where further investigation should be performed. As future work, we intend to improve the comparison between traceability proposals for {\{}SPL{\}} and single-system development. },
  doi      = {https://doi.org/10.1016/j.infsof.2016.12.004},
  keywords = {Software and systems traceability,Software product lines,Software reuse,Systematic mapping study},
  url      = {http://www.sciencedirect.com/science/article/pii/S0950584916304463},
}

@Article{Ardis2000825,
  author        = {Ardis, M and Daley, N and Hoffman, D and Siy, H and Weiss, D},
  title         = {{Software product lines: A case study}},
  journal       = {Software - Practice and Experience},
  year          = {2000},
  volume        = {30},
  number        = {7},
  pages         = {825--847},
  abstract      = {A software product line is a family of products that share common features to meet the needs of a market area. Systematic processes have been developed to dramatically reduce the cost of a product line. Such product-line engineering processes have proven practical and effective in industrial use, but are not widely understood. The Family-Oriented Abstraction, Specification and Translation (FAST) process has been used successfully at Lucent Technologies in over 25 domains, providing productivity improvements of as much as four to one. In this paper, we show how to use FAST to document precisely the key abstractions in a domain, exploit design patterns in a generic product-line architecture, generate documentation and Java code, and automate testing to reduce costs. The paper is based on a detailed case study covering all aspects from domain analysis through testing.},
  annote        = {cited By 33},
  doi           = {10.1002/(SICI)1097-024X(200006)30:7<825::AID-SPE322>3.0.CO;2-1},
  keywords      = {Computer architecture,Computer software selection,Domain engineering,Software engineering,Software product lines,case study},
  mendeley-tags = {case study},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-0033688589{\&}doi=10.1002{\%}2F{\%}28SICI{\%}291097-024X{\%}28200006{\%}2930{\%}3A7{\%}3C825{\%}3A{\%}3AAID-SPE322{\%}3E3.0.CO{\%}3B2-1{\&}partnerID=40{\&}md5=9146e51ed24f7399c09731f8d034fbf4},
}

@Article{SHIN200760,
  author   = {Shin, Michael E and Gomaa, Hassan},
  title    = {{Software requirements and architecture modeling for evolving non-secure applications into secure applications}},
  journal  = {Science of Computer Programming},
  year     = {2007},
  volume   = {66},
  number   = {1},
  pages    = {60--70},
  issn     = {0167-6423},
  abstract = {This paper describes an approach to modeling the evolution of non-secure applications into secure applications in terms of the software requirements model and software architecture model. The requirements for security services are captured separately from application requirements, and the security services are encapsulated in connectors in the software architecture, separately from the components providing functional services. The enterprise architecture is described in terms of use case models, static models, and dynamic models. The software architecture is described in terms of components and connectors, which can be deployed to distributed configurations. By separating application concerns from security concerns, the evolution from a non-secure application to a secure application can be achieved with less impact on the application. An electronic commerce system is described to illustrate the approach.},
  annote   = {Special Issue on the 5th International Workshop on System/Software Architectures (IWSSA'06)},
  doi      = {https://doi.org/10.1016/j.scico.2006.10.009},
  keywords = {Application system,Evolution,Security,Software architecture,Software requirements},
  url      = {http://www.sciencedirect.com/science/article/pii/S0167642306002486},
}

@Article{Lucrédio2008996,
  author   = {Lucr{\'{e}}dio, Daniel and {dos Santos Brito}, Kellyton and Alvaro, Alexandre and Garcia, Vinicius Cardoso and de Almeida, Eduardo Santana and {de Mattos Fortes}, Renata Pontin and Meira, Silvio Lemos},
  title    = {{Software reuse: The Brazilian industry scenario}},
  journal  = {Journal of Systems and Software},
  year     = {2008},
  volume   = {81},
  number   = {6},
  pages    = {996--1013},
  issn     = {0164-1212},
  abstract = {This paper aims at identifying some of the key factors in adopting an organization-wide software reuse program. The factors are derived from practical experience reported by industry professionals, through a survey involving 57 Brazilian small, medium and large software organizations. Some of them produce software with commonality between applications, and have mature processes, while others successfully achieved reuse through isolated, ad hoc efforts. The paper compiles the answers from the survey participants, showing which factors were more associated with reuse success. Based on this relationship, a guide is presented, pointing out which factors should be more strongly considered by small, medium and large organizations attempting to establish a reuse program. },
  annote   = {Agile Product Line Engineering},
  doi      = {https://doi.org/10.1016/j.jss.2007.08.036},
  keywords = {Best practices,Reuse success factors,Software reuse,Survey},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121207002221},
}

@Article{NuñezVarela2017164,
  author   = {Nu{\~{n}}ez-Varela, Alberto S and P{\'{e}}rez-Gonzalez, H{\'{e}}ctor G and Mart{\'{i}}nez-Perez, Francisco E and Soubervielle-Montalvo, Carlos},
  title    = {{Source code metrics: A systematic mapping study}},
  journal  = {Journal of Systems and Software},
  year     = {2017},
  volume   = {128},
  pages    = {164--197},
  issn     = {0164-1212},
  abstract = {AbstractContext Source code metrics are essential components in the software measurement process. They are extracted from the source code of the software, and their values allow us to reach conclusions about the quality attributes measured by the metrics. Objectives This paper aims to collect source code metrics related studies, review them, and perform an analysis, while providing an overview on the current state of source code metrics and their current trends. Method A systematic mapping study was conducted. A total of 226 studies, published between the years 2010 and 2015, were selected and analyzed. Results Almost 300 source code metrics were found. Object oriented programming is the most commonly studied paradigm with the Chidamber and Kemerer metrics, lines of code, McCabe's cyclomatic complexity, and number of methods and attributes being the most used metrics. Research on aspect and feature oriented programming is growing, especially for the current interest in programming concerns and software product lines. Conclusions Object oriented metrics have gained much attention, but there is a current need for more studies on aspect and feature oriented metrics. Software fault prediction, complexity and quality assessment are recurrent topics, while concerns, big scale software and software product lines represent current trends. },
  doi      = {https://doi.org/10.1016/j.jss.2017.03.044},
  keywords = {Aspect-oriented metrics,Feature-oriented metrics,Object-oriented metrics,Software metrics,Source code metrics,Systematic mapping study},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121217300663},
}

@Article{Cabanillas201555,
  author   = {Cabanillas, Cristina and Resinas, Manuel and Del-R{\'{i}}o-Ortega, Adela and Ruiz-Cort{\'{e}}s, Antonio},
  title    = {{Specification and automated design-time analysis of the business process human resource perspective}},
  journal  = {Information Systems},
  year     = {2015},
  volume   = {52},
  pages    = {55--82},
  issn     = {0306-4379},
  abstract = {Abstract The human resource perspective of a business process is concerned with the relation between the activities of a process and the actors who take part in them. Unlike other process perspectives, such as control flow, for which many different types of analyses have been proposed, such as finding deadlocks, there is an important gap regarding the human resource perspective. Resource analysis in business processes has not been defined, and only a few analysis operations can be glimpsed in previous approaches. In this paper, we identify and formally define seven design-time analysis operations related to how resources are involved in process activities. Furthermore, we demonstrate that for a wide variety of resource-aware {\{}BP{\}} models, those analysis operations can be automated by leveraging Description Logic (DL) off-the-shelf reasoners. To this end, we rely on Resource Assignment Language (RAL), a domain-specific language that enables the definition of conditions to select the candidates to participate in a process activity. We provide a complete formal semantics for {\{}RAL{\}} based on {\{}DLs{\}} and extend it to address the operations, for which the control flow of the process must also be taken into consideration. A proof-of-concept implementation has been developed and integrated in a system called CRISTAL. As a result, we can give an automatic answer to different questions related to the management of resources in business processes at design time. },
  annote   = {Special Issue on Selected Papers from {\{}SISAP{\}} 2013},
  doi      = {https://doi.org/10.1016/j.is.2015.03.002},
  keywords = {Analysis operation,Automated analysis,Business process management,Human resource perspective,RAL,Resource assignment},
  url      = {http://www.sciencedirect.com/science/article/pii/S0306437915000460},
}

@Article{Lochau2017125,
  author   = {Lochau, M and B{\"{u}}rdek, J and H{\"{o}}lzle, S and Sch{\"{u}}rr, A},
  title    = {{Specification and automated validation of staged reconfiguration processes for dynamic software product lines}},
  journal  = {Software and Systems Modeling},
  year     = {2017},
  volume   = {16},
  number   = {1},
  pages    = {125--152},
  abstract = {Dynamic software product lines (DSPLs) propose elaborated design and implementation principles for engineering highly configurable runtime-adaptive systems in a sustainable and feature-oriented way. For this, DSPLs add to classical software product lines (SPL) the notions of (1) staged (pre-)configurations with dedicated binding times for each individual feature, and (2) continuous runtime reconfigurations of dynamic features throughout the entire product life cycle. Especially in the context of safety- and mission-critical systems, the design of reliable DSPLs requires capabilities for accurately specifying and validating arbitrary complex constraints among configuration parameters and/or respective reconfiguration options. Compared to classical SPL domain analysis which is usually based on Boolean constraint solving, DSPL validation, therefore, further requires capabilities for checking temporal properties of reconfiguration processes. In this article, we present a comprehensive approach for modeling and automatically verifying essential validity properties of staged reconfiguration processes with complex binding time constraints during DSPL domain engineering. The novel modeling concepts introduced are motivated by (re-)configuration constraints apparent in a real-world industrial case study from the automation engineering domain, which are not properly expressible and analyzable using state-of-the-art SPL domain modeling approaches. We present a prototypical tool implementation based on the model checker SPIN and present evaluation results obtained from our industrial case study, demonstrating the applicability of the approach. {\textcopyright} 2015, Springer-Verlag Berlin Heidelberg.},
  annote   = {cited By 0},
  doi      = {10.1007/s10270-015-0470-4},
  keywords = {Computer software; Computer software reusability;,Configuration constraints; Configuration paramete,Life cycle},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929120403{\&}doi=10.1007{\%}2Fs10270-015-0470-4{\&}partnerID=40{\&}md5=deb247d3adad24816cc4021911e09fe4},
}

@Article{Pinto20111165,
  author   = {Pinto, M{\'{o}}nica and Fuentes, Lidia and Troya, Jos{\'{e}} Mar{\'{i}}a},
  title    = {{Specifying aspect-oriented architectures in AO-ADL}},
  journal  = {Information and Software Technology},
  year     = {2011},
  volume   = {53},
  number   = {11},
  pages    = {1165--1182},
  issn     = {0950-5849},
  abstract = {Context Architecture description languages (ADLs) are a well-accepted approach to software architecture representation. The majority of well-known {\{}ADLs{\}} are defined by means of components and connectors. Architectural connectors are mainly used to model interactions among components, specifying component communication and coordination separately. However, there are other properties that cut across several components and also affect component interactions (e.g. security). Objective It seems reasonable therefore to model how such crosscutting properties affect component interactions as part of connectors. Method Using an aspect-oriented approach, the AO-ADL architecture description language extends the classical connector semantics with enough expressiveness to model the influences of such crosscutting properties on component interactions (defined as ‘aspectual compositions' in connectors). Results This paper describes the AO-ADL language putting special emphasis on the extended connectors used to specify aspectual and non-aspectual compositions between concrete components. The contributions of AO-ADL are validated using concern-oriented metrics available in the literature. Conclusion The measured indicators show that using AO-ADL it is possible to specify more reusable and scalable software architectures. },
  annote   = {{\{}AMOST{\}} 2010AMOST 2010},
  doi      = {https://doi.org/10.1016/j.infsof.2011.04.003},
  keywords = {Aspect-Oriented Software Development,Languages,Metrics,Software Architectures,Software Engineering},
  url      = {http://www.sciencedirect.com/science/article/pii/S0950584911001005},
}

@Article{WENZEL2014776,
  author   = {Wenzel, S and Poggenpohl, D and J{\"{u}}rjens, J and Ochoa, M},
  title    = {{Specifying model changes with UMLchange to support security verification of potential evolution}},
  journal  = {Computer Standards {\&} Interfaces},
  year     = {2014},
  volume   = {36},
  number   = {4},
  pages    = {776--791},
  issn     = {0920-5489},
  abstract = {In model-based development, quality properties such as consistency of security requirements are often verified prior to code generation. Changed models have to be re-verified before re-generation. If several alternative evolutions of a model are possible, each alternative has to be modeled and verified to find the best model for further development. We present a verification strategy to analyze whether evolution preserves given security properties. The UMLchange profile is used for specifying potential evolutions of a given model simultaneously. We present a tool that reads these annotations and computes a delta containing all possible evolution paths. The paths can be verified wrt. security properties, and for each successfully verified path a new model version is generated automatically.},
  annote   = {Security in Information Systems: Advances and new Challenges.},
  doi      = {https://doi.org/10.1016/j.csi.2013.12.011},
  keywords = {Model evolution,Security verification,Tool support,UML profile},
  url      = {http://www.sciencedirect.com/science/article/pii/S0920548913001852},
}

@Article{Bouarar2015332,
  author   = {Bouarar, S and Jean, S and Siegmund, N},
  title    = {{SPL driven approach for variability in database design}},
  journal  = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year     = {2015},
  volume   = {9344},
  pages    = {332--342},
  abstract = {The evolution of computer technology has strongly impacted the database design. No phase was spared: several conceptual formalisms (e.g. ER, UML, ontological), various logical models (e.g. relational, object, key-value), a wide panoply of physical optimization structures and deployment platforms have been proposed. As a result, the database design process has become more complex involving more tasks and even more actors (as database architect or analyst). Getting inspired from software engineering in dealing with variable similar systems, we propose a methodological framework for a variability-aware design of databases, whereby this latter is henceforth devised as a Software Product Line. Doing so guarantees a high reuse, automation, and customizability in generating ready-to-be implemented databases. We also propose a solution to help users make a suitable choice among the wide panoply. Finally, a case study is presented. {\textcopyright} Springer International Publishing Switzerland 2015.},
  annote   = {cited By 1},
  doi      = {10.1007/978-3-319-23781-7_27},
  keywords = {Computer software; Database systems; Design; Softw,Computer technology; Customizability; Database de,Product design},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84951776613{\&}doi=10.1007{\%}2F978-3-319-23781-7{\_}27{\&}partnerID=40{\&}md5=da9040f7cdddcfd957d7f76bacf2c976},
}

@Article{Devroey2017153,
  author   = {Devroey, X and Perrouin, G and Cordy, M and Samih, H and Legay, A and Schobbens, P.-Y. and Heymans, P},
  title    = {{Statistical prioritization for software product line testing: an experience report}},
  journal  = {Software and Systems Modeling},
  year     = {2017},
  volume   = {16},
  number   = {1},
  pages    = {153--171},
  abstract = {Software product lines (SPLs) are families of software systems sharing common assets and exhibiting variabilities specific to each product member of the family. Commonalities and variabilities are often represented as features organized in a feature model. Due to combinatorial explosion of the number of products induced by possible features combinations, exhaustive testing of SPLs is intractable. Therefore, sampling and prioritization techniques have been proposed to generate sorted lists of products based on coverage criteria or weights assigned to features. Solely based on the feature model, these techniques do not take into account behavioural usage of such products as a source of prioritization. In this paper, we assess the feasibility of integrating usage models into the testing process to derive statistical testing approaches for SPLs. Usage models are given as Markov chains, enabling prioritization of probable/rare behaviours. We used featured transition systems, compactly modelling variability and behaviour for SPLs, to determine which products are realizing prioritized behaviours. Statistical prioritization can achieve a significant reduction in the state space, and modelling efforts can be rewarded by better automation. In particular, we used MaTeLo, a statistical test cases generation suite developed at ALL4TEC. We assess feasibility criteria on two systems: Claroline, a configurable course management system, and Sferion™, an embedded system providing helicopter landing assistance. {\textcopyright} 2015, Springer-Verlag Berlin Heidelberg.},
  annote   = {cited By 0},
  doi      = {10.1007/s10270-015-0479-8},
  keywords = {Combinatorial explosion; Course management system,Computer software; Embedded systems; Markov proces,Integration testing},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937933560{\&}doi=10.1007{\%}2Fs10270-015-0479-8{\&}partnerID=40{\&}md5=05e5d292352e727f469b0cc635961e21},
}

@Article{Barros20111355,
  author        = {Barros, Heitor and Silva, Alan and Costa, Evandro and Bittencourt, Ig Ibert and Holanda, Olavo and Sales, Leandro},
  title         = {{Steps, techniques, and technologies for the development of intelligent applications based on Semantic Web Services: A case study in e-learning systems}},
  journal       = {Engineering Applications of Artificial Intelligence},
  year          = {2011},
  volume        = {24},
  number        = {8},
  pages         = {1355--1367},
  issn          = {0952-1976},
  abstract      = {Semantic Web Services domain has gained special attention in academia and industry. It has been adopted as a promise to enable automation of all aspects of Web Services provision and uses, such as service creation, selection, discovery, composition, and invocation. However, the development of intelligent systems based on Semantic Web Services (SWS) is still a complex and time-consuming task, mainly with respect to the choice and integration of technologies. In this paper, we discuss some empirical issues associated with the development process for such systems and propose a systematic way for building intelligent applications based on {\{}SWS{\}} by providing the development process with steps, techniques and technologies. In addition, one experiment concerning the implementation of a real e-learning system using the proposed approach is described. The evaluation results from this experiment showed that our approach has been effective and relevant in terms of improvements in the development process of intelligent applications based on SWS.},
  annote        = {Semantic-based Information and Engineering Systems},
  doi           = {https://doi.org/10.1016/j.engappai.2011.05.007},
  keywords      = {Grinv Middleware,Intelligent Tutoring System,Ontology,Semantic Web Services,case study},
  mendeley-tags = {case study},
  url           = {http://www.sciencedirect.com/science/article/pii/S0952197611000893},
}

@Article{Zhang2016,
  author  = {Zhang, Hongxia and Wang, Fei and Zhang, Yang and Xu, Jiuyun},
  title   = {{STPSO: Optimal configuration for cloud environments}},
  journal = {China Communications},
  year    = {2016},
  volume  = {13},
  number  = {10},
  pages   = {198--208},
  month   = {oct},
  issn    = {1673-5447},
  doi     = {10.1109/CC.2016.7733044},
  url     = {http://ieeexplore.ieee.org/document/7733044/},
}

@Article{Bagheri2010300,
  author   = {Bagheri, E and Asadi, M and Gasevic, D and Soltani, S},
  title    = {{Stratified analytic hierarchy process: Prioritization and selection of software features}},
  journal  = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year     = {2010},
  volume   = {6287 LNCS},
  pages    = {300--315},
  abstract = {Product line engineering allows for the rapid development of variants of a domain specific application by using a common set of reusable assets often known as core assets. Variability modeling is a critical issue in product line engineering, where the use of feature modeling is one of most commonly used formalisms. To support an effective and automated derivation of concrete products for a product family, staged configuration has been proposed in the research literature. In this paper, we propose the integration of well-known requirements engineering principles into stage configuration. Being inspired by the well-established Preview requirements engineering framework, we initially propose an extension of feature models with capabilities for capturing business oriented requirements. This representation enables a more effective capturing of stakeholders' preferences over the business requirements and objectives (e.g.,. implementation costs or security) in the form of fuzzy linguistic variables (e.g., high, medium, and low). On top of this extension, we propose a novel method, the Stratified Analytic Hierarchy process, which first helps to rank and select the most relevant high level business objectives for the target stakeholders (e.g., security over implementation costs), and then helps to rank and select the most relevant features from the feature model to be used as the starting point in the staged configuration process. Besides a complete formalization of the process, we define the place of our proposal in existing software product line lifecycles as well as demonstrate the use of our proposal on the widely-used e-Shop case study. Finally, we report on the results of our user study, which indicates a high appreciation of the proposed method by the participating industrial software developers. The tool support for S-AHP is also introduced. {\textcopyright} 2010 Springer-Verlag Berlin Heidelberg.},
  annote   = {cited By 12},
  doi      = {10.1007/978-3-642-15579-6_21},
  keywords = {Analytic hierarchy process; Computer software reu,Business objectives; Business requirement; Busines,Feature extraction},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-78049414588{\&}doi=10.1007{\%}2F978-3-642-15579-6{\_}21{\&}partnerID=40{\&}md5=1c964be89a3d87cc725a2a54b3f70a9b},
}

@Article{Furtado2010316,
  author   = {Furtado, A W B and Santos, A L M and Ramalho, G L},
  title    = {{Streamlining domain analysis for digital games product lines}},
  journal  = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year     = {2010},
  volume   = {6287 LNCS},
  pages    = {316--330},
  abstract = {Digital games and their development process are quite peculiar when compared to other software in general. However, current domain engineering processes do not addresses such peculiarities and, not surprisingly, successful cases of software product lines (SPLs) for digital games cannot be found in the literature nor the industry. With such a motivation, this paper focuses on streamlining and enriching the Domain Analysis process for SPLs targeted at digital games. Guidelines are provided for making Domain Analysis tasks aware of digital games peculiarities, in order to tackle the challenges of and benefit from the unique characteristics of such a macro-domain. A case study for an SPL aimed at arcade-based games is also presented to illustrate and evaluate the proposed guidelines. {\textcopyright} 2010 Springer-Verlag Berlin Heidelberg.},
  annote   = {cited By 3},
  doi      = {10.1007/978-3-642-15579-6_22},
  keywords = {Computer software reusability,Concentration (process),Development process; Digital games; Domain analysi},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-78049405732{\&}doi=10.1007{\%}2F978-3-642-15579-6{\_}22{\&}partnerID=40{\&}md5=36dc531dd5107ba35baaa27c46fa5a4a},
}

@Article{Dhungana20101108,
  author   = {Dhungana, Deepak and Gr{\"{u}}nbacher, Paul and Rabiser, Rick and Neumayer, Thomas},
  title    = {{Structuring the modeling space and supporting evolution in software product line engineering}},
  journal  = {Journal of Systems and Software},
  year     = {2010},
  volume   = {83},
  number   = {7},
  pages    = {1108--1122},
  issn     = {0164-1212},
  abstract = {The scale and complexity of product lines means that it is practically infeasible to develop a single model of the entire system, regardless of the languages or notations used. The dynamic nature of real-world systems means that product line models need to evolve continuously to meet new customer requirements and to reflect changes of product line artifacts. To address these challenges, product line engineers need to apply different strategies for structuring the modeling space to ease the creation and maintenance of models. This paper presents an approach that aims at reducing the maintenance effort by organizing product lines as a set of interrelated model fragments defining the variability of particular parts of the system. We provide support to semi-automatically merge fragments into complete product line models. We also provide support to automatically detect inconsistencies between product line artifacts and the models representing these artifacts after changes. Furthermore, our approach supports the co-evolution of models and their respective meta-models. We discuss strategies for structuring the modeling space and show the usefulness of our approach using real-world examples from our ongoing industry collaboration. },
  annote   = {{\{}SPLC{\}} 2008},
  doi      = {https://doi.org/10.1016/j.jss.2010.02.018},
  keywords = {Model evolution,Product line engineering,Variability modeling},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121210000506},
}

@Article{Soumeya20141345,
  author   = {Soumeya, Debboub and Djamel, Meslati},
  title    = {{Study of advanced separation of concerns approaches using the GoF design patterns: A quantitative and qualitative comparison}},
  journal  = {Information and Software Technology},
  year     = {2014},
  volume   = {56},
  number   = {10},
  pages    = {1345--1359},
  issn     = {0950-5849},
  abstract = {AbstractContext Since the emergence of the aspect oriented paradigm, several studies have been conducted to test the contribution of this new paradigm compared to the object paradigm. However, in addition to this type of studies, we need also comparative studies that assess the aspect approaches mutually. The motivations of the latter include the enhancement of each aspect approach, devising hybrid approaches or merely helping developers choosing the suitable approach according to their needs. Comparing advanced separation of concerns approaches is the context of our work. Objective We aim at making an assessment of how the aspect approaches deal with crosscutting concerns. This assessment is based on quantitative attributes such as coupling and cohesion that evaluate the modularity as well as on qualitative observations. Method We selected three of well-known aspect approaches: AspectJ, {\{}JBoss{\}} {\{}AOP{\}} and CaesarJ, all the three based on Java. We conducted then, a comparative study using the GoF design patterns. In order to be fair we asked a group of Master students to achieve the implementation of all patterns with the three approaches. The use of these implementations as hypothetical benchmarks allowed us to achieve two kinds of comparison: a quantitative one based on structural and performance metrics, and qualitative one based on observations collected during the implementation phase. Results The quantitative comparison shows some advantages like the using of fewer components with AspectJ and the strong cohesion with CaesarJ and weaknesses, as the high internal coupling caused by the inner classes of CaesarJ. The qualitative comparison gives comments about the approach understandability and others qualitative concepts. Conclusion This comparison highlighted strengths and weaknesses of each approach, and provided a referential work that can help choosing the right approach during software development, enhancing aspect approaches or devising hybrid approaches that combine best features. },
  doi      = {https://doi.org/10.1016/j.infsof.2014.04.015},
  keywords = {Advanced separation of concerns,Aspect oriented programming,AspectJ,CaesarJ,Empirical assessment,JBoss AOP},
  url      = {http://www.sciencedirect.com/science/article/pii/S0950584914000962},
}

@Article{Galindo201578,
  author   = {Galindo, Jos{\'{e}} A and Dhungana, Deepak and Rabiser, Rick and Benavides, David and Botterweck, Goetz and Gr{\"{u}}nbacher, Paul},
  title    = {{Supporting distributed product configuration by integrating heterogeneous variability modeling approaches}},
  journal  = {Information and Software Technology},
  year     = {2015},
  volume   = {62},
  pages    = {78--100},
  issn     = {0950-5849},
  abstract = {AbstractContext In industrial settings products are developed by more than one organization. Software vendors and suppliers commonly typically maintain their own product lines, which contribute to a larger (multi) product line or software ecosystem. It is unrealistic to assume that the participating organizations will agree on using a specific variability modeling technique—they will rather use different approaches and tools to manage the variability of their systems. Objective We aim to support product configuration in software ecosystems based on several variability models with different semantics that have been created using different notations. Method We present an integrative approach that provides a unified perspective to users configuring products in multi product line environments, regardless of the different modeling methods and tools used internally. We also present a technical infrastructure and a prototype implementation based on web services. Results We show the feasibility of the approach and its implementation by using it with the three most widespread types of variability modeling approaches in the product line community, i.e., feature-based, OVM-style, and decision-oriented modeling. To demonstrate the feasibility and flexibility of our approach, we present an example derived from industrial experience in enterprise resource planning. We further applied the approach to support the configuration of privacy settings in the Android ecosystem based on multiple variability models. We also evaluated the performance of different model enactment strategies used in our approach. Conclusions Tools and techniques allowing stakeholders to handle variability in a uniform manner can considerably foster the initiation and growth of software ecosystems from the perspective of software reuse and configuration. },
  doi      = {https://doi.org/10.1016/j.infsof.2015.02.002},
  keywords = {Automated analysis,Product configuration,Software product lines},
  url      = {http://www.sciencedirect.com/science/article/pii/S0950584915000312},
}

@Article{Mohan2006650,
  author   = {Mohan, Kannan and Xu, Peng and Ramesh, Balasubramaniam},
  title    = {{Supporting dynamic group decision and negotiation processes: A traceability augmented peer-to-peer network approach}},
  journal  = {Information {\&} Management},
  year     = {2006},
  volume   = {43},
  number   = {5},
  pages    = {650--662},
  issn     = {0378-7206},
  abstract = {Peer-to-peer (P2P) networks are gaining popularity in supporting group decision and negotiation (GDN) activities in which ad hoc, transient groups participate. In these, multiple stakeholders create and use knowledge that is fragmented and distributed across different locations. While {\{}P2P{\}} networks help establish physical links across participants, they lack the capability to integrate knowledge fragments embedded in documents and artifacts distributed across peers. We augmented the {\{}P2P{\}} architecture with traceability to provide a way of integrating distributed knowledge. We implemented this approach in a prototype system that used a {\{}P2P{\}} networking tool. Using a case study of software development outsourcing, we showed how our approach supported critical {\{}GDN{\}} activities. Qualitative evaluation of our approach in supporting {\{}GDN{\}} was also demonstrated. },
  doi      = {https://doi.org/10.1016/j.im.2006.04.001},
  keywords = {Group decision and negotiation,Knowledge integration,Peer-to-peer networks,Traceability},
  url      = {http://www.sciencedirect.com/science/article/pii/S0378720606000449},
}

@Article{Stoica201542,
  author   = {Stoica, Anca-Juliana and Pelckmans, Kristiaan and Rowe, William},
  title    = {{System components of a general theory of software engineering}},
  journal  = {Science of Computer Programming},
  year     = {2015},
  volume   = {101},
  pages    = {42--65},
  issn     = {0167-6423},
  abstract = {Abstract The contribution of this paper to a general theory of software engineering is twofold: it presents the model system concept, and it integrates the software engineering design process into a decision making theory and a value-based decision-under-risk process. The model system concept is defined as a collection of interconnected and consistent components that work together for defining, developing, and delivering a software system. This model system concept is used to represent the multiple facets of a software engineering project such as stakeholders and models related to domain/environment, success, decision, product, process, and property. The model system concept is derived from software development practices in the industry and academia. The theoretical decision framework acts as a central governance component for a given software engineering project. Applying this decision framework allows for effectively managing risks and uncertainties related to success in the project building stage. Especially, this puts the design process in an economic perspective, where concepts such as value-of-waiting, value-of-information and possible outcomes can be coped with explicitly. In practice, the decision framework allows for the optimal control of modern adaptive software development. In particular, one can use dynamic programming to find the optimal sequence of decisions to be made considering a defined time horizon. In this way we can relate our contribution to a theory of software engineering to the well-studied areas of automatic control, optimization, decision theory and Bayesian analysis. Computational case studies exemplify the conceptual innovations proposed in this paper. },
  annote   = {Towards general theories of software engineering},
  doi      = {https://doi.org/10.1016/j.scico.2014.11.008},
  keywords = {Adaptive software development,General theory of software engineering,Model systems,Optimal decision-under-risk process,Theoretic Decision Framework},
  url      = {http://www.sciencedirect.com/science/article/pii/S0167642314005401},
}

@Article{Tarhan2014477,
  author   = {Tarhan, Ayca and Yilmaz, Seda Gunes},
  title    = {{Systematic analyses and comparison of development performance and product quality of Incremental Process and Agile Process}},
  journal  = {Information and Software Technology},
  year     = {2014},
  volume   = {56},
  number   = {5},
  pages    = {477--494},
  issn     = {0950-5849},
  abstract = {AbstractContext Although Agile software development models have been widely used as a base for the software project life-cycle since 1990s, the number of studies that follow a sound empirical method and quantitatively reveal the effect of using these models over Traditional models is scarce. Objective This article explains the empirical method of and the results from systematic analyses and comparison of development performance and product quality of Incremental Process and Agile Process adapted in two projects of a middle-size, telecommunication software development company. The Incremental Process is an adaption of the Waterfall Model whereas the newly introduced Agile Process is a combination of the Unified Software Development Process, Extreme Programming, and Scrum. Method The method followed to perform the analyses and comparison is benefited from the combined use of qualitative and quantitative methods. It utilizes; {\{}GQM{\}} Approach to set measurement objectives, {\{}CMMI{\}} as the reference model to map the activities of the software development processes, and a pre-defined assessment approach to verify consistency of process executions and evaluate measure characteristics prior to quantitative analysis. Results The results of the comparison showed that the Agile Process had performed better than the Incremental Process in terms of productivity (79{\%}), defect density (57{\%}), defect resolution effort ratio (26{\%}), Test Execution V{\&}V Effectiveness (21{\%}), and effort prediction capability (4{\%}). These results indicate that development performance and product quality achieved by following the Agile Process was superior to those achieved by following the Incremental Process in the projects compared. Conclusion The acts of measurement, analysis, and comparison enabled comprehensive review of the two development processes, and resulted in understanding their strengths and weaknesses. The comparison results constituted objective evidence for organization-wide deployment of the Agile Process in the company. },
  annote   = {Performance in Software Development},
  doi      = {https://doi.org/10.1016/j.infsof.2013.12.002},
  keywords = {Agile development,Empirical method,Process performance,Qualitative analysis,Quantitative analysis,Software measurement},
  url      = {http://www.sciencedirect.com/science/article/pii/S0950584913002310},
}

@Article{Midtgaard2015145,
  author   = {Midtgaard, Jan and Dimovski, Aleksandar S and Brabrand, Claus and W{\c{a}}sowski, Andrzej},
  title    = {{Systematic derivation of correct variability-aware program analyses}},
  journal  = {Science of Computer Programming},
  year     = {2015},
  volume   = {105},
  pages    = {145--170},
  issn     = {0167-6423},
  abstract = {Abstract A recent line of work lifts particular verification and analysis methods to Software Product Lines (SPL). In an effort to generalize such case-by-case approaches, we develop a systematic methodology for lifting single-program analyses to {\{}SPLs{\}} using abstract interpretation. Abstract interpretation is a classical framework for deriving static analyses in a compositional, step-by-step manner. We show how to take an analysis expressed as an abstract interpretation and lift each of the abstract interpretation steps to a family of programs (SPL). This includes schemes for lifting domain types, and combinators for lifting analyses and Galois connections. We prove that for analyses developed using our method, the soundness of lifting follows by construction. The resulting variational abstract interpretation is a conceptual framework for understanding, deriving, and validating static analyses for SPLs. Then we show how to derive the corresponding variational dataflow equations for an example static analysis, a constant propagation analysis. We also describe how to approximate variability by applying variability-aware abstractions to {\{}SPL{\}} analysis. Finally, we discuss how to efficiently implement our method and present some evaluation results. },
  doi      = {https://doi.org/10.1016/j.scico.2015.04.005},
  keywords = {Abstract interpretation,Software Product Lines,Software variability,Static analysis,Verification},
  url      = {http://www.sciencedirect.com/science/article/pii/S0167642315000714},
}

@Article{Marew200733,
  author   = {Marew, T and Kim, J and Bae, D H},
  title    = {{Systematic functional decomposition in a product line using aspect-oriented software development: A case study}},
  journal  = {International Journal of Software Engineering and Knowledge Engineering},
  year     = {2007},
  volume   = {17},
  number   = {1},
  pages    = {33--55},
  abstract = {Systematic configuration management is important for successful software product lines. We can use aspect-oriented software development to decompose software product lines based on features that can ease configuration management. In this paper, we present a military maintenance product line that employs such strategy. In particular, we applied a specific approach, feature based modeling (FBM), in the construction of the system. We have extended FBM to address properties specific to product line. We will discuss the advantages of FBM when applied to product lines. Such gains include the functional decomposition of the system along user requirements (features) as aspects. Moreover, those features exhibit unidirectional dependency (i.e. among any two features, at most one depend on another) that enables developers to analyze the effect of any modification they may make on any feature. In addition, any variations can be captured as aspects which can also be incorporated easily into the core asset if such variation is deemed to be important enough to be included in the product line for further evolution. {\textcopyright} World Scientific Publishing Company.},
  annote   = {cited By 2},
  doi      = {10.1142/S0218194007003112},
  keywords = {Aspect oriented software engineering; Feature bas,Feature extraction; Function evaluation; Mathemati,Product design},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-33947683339{\&}doi=10.1142{\%}2FS0218194007003112{\&}partnerID=40{\&}md5=1643033c2adca039352c12f0b2e89a0e},
}

@Article{Biffl20141533,
  author   = {Biffl, S and Kalinowski, M and Rabiser, R and Ekaputra, F and Winkler, D},
  title    = {{Systematic knowledge engineering: Building bodies of knowledge from published research}},
  journal  = {International Journal of Software Engineering and Knowledge Engineering},
  year     = {2014},
  volume   = {24},
  number   = {10},
  pages    = {1533--1571},
  abstract = {Context. Software engineering researchers conduct systematic literature reviews (SLRs) to build bodies of knowledge (BoKs). Unfortunately, relevant knowledge collected in the SLR process is not publicly available, which considerably slows down building BoKs incrementally. Objective. We present and evaluate the Systematic Knowledge Engineering (SKE) process to support efficiently building BoKs from published research. Method. SKE is based on the SLR process and on Knowledge Engineering practices to build a Knowledge Base (KB) by reusing intermediate data extraction results from SLRs. We evaluated the feasibility of applying SKE by building a Software Inspection BoK KB from published experiments and a Software Product Line BoK KB from published experience reports. We compared the effort, benefits, and risks of building BoK KBs regarding the SKE and the traditional SLR processes. Results. The application of SKE for incrementally collecting and organizing knowledge in the context of a BoK was feasible for different domains and different types of evidence. While the efforts for conducting the SKE and traditional SLR processes are comparable, SKE provides significant benefits for building BoKs. Conclusions. SKE enables researchers in a scientific community to reuse and incrementally build knowledge in a BoK. SKE is ready to be evaluated in other software engineering domains. {\textcopyright} 2014 World Scientific Publishing Company.},
  annote   = {cited By 2},
  doi      = {10.1142/S021819401440018X},
  keywords = {Body of knowledge; Empirical Software Engineering,Buildings,Computer software; Computer software selection and},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84928562233{\&}doi=10.1142{\%}2FS021819401440018X{\&}partnerID=40{\&}md5=25738e99352373633d4b6ba562f575c9},
}

@Article{Hoda20171339,
  author   = {Hoda, R and Salleh, N and Grundy, J and Tee, H M},
  title    = {{Systematic literature reviews in agile software development: A tertiary study}},
  journal  = {Information and Software Technology},
  year     = {2017},
  volume   = {85},
  pages    = {1339--1351},
  abstract = {Context A number of systematic literature reviews and mapping studies (SLRs) covering numerous primary research studies on various aspects of agile software development (ASD) exist. Objective The aim of this paper is to provide an overview of the SLRs on ASD research topics for software engineering researchers and practitioners. Method We followed the tertiary study guidelines by Kitchenham et al. to find SLRs published between late 1990s to December 2015. Results We found 28 SLRs focusing on ten different ASD research areas: adoption, methods, practices, human and social aspects, CMMI, usability, global software engineering (GSE), organizational agility, embedded systems, and software product line engineering. The number of SLRs on ASD topics, similar to those on software engineering (SE) topics in general, is on the rise. A majority of the SLRs applied standardized guidelines and the quality of these SLRs on ASD topics was found to be slightly higher for journal publications than for conferences. While some individuals and institutions seem to lead this area, the spread of authors and institutions is wide. With respect to prior review recommendations, significant progress was noticed in the area of connecting agile to established domains such as usability, CMMI, and GSE; and considerable progress was observed in focusing on management-oriented approaches as Scrum and sustaining ASD in different contexts such as embedded systems. Conclusion SLRs of ASD studies are on the rise and cover a variety of ASD aspects, ranging from early adoption issues to newer applications of ASD such as in product line engineering. ASD research can benefit from further primary and secondary studies on evaluating benefits and challenges of ASD methods, agile hybrids in large-scale setups, sustainability, motivation, teamwork, and project management; as well as a fresh review of empirical studies in ASD to cover the period post 2008. {\textcopyright} 2017 Elsevier B.V.},
  annote   = {cited By 0},
  doi      = {10.1016/j.infsof.2017.01.007},
  keywords = {Agile manufacturing systems; Embedded systems; Hum,Agile software development; Global software engin,Software design},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009961943{\&}doi=10.1016{\%}2Fj.infsof.2017.01.007{\&}partnerID=40{\&}md5=6a85ea7243f2d03424972d48f898f921},
}

@Article{Lamancha201158,
  author   = {Lamancha, B P and Polo, M and Piattini, M},
  title    = {{Systematic review on Software Product Line Testing}},
  journal  = {Communications in Computer and Information Science},
  year     = {2011},
  volume   = {170},
  pages    = {58--71},
  abstract = {This article presents a systematic review of the literature about Testing in Software Product Lines. The objective is to analyze the existing approaches to testing in software product lines, discussing the significant issues related to this area of knowledge and providing an up-to-date state of the art, which can serve as a basis for innovative research activities. The paper includes an analysis on how SPL research can contribute to dynamize the research in software testing. {\textcopyright} Springer-Verlag Berlin Heidelberg 2011.},
  annote   = {cited By 2},
  doi      = {10.1007/978-3-642-29578-2},
  keywords = {Innovative research; Software Product Line; Softwa,Research; Software testing; Surveying; Testing,Software design},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879472096{\&}doi=10.1007{\%}2F978-3-642-29578-2{\&}partnerID=40{\&}md5=c62c91ce78008def01df926a79a396c5},
}

@Article{ZHANG20131341,
  author   = {Zhang, He and Babar, Muhammad Ali},
  title    = {{Systematic reviews in software engineering: An empirical investigation}},
  journal  = {Information and Software Technology},
  year     = {2013},
  volume   = {55},
  number   = {7},
  pages    = {1341--1354},
  issn     = {0950-5849},
  abstract = {Background
Systematic Literature Reviews (SLRs) have gained significant popularity among Software Engineering (SE) researchers since 2004. Several researchers have also been working on improving the scientific and methodological infrastructure to support SLRs in SE. We argue that there is also an apparent and essential need for evidence-based body of knowledge about different aspects of the adoption of SLRs in SE.
Objective
The main objective of this research is to empirically investigate the adoption, value, and use of SLRs in SE research from various perspectives.
Method
We used mixed-methods approach (systematically integrating tertiary literature review, semi-structured interviews and questionnaire-based survey) as it is based on a combination of complementary research methods which are expected to compensate each others' limitations.
Results
A large majority of the participants are convinced of the value of using a rigourous and systematic methodology for literature reviews in SE research. However, there are concerns about the required time and resources for SLRs. One of the most important motivators for performing SLRs is new findings and inception of innovative ideas for further research. The reported SLRs are more influential compared to the traditional literature reviews in terms of number of citations. One of the main challenges of conducting SLRs is drawing a balance between methodological rigour and required effort.
Conclusions
SLR has become a popular research methodology for conducting literature review and evidence aggregation in SE. There is an overall positive perception about this relatively new methodology to SE research. The findings provide interesting insights into different aspects of SLRs. We expect that the findings can provide valuable information to readers about what can be expected from conducting SLRs and the potential impact of such reviews.},
  doi      = {https://doi.org/10.1016/j.infsof.2012.09.008},
  keywords = {Evidence-based software engineering,Methodology adoption,Mixed-methods research,Research methodology,Systematic (literature) reviews,Tertiary study},
  url      = {http://www.sciencedirect.com/science/article/pii/S0950584912002029},
}

@Article{Preuveneers2016162,
  author   = {Preuveneers, Davy and Heyman, Thomas and Berbers, Yolande and Joosen, Wouter},
  title    = {{Systematic scalability assessment for feature oriented multi-tenant services}},
  journal  = {Journal of Systems and Software},
  year     = {2016},
  volume   = {116},
  pages    = {162--176},
  issn     = {0164-1212},
  abstract = {Abstract Recent software engineering paradigms such as software product lines, supporting development techniques like feature modeling, and cloud provisioning models such as platform and infrastructure as a service, allow for great flexibility during both software design and deployment, resulting in potentially large cost savings. However, all this flexibility comes with a catch: as the combinatorial complexity of optional design features and deployment variability increases, the difficulty of assessing system qualities such as scalability and quality of service increases too. And if the software itself is not scalable (for instance, because of a specific set of selected features), deploying additional service instances is a futile endeavor. Clearly there is a need to systematically measure the impact of feature selection on scalability, as the potential cost savings can be completely mitigated by the risk of having a system that is unable to meet service demand. In this work, we document our results on systematic load testing for automated quality of service and scalability analysis. The major contribution of our work is tool support and a methodology to analyze the scalability of these distributed, feature oriented multi-tenant software systems in a continuous integration process. We discuss our approach to select features for load testing such that a representative set of feature combinations is used to elicit valuable information on the performance impact and feature interactions. Additionally, we highlight how our methodology and framework for performance and scalability prediction differs from state-of-practice solutions. We take the viewpoint of both the tenant of the service and the service provider, and report on our experiences applying the approach to an industrial use case in the domain of electronic payments. We conclude that the integration of systematic scalability tests in a continuous integration process offers strong advantages to software developers and service providers, such as the ability to quantify the impact of new features in existing service compositions, and the early detection of hidden feature interactions that may negatively affect the overall performance of multi-tenant services. },
  doi      = {https://doi.org/10.1016/j.jss.2015.12.024},
  keywords = {Distributed systems,Scalability,Tool support},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121215002897},
}

@Article{Haber2015601,
  author   = {Haber, A and H{\"{o}}lldobler, K and Kolassa, C and Look, M and M{\"{u}}ller, K and Rumpe, B and Schaefer, I and Schulze, C},
  title    = {{Systematic synthesis of delta modeling languages}},
  journal  = {International Journal on Software Tools for Technology Transfer},
  year     = {2015},
  volume   = {17},
  number   = {5},
  pages    = {601--626},
  abstract = {Delta modeling is a modular, yet flexible approach to capture variability by explicitly representing differences between system variants or versions. The conceptual idea of delta modeling is language-independent. But, to apply delta modeling to a concrete language, either a generic transformation language has to be used or the corresponding delta language has to be manually developed for each considered base language. Generic languages and their tool support often lack readability and specific context condition checking, since they are unrelated to the base language. In this paper, we present a process that allows synthesizing a delta language from the grammar of a given base language. Our method relies on an automatically generated language extension that can be manually adapted to meet domain-specific needs. We illustrate our method using delta modeling on a textual variant of architecture diagrams. Furthermore, we evaluate our method using a comparative case study. This case study covers an architectural, a structural, and a behavioral language and compares the preexisting handwritten grammars to the generated grammars as well as the manually tailored grammars. This paper is an extension of Haber et al. (Proceedings of the 17th international software product line conference (SPLC'13), pp 22–31, 2013). {\textcopyright} 2015, Springer-Verlag Berlin Heidelberg.},
  annote   = {cited By 1},
  doi      = {10.1007/s10009-015-0387-9},
  keywords = {Computational linguistics; Computer programming la,Delta model; Domain specific languages; Generatio,Modeling languages},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941426977{\&}doi=10.1007{\%}2Fs10009-015-0387-9{\&}partnerID=40{\&}md5=b20a162f1709303e9a6be78314750243},
}

@Article{HUANG2008353,
  author   = {Huang, Hui-Wen and Shih, Chunkuan and Yih, Swu and Chen, Ming-Huei},
  title    = {{System-level hazard analysis using the sequence-tree method}},
  journal  = {Annals of Nuclear Energy},
  year     = {2008},
  volume   = {35},
  number   = {3},
  pages    = {353--362},
  issn     = {0306-4549},
  abstract = {A system-level PHA using the sequence-tree method is presented to perform safety-related digital I{\&}C system SSA. The conventional PHA involves brainstorming among experts on various portions of the system to identify hazards through discussions. However, since the conventional PHA is not a systematic technique, the analysis results depend strongly on the experts' subjective opinions. The quality of analysis cannot be appropriately controlled. Therefore, this study presents a system-level sequence tree based PHA, which can clarify the relationship among the major digital I{\&}C systems. This sequence-tree-based technique has two major phases. The first phase adopts a table to analyze each event in SAR Chapter 15 for a specific safety-related I{\&}C system, such as RPS. The second phase adopts a sequence tree to recognize the I{\&}C systems involved in the event, the working of the safety-related systems and how the backup systems can be activated to mitigate the consequence if the primary safety systems fail. The defense-in-depth echelons, namely the Control echelon, Reactor trip echelon, ESFAS echelon and Monitoring and indicator echelon, are arranged to build the sequence-tree structure. All the related I{\&}C systems, including the digital systems and the analog back-up systems, are allocated in their specific echelons. This system-centric sequence-tree analysis not only systematically identifies preliminary hazards, but also vulnerabilities in a nuclear power plant. Hence, an effective simplified D3 evaluation can also be conducted.},
  doi      = {https://doi.org/10.1016/j.anucene.2007.07.010},
  url      = {http://www.sciencedirect.com/science/article/pii/S0306454907001764},
}

@Article{Zdun200656,
  author   = {Zdun, Uwe},
  title    = {{Tailorable language for behavioral composition and configuration of software components}},
  journal  = {Computer Languages, Systems {\&} Structures},
  year     = {2006},
  volume   = {32},
  number   = {1},
  pages    = {56--82},
  issn     = {1477-8424},
  abstract = {Many software systems suffer from missing support for behavioral (runtime) composition and configuration of software components. The concern “behavioral composition and configuration” is not treated as a first-class entity, but instead it is hard-coded in different programming styles, leading to tangled composition and configuration code that is hard to understand and maintain. We propose to embed a dynamic language with a tailorable object and class concept into the host language in which the components are written, and use the tailorable language for behavioral composition and configuration tasks. Using this approach we can separate the concerns “behavioral composition and configuration” from the rest of the software system, leading to a more reusable, understandable, and maintainable composition and configuration of software components. },
  doi      = {https://doi.org/10.1016/j.cl.2005.04.001},
  keywords = {Component composition,Component configuration,Software components,Tailorable language},
  url      = {http://www.sciencedirect.com/science/article/pii/S1477842405000205},
}

@Article{Rosenmüller20091493,
  author        = {Rosenm{\"{u}}ller, Marko and Apel, Sven and Leich, Thomas and Saake, Gunter},
  title         = {{Tailor-made data management for embedded systems: A case study on Berkeley {\{}DB{\}}}},
  journal       = {Data {\&} Knowledge Engineering},
  year          = {2009},
  volume        = {68},
  number        = {12},
  pages         = {1493--1512},
  issn          = {0169-023X},
  abstract      = {Applications in the domain of embedded systems are diverse and store an increasing amount of data. In order to satisfy the varying requirements of these applications, data management functionality is needed that can be tailored to the applications' needs. Furthermore, the resource restrictions of embedded systems imply a need for data management that is customized to the hardware platform. In this paper, we present an approach for decomposing data management software for embedded systems using feature-oriented programming. The result of such a decomposition is a software product line that allows us to generate tailor-made data management systems. While existing approaches for tailoring software have significant drawbacks regarding customizability and performance, a feature-oriented approach overcomes these limitations, as we will demonstrate. In a non-trivial case study on Berkeley DB, we evaluate our approach and compare it to other approaches for tailoring DBMS.},
  annote        = {Including Special Section: 21st {\{}IEEE{\}} International Symposium on Computer-Based Medical Systems (IEEE {\{}CBMS{\}} 2008) – Seven selected and extended papers on Biomedical Data Mining},
  doi           = {https://doi.org/10.1016/j.datak.2009.07.013},
  keywords      = {Embedded systems,Feature-oriented programming,FeatureC++,Software product lines,Tailor-made data management,case study},
  mendeley-tags = {case study},
  url           = {http://www.sciencedirect.com/science/article/pii/S0169023X09001128},
}

@Article{GarcíaDíaz20101179,
  author   = {Garc{\'{i}}a-D{\'{i}}az, Vicente and Fern{\'{a}}ndez-Fern{\'{a}}ndez, H{\'{e}}ctor and Palacios-Gonz{\'{a}}lez, El{\'{i}}as and G-Bustelo, B Cristina Pelayo and Sanju{\'{a}}n-Mart{\'{i}}nez, Oscar and Lovelle, Juan Manuel Cueva},
  title    = {{{\{}TALISMAN{\}} MDE: Mixing {\{}MDE{\}} principles}},
  journal  = {Journal of Systems and Software},
  year     = {2010},
  volume   = {83},
  number   = {7},
  pages    = {1179--1191},
  issn     = {0164-1212},
  abstract = {The Model-Driven Engineering approach is progressively gaining popularity in the software engineering community as it raises the level of abstraction in software development. In {\{}TALISMAN{\}} {\{}MDE{\}} framework, we combine the principles of the two most important initiatives, Model-Driven Architecture and Software Factories. Both have their pros and cons, and we select the best from each in {\{}TALISMAN{\}} MDE. To show the advantages of {\{}TALISMAN{\}} MDE, we have developed a systems generator and used it to create applications for controlling food traceability. The applications are being used in dairies with different manufacturing processes, using software developed specifically for each dairy by working only with models, without additional programming. },
  annote   = {{\{}SPLC{\}} 2008},
  doi      = {https://doi.org/10.1016/j.jss.2010.01.010},
  keywords = {MDA,MDE,Model-Driven,Software factory,TALISMAN,TMDE},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121210000075},
}

@Article{Acher:2017:TSP:3155324.3088440,
  author    = {Acher, Mathieu and Lopez-Herrejon, Roberto E and Rabiser, Rick},
  title     = {{Teaching Software Product Lines: A Snapshot of Current Practices and Challenges}},
  journal   = {ACM Trans. Comput. Educ.},
  year      = {2017},
  volume    = {18},
  number    = {1},
  pages     = {2:1----2:31},
  issn      = {1946-6226},
  address   = {New York, NY, USA},
  doi       = {10.1145/3088440},
  keywords  = {Software product lines,software engineering teaching,software product line teaching,variability modeling},
  publisher = {ACM},
  url       = {http://doi.acm.org/10.1145/3088440},
}

@Article{Tsai2016150,
  author   = {Tsai, Wei-Tek and Zhong, Peide and Chen, Yinong},
  title    = {{Tenant-centric Sub-Tenancy Architecture in Software-as-a-Service}},
  journal  = {{\{}CAAI{\}} Transactions on Intelligence Technology},
  year     = {2016},
  volume   = {1},
  number   = {2},
  pages    = {150--161},
  issn     = {2468-2322},
  abstract = {Abstract Multi-tenancy architecture (MTA) is often used in Software-as-a-Service (SaaS) and the central idea is that multiple tenant applications can be developed using components stored in the SaaS infrastructure. Recently, {\{}MTA{\}} has been extended to allow a tenant application to have its own sub-tenants, where the tenant application acts like a SaaS infrastructure. In other words, {\{}MTA{\}} is extended to {\{}STA{\}} (Sub-Tenancy Architecture). In STA, each tenant application needs not only to develop its own functionalities, but also to prepare an infrastructure to allow its sub-tenants to develop customized applications. This paper applies Crowdsourcing as the core to {\{}STA{\}} component in the development life cycle. In addition, to discovering adequate fit tenant developers or components to help build and compose new components, dynamic and static ranking models are proposed. Furthermore, rank computation architecture is presented to deal with the case when the number of tenants and components becomes huge. Finally, experiments are performed to demonstrate that the ranking models and the rank computation architecture work as design. },
  doi      = {https://doi.org/10.1016/j.trit.2016.08.002},
  keywords = {Crowdsourcing,MTA,Ranking,STA,SaaS,Sub-tenant,Tenant},
  url      = {http://www.sciencedirect.com/science/article/pii/S2468232216300154},
}

@Article{Srikanth2016122,
  author   = {Srikanth, Hema and Cashman, Mikaela and Cohen, Myra B},
  title    = {{Test case prioritization of build acceptance tests for an enterprise cloud application: An industrial case study}},
  journal  = {Journal of Systems and Software},
  year     = {2016},
  volume   = {119},
  pages    = {122--135},
  issn     = {0164-1212},
  abstract = {Abstract The use of cloud computing brings many new opportunities for companies to deliver software in a highly-customizable and dynamic way. One such paradigm, Software as a Service (SaaS), allows users to subscribe and unsubscribe to services as needed. While beneficial to both subscribers and SaaS service providers, failures escaping to the field in these systems can potentially impact an entire customer base. Build Acceptance Testing (BAT) is a black box technique performed to validate the quality of a SaaS system every time a build is generated. In BAT, the same set of test cases is executed simultaneously across many different servers, making this a time consuming test process. Since {\{}BAT{\}} contains the most critical use cases, it may not be obvious which tests to perform first, given that the time to complete all test cases across different servers in any given day may be insufficient. While all tests must be eventually run, it is critical to run those tests first which are likely to find failures. In this work, we ask if it is possible to prioritize {\{}BAT{\}} tests for improved time to fault detection and present several different approaches, each based on the services executed when running each BAT. In an empirical study on a production enterprise system, we first analyze the historical data from several months in the field, and then use that data to derive the prioritization order for the current development BATs. We then examine if the orders change significantly when we consider fault severity using a cost-based prioritization metric. We find that the prioritization order in which we run the tests does matter, and that the use of historical information is a good heuristic for this order. Prioritized tests have an increase in the rate of fault detection, with the average percent of faults detected (APFD) increasing from less than 0.30 to as high as 0.77 on a scale of zero to one. Although severity slightly changes which order performs best, we see that there are clusters of orderings, ones which improve time to early fault detection ones which don't. },
  doi      = {https://doi.org/10.1016/j.jss.2016.06.017},
  keywords = {Cloud computing,Prioritization,Regression testing,Software as a service},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121216300851},
}

@Article{Engström2013581,
  author        = {Engstr{\"{o}}m, E and Runeson, P},
  title         = {{Test overlay in an emerging software product line-An industrial case study}},
  journal       = {Information and Software Technology},
  year          = {2013},
  volume        = {55},
  number        = {3},
  pages         = {581--594},
  abstract      = {Context: In large software organizations with a product line development approach, system test planning and scope selection is a complex task. Due to repeated testing: across different testing levels, over time (test for regression) as well as of different variants, the risk of redundant testing is large as well as the risk of overlooking important tests, hidden by the huge amount of possible tests. Aims: This study assesses the amount and type of overlaid manual testing across feature, integration and system test in such context, it explores the causes of potential redundancy and elaborates on how to provide decision support in terms of visualization for the purpose of avoiding redundancy. Method: An in-depth case study was launched including both qualitative and quantitative observations. Results: A high degree of test overlay is identified originating from distributed test responsibilities, poor documentation and structure of test cases, parallel work and insufficient delta analysis. The amount of test overlay depends on which level of abstraction is studied. Conclusions: Avoiding redundancy requires tool support, e.g. visualization of test design coverage, test execution progress, priorities of coverage items as well as visualized priorities of variants to support test case selection. {\textcopyright} 2012 Elsevier B.V. All rights reserved.},
  annote        = {cited By 4},
  doi           = {10.1016/j.infsof.2012.04.009},
  file          = {:Users/mac/ownCloud/Anita/SLR/REVISION DE JOURNALS/PAPERS{\_}D/SELECT DAVID/checked/analizados/casos de estudios analizados/5Test overlay in an emerging software product line-An industrial case study.pdf:pdf},
  keywords      = {Complex task,Decision support systems,Decision supports,Efficiency,Industrial,Industrial case s,Software testing,case study},
  mendeley-tags = {case study},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872969667{\&}doi=10.1016{\%}2Fj.infsof.2012.04.009{\&}partnerID=40{\&}md5=f0761683155fbeab04ba6c98af239ed2},
}

@Article{Kitamura2012458,
  author   = {Kitamura, T and Do, N T B and Ohsaki, H and Fang, L and Yatabe, S},
  title    = {{Test-case design by feature trees}},
  journal  = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year     = {2012},
  volume   = {7609 LNCS},
  number   = {PART 1},
  pages    = {458--473},
  abstract = {This paper proposes a test-case design method for black-box testing, called "Feature Oriented Testing (FOT)". The method is realized by applying Feature Models (FMs) developed in software product line engineering to test-case designs. We develop a graphical language for test-case design called "Feature Trees for Testing (FTT)" based on FMs. To firmly underpin the method, we provide a formal semantics of FTT, by means of test-cases derived from test-case designs modelled with FTT. Based on the semantics we develop an automated test-suite generation and correctness checking of test-case designs using SAT, as computer-aided analysis techniques of the method. Feasibility of the method is demonstrated from several viewpoints including its implementation, complexity analysis, experiments, a case study, and an assistant tool. {\textcopyright} 2012 Springer-Verlag.},
  annote   = {cited By 6},
  doi      = {10.1007/978-3-642-34026-0_34},
  keywords = {Black-box testing; Complexity analysis; Design by,Cad Cam; Computer Programs; Experimentation; Math,Computer aided analysis; Forestry; Semantics; Sof,Testing},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868281704{\&}doi=10.1007{\%}2F978-3-642-34026-0{\_}34{\&}partnerID=40{\&}md5=ca10f8a5ee5220d0da6717a0b2094712},
}

@Article{Sen2013657,
  author        = {Sen, S and Gotlieb, A},
  title         = {{Testing a data-intensive system with generated data interactions: The Norwegian customs and excise case study}},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year          = {2013},
  volume        = {7908 LNCS},
  pages         = {657--671},
  abstract      = {Testing data-intensive systems is paramount to increase our reliance on e-governance services. An incorrectly computed tax can have catastrophic consequences in terms of public image. Testers at Norwegian Customs and Excise reveal that faults occur from interactions between database features such as field values. Taxation rules, for example, are triggered due to an interaction between 10,000 items, 88 country groups, and 934 tax codes. There are about 12.9 trillion 3-wise interactions. Finding interactions to uncover specific faults is like finding a needle in a haystack. Can we surgically generate a test database for interactions that interest testers? We address this question with a methodology and tool Faktum to automatically populate a test database that covers all T-wise interactions for selected features. Faktum generates a constraint model of interactions in Alloy and solves it using a divide-and-combine strategy. Our experiments demonstrate scalability of our methodology and we project its industrial applications. {\textcopyright} 2013 Springer-Verlag.},
  annote        = {cited By 3},
  doi           = {10.1007/978-3-642-38709-8_42},
  keywords      = {Alloying,Cerium alloys,Database schemas,Database systems,Entity relationship diagrams,Fe,Industrial applications,case study},
  mendeley-tags = {case study},
  url           = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879862386{\&}doi=10.1007{\%}2F978-3-642-38709-8{\_}42{\&}partnerID=40{\&}md5=52156c86c764b18c8dfc654a55c61079},
}

@Article{Kuroiwa20161341,
  author   = {Kuroiwa, Takeru and Aoyama, Yusuke and Kushiro, Noriyuki},
  title    = {{Testing Environment for {\{}CPS{\}} by Cooperating Model Checking with Execution Testing}},
  journal  = {Procedia Computer Science},
  year     = {2016},
  volume   = {96},
  pages    = {1341--1350},
  issn     = {1877-0509},
  abstract = {Abstract In this study, we propose a testing environment for cyber-physical systems (CPS). In system testing for CPS, many tests are difficult to design or implement because of these systems' many product variations. The proposed environment executes the tests and guarantees that these systems operate reliably using two methods. The first method provides easy management of test cases by managing functions to be tested and configurations to be tested separately. The second method involves automatic testing of real devices based on model checking technologies. The authors have developed a horizontal prototype of the proposed environment and confirmed its feasibility and applicability. },
  annote   = {Knowledge-Based and Intelligent Information {\&} Engineering Systems: Proceedings of the 20th International Conference KES-2016},
  doi      = {https://doi.org/10.1016/j.procs.2016.08.179},
  keywords = {Cyber-physical systems,Model cheking ;,System testing},
  url      = {http://www.sciencedirect.com/science/article/pii/S1877050916319895},
}

@Article{Wong2012567,
  author   = {Wong, P Y H and Albert, E and Muschevici, R and Proen{\c{c}}a, J and Sch{\"{a}}fer, J and Schlatte, R},
  title    = {{The ABS tool suite: Modelling, executing and analysing distributed adaptable object-oriented systems}},
  journal  = {International Journal on Software Tools for Technology Transfer},
  year     = {2012},
  volume   = {14},
  number   = {5},
  pages    = {567--588},
  abstract = {Modern software systems must support a high degree of variability to accommodate a wide range of requirements and operating conditions. This paper introduces the Abstract Behavioural Specification (ABS) language and tool suite, a comprehensive platform for developing and analysing highly adaptable distributed concurrent software systems. The ABS language has a hybrid functional and object- oriented core, and comes with extensions that support the development of systems that are adaptable to diversified requirements, yet capable to maintain a high level of trustworthiness. Using ABS, system variability is consistently traceable from the level of requirements engineering down to object behaviour. This facilitates temporal evolution, as changes to the required set of features of a system are automatically reflected by functional adaptation of the system's behaviour. The analysis capabilities of ABS stretch from debugging, observing and simulating to resource analysis of ABS models and help ensure that a system will remain dependable throughout its evolutionary lifetime. We report on the experience of using the ABS language and the ABS tool suite in an industrial case study. {\textcopyright} 2012 Springer-Verlag.},
  annote   = {cited By 17},
  doi      = {10.1007/s10009-012-0250-1},
  keywords = {Computer software; Industrial applications,Concurrency; Feature modelling; Formal modelling;,Program debugging},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866277516{\&}doi=10.1007{\%}2Fs10009-012-0250-1{\&}partnerID=40{\&}md5=b359c6bba10bf37ad3cd65f572643aba},
}

@Article{Jarke2011992,
  author   = {Jarke, Matthias and Loucopoulos, Pericles and Lyytinen, Kalle and Mylopoulos, John and Robinson, William},
  title    = {{The brave new world of design requirements}},
  journal  = {Information Systems},
  year     = {2011},
  volume   = {36},
  number   = {7},
  pages    = {992--1008},
  issn     = {0306-4379},
  abstract = {Despite its success over the last 30 years, the field of Requirements Engineering (RE) is still experiencing fundamental problems that indicate a need for a change of focus to better ground its research on issues underpinning current practices. We posit that these practices have changed significantly in recent years. To this end we explore changes in software system operational environments, targets, and the process of RE. Our explorations include a field study, as well as two workshops that brought together experts from academia and industry. We recognize that these changes influence the nature of central {\{}RE{\}} research questions. We identify four new principles that underlie contemporary requirements processes, namely: (1) intertwining of requirements with implementation and organizational contexts, (2) dynamic evolution of requirements, (3) emergence of architectures as a critical stabilizing force, and (4) need to recognize unprecedented levels of design complexity. We recommend a re-focus of {\{}RE{\}} research based on a review and analysis of these four principles, and identify several theoretical and practical implications that flow from this analysis. },
  annote   = {Special Issue: Advanced Information Systems Engineering (CAiSE'10)},
  doi      = {https://doi.org/10.1016/j.is.2011.04.003},
  keywords = {Architectures,Complexity,Evolution,Future of requirements engineering,Requirements,Requirements engineering,Requirements principles},
  url      = {http://www.sciencedirect.com/science/article/pii/S0306437911000548},
}

@Article{Hartmann2012178,
  author   = {Hartmann, Herman and Trew, Tim and Bosch, Jan},
  title    = {{The changing industry structure of software development for consumer electronics and its consequences for software architectures}},
  journal  = {Journal of Systems and Software},
  year     = {2012},
  volume   = {85},
  number   = {1},
  pages    = {178--192},
  issn     = {0164-1212},
  abstract = {During the last decade the structure of the consumer electronics industry has been changing profoundly. Current consumer electronics products are built using components from a large variety of specialized firms, whereas previously each product was developed by a single, vertically integrated company. Taking a software development perspective, we analyze the transition in the consumer electronics industry using case studies from digital televisions and mobile phones. We introduce a model consisting of five industry structure types and describe the forces that govern the transition between types and we describe the consequences for software architectures. We conclude that, at this point in time, software supply chains are the dominant industry structure for developing consumer electronics products. This is because the modularization of the architecture is limited, due to the lack of industry-wide standards and because resource constrained devices require variants of supplied software that are optimized for different hardware configurations. Due to these characteristics open ecosystems have not been widely adopted. The model and forces can serve the decision making process for individual companies that consider the transition to a different type of industry structure as well as provide a framework for researchers studying the software-intensive industries. },
  annote   = {Dynamic Analysis and Testing of Embedded Software},
  doi      = {https://doi.org/10.1016/j.jss.2011.08.007},
  keywords = {Case study,Consumer electronics,Ecosystems,Embedded systems,Industry structures,Mobile phones,Software architecture,Software evolution,Software management,Software supply chains},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121211002081},
}

@Article{PAROLIA2011313,
  author   = {Parolia, Neeraj and Jiang, James J and Klein, Gary and Sheu, Tsong Shin},
  title    = {{The contribution of resource interdependence to IT program performance: A social interdependence perspective}},
  journal  = {International Journal of Project Management},
  year     = {2011},
  volume   = {29},
  number   = {3},
  pages    = {313--324},
  issn     = {0263-7863},
  abstract = {Combinations of multiple, related projects into interdependent programs are becoming common in the field of information technology. However, little work exists to study the impact of interdependence in the program environment to achieve collective success. Social interdependence theory provides a structure to examine whether collaborative efforts promote behaviors that result in higher levels of success. Using resource interdependence as an indicator of collaboration, a model of promotive behaviors is developed using the lens of social interdependence. Expectations are that resource interdependence conditions will promote more mutual support, effort, and communication. In turn, these behaviors will lead to an improvement in the business objectives and operational efficiency of the organization. A survey of program and project managers in information system vendors located in India support the model. The results provide support for the argument that programs are effective organization structures that capitalize on interdependencies and that the social interdependence theory provides a consistent model to explain the benefits of resource interdependence.},
  doi      = {https://doi.org/10.1016/j.ijproman.2010.03.004},
  keywords = {Business objectives,Operational effectiveness,Program management,Program performance,Promotive interaction,Resource interdependence},
  url      = {http://www.sciencedirect.com/science/article/pii/S0263786310000542},
}

@Article{Kulesza2013905,
  author   = {Kulesza, Uir{\'{a}} and Soares, S{\'{e}}rgio and Chavez, Christina and Castor, Fernando and Borba, Paulo and Lucena, Carlos and Masiero, Paulo and Sant'Anna, Claudio and Ferrari, Fabiano and Alves, Vander and Coelho, Roberta and Figueiredo, Eduardo and Pires, Paulo F and Delicato, Fl{\'{a}}via and Piveta, Eduardo and Silva, Carla and Camargo, Valter and Braga, Rosana and Leite, Julio and Lemos, Ot{\'{a}}vio and Mendon{\c{c}}a, Nabor and Batista, Thais and Bonif{\'{a}}cio, Rodrigo and Cacho, N{\'{e}}lio and Silva, Lyrene and von Staa, Arndt and Silveira, F{\'{a}}bio and Valente, Marco T{\'{u}}lio and Alencar, Fernanda and Castro, Jaelson and Ramos, Ricardo and Penteado, Rosangela and Rubira, Cec{\'{i}}lia},
  title    = {{The crosscutting impact of the {\{}AOSD{\}} Brazilian research community}},
  journal  = {Journal of Systems and Software},
  year     = {2013},
  volume   = {86},
  number   = {4},
  pages    = {905--933},
  issn     = {0164-1212},
  abstract = {Background Aspect-Oriented Software Development (AOSD) is a paradigm that promotes advanced separation of concerns and modularity throughout the software development lifecycle, with a distinctive emphasis on modular structures that cut across traditional abstraction boundaries. In the last 15 years, research on {\{}AOSD{\}} has boosted around the world. The AOSD-BR research community (AOSD-BR stands for {\{}AOSD{\}} in Brazil) emerged in the last decade, and has provided different contributions in a variety of topics. However, despite some evidence in terms of the number and quality of its outcomes, there is no organized characterization of the AOSD-BR community that positions it against the international {\{}AOSD{\}} Research community and the Software Engineering Research community in Brazil. Aims In this paper, our main goal is to characterize the AOSD-BR community with respect to the research developed in the last decade, confronting it with the {\{}AOSD{\}} international community and the Brazilian Software Engineering community. Method Data collection, validation and analysis were performed in collaboration with several researchers of the AOSD-BR community. The characterization was presented from three different perspectives: (i) a historical timeline of events and main milestones achieved by the community; (ii) an overview of the research developed by the community, in terms of key challenges, open issues and related work; and (iii) an analysis on the impact of the AOSD-BR community outcomes in terms of well-known indicators, such as number of papers and number of citations. Results Our analysis showed that the AOSD-BR community has impacted both the international {\{}AOSD{\}} Research community and the Software Engineering Research community in Brazil. },
  annote   = {{\{}SI{\}} : Software Engineering in Brazil: Retrospective and Prospective Views},
  doi      = {https://doi.org/10.1016/j.jss.2012.08.031},
  keywords = {Aspect-Oriented Software Development,Modularity,Research impact},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121212002427},
}

@Article{Asadi20161706,
  author   = {Asadi, M and Soltani, S and Ga{\v{s}}evi{\'{c}}, D and Hatala, M},
  title    = {{The effects of visualization and interaction techniques on feature model configuration}},
  journal  = {Empirical Software Engineering},
  year     = {2016},
  volume   = {21},
  number   = {4},
  pages    = {1706--1743},
  abstract = {A Software Product Line is a set of software systems of a domain, which share some common features but also have significant variability. A feature model is a variability modeling artifact which represents differences among software products with respect to variability relationships among their features. Having a feature model along with a reference model developed in the domain engineering lifecycle, a concrete product of the family is derived by selecting features in the feature model (referred to as the configuration process) and by instantiating the reference model. However, feature model configuration can be a cumbersome task because: 1) feature models may consist of a large number of features, which are hard to comprehend and maintain; and 2) many factors including technical limitations, implementation costs, stakeholders' requirements and expectations must be considered in the configuration process. Recognizing these issues, a significant amount of research efforts has been dedicated to different aspects of feature model configuration such as automating the configuration process. Several approaches have been proposed to alleviate the feature model configuration challenges through applying visualization and interaction techniques. However, there have been limited empirical insights available into the impact of visualization and interaction techniques on the feature model configuration process. In this paper, we present a set of visualization and interaction interventions for representing and configuring feature models, which are then empirically validated to measure the impact of the proposed interventions. An empirical study was conducted by following the principles of control experiments in software engineering and by applying the well-known software quality standard ISO 9126 to operationalize the variables investigated in the experiment. The results of the empirical study revealed that the employed visualization and interaction interventions significantly improved completion time of comprehension and changing of the feature model configuration. Additionally, according to results, the proposed interventions are easy-to-use and easy-to-learn for the participants. {\textcopyright} 2015, Springer Science+Business Media New York.},
  annote   = {cited By 2},
  doi      = {10.1007/s10664-014-9353-5},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84925275030{\&}doi=10.1007{\%}2Fs10664-014-9353-5{\&}partnerID=40{\&}md5=985f277899e4fd1ea4af6e2b70d33f32},
}

@Article{Kazman20061207,
  author   = {Kazman, Rick and Bass, Len and Klein, Mark},
  title    = {{The essential components of software architecture design and analysis}},
  journal  = {Journal of Systems and Software},
  year     = {2006},
  volume   = {79},
  number   = {8},
  pages    = {1207--1216},
  issn     = {0164-1212},
  abstract = {Architecture analysis and design methods such as ATAM, QAW, {\{}ADD{\}} and {\{}CBAM{\}} have enjoyed modest success and are being adopted by many companies as part of their standard software development processes. They are used in the lifecycle, as a means of understanding business goals and stakeholders concerns, mapping these onto an architectural representation, and assessing the risks associated with this mapping. These methods have evolved a set of shared component techniques. In this paper we show how these techniques can be combined in countless ways to create needs-specific methods in an agile way. We demonstrate the generality of these techniques by describing a new architecture improvement method called {\{}APTIA{\}} (Analytic Principles and Tools for the Improvement of Architectures). {\{}APTIA{\}} almost entirely reuses pre-existing techniques but in a new combination, with new goals and results. We exemplify APTIA's use in improving the architecture of a commercial information system. },
  doi      = {https://doi.org/10.1016/j.jss.2006.05.001},
  keywords = {Analysis methodologies,Design methodologies,Software architecture},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121206001439},
}

@Article{Saeed20161094,
  author   = {Saeed, Aneesa and Hamid, Siti Hafizah Ab and Mustafa, Mumtaz Begum},
  title    = {{The experimental applications of search-based techniques for model-based testing: Taxonomy and systematic literature review}},
  journal  = {Applied Soft Computing},
  year     = {2016},
  volume   = {49},
  pages    = {1094--1117},
  issn     = {1568-4946},
  abstract = {AbstractContext Model-based testing (MBT) aims to generate executable test cases from behavioral models of software systems. {\{}MBT{\}} gains interest in industry and academia due to its provision of systematic, automated, and comprehensive testing. Researchers have successfully applied search-based techniques (SBTs) by automating the search for an optimal set of test cases at reasonable cost compared to other more expensive techniques. Thus, there is a recent surge toward the applications of {\{}SBTs{\}} for {\{}MBT{\}} because the generated test cases are optimal and have low computational cost. However, successful, future {\{}SBTs{\}} for {\{}MBT{\}} applications demand deep insight into its existing experimental applications that underlines stringent issues and challenges, which is lacking in the literature. Objective The objective of this study is to comprehensively analyze the current state-of-the-art of the experimental applications of {\{}SBTs{\}} for {\{}MBT{\}} and present the limitations of the current literature to direct future research. Method We conducted a systematic literature review (SLR) using 72 experimental papers from six data sources. We proposed a taxonomy based on the literature to categorize the characteristics of the current applications. Results The results indicate that the majority of the existing applications of {\{}SBTs{\}} for {\{}MBT{\}} focus on functional and structural coverage purposes, as opposed to stress testing, regression testing and graphical user interface (GUI) testing. We found research gaps in the existing applications in five areas: applying multi-objective SBTs, proposing hybrid techniques, handling complex constraints, addressing data and requirement-based adequacy criteria, and adapting landscape visualization. Only twelve studies proposed and empirically evaluated the {\{}SBTs{\}} for complex systems in MBT. Conclusion This extensive systematic analysis of the existing literature based on the proposed taxonomy enables to assist researchers in exploring the existing research efforts and reveal the limitations that need additional investigation. },
  doi      = {https://doi.org/10.1016/j.asoc.2016.08.030},
  keywords = {Model-based testing,Search-based techniques,Software testing,Systematic literature review,Taxonomy,Test case generation},
  url      = {http://www.sciencedirect.com/science/article/pii/S1568494616304240},
}

@Article{Nurdiani2016162,
  author   = {Nurdiani, Indira and B{\"{o}}rstler, J{\"{u}}rgen and Fricker, Samuel A},
  title    = {{The impacts of agile and lean practices on project constraints: A tertiary study}},
  journal  = {Journal of Systems and Software},
  year     = {2016},
  volume   = {119},
  pages    = {162--183},
  issn     = {0164-1212},
  abstract = {Abstract The growing interest in Agile and Lean software development is reflected in the increasing number of secondary studies on the benefits and limitations of Agile and Lean processes and practices. The aim of this tertiary study is to consolidate empirical evidence regarding Agile and Lean practices and their respective impacts on project constraints as defined in the Project Management Body of Knowledge (PMBOK): scope, quality, schedule, budget, resources, communication, and risk. In this tertiary study, 13 secondary studies were included for detailed analysis. Given the heterogeneity of the data, we were unable to perform a rigorous synthesis. Instead, we mapped the identified Agile and Lean practices, and their impacts on the project constraints described in PMBOK. From 13 secondary studies, we identified 13 Agile and Lean practices. Test-Driven Development (TDD) is studied in ten secondary studies, meanwhile other practices are studied in only one or two secondary studies. This tertiary study provides a consolidated view of the impacts of Agile and Lean practices. The result of this tertiary study indicates that {\{}TDD{\}} has a positive impact on external quality. However, due to insufficient data or contradictory results, we were unable to make inferences on other Agile and Lean practices. Implications for research and practice are further discussed in the paper. },
  doi      = {https://doi.org/10.1016/j.jss.2016.06.043},
  keywords = {Agile software development,Lean software development,Project constraints,Tertiary study},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121216300863},
}

@Article{Wendler20121317,
  author   = {Wendler, Roy},
  title    = {{The maturity of maturity model research: A systematic mapping study}},
  journal  = {Information and Software Technology},
  year     = {2012},
  volume   = {54},
  number   = {12},
  pages    = {1317--1339},
  issn     = {0950-5849},
  abstract = {Context Maturity models offer organizations a simple but effective possibility to measure the quality of their processes. Emerged out of software engineering, the application fields have widened and maturity model research is becoming more important. During the last two decades the publication amount steadily rose as well. Until today, no studies have been available summarizing the activities and results of the field of maturity model research. Objective The objective of this paper is to structure and analyze the available literature of the field of maturity model research to identify the state-of-the-art research as well as research gaps. Method A systematic mapping study was conducted. It included relevant publications of journals and {\{}IS{\}} conferences. Mapping studies are a suitable method for structuring a broad research field concerning research questions about contents, methods, and trends in the available publications. Results The mapping of 237 articles showed that current maturity model research is applicable to more than 20 domains, heavily dominated by software development and software engineering. The study revealed that most publications deal with the development of maturity models and empirical studies. Theoretical reflective publications are scarce. Furthermore, the relation between conceptual and design-oriented maturity model development was analyzed, indicating that there is still a gap in evaluating and validating developed maturity models. Finally, a comprehensive research framework was derived from the study results and implications for further research are given. Conclusion The mapping study delivers the first systematic summary of maturity model research. The categorization of available publications helps researchers gain an overview of the state-of-the-art research and current research gaps. The proposed research framework supports researchers categorizing their own projects. In addition, practitioners planning to use a maturity model may use the study as starting point to identify which maturity models are suitable for their domain and where limitations exist. },
  annote   = {Special Section on Software Reliability and Security},
  doi      = {https://doi.org/10.1016/j.infsof.2012.07.007},
  keywords = {Design-oriented research,Maturity models,Software management,Systematic mapping study},
  url      = {http://www.sciencedirect.com/science/article/pii/S0950584912001334},
}

@Article{Slabospitskaya2013162,
  author   = {Slabospitskaya, O and Kolesnyk, A},
  title    = {{The model for enhanced variability management process in software product line}},
  journal  = {Lecture Notes in Business Information Processing},
  year     = {2013},
  volume   = {137},
  pages    = {162--171},
  abstract = {The paper presents a novel model for the process of managing software Variability – the ability of a software system or artefact to be extended, changed, customized or configured for use in a specific context – in Software Product Line (PL). The process pretends to be enhanced (i. e. consistent, scalable, traceable, visible and rational as for the decisions being made on Variability) to mitigate some its limitations. To this end the Model proposed composes: Management Functions uniformly combining all the actions on variability into single cycle like Doeming Plan-Do-Check-Act one; due quality Demands for the Functions; their Environment driven with another novel Model of Variability in PL. It consistently represents variability both in PL structure and artefacts across all PL development stages and stakeholders' viewpoints along with the dedicated assessment submodel purposing at the decisions' rationality. Presented sample Case Study with trial Workflow-based Configurator tool just developed in the Institute of Software Systems of NAS promises availability of the process constructed for efficient automated support. {\textcopyright} Springer-Verlag Berlin Heidelberg 2013.},
  annote   = {cited By 0; Conference of 4th International United Information Systems Conference, UNISCON 2012 ; Conference Date: 1 June 2012 Through 3 June 2012; Conference Code:194499},
  doi      = {10.1007/978-3-642-38370-0_15},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025114764{\&}doi=10.1007{\%}2F978-3-642-38370-0{\_}15{\&}partnerID=40{\&}md5=e3812ccb458c77636043dd094e869ce0},
}

@Article{O’Leary20121014,
  author   = {O'Leary, P{\'{a}}draig and {De Almeida}, Eduardo Santana and Richardson, Ita and O'Leary, P{\'{a}}draig and {De Almeida}, Eduardo Santana and Richardson, Ita},
  title    = {{The Pro-PD Process Model for Product Derivation within software product lines}},
  journal  = {Information and Software Technology},
  year     = {2012},
  volume   = {54},
  number   = {9},
  pages    = {1014--1028},
  issn     = {0950-5849},
  abstract = {Background The derivation of products from a software product line is a time consuming and expensive activity. Despite recognition that an effective process could alleviate many of the difficulties associated with product derivation, existing approaches have different scope, emphasise different aspects of the derivation process and are frequently too specialised to serve as a general solution. Objective To define a systematic process that will provide a structured approach to the derivation of products from a software product line, based on a set of tasks, roles and artefacts. Method Through a series of research stages using sources in industry and academia, this research has developed a Process Model for Product Derivation (Pro-PD). We document the evidence for the construction of Pro-PD and the design decisions taken. We evaluate Pro-PD through comparison with prominent existing approaches and standards. Results This research presents a Process Model for Product Derivation (Pro-PD). Pro-PD describes the tasks, roles and work artefacts used to derive products from a software product line. Conclusion In response to a need for methodological support, we developed Pro-PD (Process Model for Product Derivation). Pro-PD was iteratively developed and evaluated through four research stages. Our research is a first step toward an evidence-based methodology for product derivation and a starting point for the definition of a product derivation approach. },
  doi      = {https://doi.org/10.1016/j.infsof.2012.03.008},
  file     = {:Users/mac/Library/Application Support/Mendeley Desktop/Downloaded/O'Leary, De Almeida, Richardson - 2012 - The Pro-PD Process Model for Product Derivation within software product lines.pdf:pdf},
  keywords = {Process,Product derivation,Software product lines},
  url      = {http://www.sciencedirect.com/science/article/pii/S0950584912000572},
}

@Article{Fleischmann201683,
  author   = {Fleischmann, Marvin and Amirpur, Miglena and Grupp, Tillmann and Benlian, Alexander and Hess, Thomas},
  title    = {{The role of software updates in information systems continuance — An experimental study from a user perspective}},
  journal  = {Decision Support Systems},
  year     = {2016},
  volume   = {83},
  pages    = {83--96},
  issn     = {0167-9236},
  abstract = {Abstract Although software updates are a ubiquitous phenomenon in professional and private {\{}IT{\}} usage, they have to date received little attention in the {\{}IS{\}} post-adoption literature. Drawing on expectation–confirmation theory and the {\{}IS{\}} continuance literature, we investigate whether, when and how software updates affect users' continuance intentions (CI). Based on a controlled laboratory experiment, we find a positive effect of feature updates on users' CI. According to this effect, software vendors can increase their users' {\{}CI{\}} by delivering features through updates after a software has been released and is already used by customers. We also find that users prefer frequent feature updates over less frequent update packages that bundle several features in one update. However, the positive effect from updates occurs only with functional feature updates and not with technical non-feature updates, disclosing update frequency and update type as crucial moderators to this effect. Furthermore, we unveil that this beneficial effect of feature updates operates through positive disconfirmation of expectations, resulting in increased perceived usefulness and satisfaction. Implications for research and practice as well as directions for future research are discussed. },
  doi      = {https://doi.org/10.1016/j.dss.2015.12.010},
  keywords = {Expectation–confirmation theory,IS continuance,IS post-adoption,IT features,Software updates},
  url      = {http://www.sciencedirect.com/science/article/pii/S0167923616000026},
}

@Article{Geppert2001627,
  author   = {Geppert, Birgit and R{\"{o}}{\ss}ler, Frank},
  title    = {{The {\{}SDL{\}} pattern approach – a reuse-driven {\{}SDL{\}} design methodology}},
  journal  = {Computer Networks},
  year     = {2001},
  volume   = {35},
  number   = {6},
  pages    = {627--645},
  issn     = {1389-1286},
  abstract = {There are several {\{}SDL{\}} methodologies that offer full system life-cycle support. Only few of them consider software reuse, not to mention high-level reuse of architecture and design. However, software reuse is a proven software engineering paradigm leading to high quality and reduced development effort. Experience made it apparent that – beyond the more traditional reuse of code – especially high-level reuse of architecture and design (as in the case of design patterns or frameworks) has the potential of achieving more systematic and widespread reuse. This paper presents the {\{}SDL{\}} pattern approach, a design methodology for distributed systems which integrates SDL-based system development with the pattern paradigm. It supports reuse of design knowledge modeled as {\{}SDL{\}} patterns and concentrates on the design phase of SDL-based system development. In order to get full life-cycle support, the pattern-based design process can be integrated within existing {\{}SDL{\}} methodologies. },
  doi      = {https://doi.org/10.1016/S1389-1286(00)00202-4},
  keywords = {Design methodology,Distributed systems,Patterns,Process model,SDL,Software reuse},
  url      = {http://www.sciencedirect.com/science/article/pii/S1389128600002024},
}

@Article{AHMED20081098,
  author   = {Ahmed, Faheem and Capretz, Luiz Fernando},
  title    = {{The software product line architecture: An empirical investigation of key process activities}},
  journal  = {Information and Software Technology},
  year     = {2008},
  volume   = {50},
  number   = {11},
  pages    = {1098--1113},
  issn     = {0950-5849},
  abstract = {Software architecture has been a key area of concern in software industry due to its profound impact on the productivity and quality of software products. This is even more crucial in case of software product line, because it deals with the development of a line of products sharing common architecture and having controlled variability. The main contributions of this paper is to increase the understanding of the influence of key software product line architecture process activities on the overall performance of software product line by conducting a comprehensive empirical investigation covering a broad range of organizations currently involved in the business of software product lines. This is the first study to empirically investigate and demonstrate the relationships between some of the software product line architecture process activities and the overall software product line performance of an organization at the best of our knowledge. The results of this investigation provide empirical evidence that software product line architecture process activities play a significant role in successfully developing and managing a software product line.},
  doi      = {https://doi.org/10.1016/j.infsof.2007.10.013},
  keywords = {Domain engineering,Empirical study,Software architecture,Software engineering,Software product line},
  url      = {http://www.sciencedirect.com/science/article/pii/S0950584907001243},
}

@Article{Mannaert20111210,
  author   = {Mannaert, Herwig and Verelst, Jan and Ven, Kris},
  title    = {{The transformation of requirements into software primitives: Studying evolvability based on systems theoretic stability}},
  journal  = {Science of Computer Programming},
  year     = {2011},
  volume   = {76},
  number   = {12},
  pages    = {1210--1222},
  issn     = {0167-6423},
  abstract = {Evolvability is widely considered to be a crucial characteristic of software architectures, particularly in the area of information systems. Although many approaches have been proposed for improving evolvability, most indications are that it remains challenging to deliver the required levels of evolvability. In this paper, we present a theoretical approach to how the concept of systems theoretic stability can be applied to the evolvability of software architectures of information systems. We define and formalize the transformation of a set of basic functional requirements into a set of instantiations of software constructs. We define this transformation using both a static and a dynamic perspective. In the latter perspective, we formulate the postulate that information systems should be stable against new requirements. Based on this postulate, we derive a number of design theorems for software implementation. Using this transformation we use theoretical arguments to derive that these theorems contribute to achieving stability. },
  annote   = {Special Issue on Software Evolution, Adaptability and Variability},
  doi      = {https://doi.org/10.1016/j.scico.2010.11.009},
  keywords = {Normalized systems,Stability,Systems theory},
  url      = {http://www.sciencedirect.com/science/article/pii/S016764231000208X},
}

@Article{Anaya201099,
  author   = {Anaya, Victor and Berio, Giuseppe and Harzallah, Mounira and Heymans, Patrick and Matulevi{\v{c}}ius, Raimundas and Opdahl, Andreas L and Panetto, Herv{\'{e}} and Verdecho, Maria Jose},
  title    = {{The Unified Enterprise Modelling Language—Overview and further work}},
  journal  = {Computers in Industry},
  year     = {2010},
  volume   = {61},
  number   = {2},
  pages    = {99--111},
  issn     = {0166-3615},
  abstract = {The Unified Enterprise Modelling Language (UEML) aims at supporting integrated use of enterprise and {\{}IS{\}} models expressed using different languages. To achieve this aim, {\{}UEML{\}} offers a hub through which modelling languages can be connected, thereby paving the way for also connecting the models expressed in those languages. This paper motivates and presents the most central parts of the {\{}UEML{\}} approach: a structured path to describing enterprise and {\{}IS{\}} modelling constructs; a common ontology to interrelate construct descriptions at the semantic level; a correspondence analysis approach to estimate semantic construct similarity; a quality framework to aid selection of languages; a meta-meta model to integrate the different parts of the approach; and a set of tools to aid its use and evolution. The paper also discusses the benefits of {\{}UEML{\}} and points to paths for further work. },
  annote   = {Integration and Information in Networked Enterprises},
  doi      = {https://doi.org/10.1016/j.compind.2009.10.013},
  keywords = {Bunge–Wand–Weber Model,Enterprise modelling,Information Systems Modelling,Interoperability,Modelling Languages,OWL,Ontology,Unified Enterprise Modelling Language (UEML)},
  url      = {http://www.sciencedirect.com/science/article/pii/S0166361509002061},
}

@Article{Päivärinta2015124,
  author   = {P{\"{a}}iv{\"{a}}rinta, Tero and Smolander, Kari},
  title    = {{Theorizing about software development practices}},
  journal  = {Science of Computer Programming},
  year     = {2015},
  volume   = {101},
  pages    = {124--135},
  issn     = {0167-6423},
  abstract = {Abstract The paper focuses on the challenge of generating theoretical support for software development, especially when human software developers are involved in the software development process. We outline a model, “Coat Hanger”, for theorizing about development practices. The model focuses on the intended rationale for the actual realization and resulting impacts of using particular practices in varying contexts. To illustrate the use of the model, we have studied recent practice-oriented articles in the journal Science of Computer Programming. A survey of articles in the journal between 2010 and 2013 showed that out of 371 articles, only four studied software development in professional organizations with actual software practitioners as informants. The Coat Hanger model was then used to identify the theoretical strengths and weaknesses of these four practice descriptions. The analysis is used as the basis to declare the potential of our model as a conceptual aid for more structured theorizing about software development practices. The contribution of the model is the introduction of a concretization of how theorizing can be done through reflection-in-action, instead of regarding research on software practices plainly from the prevailing viewpoint of technical rationality. },
  annote   = {Towards general theories of software engineering},
  doi      = {https://doi.org/10.1016/j.scico.2014.11.012},
  keywords = {Practice,Reflection-in-action,Software development,Theorizing,Theory},
  url      = {http://www.sciencedirect.com/science/article/pii/S0167642314005449},
}

@Article{Berger2014302,
  author   = {Berger, T and Nair, D and Rublack, R and Atlee, J M and Czarnecki, K and W{\c{a}}sowski, A},
  title    = {{Three cases of feature-based variability modeling in industry}},
  journal  = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year     = {2014},
  volume   = {8767},
  pages    = {302--319},
  abstract = {Large software product lines need to manage complex variability. A common approach is variability modeling—creating and maintaining models that abstract over the variabilities inherent in such systems. While many variability modeling techniques and notations have been proposed, little is known about industrial practices and how industry values or criticizes this class of modeling. We attempt to address this gap with an exploratory case study of three companies that apply variability modeling. Among others, our study shows that variability models are valued for their capability to organize knowledge and to achieve an overview understanding of codebases. We observe centralized model governance, pragmatic versioning, and surprisingly little constraint modeling, indicating that the effort of declaring and maintaining constraints does not always pay off. {\textcopyright} Springer International Publishing Switzerland 2014.},
  annote   = {cited By 1},
  keywords = {Artificial intelligence,Centralized models; Constraint model; Exploratory,Computers},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921634156{\&}partnerID=40{\&}md5=ba4de4c74bcc80924ed6d87cf2fb9984},
}

@Article{Asadi20141144,
  author   = {Asadi, Mohsen and Soltani, Samaneh and Gasevic, Dragan and Hatala, Marek and Bagheri, Ebrahim},
  title    = {{Toward automated feature model configuration with optimizing non-functional requirements}},
  journal  = {Information and Software Technology},
  year     = {2014},
  volume   = {56},
  number   = {9},
  pages    = {1144--1165},
  issn     = {0950-5849},
  abstract = {AbstractContext A software product line is a family of software systems that share some common features but also have significant variabilities. A feature model is a variability modeling artifact, which represents differences among software products with respect to the variability relationships among their features. Having a feature model along with a reference model developed in the domain engineering lifecycle, a concrete product of the family is derived by binding the variation points in the feature model (called configuration process) and by instantiating the reference model. Objective In this work we address the feature model configuration problem and propose a framework to automatically select suitable features that satisfy both the functional and non-functional preferences and constraints of stakeholders. Additionally, interdependencies between various non-functional properties are taken into account in the framework. Method The proposed framework combines Analytical Hierarchy Process (AHP) and Fuzzy Cognitive Maps (FCM) to compute the non-functional properties weights based on stakeholders' preferences and interdependencies between non-functional properties. Afterwards, Hierarchical Task Network (HTN) planning is applied to find the optimal feature model configuration. Result Our approach improves state-of-art of feature model configuration by considering positive or negative impacts of the features on non-functional properties, the stakeholders' preferences, and non-functional interdependencies. The approach presented in this paper extends earlier work presented in [1] from several distinct perspectives including mechanisms handling interdependencies between non-functional properties, proposing a novel tooling architecture, and offering visualization and interaction techniques for representing functional and non-functional aspects of feature models. Conclusion our experiments show the scalability of our configuration approach when considering both functional and non-functional requirements of stakeholders. },
  annote   = {Special Sections from “Asia-Pacific Software Engineering Conference (APSEC), 2012” and “ Software Product Line conference (SPLC), 2012”},
  doi      = {https://doi.org/10.1016/j.infsof.2014.03.005},
  keywords = {Feature model configuration,Non-functional interdependencies,Software product lines,Stakeholders' preferences},
  url      = {http://www.sciencedirect.com/science/article/pii/S0950584914000640},
}

@Article{Zhao2015370,
  author   = {Zhao, Xin and Shen, Liwei and Peng, Xin and Zhao, Wenyun},
  title    = {{Toward SLA-constrained service composition: An approach based on a fuzzy linguistic preference model and an evolutionary algorithm}},
  journal  = {Information Sciences},
  year     = {2015},
  volume   = {316},
  pages    = {370--396},
  issn     = {0020-0255},
  abstract = {Abstract In a market-oriented service computing environment, both back-end {\{}SLA{\}} (service level agreement) offers and front-end {\{}SLA{\}} requirements should be considered when performing service composition. In this paper, we address the optimization problem of SLA-constrained service composition and focus on the following issues: the difficulties related to preference definition and to weight assignment, the limitation of linear utility functions in identifying preferred skyline solutions, and the efficiency and scalability requirements of the optimization algorithm. We present a systematic approach based on a fuzzy preference model and on evolutionary algorithms. Specifically, we first model this multi-objective optimization problem using the weighted Tchebycheff distance rather than a linear utility function. We then present a fuzzy preference model for preference representation and weight assignment. In the model, a set of fuzzy linguistic preference terms and their properties are introduced for establishing consistent preference order of multiple QoS dimensions, and a weighting procedure is proposed to transform the preference into numeric weights. Finally, we present two evolutionary algorithms, i.e., single{\_}EA and hybrid{\_}EA, that implement different optimization objectives and that can be used in different {\{}SLA{\}} management scenarios for service composition. We conduct a set of experimental studies to evaluate the effectiveness of the proposed algorithms in determining the optimal solutions, and to evaluate their efficiency and scalability for different problem scales. },
  annote   = {Nature-Inspired Algorithms for Large Scale Global Optimization},
  doi      = {https://doi.org/10.1016/j.ins.2014.11.016},
  keywords = {Evolutionary algorithm,Linguistic preference,Multi-objective optimization,SLA,Service composition,Weighted Tchebycheff distance},
  url      = {http://www.sciencedirect.com/science/article/pii/S002002551401086X},
}

@Article{Nassif2013144,
  author   = {Nassif, Ali Bou and Ho, Danny and Capretz, Luiz Fernando},
  title    = {{Towards an early software estimation using log-linear regression and a multilayer perceptron model}},
  journal  = {Journal of Systems and Software},
  year     = {2013},
  volume   = {86},
  number   = {1},
  pages    = {144--160},
  issn     = {0164-1212},
  abstract = {Software estimation is a tedious and daunting task in project management and software development. Software estimators are notorious in predicting software effort and they have been struggling in the past decades to provide new models to enhance software estimation. The most critical and crucial part of software estimation is when estimation is required in the early stages of the software life cycle where the problem to be solved has not yet been completely revealed. This paper presents a novel log-linear regression model based on the use case point model (UCP) to calculate the software effort based on use case diagrams. A fuzzy logic approach is used to calibrate the productivity factor in the regression model. Moreover, a multilayer perceptron (MLP) neural network model was developed to predict software effort based on the software size and team productivity. Experiments show that the proposed approach outperforms the original {\{}UCP{\}} model. Furthermore, a comparison between the {\{}MLP{\}} and log-linear regression models was conducted based on the size of the projects. Results demonstrate that the {\{}MLP{\}} model can surpass the regression model when small projects are used, but the log-linear regression model gives better results when estimating larger projects. },
  doi      = {https://doi.org/10.1016/j.jss.2012.07.050},
  keywords = {Log-linear regression model,Multilayer perceptron,Software effort estimation,Use case points},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121212002221},
}

@Article{ReinhartzBerger2010491,
  author   = {Reinhartz-Berger, Iris},
  title    = {{Towards automatization of domain modeling}},
  journal  = {Data {\&} Knowledge Engineering},
  year     = {2010},
  volume   = {69},
  number   = {5},
  pages    = {491--515},
  issn     = {0169-023X},
  abstract = {A domain model, which captures the common knowledge and the possible variability allowed among applications in a domain, may assist in the creation of other valid applications in that domain. However, to create such domain models is not a trivial task: it requires expertise in the domain, reaching a very high level of abstraction, and providing flexible, yet formal, artifacts. In this paper an approach, called Semi-automated Domain Modeling (SDM), to create draft domain models from applications in those domains, is presented. {\{}SDM{\}} takes a repository of application models in a domain and matches, merges, and generalizes them into sound draft domain models that include the commonality and variability allowed in these domains. The similarity of the different elements is measured, with consideration of syntactic, semantic, and structural aspects. Unlike ontology and schema integration, these models capture both structural and behavioral aspects of the domain. Running {\{}SDM{\}} on small repositories of project management applications and scheduling systems, we found that the approach may provide reasonable draft domain models, whose comprehensibility, correctness, completeness, and consistency levels are satisfactory. },
  doi      = {https://doi.org/10.1016/j.datak.2010.01.002},
  keywords = {DSL,Domain analysis,Domain engineering,Metamodeling,Product line engineering,UML},
  url      = {http://www.sciencedirect.com/science/article/pii/S0169023X10000030},
}

@Article{Martinez201823,
  author   = {Martinez, J and Sottet, J.-S. and Frey, A G and Bissyand{\'{e}}, T F and Ziadi, T and Klein, J and Temple, P and Acher, M and le Traon, Y},
  title    = {{Towards Estimating and Predicting User Perception on Software Product Variants}},
  journal  = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year     = {2018},
  volume   = {10826 LNCS},
  pages    = {23--40},
  abstract = {Estimating and predicting user subjective perceptions on software products is a challenging, yet increasingly important, endeavour. As an extreme case study, we consider the problem of exploring computer-generated art object combinations that will please the maximum number of people. Since it is not feasible to gather feedbacks for all art products because of a combinatorial explosion of possible configurations as well as resource and time limitations, the challenging objective is to rank and identify optimal art product variants that can be generated based on their average likability. We present the use of Software Product Line (SPL) techniques for gathering and leveraging user feedbacks within the boundaries of a variability model. Our approach is developed in two phases: (1) the creation of a data set using a genetic algorithm and real feedback and (2) the application of a data mining technique on this data set to create a ranking enriched with confidence metrics. We perform a case study of a real-world computer-generated art system. The results of our approach on the arts domain reveal interesting directions for the analysis of user-specific qualities of SPLs. {\textcopyright} 2018, Springer International Publishing AG, part of Springer Nature.},
  annote   = {cited By 0; Conference of 17th International Conference on Software Reuse, ICSR 2018 ; Conference Date: 21 May 2018 Through 23 May 2018; Conference Code:213459},
  doi      = {10.1007/978-3-319-90421-4_2},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047137471{\&}doi=10.1007{\%}2F978-3-319-90421-4{\_}2{\&}partnerID=40{\&}md5=96b468473b8a580fa167065c992f4633},
}

@Article{Bessling2014217,
  author   = {Bessling, S and Huhn, M},
  title    = {{Towards formal safety analysis in feature-oriented product line development}},
  journal  = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year     = {2014},
  volume   = {8315},
  pages    = {217--235},
  abstract = {Feature-orientation has proven beneficial in the development of software product lines. We investigate formal safety analysis and verification for product lines of software-intensive embedded systems. We show how to uniformly augment a feature-oriented, model-based design approach with the specification of safety requirements, failure models and fault injection. Therefore we analyze system hazards and identify the causes, i.e. failures and inadequate control systematically. As features are themain concept of functional decomposition in the product line approach, features also direct the safety analysis and the specification of systemlevel safety requirements: Safety (design) constraints are allocated to features. Subsequently, the behavior including possible faults is formally modeled. Then formal verification techniques are employed in order to prove that the safety constraints are satisfied and the system level hazards are prevented. We demonstrate our method using SCADE Suite for the model-based product line design of cardiac pacemakers. VIATRA is employed for the model graph transformation generating the individual products. Formal safety analysis is performed by using SCADE Design Verifier. The case study shows that our approach leads to a fine-grained safety analysis and is capable of uncovering unwanted feature interactions. {\textcopyright} Springer-Verlag Berlin Heidelberg 2014.},
  annote   = {cited By 3},
  keywords = {Design; Embedded systems; Formal verification; Haz,Feature interactions; Functional decomposition; G,Safety engineering},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924330620{\&}partnerID=40{\&}md5=d2e3a6f4124aeb45d19f9f1fe5f6b65f},
}

@Article{Veerman2005129,
  author   = {Veerman, Niels},
  title    = {{Towards lightweight checks for mass maintenance transformations}},
  journal  = {Science of Computer Programming},
  year     = {2005},
  volume   = {57},
  number   = {2},
  pages    = {129--163},
  issn     = {0167-6423},
  abstract = {We propose a lightweight, practical approach to check mass maintenance transformations. We present checks for both transformation tools and transformed source code, and illustrate them using examples of real-world transformations. Our approach is not a fully fledged, formal one but provides circumstantial evidence for transformation correctness, and has been applied to the mass maintenance of industrial Cobol systems. },
  doi      = {https://doi.org/10.1016/j.scico.2005.01.001},
  keywords = {Lightweight checks,Mass maintenance,Transformations},
  url      = {http://www.sciencedirect.com/science/article/pii/S0167642305000134},
}

@Article{Rothberg:2016:TSC:3093335.2993252,
  author    = {Rothberg, Valentin and Dietrich, Christian and Ziegler, Andreas and Lohmann, Daniel},
  title     = {{Towards Scalable Configuration Testing in Variable Software}},
  journal   = {SIGPLAN Not.},
  year      = {2016},
  volume    = {52},
  number    = {3},
  pages     = {156--167},
  issn      = {0362-1340},
  address   = {New York, NY, USA},
  doi       = {10.1145/3093335.2993252},
  keywords  = {Configurability,Linux,Sampling,Software Product Lines,Software Testing},
  publisher = {ACM},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/3093335.2993252},
}

@Article{Mellado2008361,
  author   = {Mellado, D and Fern{\'{a}}ndez-Medina, E and Piattini, M},
  title    = {{Towards security requirements management for software product lines: A security domain requirements engineering process}},
  journal  = {Computer Standards and Interfaces},
  year     = {2008},
  volume   = {30},
  number   = {6},
  pages    = {361--371},
  abstract = {Security and requirements engineering are one of the most important factors of success in the development of a software product line due to the complexity and extensive nature of them, given that a weakness in security can cause problems throughout the products of a product line. The main contribution of this work is that of providing a security standard-based process for software product line development, which is an add-in of activities in the domain engineering. This process deals with security requirements from the early stages of the product line lifecycle in a systematic and intuitive way especially adapted for product line based development. It is based on the use of the latest security requirements techniques, together with the integration of the Common Criteria (ISO/IEC 15408) and the ISO/IEC 17799 controls into the product line lifecycle. Additionally, it deals with security artefacts variability and traceability, providing us with a Security Core Assets Repository. Moreover, it facilitates the conformance to the most relevant security standards with regard to the management of security requirements, such as ISO/IEC 27001 and ISO/IEC 17799. Finally, we will illustrate our proposed process by describing part of a real case study, as a preliminary validation of it. {\textcopyright} 2008 Elsevier B.V. All rights reserved.},
  annote   = {cited By 23},
  doi      = {10.1016/j.csi.2008.03.004},
  keywords = {(algorithmic) complexity; (e,3e) process; case studies; Common criteria (CC); c,Computer software; Computer software reusability;,Product development},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-44949241673{\&}doi=10.1016{\%}2Fj.csi.2008.03.004{\&}partnerID=40{\&}md5=ec78211e6b7f4d0935b70d5d7c220c10},
}

@Article{Cavalcanti201682,
  author   = {Cavalcanti, Yguarat{\~{a}} Cerqueira and {do Carmo Machado}, Ivan and {da Motal S. Neto}, Paulo Anselmo and de Almeida, Eduardo Santana},
  title    = {{Towards semi-automated assignment of software change requests}},
  journal  = {Journal of Systems and Software},
  year     = {2016},
  volume   = {115},
  pages    = {82--101},
  issn     = {0164-1212},
  abstract = {Abstract Change Requests (CRs) are key elements to software maintenance and evolution. Finding the appropriate developer to a {\{}CR{\}} is crucial for obtaining the lowest, economically feasible, fixing time. Nevertheless, assigning {\{}CRs{\}} is a labor-intensive and time consuming task. In this paper, we report on a questionnaire-based survey with practitioners to understand the characteristics of {\{}CR{\}} assignment, and on a semi-automated approach for {\{}CR{\}} assignment which combines rule-based and machine learning techniques. In accordance with the results of the survey, the proposed approach emphasizes the use of contextual information, essential to effective assignments, and puts the development team in control of the assignment rules, toward making its adoption easier. The assignment rules can be either extracted from the assignment history or created from scratch. An empirical validation was performed through an offline experiment with {\{}CRs{\}} from a large software project. The results pointed out that the approach is up to 46.5{\%} more accurate than other approaches which relying solely on machine learning techniques. This indicates that a rule-based approach is a viable and simple method to leverage {\{}CR{\}} assignments. },
  doi      = {https://doi.org/10.1016/j.jss.2016.01.038},
  keywords = {Automatic change request assignment,Bug triage,Change request management,Software maintenance and evolution},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121216000352},
}

@Article{Buccella2013,
  author    = {Buccella, Agustina and Cechich, Alejandra and Arias, Maximiliano and Pol'la, Matias and Doldan, Maria del Socorro and Morsan, Enrique},
  title     = {{Towards systematic software reuse of GIS: Insights from a case study}},
  journal   = {Computers and Geosciences},
  year      = {2013},
  volume    = {54},
  pages     = {9--20},
  issn      = {00983004},
  abstract  = {With the development and adoption of geographic information systems, there is an increasingly amount of software resources being stored or recorded as products to be reused. At the same time, complexity of geographic services is addressed through standardization, which allows developers reaching higher quality levels. In this paper, we introduce our domain-oriented approach to developing geographic software product lines focusing on the experiences collected from a case study. It was developed in the Marine Ecology Domain (Patagonia, Argentina) and illustrates insights of the process. ?? 2013.},
  doi       = {10.1016/j.cageo.2012.11.014},
  file      = {:Users/mac/Library/Application Support/Mendeley Desktop/Downloaded/Buccella et al. - 2013 - Towards systematic software reuse of GIS Insights from a case study.pdf:pdf},
  isbn      = {0098-3004},
  keywords  = {Geographic information systems,Geographic open source tools,ISO 19119 std.,Marine ecology,Reused services,Software product lines},
  publisher = {Elsevier},
  url       = {http://dx.doi.org/10.1016/j.cageo.2012.11.014},
}

@Article{VarelaVaca20131948,
  author   = {Varela-Vaca, Angel Jesus and Gasca, Rafael M},
  title    = {{Towards the automatic and optimal selection of risk treatments for business processes using a constraint programming approach}},
  journal  = {Information and Software Technology},
  year     = {2013},
  volume   = {55},
  number   = {11},
  pages    = {1948--1973},
  issn     = {0950-5849},
  abstract = {AbstractContext The use of Business Process Management Systems (BPMS) has emerged in the {\{}IT{\}} arena for the automation of business processes. In the majority of cases, the issue of security is overlooked by default in these systems, and hence the potential cost and consequences of the materialization of threats could produce catastrophic loss for organizations. Therefore, the early selection of security controls that mitigate risks is a real and important necessity. Nevertheless, there exists an enormous range of {\{}IT{\}} security controls and their configuration is a human, manual, time-consuming and error-prone task. Furthermore, configurations are carried out separately from the organization perspective and involve many security stakeholders. This separation makes difficult to ensure the effectiveness of the configuration with regard to organizational requirements. Objective In this paper, we strive to provide security stakeholders with automated tools for the optimal selection of {\{}IT{\}} security configurations in accordance with a range of business process scenarios and organizational multi-criteria. Method An approach based on feature model analysis and constraint programming techniques is presented, which enable the automated analysis and selection of optimal security configurations. Results A catalogue of feature models is determined by analyzing typical {\{}IT{\}} security controls for {\{}BPMSs{\}} for the enforcement of the standard goals of security: integrity, confidentiality, availability, authorization, and authentication. These feature models have been implemented through constraint programs, and Constraint Programming techniques based on optimized and non-optimized searches are used to automate the selection and generation of configurations. In order to compare the results of the determination of configuration a comparative analysis is given. Conclusion In this paper, we present innovative tools based on feature models, Constraint Programming and multi-objective techniques that enable the agile, adaptable and automatic selection and generation of security configurations in accordance with the needs of the organization. },
  doi      = {https://doi.org/10.1016/j.infsof.2013.05.007},
  keywords = {Business process,Business process management systems,Constraint programming,Feature model,Risk treatments,Security},
  url      = {http://www.sciencedirect.com/science/article/pii/S0950584913001286},
}

@Article{Ferrari20131639,
  author   = {Ferrari, Fabiano Cutigi and Rashid, Awais and Maldonado, Jos{\'{e}} Carlos},
  title    = {{Towards the practical mutation testing of AspectJ programs}},
  journal  = {Science of Computer Programming},
  year     = {2013},
  volume   = {78},
  number   = {9},
  pages    = {1639--1662},
  issn     = {0167-6423},
  abstract = {Abstract Mutation testing is a test selection criterion that relies on the assumption that test cases which can reveal artificial faults in the software are also good to reveal the real ones. It helps to expose faults which would go otherwise unnoticed. This criterion has been shown to be a promising means to deal with testing-related specificities of contemporary programming techniques such as Aspect-Oriented Programming. However, to date the few initiatives for customising mutation testing for aspect-oriented (AO) programs show either limited coverage with respect to the range of simulated faults, or a need for both adequate tool support and proper evaluation in regard to properties like application cost and effectiveness. This article tackles these limitations by describing a comprehensive mutation-based testing approach for programs written in AspectJ, which represents the most investigated {\{}AO{\}} programming language to date. The approach encompasses the definition of a set of mutation operators for AspectJ-specific constructs and the implementation of a tool that automates the approach. The results of a preliminary evaluation study show that the mutation operators are able to simulate faults that may not be revealed by pre-existing, non-mutation-based test suites. The results also suggest that the approach seems not to overwhelm the testers and hence represents a step towards the practical fault-based testing of AspectJ-like programs. },
  doi      = {https://doi.org/10.1016/j.scico.2013.02.011},
  keywords = {Aspect-oriented programming,AspectJ,Mutation testing,Test evaluation,Testing AspectJ programs},
  url      = {http://www.sciencedirect.com/science/article/pii/S0167642313000713},
}

@Article{Mohan2007968,
  author   = {Mohan, Kannan and Ramesh, Balasubramaniam},
  title    = {{Traceability-based knowledge integration in group decision and negotiation activities}},
  journal  = {Decision Support Systems},
  year     = {2007},
  volume   = {43},
  number   = {3},
  pages    = {968--989},
  issn     = {0167-9236},
  abstract = {Group decision and negotiation (GDN) in distributed collaborative environments involves the acquisition and use of extensive knowledge. Knowledge elements that play a critical role in guiding {\{}GDN{\}} activities are distributed across different work environments that are not seamlessly integrated with each other. We argue that integrating fragmented knowledge will improve the process of {\{}GDN{\}} in software development. In this paper, we present an approach to knowledge integration using traceability. Our approach comprises of: (a) a traceability framework that identifies the key knowledge elements that are to be integrated, and (b) a prototype system that supports the acquisition, integration, and use of knowledge elements represented by the traceability framework. We illustrate the usefulness of our approach with a case study in a software development organization. },
  annote   = {Integrated Decision Support},
  doi      = {https://doi.org/10.1016/j.dss.2005.05.026},
  keywords = {Collaborative software development,Decision making,Group decision and negotiation,Knowledge integration,Traceability,Work processes},
  url      = {http://www.sciencedirect.com/science/article/pii/S0167923605000916},
}

@Article{Bettini2013521,
  author   = {Bettini, Lorenzo and Damiani, Ferruccio and Schaefer, Ina and Strocco, Fabio},
  title    = {{TraitRecordJ: A programming language with traits and records}},
  journal  = {Science of Computer Programming},
  year     = {2013},
  volume   = {78},
  number   = {5},
  pages    = {521--541},
  issn     = {0167-6423},
  abstract = {Traits have been designed as units for fine-grained reuse of behavior in the object-oriented paradigm. Records have been devised to complement traits for fine-grained reuse of state. In this paper, we present the language TraitRecordJ, a Java dialect with records and traits. Records and traits can be composed by explicit linguistic operations, allowing code manipulations to achieve fine-grained code reuse. Classes are assembled from (composite) records and traits and instantiated to generate objects. We introduce the language through examples and illustrate the prototypical implementation of TraitRecordJ using Xtext, an Eclipse framework for the development of programming languages as well as other domain-specific languages. Our implementation comprises an Eclipse-based editor for TraitRecordJ with typical {\{}IDE{\}} functionalities, and a stand-alone compiler, which translates TraitRecordJ programs into standard Java programs. As a case study, we present the TraitRecordJ implementation of a part of the software used in a web-based information system previously implemented in Java. },
  annote   = {Special section: Principles and Practice of Programming in Java 2009/2010 {\&} Special section: Self-Organizing Coordination},
  doi      = {https://doi.org/10.1016/j.scico.2011.06.007},
  keywords = {Eclipse,Implementation,Java,Trait,Type system},
  url      = {http://www.sciencedirect.com/science/article/pii/S0167642311001572},
}

@Article{Moros2013941,
  author   = {Moros, Bego{\~{n}}a and Toval, Ambrosio and Rosique, Francisca and S{\'{a}}nchez, Pedro},
  title    = {{Transforming and tracing reused requirements models to home automation models}},
  journal  = {Information and Software Technology},
  year     = {2013},
  volume   = {55},
  number   = {6},
  pages    = {941--965},
  issn     = {0950-5849},
  abstract = {AbstractContext Model-Driven Software Development (MDSD) has emerged as a very promising approach to cope with the inherent complexity of modern software-based systems. Furthermore, it is well known that the Requirements Engineering (RE) stage is critical for a project's success. Despite the importance of RE, {\{}MDSD{\}} approaches commonly leave textual requirements specifications to one side. Objective Our aim is to integrate textual requirements specifications into the {\{}MDSD{\}} approach by using the {\{}MDSD{\}} techniques themselves, including metamodelling and model transformations. The proposal is based on the assumption that a reuse-based Model-Driven Requirements Engineering (MDRE) approach will improve the requirements engineering stage, the quality of the development models generated from requirements models, and will enable the traces from requirements to other development concepts (such as analysis or design) to be maintained. Method The approach revolves around the Requirements Engineering Metamodel, denominated as REMM, which supports the definition of the boilerplate based textual requirements specification languages needed for the definition of model transformation from application requirements models to platform-specific application models and code. Results The approach has been evaluated through its application to Home Automation (HA) systems. The {\{}HA{\}} Requirement Specification Language denominated as {\{}HAREL{\}} is used to define application requirements models which will be automatically transformed and traced to the application model conforming to the {\{}HA{\}} Domain Specific Language. Conclusions An anonymous online survey has been conducted to evaluate the degree of acceptance by both {\{}HA{\}} application developers and {\{}MDSD{\}} practitioners. The main conclusion is that 66.7{\%} of the {\{}HA{\}} experts polled strongly agree that the automatic transformation of the requirements models to {\{}HA{\}} models improves the quality of the {\{}HA{\}} models. Moreover, 58.3{\%} of the {\{}HA{\}} participants strongly agree with the usefulness of the traceability matrix which links requirements to {\{}HA{\}} functional units in order to discover which devices are related to a specific requirement. We can conclude that the experts we have consulted agree with the proposal we are presenting here, since the average mark given is 4 out of 5. },
  doi      = {https://doi.org/10.1016/j.infsof.2012.12.003},
  keywords = {Home automation models,Model driven software development,Models transformation,Requirements metamodel,Requirements reuse,Requirements traceability},
  url      = {http://www.sciencedirect.com/science/article/pii/S0950584912002388},
}

@Article{HUTCHESSON2013525,
  author   = {Hutchesson, Stuart and McDermid, John},
  title    = {{Trusted Product Lines}},
  journal  = {Information and Software Technology},
  year     = {2013},
  volume   = {55},
  number   = {3},
  pages    = {525--540},
  issn     = {0950-5849},
  abstract = {Context
The paper addresses the use of a Software Product Line approach in the context of developing software for a high-integrity, regulated domain such as civil aerospace. The success of a Software Product Line approach must be judged on whether useful products can be developed more effectively (lower cost, reduced schedule) than with traditional single-system approaches. When developing products for regulated domains, the usefulness of the product is critically dependent on the ability of the development process to provide approval evidence for scrutiny by the regulating authority.
Objective
The objective of the work described is to propose a framework for arguing that a product instantiated using a Software Product Line approach can be approved and used within a regulated domain, such that the development cost of that product would be less than if it had been developed in isolation.
Method
The paper identifies and surveys the issues relating the adoption of Software Product Lines as currently understood (including related technologies such as feature modelling, component-based development and model transformation) when applied to high-integrity software development. We develop an argument framework using Goal Structuring Notation to structure the claims made and the evidence required to support the approval of an instantiated product in such domains. Any unsubstantiated claims or missing/sub-standard evidence is identified, and we propose potential approaches or pose research questions to help address this.
Results
The paper provides an argument framework supporting the use of a Software Product Line approach within a high-integrity regulated domain. It shows how lifecycle evidence can be collected, managed and used to credibly support a regulatory approval process, and provides a detailed example showing how claims regarding model transformation may be supported. Any attempt to use a Software Product Line approach in a regulated domain will need to provide evidence to support their approach in accordance with the argument outlined in the paper.
Conclusion
Product Line practices may complicate the generation of convincing evidence for approval of instantiated products, but it is possible to define a credible Trusted Product Line approach.},
  annote   = {Special Issue on Software Reuse and Product Lines},
  doi      = {https://doi.org/10.1016/j.infsof.2012.06.005},
  keywords = {DO-178B/ED-12B,GSN,High-integrity software,Model transformation,SPARK,Software Product Lines},
  url      = {http://www.sciencedirect.com/science/article/pii/S0950584912001085},
}

@Article{Vale2016128,
  author   = {Vale, Tassio and Crnkovic, Ivica and de Almeida, Eduardo Santana and {da Mota Silveira Neto}, Paulo Anselmo and Cavalcanti, Yguarat{\~{a}} Cerqueira and {de Lemos Meira}, Silvio Romero},
  title    = {{Twenty-eight years of component-based software engineering}},
  journal  = {Journal of Systems and Software},
  year     = {2016},
  volume   = {111},
  pages    = {128--148},
  issn     = {0164-1212},
  abstract = {Abstract The idea of developing software components was envisioned more than forty years ago. In the past two decades, Component-Based Software Engineering (CBSE) has emerged as a distinguishable approach in software engineering, and it has attracted the attention of many researchers, which has led to many results being published in the research literature. There is a huge amount of knowledge encapsulated in conferences and journals targeting this area, but a systematic analysis of that knowledge is missing. For this reason, we aim to investigate the state-of-the-art of the {\{}CBSE{\}} area through a detailed literature review. To do this, 1231 studies dating from 1984 to 2012 were analyzed. Using the available evidence, this paper addresses five dimensions of CBSE: main objectives, research topics, application domains, research intensity and applied research methods. The main objectives found were to increase productivity, save costs and improve quality. The most addressed application domains are homogeneously divided between commercial-off-the-shelf (COTS), distributed and embedded systems. Intensity of research showed a considerable increase in the last fourteen years. In addition to the analysis, this paper also synthesizes the available evidence, identifies open issues and points out areas that call for further research. },
  doi      = {https://doi.org/10.1016/j.jss.2015.09.019},
  keywords = {Component-based software development,Component-based software engineering,Software component,Systematic mapping study},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121215002095},
}

@Article{Mockus:2002:TCS:567793.567795,
  author        = {Mockus, Audris and Fielding, Roy T and Herbsleb, James D},
  title         = {{Two Case Studies of Open Source Software Development: Apache and Mozilla}},
  journal       = {ACM Trans. Softw. Eng. Methodol.},
  year          = {2002},
  volume        = {11},
  number        = {3},
  pages         = {309--346},
  issn          = {1049-331X},
  address       = {New York, NY, USA},
  doi           = {10.1145/567793.567795},
  keywords      = {Apache,Mozilla,Open source software,case study,code ownership,defect density,repair interval},
  mendeley-tags = {case study},
  publisher     = {ACM},
  url           = {http://0-doi.acm.org.fama.us.es/10.1145/567793.567795},
}

@Article{Kastner:2012:TCA:2211616.2211617,
  author    = {K{\"{a}}stner, Christian and Apel, Sven and Th{\"{u}}m, Thomas and Saake, Gunter},
  title     = {{Type Checking Annotation-based Product Lines}},
  journal   = {ACM Trans. Softw. Eng. Methodol.},
  year      = {2012},
  volume    = {21},
  number    = {3},
  pages     = {14:1----14:39},
  issn      = {1049-331X},
  address   = {New York, NY, USA},
  doi       = {10.1145/2211616.2211617},
  keywords  = {{\#}ifdef,CFJ,CIDE,Featherweight Java,conditional compilation,software product lines,type system},
  publisher = {ACM},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/2211616.2211617},
}

@Article{Laguna20102313,
  author   = {Laguna, M A and Marqu{\'{e}}s, J M},
  title    = {{UML support for designing software product lines: The package merge mechanism}},
  journal  = {Journal of Universal Computer Science},
  year     = {2010},
  volume   = {16},
  number   = {17},
  pages    = {2313--2332},
  abstract = {Software product lines have become a successful but challenging approach to software reuse. Some of the problems that hinder the adoption of this development paradigm are the conceptual gap between the variability and design models, as well as the complexity of the traceability management between them. Most current development methods use UML stereotypes or modify UML to face variability and traceability issues. Commercial tools focus mainly on code management, at a fine-grained level. However, the use of specialized techniques and tools represent additional barriers for the widespread introduction of product lines in software companies. In this paper, we propose an alternative based on the UML package merge mechanisms to reflect the structure of the variability models in product line package architecture, thus making the traceability of the configuration decisions straightforward. This package architecture and the configuration of the concrete products are automatically generated (using Model Driven Engineering techniques) from the variability models. As an additional advantage, the package merge mechanism can be directly implemented at code level using partial classes (present in languages such as C{\#}). To support the proposal, we have developed a tool incorporated into MS Visual Studio. This tool permits the product line variability to be modeled and the required transformations to be automated, including the final compilation of concrete products. A case study of a successful experience is described in the article as an example of applying these techniques and tools. The proposed approach, a combination of UML techniques and conventional IDE tools, can make the development of product lines easier for an organization as it removes the need for specialized tools and personnel. {\textcopyright} J.UCS.},
  annote   = {cited By 6},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650294973{\&}partnerID=40{\&}md5=10049679947c53bf4832416b3646fedf},
}

@Article{Chen2014114,
  author   = {Chen, Bihuan and Peng, Xin and Yu, Yijun and Zhao, Wenyun},
  title    = {{Uncertainty handling in goal-driven self-optimization – Limiting the negative effect on adaptation}},
  journal  = {Journal of Systems and Software},
  year     = {2014},
  volume   = {90},
  pages    = {114--127},
  issn     = {0164-1212},
  abstract = {Abstract Goal-driven self-optimization through feedback loops has shown effectiveness in reducing oscillating utilities due to a large number of uncertain factors in the runtime environments. However, such self-optimization is less satisfactory when there contains uncertainty in the predefined requirements goal models, such as imprecise contributions and unknown quality preferences, or during the switches of goal solutions, such as lack of understanding about the time for the adaptation actions to take effect. In this paper, we propose to handle such uncertainty in goal-driven self-optimization without interrupting the services. Taking the monitored quality values as the feedback, and the estimated earned value as the global indicator of self-optimization, our approach dynamically updates the quantitative contributions from alternative functionalities to quality requirements, tunes the preferences of relevant quality requirements, and determines a proper timing delay for the last adaptation action to take effect. After applying these runtime measures to limit the negative effect of the uncertainty in goal models and their suggested switches, an experimental study on a real-life online shopping system shows the improvements over goal-driven self-optimization approaches without uncertainty handling. },
  doi      = {https://doi.org/10.1016/j.jss.2013.12.033},
  keywords = {Goal-driven self-optimization,Requirements goal models,Uncertainty},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121214000065},
}

@Article{Parra20111247,
  author   = {Parra, Carlos and Blanc, Xavier and Cleve, Anthony and Duchien, Laurence},
  title    = {{Unifying design and runtime software adaptation using aspect models}},
  journal  = {Science of Computer Programming},
  year     = {2011},
  volume   = {76},
  number   = {12},
  pages    = {1247--1260},
  issn     = {0167-6423},
  abstract = {Software systems are seen more and more as evolutive systems. At the design phase, software is constantly in adaptation by the building process itself, and at runtime, it can be adapted in response to changing conditions in the executing environment such as location or resources. Adaptation is generally difficult to specify because of its cross-cutting impact on software. This article introduces an approach to unify adaptation at design and at runtime based on Aspect Oriented Modeling. Our approach proposes a unified aspect metamodel and a platform that realizes two different weaving processes to achieve design and runtime adaptations. This approach is used in a Dynamic Software Product Line which derives products that can be configured at design time and adapted at runtime in order to dynamically fit new requirements or resource changes. Such products are implemented using the Service Component Architecture and Java. Finally, we illustrate the use of our approach based on an adaptive e-shopping scenario. The main advantages of this unification are: a clear separation of concerns, the self-contained aspect model that can be weaved during the design and execution, and the platform independence guaranteed by two different types of weaving. },
  annote   = {Special Issue on Software Evolution, Adaptability and Variability},
  doi      = {https://doi.org/10.1016/j.scico.2010.12.005},
  file     = {:Users/mac/Library/Application Support/Mendeley Desktop/Downloaded/Parra et al. - 2011 - Unifying design and runtime software adaptation using aspect models.pdf:pdf},
  keywords = {Aspect oriented modeling,Software product lines},
  url      = {http://www.sciencedirect.com/science/article/pii/S0167642310002303},
}

@Article{Coelho20112700,
  author   = {Coelho, Roberta and von Staa, Arndt and Kulesza, Uir{\'{a}} and Rashid, Awais and Lucena, Carlos},
  title    = {{Unveiling and taming liabilities of aspects in the presence of exceptions: A static analysis based approach}},
  journal  = {Information Sciences},
  year     = {2011},
  volume   = {181},
  number   = {13},
  pages    = {2700--2720},
  issn     = {0020-0255},
  abstract = {As aspects extend or replace existing functionality at specific join points in the code, their behavior may raise new exceptions, which can flow through the program execution in unexpected ways. Assuring the reliability of exception handling code in aspect-oriented (AO) systems is a challenging task. Testing the exception handling code is inherently difficult, since it is tricky to provoke all exceptions during tests, and the large number of different exceptions that can happen in a system may lead to the test-case explosion problem. Moreover, we have observed that some properties of {\{}AO{\}} programming (e.g., quantification, obliviousness) may conflict with characteristics of exception handling mechanisms, exacerbating existing problems (e.g., uncaught exceptions). The lack of verification approaches for exception handling code in {\{}AO{\}} systems stimulated the present work. This work presents a verification approach based on a static analysis tool, called SAFE, to check the reliability of exception handling code in AspectJ programs. We evaluated the effectiveness and feasibility of our approach in two complementary ways (i) by investigating if the {\{}SAFE{\}} tool is precise enough to uncover exception flow information and (ii) by applying the approach to three medium-sized ApectJ systems from different application domains. },
  annote   = {Including Special Section on Databases and Software Engineering},
  doi      = {https://doi.org/10.1016/j.ins.2010.06.002},
  keywords = {Aspect-oriented programming,Exception flow analysis,Exception handling,Exception handling rules conformance,Static analysis},
  url      = {http://www.sciencedirect.com/science/article/pii/S0020025510002525},
}

@Article{GAROUSI2015664,
  author   = {Garousi, Golara and Garousi-Yusifoğlu, Vahid and Ruhe, Guenther and Zhi, Junji and Moussavi, Mahmoud and Smith, Brian},
  title    = {{Usage and usefulness of technical software documentation: An industrial case study}},
  journal  = {Information and Software Technology},
  year     = {2015},
  volume   = {57},
  pages    = {664--682},
  issn     = {0950-5849},
  abstract = {Context
Software documentation is an integral part of any software development process. However, software practitioners are often concerned about the value, degree of usage and usefulness of documentation during development and maintenance.
Objective
Motivated by the needs of NovAtel Inc. (NovAtel), a world-leading company developing software systems in support of global navigation satellite systems, and based on the results of a former systematic mapping study, we aimed at better understanding of the usage and the usefulness of various technical documents during software development and maintenance.
Method
We utilized the results of a former systematic mapping study and performed an industrial case study at NovAtel. From the joint definition of the analysis goals, the research method incorporates qualitative and quantitative analysis of 55 documents (design, test and process related) and 1630 of their revisions. In addition, we conducted a survey on the usage and usefulness of documents. A total of 25 staff members from the industrial partner, all having a medium to high level of experience, participated in the survey.
Results
In the context of the case study, a number of findings were derived. They include that (1) technical documentation was consulted least frequently for maintenance purpose and most frequently as an information source for development, (2) source code was considered most frequently as the preferred information source during software maintenance, (3) there is no significant difference between the usage of various documentation types during both development and maintenance, and (4) initial hypotheses stating that up-to-date information, accuracy and preciseness have the highest impact on usefulness of technical documentation.
Conclusions
It is concluded that the usage of documentation differs for various purposes and it depends on the type of the information needs as well as the tasks to be completed (e.g., development and maintenance). The results have been confirmed to be helpful for the company under study, and the firm is currently implementing some of the recommendations given.},
  doi      = {https://doi.org/10.1016/j.infsof.2014.08.003},
  keywords = {Case study,Industrial context,Technical software documentation,Usage,Usefulness},
  url      = {http://www.sciencedirect.com/science/article/pii/S095058491400192X},
}

@Article{Lumertz20161,
  author   = {Lumertz, Paulo Roberto and Ribeiro, Leila and Duarte, Lucio Mauro},
  title    = {{User interfaces metamodel based on graphs}},
  journal  = {Journal of Visual Languages {\&} Computing},
  year     = {2016},
  volume   = {32},
  pages    = {1--34},
  issn     = {1045-926X},
  abstract = {Abstract Information systems are widely used in all business areas. These systems typically integrate a set of functionalities that implement business rules and maintain databases. Users interact with these systems and use these features through user interfaces (UI). Each {\{}UI{\}} is usually composed of menus where the user can select the desired functionality, thus accessing a new {\{}UI{\}} that corresponds to the desired feature. Hence, a system normally contains multiple UIs. However, keeping consistency between these {\{}UIs{\}} of a system from a visual (organisation, component style, etc.) and behavioral perspective is usually difficult. This problem also appears in software production lines, where it would be desirable to have patterns to guide the construction and maintenance of UIs. One possible way of defining such patterns is to use model-driven engineering (MDE). In MDE, models are defined at different levels, where the bottom level is called a metamodel. The metamodel determines the main characteristics of the models of the upper levels, serving as a guideline. Each new level must adhere to the rules defined by the lower levels. This way, if anything changes in a lower level, these changes are propagated to the levels above it. The goal of this work is to define and validate a metamodel that allows the modeling of {\{}UIs{\}} of software systems, thus allowing the definition of patterns of interface and supporting system evolution. To build this metamodel, we use a graph structure. This choice is due to the fact that a {\{}UI{\}} can be easily represented as a graph, where each {\{}UI{\}} component is a vertex and edges represent dependencies between these components. Moreover, graph theory provides support for a great number of operations and transformations that can be useful for UIs. The metamodel was defined based on the investigation of patterns that occur in UIs. We used a sample of information systems containing different types of {\{}UIs{\}} to obtain such patterns. To validate the metamodel, we built the complete {\{}UI{\}} models of one new system and of four existing real systems. This shows not only the expressive power of the metamodel, but also its versatility, since our validation was conducted using different types of systems (a desktop system, a web system, mobile system, and a multiplatform system). Moreover, it also demonstrated that the proposed approach can be used not only to build new models, but also to describe existing ones (by reverse engineering). },
  doi      = {https://doi.org/10.1016/j.jvlc.2015.10.026},
  keywords = {Graph transformation,Graphs,Metamodel,User interface,User interface patterns},
  url      = {http://www.sciencedirect.com/science/article/pii/S1045926X1500083X},
}

@Article{Nieke2016563,
  author   = {Nieke, M and Mauro, J and Seidl, C and Yu, I C},
  title    = {{User profiles for context-aware reconfiguration in software product lines}},
  journal  = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year     = {2016},
  volume   = {9953 LNCS},
  pages    = {563--578},
  abstract = {Software Product Lines (SPLs) are a mechanism to capture families of closely related software systems by modeling commonalities and variability. Although user customization has a growing importance in software systems and is a vital sales argument, SPLs currently only allow user customization at deploy-time. In this paper, we extend the notion of context-aware SPLs by means of user profiles, containing a linearly ordered set of preferences. Preferences have priorities, meaning that a low priority preference can be neglected in favor of a higher prioritized one. We present a reconfiguration engine checking the validity of the current configuration and, if necessary, reconfiguring the SPL while trying to fulfill the preferences of the active user profile. Thus, users can be assured about the reconfiguration engine providing the most suitable configuration for them. Moreover, we demonstrate the feasibility of our approach using a case study based on existing car customizability. {\textcopyright} Springer International Publishing AG 2016.},
  annote   = {cited By 1},
  doi      = {10.1007/978-3-319-47169-3_44},
  keywords = {Computer software,Context- awareness; Dynamic software product line,Engines; Formal methods; Reconfigurable hardware;},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994052020{\&}doi=10.1007{\%}2F978-3-319-47169-3{\_}44{\&}partnerID=40{\&}md5=0d33fcf39f92d98c12a680d97977251d},
}

@Article{DaSilva2015527,
  author    = {{Da Silva}, Ivonei Freitas and {Da Mota Silveira Neto}, Paulo Anselmo and O'Leary, P??draig and {De Almeida}, Eduardo Santana and {De Lemos Meira}, Silvio Romero},
  title     = {{Using a multi-method approach to understand agile software product lines}},
  journal   = {Information and Software Technology},
  year      = {2015},
  volume    = {57},
  number    = {1},
  pages     = {527--542},
  issn      = {09505849},
  abstract  = {Context: Software product lines (SPLs) and Agile are approaches that share similar objectives. The main difference is the way in which these objectives are met. Typically evidence on what activities of Agile and SPL can be combined and how they can be integrated stems from different research methods performed separately. The generalizability of this evidence is low, as the research topic is still relatively new and previous studies have been conducted using only one research method. Objective: This study aims to increase understanding of Agile SPL and improve the generalizability of the identified evidence through the use of a multi-method approach. Method: Our multi-method research combines three complementary methods (Mapping Study, Case Study and Expert Opinion) to consolidate the evidence. Results: This combination results in 23 findings that provide evidence on how Agile and SPL could be combined. Conclusion: Although multi-method research is time consuming and requires a high degree of effort to plan, design, and perform, it helps to increase the understanding on Agile SPL and leads to more generalizable evidence. The findings confirm a synergy between Agile and SPL and serve to improve the body of evidence in Agile SPL. When researchers and practitioners develop new Agile SPL approaches, it will be important to consider these synergies. {\textcopyright} 2014 Elsevier B.V. All rights reserved.},
  annote    = {From Duplicate 1 (Using a multi-method approach to understand agile software product lines - Da Silva, I F; Da Mota Silveira Neto, P A; O'Leary, P; De Almeida, E S; De Lemos Meira, S R) cited By 1},
  doi       = {10.1016/j.infsof.2014.06.004},
  file      = {:Users/mac/Library/Application Support/Mendeley Desktop/Downloaded/Da Silva et al. - 2015 - Using a multi-method approach to understand agile software product lines.pdf:pdf},
  keywords  = {Agile,Case study,Computer software,Expert opinion,Mapping,Mapping studies,Mapping study,Mergers and acquisitions,Multi-met,Multi-method approach,Software product lines},
  publisher = {Elsevier B.V.},
  url       = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84940244978{\&}doi=10.1016{\%}2Fj.infsof.2014.06.004{\&}partnerID=40{\&}md5=c5776a865b7685023c2704b245211ad2 http://dx.doi.org/10.1016/j.infsof.2014.06.004},
}

@Article{REINAQUINTERO200729,
  author   = {Quintero, A M Reina and Valderrama, J Torres},
  title    = {{Using Aspect-orientation Techniques to Improve Reuse of Metamodels}},
  journal  = {Electronic Notes in Theoretical Computer Science},
  year     = {2007},
  volume   = {163},
  number   = {2},
  pages    = {29--43},
  issn     = {1571-0661},
  abstract = {Metamodelling is an activity that attracts attention of the research community dealing with the Model-Driven Development (MDD). To be reusable in different MDD approaches a metamodel should be unaware of being extended by another metamodel. This property of metamodel is called obliviousness. This paper shows that current techniques implementing metamodels do not maintain obliviousness when some elements of the extended metamodel and the elements of the original model have association relations. Three different approaches to reuse of metamodels are analyzed. One of the approaches uses traditional object-oriented techniques. Two other approaches use aspect-oriented techniques. The paper shows that the third approach, which considers relationships as first-class citizens at the implementation level by using relationship aspects, guarantees obliviousness of metamodels.},
  annote   = {Proceedings of the Second International Workshop on Aspect-Based and Model-Based Separation of Concerns in Software Systems (ABMB 2006)},
  doi      = {https://doi.org/10.1016/j.entcs.2006.10.014},
  keywords = {Aspect-Oriented Programming,Metamodelling,Model-Driven Architecture},
  url      = {http://www.sciencedirect.com/science/article/pii/S1571066107001466},
}

@Article{Cunningham2006147,
  author   = {Cunningham, H Conrad and Liu, Yi and Zhang, Cuihua},
  title    = {{Using classic problems to teach Java framework design}},
  journal  = {Science of Computer Programming},
  year     = {2006},
  volume   = {59},
  number   = {1–2},
  pages    = {147--169},
  issn     = {0167-6423},
  abstract = {All programmers should understand the concept of software families and know the techniques for constructing them. This paper suggests that classic problems, such as well-known algorithms and data structures, are good sources for examples to use in a study of software family design. The paper describes two case studies that can be used to introduce students in a Java software design course to the construction of software families using software frameworks. The first is the family of programs that use the well-known divide and conquer algorithmic strategy. The second is the family of programs that carry out traversals of binary trees. },
  annote   = {Special Issue on Principles and Practices of Programming in Java (PPPJ 2004)Special Issue on Principles and Practices of Programming in Java (PPPJ 2004)},
  doi      = {https://doi.org/10.1016/j.scico.2005.07.009},
  keywords = {Design pattern,Divide and conquer,Hot spot,Software family,Software framework,Tree traversal},
  url      = {http://www.sciencedirect.com/science/article/pii/S0167642305000900},
}

@Article{Khosravi201274,
  author   = {Khosravi, R and Sabouri, H},
  title    = {{Using coordinated actors to model families of distributed systems}},
  journal  = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year     = {2012},
  volume   = {7274 LNCS},
  pages    = {74--88},
  abstract = {Software product line engineering enables strategic reuse in development of families of related products. In a component-based approach to product line development, components capture functionalities appearing in one or more products in the family and different assemblies of components yield to various products or configurations. In this approach, an interaction model which effectively factors out the logic handling variability from the functionality of the system greatly enhances the reusability of components. We study the problem of variability modeling for a family of distributed systems expressed in actor model. We define a special type of actors called coordinators whose behavior is described as Reo circuits with the aim of encapsulating the variability logic. We have the benefits of Reo language for expressing coordination logic, while modeling the entire system as an actor-based distributed model. We have applied this model to a case study extracted from an industrial software family in the domain of interactive TV. {\textcopyright} 2012 IFIP International Federation for Information Processing.},
  annote   = {cited By 1},
  doi      = {10.1007/978-3-642-30829-1_6},
  keywords = {Artificial intelligence,Component based approach; Distributed models; Dist,Reusability},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862733409{\&}doi=10.1007{\%}2F978-3-642-30829-1{\_}6{\&}partnerID=40{\&}md5=7af824eb3f66a995eea78f1c10a7643d},
}

@Article{Hartmann20132313,
  author   = {Hartmann, H and Keren, M and Matsinger, A and Rubin, J and Trew, T and Yatzkar-Haham, T},
  title    = {{Using MDA for integration of heterogeneous components in software supply chains}},
  journal  = {Science of Computer Programming},
  year     = {2013},
  volume   = {78},
  number   = {12},
  pages    = {2313--2330},
  abstract = {Software product lines are increasingly built using components from specialized suppliers. A company that is in the middle of a supply chain has to integrate components from its suppliers and offer (partially configured) products to its customers. To satisfy both the variability required by each customer and the variability required to satisfy different customers' needs, it may be necessary for such a company to use components from different suppliers, partly offering the same feature set. This leads to a product line with alternative components, possibly using different mechanisms for interfacing, binding and variability, which commonly occurs in embedded software development. In this paper, we describe the limitations of the current practice of combining heterogeneous components in a product line and describe the challenges that arise from software supply chains. We introduce a model-driven approach for automating the integration between components that can generate a partially or fully configured variant, including glue between mismatched components. We analyze the consequences of using this approach in an industrial context, using a case study derived from an existing supply chain and describe the process and roles associated with this approach. {\textcopyright} 2012 Elsevier B.V. All rights reserved.},
  annote   = {cited By 5},
  doi      = {10.1016/j.scico.2012.04.004},
  file     = {:Users/mac/Library/Application Support/Mendeley Desktop/Downloaded/Hartmann et al. - 2013 - Using MDA for integration of heterogeneous components in software supply chains.pdf:pdf},
  keywords = {Component technologies; Model-driven Engineering;,Computer software; Customer satisfaction; Industr,Supply chains},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884673619{\&}doi=10.1016{\%}2Fj.scico.2012.04.004{\&}partnerID=40{\&}md5=84e11f2c4726419a91a5e5660508d0e6},
}

@Article{Esfahani2012786,
  author   = {Esfahani, Naeem and Malek, Sam},
  title    = {{Utilizing architectural styles to enhance the adaptation support of middleware platforms}},
  journal  = {Information and Software Technology},
  year     = {2012},
  volume   = {54},
  number   = {7},
  pages    = {786--801},
  issn     = {0950-5849},
  abstract = {Context Modern middleware platforms provide the applications deployed on top of them with facilities for their adaptation. However, the level of adaptation support provided by the state-of-the-art middleware solutions is often limited to dynamically loading and off-loading of software components. Therefore, it is left to the application developers to handle the details of change such that the system's consistency is not jeopardized. Objective We aim to change the status quo by providing the middleware facilities necessary to ensure the consistency of software after adaptation. We would like these facilities to be reusable across different applications, such that the middleware can streamline the process of achieving safe adaptation. Method Our approach addresses the current shortcomings by utilizing the information encoded in a software system's architectural style. This information drives the development of reusable adaptation patterns. The patterns specify both the exact sequence of changes and the time at which those changes need to occur. We use the patterns to provide advanced adaptation support on top of an existing architectural middleware platform. Results Our experience shows the feasibility of deriving detailed adaptation patterns for several architectural styles. Applying the middleware to adapt two real-world software systems shows the approach is effective in consistently adapting these systems without jeopardizing their consistency. Conclusion We conclude the approach is effective in alleviating the application developers from the responsibility of managing the adaptation process at the application-level. Moreover, we believe this study provides the foundation for changing the way adaptation support is realized in middleware solutions. },
  doi      = {https://doi.org/10.1016/j.infsof.2012.02.001},
  keywords = {Adaptation patterns,Architectural style,Middleware,Software architecture},
  url      = {http://www.sciencedirect.com/science/article/pii/S0950584912000328},
}

@Article{ReinhartzBerger20091275,
  author   = {Reinhartz-Berger, Iris and Sturm, Arnon},
  title    = {{Utilizing domain models for application design and validation}},
  journal  = {Information and Software Technology},
  year     = {2009},
  volume   = {51},
  number   = {8},
  pages    = {1275--1289},
  issn     = {0950-5849},
  abstract = {Domain analysis enables identifying families of applications and capturing their terminology in order to assist and guide system developers to design valid applications in the domain. One major way of carrying out the domain analysis is modeling. Several studies suggest using metamodeling techniques, feature-oriented approaches, or architectural-based methods for modeling domains and specifying applications in those domains. However, these methods mainly focus on representing the domain knowledge, providing insufficient guidelines (if any) for creating application models that satisfy the domain rules and constraints. In particular, validation of the application models which include application-specific knowledge is insufficiently dealt. In order to fill these lacks, we propose a general approach, called Application-based {\{}DOmain{\}} Modeling (ADOM), which enables specifying domains and applications similarly, (re)using domain knowledge in application models, and validating the application models against the relevant domain models. In this paper we present the {\{}ADOM{\}} approach, demonstrating its application to {\{}UML{\}} 2.0 class and sequence diagrams. },
  doi      = {https://doi.org/10.1016/j.infsof.2009.03.005},
  keywords = {Domain analysis,Domain engineering,Feature oriented,Metamodeling,Software product line engineering,Variability management},
  url      = {http://www.sciencedirect.com/science/article/pii/S0950584909000366},
}

@Article{HafemannFragal2017210,
  author   = {{Hafemann Fragal}, V and Simao, A and Mousavi, M R},
  title    = {{Validated test models for software product lines: Featured finite state machines}},
  journal  = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year     = {2017},
  volume   = {10231 LNCS},
  pages    = {210--227},
  abstract = {Variants of the finite state machine (FSM) model have been extensively used to describe the behaviour of reactive systems. In particular, several model-based testing techniques have been developed to support test case generation and test case executions from FSMs. Most such techniques require several validation properties to hold for the underlying test models. In this paper, we propose an extension of the FSM test model for software product lines (SPLs), named featured finite state machine (FFSM). As the first step towards using FFSMs as test models, we define feature-oriented variants of basic test model validation criteria. We show how the high-level validation properties coincide with the necessary properties on the product FSMs. Moreover, we provide a mechanised tool prototype for checking the feature-oriented properties using satisfiability modulo theory (SMT) solver tools. We investigate the applicability of our approach by applying it to both randomly generated FFSMs as well as those from a realistic case study (the Body Comfort System). The results of our study show that for random FFSMs over 16 independent non-mandatory features, our technique provides substantial efficiency gains for the set of proposed validity checks. {\textcopyright} Springer International Publishing AG 2017.},
  annote   = {cited By 0},
  doi      = {10.1007/978-3-319-57666-4_13},
  keywords = {Computer software; Finite automata; Model checking,Feature-oriented; Formal modelling; Model based t,Software testing},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018251728{\&}doi=10.1007{\%}2F978-3-319-57666-4{\_}13{\&}partnerID=40{\&}md5=8f239750161fe96551f1877d8040b11a},
}

@Article{GONZALEZHUERTA2015405,
  author   = {Gonzalez-Huerta, Javier and Insfran, Emilio and Abrah{\~{a}}o, Silvia and Scanniello, Giuseppe},
  title    = {{Validating a model-driven software architecture evaluation and improvement method: A family of experiments}},
  journal  = {Information and Software Technology},
  year     = {2015},
  volume   = {57},
  pages    = {405--429},
  issn     = {0950-5849},
  abstract = {Context
Software architectures should be evaluated during the early stages of software development in order to verify whether the non-functional requirements (NFRs) of the product can be fulfilled. This activity is even more crucial in software product line (SPL) development, since it is also necessary to identify whether the NFRs of a particular product can be achieved by exercising the variation mechanisms provided by the product line architecture or whether additional transformations are required. These issues have motivated us to propose QuaDAI, a method for the derivation, evaluation and improvement of software architectures in model-driven SPL development.
Objective
We present in this paper the results of a family of four experiments carried out to empirically validate the evaluation and improvement strategy of QuaDAI.
Method
The family of experiments was carried out by 92 participants: Computer Science Master's and undergraduate students from Spain and Italy. The goal was to compare the effectiveness, efficiency, perceived ease of use, perceived usefulness and intention to use with regard to participants using the evaluation and improvement strategy of QuaDAI as opposed to the Architecture Tradeoff Analysis Method (ATAM).
Results
The main result was that the participants produced their best results when applying QuaDAI, signifying that the participants obtained architectures with better values for the NFRs faster, and that they found the method easier to use, more useful and more likely to be used. The results of the meta-analysis carried out to aggregate the results obtained in the individual experiments also confirmed these results.
Conclusions
The results support the hypothesis that QuaDAI would achieve better results than ATAM in the experiments and that QuaDAI can be considered as a promising approach with which to perform architectural evaluations that occur after the product architecture derivation in model-driven SPL development processes when carried out by novice software evaluators.},
  doi      = {https://doi.org/10.1016/j.infsof.2014.05.018},
  keywords = {ATAM,Family of experiments,Meta-analysis,Quality attributes,Software architecture evaluation methods,Software architectures},
  url      = {http://www.sciencedirect.com/science/article/pii/S0950584914001359},
}

@Article{Gröner201483,
  author   = {Gr{\"{o}}ner, Gerd and Asadi, Mohsen and Mohabbati, Bardia and Ga{\v{s}}evi{\'{c}}, Dragan and Bo{\v{s}}kovi{\'{c}}, Marko and Parreiras, Fernando Silva},
  title    = {{Validation of user intentions in process orchestration and choreography}},
  journal  = {Information Systems},
  year     = {2014},
  volume   = {43},
  pages    = {83--99},
  issn     = {0306-4379},
  abstract = {Abstract Goal models and business process models are complementary artifacts for capturing the requirements and their execution flow in software engineering. In this case, goal models serve as input for designing business process models. This requires mappings between both types of models in order to describe which user goals are implemented by which activities in a business process. Due to the large number of possible relationships among goals in the goal model and possible control flows of activities, developers struggle with the challenge of maintaining consistent configurations of both models and their mappings. Managing these mappings manually is error-prone. In our work, we propose an automated solution that relies on Description Logics and automated reasoners for validating mappings that describe the realization of goals by activities in business process models. The results are the identification of two inconsistency patterns – orchestration inconsistency and choreography inconsistency – and the development of the corresponding algorithms for detecting these inconsistencies. },
  doi      = {https://doi.org/10.1016/j.is.2013.05.006},
  keywords = {Goal-oriented process design,Goal-oriented process engineering,Inconsistency detection,Requirement modeling},
  url      = {http://www.sciencedirect.com/science/article/pii/S0306437913000756},
}

@Article{Itzik2016,
  author  = {Itzik, Nili and Reinhartz-Berger, Iris and Wand, Yair},
  title   = {{Variability Analysis of Requirements: Considering Behavioral Differences and Reflecting Stakeholders? Perspectives}},
  journal = {IEEE Transactions on Software Engineering},
  year    = {2016},
  volume  = {42},
  number  = {7},
  pages   = {687--706},
  month   = {jul},
  issn    = {0098-5589},
  doi     = {10.1109/TSE.2015.2512599},
  file    = {:Users/mac/Library/Application Support/Mendeley Desktop/Downloaded/Itzik, Reinhartz-Berger, Wand - 2016 - Variability Analysis of Requirements Considering Behavioral Differences and Reflecting Stakeholde.pdf:pdf},
  url     = {http://ieeexplore.ieee.org/document/7366597/},
}

@Article{DEELSTRA2009195,
  author   = {Deelstra, Sybren and Sinnema, Marco and Bosch, Jan},
  title    = {{Variability assessment in software product families}},
  journal  = {Information and Software Technology},
  year     = {2009},
  volume   = {51},
  number   = {1},
  pages    = {195--218},
  issn     = {0950-5849},
  abstract = {Software variability management is a key factor in the success of software systems and software product families. An important aspect of software variability management is the evolution of variability in response to changing markets, business needs, and advances in technology. To be able to determine whether, when, and how variability should evolve, we have developed the COVAMOF software variability assessment method (COSVAM). The contribution of COSVAM is that it is a novel, and industry-strength assessment process that addresses the issues that are associated to the current variability assessment practice. In this paper, we present the successful validation of COSVAM in an industrial software product family.},
  annote   = {Special Section - Most Cited Articles in 2002 and Regular Research Papers},
  doi      = {https://doi.org/10.1016/j.infsof.2008.04.002},
  keywords = {Assessment,Evolution,Software product families,Variability},
  url      = {http://www.sciencedirect.com/science/article/pii/S0950584908000542},
}

@Article{Jaring200481,
  author   = {Jaring, M and Bosch, J},
  title    = {{Variability dependencies in product family engineering}},
  journal  = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year     = {2004},
  volume   = {3014},
  pages    = {81--97},
  abstract = {In a product family context, software architects anticipate product diversification and design architectures that support variants in both space (multiple contexts) and time (changing contexts). Product diversification is based on the concept of variability: a single architecture and a set of components support a family of products. Software product families need to support increasing amounts of variability, leading to a situation where variability dependencies become of primary concern. This paper discusses (1) a taxonomy of variability dependencies and (2) a case study in designing a program monitor and exception handler for a legacy system. The study shows that the types of variability dependencies in a system depend on how the system is designed and architected. {\textcopyright} Springer-Verlag Berlin Heidelberg 2004.},
  annote   = {cited By 9},
  keywords = {Design architecture; Exception handlers; Multiple,Legacy systems; Software architecture,Product design},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-35048866917{\&}partnerID=40{\&}md5=ccfba4d47481eb567d6b15890b866eec},
}

@Article{Linsbauer20171179,
  author   = {Linsbauer, L and Lopez-Herrejon, R E and Egyed, A},
  title    = {{Variability extraction and modeling for product variants}},
  journal  = {Software and Systems Modeling},
  year     = {2017},
  volume   = {16},
  number   = {4},
  pages    = {1179--1199},
  abstract = {Fast-changing hardware and software technologies in addition to larger and more specialized customer bases demand software tailored to meet very diverse requirements. Software development approaches that aim at capturing this diversity on a single consolidated platform often require large upfront investments, e.g., time or budget. Alternatively, companies resort to developing one variant of a software product at a time by reusing as much as possible from already-existing product variants. However, identifying and extracting the parts to reuse is an error-prone and inefficient task compounded by the typically large number of product variants. Hence, more disciplined and systematic approaches are needed to cope with the complexity of developing and maintaining sets of product variants. Such approaches require detailed information about the product variants, the features they provide and their relations. In this paper, we present an approach to extract such variability information from product variants. It identifies traces from features and feature interactions to their implementation artifacts, and computes their dependencies. This work can be useful in many scenarios ranging from ad hoc development approaches such as clone-and-own to systematic reuse approaches such as software product lines. We applied our variability extraction approach to six case studies and provide a detailed evaluation. The results show that the extracted variability information is consistent with the variability in our six case study systems given by their variability models and available product variants. {\textcopyright} 2016, The Author(s).},
  annote   = {cited By 4},
  doi      = {10.1007/s10270-015-0512-y},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84955565242{\&}doi=10.1007{\%}2Fs10270-015-0512-y{\&}partnerID=40{\&}md5=42f24866de27f621c0ed588f40db1a32},
}

@Article{MahdaviHezavehi2013320,
  author   = {Mahdavi-Hezavehi, Sara and Galster, Matthias and Avgeriou, Paris},
  title    = {{Variability in quality attributes of service-based software systems: A systematic literature review}},
  journal  = {Information and Software Technology},
  year     = {2013},
  volume   = {55},
  number   = {2},
  pages    = {320--343},
  issn     = {0950-5849},
  abstract = {Context Variability is the ability of a software artifact (e.g., a system, component) to be adapted for a specific context, in a preplanned manner. Variability not only affects functionality, but also quality attributes (e.g., security, performance). Service-based software systems consider variability in functionality implicitly by dynamic service composition. However, variability in quality attributes of service-based systems seems insufficiently addressed in current design practices. Objective We aim at (a) assessing methods for handling variability in quality attributes of service-based systems, (b) collecting evidence about current research that suggests implications for practice, and (c) identifying open problems and areas for improvement. Method A systematic literature review with an automated search was conducted. The review included studies published between the year 2000 and 2011. We identified 46 relevant studies. Results Current methods focus on a few quality attributes, in particular performance and availability. Also, most methods use formal techniques. Furthermore, current studies do not provide enough evidence for practitioners to adopt proposed approaches. So far, variability in quality attributes has mainly been studied in laboratory settings rather than in industrial environments. Conclusions The product line domain as the domain that traditionally deals with variability has only little impact on handling variability in quality attributes. The lack of tool support, the lack of practical research and evidence for the applicability of approaches to handle variability are obstacles for practitioners to adopt methods. Therefore, we suggest studies in industry (e.g., surveys) to collect data on how practitioners handle variability of quality attributes in service-based systems. For example, results of our study help formulate hypotheses and questions for such surveys. Based on needs in practice, new approaches can be proposed. },
  annote   = {Special Section: Component-Based Software Engineering (CBSE), 2011},
  doi      = {https://doi.org/10.1016/j.infsof.2012.08.010},
  keywords = {Quality attributes,Service-based systems,Systematic literature review,Variability},
  url      = {http://www.sciencedirect.com/science/article/pii/S0950584912001772},
}

@Article{deOliveira20183,
  author   = {de Oliveira, A L and Braga, R T V and Masiero, P C and Papadopoulos, Y and Habli, I and Kelly, T},
  title    = {{Variability Management in Safety-Critical Software Product Line Engineering}},
  journal  = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year     = {2018},
  volume   = {10826 LNCS},
  pages    = {3--22},
  abstract = {Safety-critical systems developed upon SPLE approach have to address safety standards, which establish guidance for analyzing and demonstrating dependability properties of the system at different levels of abstraction. However, the adoption of an SPLE approach for developing safety-critical systems demands the integration of safety engineering into SPLE processes. Thus, variability management in both system design and dependability analysis should be considered through SPLE life-cycle. Variation in design and context may impact on dependability properties during Hazard Analysis and Risk Assessment (HARA), allocation of functional and non-functional safety requirements, and component fault analysis. This paper presents DEPendable-SPLE, a model-based approach that extends traditional SPLE methods, to support variability modeling/management in dependability analysis. The approach is illustrated in a case study from the aerospace domain. As a result, the approach enabled efficient management of the impact of design and context variations on HARA and component fault modeling. {\textcopyright} 2018, Springer International Publishing AG, part of Springer Nature.},
  annote   = {cited By 1; Conference of 17th International Conference on Software Reuse, ICSR 2018 ; Conference Date: 21 May 2018 Through 23 May 2018; Conference Code:213459},
  doi      = {10.1007/978-3-319-90421-4_1},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047104557{\&}doi=10.1007{\%}2F978-3-319-90421-4{\_}1{\&}partnerID=40{\&}md5=864173ccdfe2e90d9da48b09bb289d91},
}

@Article{Chen2010166,
  author   = {Chen, L and Babar, M A},
  title    = {{Variability management in software product lines: An investigation of contemporary industrial challenges}},
  journal  = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year     = {2010},
  volume   = {6287 LNCS},
  pages    = {166--180},
  abstract = {Variability management is critical for achieving the large scale reuse promised by the software product line paradigm. It has been studied for almost 20 years. We assert that it is important to explore how well the body of knowledge of variability management solves the challenges faced by industrial practitioners, and what are the remaining and (or) emerging challenges. To gain such understanding of the challenges of variability management faced by practitioners, we have conducted an empirical study using focus group as data collection method. The results of the study highlight several technical challenges that are often faced by practitioners in their daily practices. Different from previous studies, the results also reveal and shed light on several non-technical challenges that were almost neglected by existing research. {\textcopyright} 2010 Springer-Verlag Berlin Heidelberg.},
  annote   = {cited By 10},
  doi      = {10.1007/978-3-642-15579-6_12},
  keywords = {Body of knowledge; Data collection method; Empiric,Computer software reusability,Network architecture; Software design},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-78049366809{\&}doi=10.1007{\%}2F978-3-642-15579-6{\_}12{\&}partnerID=40{\&}md5=7675a95c5d6d22be26b635caf04a0f06},
}

@Article{Berger20141520,
  author   = {Berger, T and Pfeiffer, R.-H. and Tartler, R and Dienst, S and Czarnecki, K and Wa̧sowski, A and She, S},
  title    = {{Variability mechanisms in software ecosystems}},
  journal  = {Information and Software Technology},
  year     = {2014},
  volume   = {56},
  number   = {11},
  pages    = {1520--1535},
  abstract = {Context Software ecosystems are increasingly popular for their economic, strategic, and technical advantages. Application platforms such as Android or iOS allow users to highly customize a system by selecting desired functionality from a large variety of assets. This customization is achieved using variability mechanisms. Objective Variability mechanisms are well-researched in the context of software product lines. Although software ecosystems are often seen as conceptual successors, the technology that sustains their success and growth is much less understood. Our objective is to improve empirical understanding of variability mechanisms used in successful software ecosystems. Method We analyze five ecosystems, ranging from the Linux kernel through Eclipse to Android. A qualitative analysis identifies and characterizes variability mechanisms together with their organizational context. This analysis leads to a conceptual framework that unifies ecosystem-specific aspects using a common terminology. A quantitative analysis investigates scales, growth rates, and - most importantly - dependency structures of the ecosystems. Results In all the studied ecosystems, we identify rich dependency languages and variability descriptions that declare many direct and indirect dependencies. Indirect dependencies to abstract capabilities, as opposed to concrete variability units, are used predominantly in fast-growing ecosystems. We also find that variability models - while providing system-wide abstractions over code - work best in centralized variability management and are, thus, absent in ecosystems with large free markets. These latter ecosystems tend to emphasize maintaining capabilities and common vocabularies, dynamic discovery, and binding with strong encapsulation of contributions, together with uniform distribution channels. Conclusion The use of specialized mechanisms in software ecosystems with large free markets, as opposed to software product lines, calls for recognition of a new discipline - variability encouragement. {\textcopyright} 2014 Elsevier B.V. All rights reserved.},
  annote   = {cited By 13},
  doi      = {10.1016/j.infsof.2014.05.005},
  keywords = {Android (operating system); Commerce; Computer sof,Ecosystems,Empirical Software Engineering; Mining software r},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905441720{\&}doi=10.1016{\%}2Fj.infsof.2014.05.005{\&}partnerID=40{\&}md5=be0733bc2c3d76e97ea11b75ee18fb46},
}

@Article{Horcas2018147,
  author   = {Horcas, J.-M. and Pinto, M and Fuentes, L},
  title    = {{Variability models for generating efficient configurations of functional quality attributes}},
  journal  = {Information and Software Technology},
  year     = {2018},
  volume   = {95},
  pages    = {147--164},
  abstract = {Context: Quality attributes play a critical role in the architecture elicitation phase. Software Sustainability and energy efficiency is becoming a critical quality attribute that can be used as a selection criteria to choose from among different design or implementation alternatives. Energy efficiency usually competes with other non-functional requirements, like for instance, performance. Objective: This paper presents a process that helps developers to automatically generate optimum configurations of functional quality attributes in terms of energy efficiency and performance. Functional quality attributes refer to the behavioral properties that need to be incorporated inside a software architecture to fulfill a particular quality attribute (e.g., encryption and authentication for the security quality attribute, logging for the usability quality attribute). Method: Quality attributes are characterized to identify their design and implementation variants and how the different configurations influence both energy efficiency and performance. A usage model for each characterized quality attribute is defined. The variability of quality attributes, as well as the energy efficiency and performance experiment results, are represented as a constraint satisfaction problem with the goal of formally reasoning about it. Then, a configuration of the selected functional quality attributes is automatically generated, which is optimum with respect to a selected objective function. Results: Software developers can improve the energy efficiency and/or performance of their applications by using our approach to perform a richer analysis of the energy consumption and performance of different alternatives for functional quality attributes. We show quantitative values of the benefits of using our approach and discuss the threats to validity. Conclusions: The process presented in this paper will help software developers to build more energy efficient software, whilst also being aware of how their decisions affect other quality attributes, such as performance. {\textcopyright} 2017 The Authors},
  annote   = {cited By 2},
  doi      = {10.1016/j.infsof.2017.10.018},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032894007{\&}doi=10.1016{\%}2Fj.infsof.2017.10.018{\&}partnerID=40{\&}md5=20a198c05b43337b0a1f3933d5b3d88c},
}

@Article{7128659,
  author   = {Thurimella, A K and Bruegge, B and Janzen, D},
  title    = {{Variability Plug-Ins for Requirements Tools: A Case-Based Theory Building Approach}},
  journal  = {IEEE Systems Journal},
  year     = {2017},
  volume   = {11},
  number   = {4},
  pages    = {1935--1946},
  issn     = {1932-8184},
  abstract = {Product line engineering is a promising discipline for developing a family of systems based on a reusable asset base by systematically managing variability. Requirements tools that deal with variability are a key in the development of product lines. Because of the weak tool support to reuse system requirements and heterogeneity in the representations and processes, we identify a need for guidance and empirical evidence in order to extend a requirements tool with variability. We believe that existing requirement tools can be reused for product line requirements engineering (RE) by adding plug-ins. In this paper, we report a retrospective multiple case study with two diverse cases on developing and deploying variability plug-ins. Our study is based on the theory building process from social sciences and organizational theory. We show how our results can be used by practitioners to extend an RE tool with minimal implementation effort to manage variability.},
  doi      = {10.1109/JSYST.2015.2418290},
  keywords = {formal specification;software development manageme},
}

@Article{Rhein:2018:VSA:3287303.3280986,
  author    = {Rhein, Alexander Von and Liebig, J{\"{o}}RG and Janker, Andreas and K{\"{a}}stner, Christian and Apel, Sven},
  title     = {{Variability-Aware Static Analysis at Scale: An Empirical Study}},
  journal   = {ACM Trans. Softw. Eng. Methodol.},
  year      = {2018},
  volume    = {27},
  number    = {4},
  pages     = {18:1----18:33},
  issn      = {1049-331X},
  address   = {New York, NY, USA},
  doi       = {10.1145/3280986},
  keywords  = {Highly configurable systems,TypeChef,configuration sampling,variability-aware analysis},
  publisher = {ACM},
  url       = {http://doi.acm.org/10.1145/3280986},
}

@Article{ZIELINSKI2017226,
  author   = {Zieli{\'{n}}ski, Cezary and Stefa{\'{n}}czyk, Maciej and Kornuta, Tomasz and Figat, Maksym and Dudek, Wojciech and Szynkiewicz, Wojciech and Kasprzak, W{\l}odzimierz and Figat, Jan and Szlenk, Marcin and Winiarski, Tomasz and Banachowicz, Konrad and Zieli{\'{n}}ska, Teresa and Tsardoulias, Emmanouil G and Symeonidis, Andreas L and Psomopoulos, Fotis E and Kintsakis, Athanassios M and Mitkas, Pericles A and Thallas, Aristeidis and Reppou, Sofia E and Karagiannis, George T and Panayiotou, Konstantinos and Prunet, Vincent and Serrano, Manuel and Merlet, Jean-Pierre and Arampatzis, Stratos and Giokas, Alexandros and Penteridis, Lazaros and Trochidis, Ilias and Daney, David and Iturburu, Miren},
  title    = {{Variable structure robot control systems: The RAPP approach}},
  journal  = {Robotics and Autonomous Systems},
  year     = {2017},
  volume   = {94},
  pages    = {226--244},
  issn     = {0921-8890},
  abstract = {This paper presents a method of designing variable structure control systems for robots. As the on-board robot computational resources are limited, but in some cases the demands imposed on the robot by the user are virtually limitless, the solution is to produce a variable structure system. The task dependent part has to be exchanged, however the task governs the activities of the robot. Thus not only exchange of some task-dependent modules is required, but also supervisory responsibilities have to be switched. Such control systems are necessary in the case of robot companions, where the owner of the robot may demand from it to provide many services.},
  doi      = {https://doi.org/10.1016/j.robot.2017.05.002},
  keywords = {Cloud robotics,RAPP,Robot controllers,Variable structure controllers},
  url      = {http://www.sciencedirect.com/science/article/pii/S0921889016306248},
}

@Article{Kienzle2016122,
  author   = {Kienzle, J and Mussbacher, G and Alam, O and Sch{\"{o}}ttle, M and Belloir, N and Collet, P and Combemale, B and DeAntoni, J and Klein, J and Rumpe, B},
  title    = {{VCU: The three dimensions of reuse}},
  journal  = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year     = {2016},
  volume   = {9679},
  pages    = {122--137},
  abstract = {Reuse, enabled by modularity and interfaces, is one of the most important concepts in software engineering. This is evidenced by an increasingly large number of reusable artifacts, ranging from small units such as classes to larger, more sophisticated units such as components, services, frameworks, software product lines, and concerns. This paper presents evidence that a canonical set of reuse interfaces has emerged over time: the variation, customization, and usage interfaces (VCU). A reusable artifact that provides all three interfaces reaches the highest potential of reuse, as it explicitly exposes how the artifact can be manipulated during the reuse process along these three dimensions. We demonstrate the wide applicability of the VCU interfaces along two axes: across abstraction layers of a system specification and across existing reuse techniques. The former is shown with the help of a comprehensive case study including reusable requirements, software, and hardware models for the authorization domain. The latter is shown with a discussion on how the VCU interfaces relate to existing reuse techniques. {\textcopyright} Springer International Publishing Switzerland 2016.},
  annote   = {cited By 1},
  doi      = {10.1007/978-3-319-35122-3_9},
  keywords = {Abstracting; Interfaces (materials); Software engi,Computer software reusability,Concern-oriented reuse; Configuration; Customizat},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84977507448{\&}doi=10.1007{\%}2F978-3-319-35122-3{\_}9{\&}partnerID=40{\&}md5=196c3a40799350d9ef9bf7971326ec02},
}

@Article{Wang2007117,
  author   = {Wang, Hai H and Li, Yuan Fang and Sun, Jing and Zhang, Hongyu and Pan, Jeff},
  title    = {{Verifying feature models using {\{}OWL{\}}}},
  journal  = {Web Semantics: Science, Services and Agents on the World Wide Web},
  year     = {2007},
  volume   = {5},
  number   = {2},
  pages    = {117--129},
  issn     = {1570-8268},
  abstract = {Feature models are widely used in domain engineering to capture common and variant features among systems in a particular domain. However, the lack of a formal semantics and reasoning support of feature models has hindered the development of this area. Industrial experiences also show that methods and tools that can support feature model analysis are badly appreciated. Such reasoning tool should be fully automated and efficient. At the same time, the reasoning tool should scale up well since it may need to handle hundreds or even thousands of features a that modern software systems may have. This paper presents an approach to modeling and verifying feature diagrams using Semantic Web {\{}OWL{\}} ontologies. We use {\{}OWL{\}} {\{}DL{\}} ontologies to precisely capture the inter-relationships among the features in a feature diagram. {\{}OWL{\}} reasoning engines such as FaCT++ are deployed to check for the inconsistencies of feature configurations fully automatically. Furthermore, a general {\{}OWL{\}} debugger has been developed to tackle the disadvantage of lacking debugging aids for the current {\{}OWL{\}} reasoner and to complement our verification approach. We also developed a {\{}CASE{\}} tool to facilitate visual development, interchange and reasoning of feature diagrams in the Semantic Web environment. },
  annote   = {Software Engineering and the Semantic Web},
  doi      = {https://doi.org/10.1016/j.websem.2006.11.006},
  keywords = {Feature modeling,OWL,Ontologies,Semantic Web},
  url      = {http://www.sciencedirect.com/science/article/pii/S1570826807000042},
}

@Article{Huysegoms2013189,
  author   = {Huysegoms, Tom and Snoeck, Monique and Dedene, Guido and Goderis, Antoon and Stumpe, Frank},
  title    = {{Visualizing Variability Management in Requirements Engineering through Formal Concept Analysis}},
  journal  = {Procedia Technology},
  year     = {2013},
  volume   = {9},
  pages    = {189--199},
  issn     = {2212-0173},
  abstract = {Abstract While research on the visualization and documentation of variability in software artefacts by means of e.g. feature diagrams is well established, most of these documentation methods in the field of variability management assume the presence of variability as a given fact. The decision whether variability within the requirements should actually give rise to variability in the envisaged software artefact is often taken unconsciously and as a result techniques to visualize and document the amount, the structure and the impact of requirements evolution on variability are scarce. This paper provides a real life proof of concept that formal concept analysis (FCA) can be used for the visualization and documentation of variability re- lated decisions during (early) requirements engineering. {\{}FCA{\}} is used in a real-life case study to check the usability of {\{}FCA{\}} as a visualization method to support variability management during requirements engineering. The real-life case study also provides initial proof that useful documentation can be obtained by representing the requirements in a {\{}FCA{\}} concept lattice. },
  annote   = {{\{}CENTERIS{\}} 2013 - Conference on {\{}ENTERprise{\}} Information Systems / ProjMAN 2013 - International Conference on Project MANagement/ {\{}HCIST{\}} 2013 - International Conference on Health and Social Care Information Systems and Technologies},
  doi      = {https://doi.org/10.1016/j.protcy.2013.12.021},
  keywords = {Requirements management,formal concept analysis,harmonization,variability management,variabilization},
  url      = {http://www.sciencedirect.com/science/article/pii/S2212017313001758},
}

@Article{Koning2009258,
  author   = {Koning, Michiel and Sun, Chang-ai and Sinnema, Marco and Avgeriou, Paris},
  title    = {{VxBPEL: Supporting variability for Web services in {\{}BPEL{\}}}},
  journal  = {Information and Software Technology},
  year     = {2009},
  volume   = {51},
  number   = {2},
  pages    = {258--269},
  issn     = {0950-5849},
  abstract = {Web services provide a way to facilitate the business integration over the Internet. Flexibility is an important and desirable property of Web service-based systems due to dynamic business environments. The flexibility can be provided or addressed by incorporating variability into a system. In this study, we investigate how variability can be incorporated into service-based systems. We propose a language, VxBPEL, which is an adaptation of an existing language, BPEL, and able to capture variability in these systems. We develop a prototype to interpret this language. Finally, we illustrate our method by using it to handle variability of an example. },
  doi      = {https://doi.org/10.1016/j.infsof.2007.12.002},
  keywords = {Business Process Execution Language,Service-based system,Variability,Web service},
  url      = {http://www.sciencedirect.com/science/article/pii/S0950584908000207},
}

@Article{Fantinato2006290,
  author   = {Fantinato, M and {De S. Gimenes}, I M and {De Toledo}, M B F},
  title    = {{Web service E-contract establishment using features}},
  journal  = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year     = {2006},
  volume   = {4102 LNCS},
  pages    = {290--305},
  abstract = {Electronic contracts describe inter-organizational business processes in terms of supply and consumption of electronic services (commonly Web services). In a given contract domain, it is usually possible to identify a set of well-defined common and variation points. Feature modeling is an ontology-like technique that has been widely used for capturing and managing commonalities and variabilities of product families in the context of software product line. This paper proposes a feature-based approach in order to decrease the complexity in Web service e-contract establishment. The feasibility of the approach is shown by a case study carried out within the telecom context and based on experimental software engineering concepts. {\textcopyright} Springer-Verlag Berlin Heidelberg 2006.},
  annote   = {cited By 2},
  keywords = {Computer simulation; Computer software; Computer s,Electronic commerce,e-contract; Feature modeling; Ontology; Web servi},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-33750013759{\&}partnerID=40{\&}md5=6eb690c8a307d019593534f057352d8b},
}

@Article{Lee2016199,
  author   = {Lee, Seonah and Kang, Sungwon},
  title    = {{What situational information would help developers when using a graphical code recommender?}},
  journal  = {Journal of Systems and Software},
  year     = {2016},
  volume   = {117},
  pages    = {199--217},
  issn     = {0164-1212},
  abstract = {Abstract Developers spend a significant amount of time trying to understand code bases. To aid developers' comprehension of code, researchers have developed software visualization tools. However, the uses of these tools in situ have rarely been investigated. To make matters worse, as studies have revealed, developers seldom use diagramming tools, making such investigations a challenge. To determine the possible uses of such tools in real practice, we conduct a diary study in which eleven developers in real-world developments use a novel visualization tool (a graphical code recommender) for one month. In the study, we ask what information and features the visualization and diagramming tools should provide to aid developers' work according to their situations. The study reveals the situations in which developers would use such visualization and diagramming tools and also the concrete requirements for such tools that would make them useful. },
  doi      = {https://doi.org/10.1016/j.jss.2016.02.050},
  keywords = {Code navigation,Design requirement,Diagramming tool,Diary study,Software visualization},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121216000819},
}

@Article{Barros-Justo20181,
  author   = {Barros-Justo, J L and Pinciroli, F and Matalonga, S and Mart{\'{i}}nez-Araujo, N},
  title    = {{What software reuse benefits have been transferred to the industry? A systematic mapping study}},
  journal  = {Information and Software Technology},
  year     = {2018},
  volume   = {103},
  pages    = {1--21},
  abstract = {Context: The term software reuse was first used in 1968 at the NATO conference. Since then, work in the scientific literature has stated that the application of software reuse offers benefits such as increase in quality and productivity. Nonetheless, in spite of many publications reporting software reuse experiences, evidence that such benefits having reached industrial settings is scarce. Objective: To identify and classify the benefits transferred to real-world settings by the application of software reuse strategies. Method: We conducted a systematic mapping study (SMS). Our search strategies retrieved a set of 2,413 papers out of which 49 were selected as primary studies. We defined five facets to classify these studies: (a) the type of benefit, (b) the reuse process, (c) the industry's domain, (d) the type of reuse and (e) the type of research reported. Results: Quality increase (28 papers) and Productivity increase (25 papers) were the two most mentioned benefits. Component-Based Development (CBD) was the most reported reuse strategy (41{\%}), followed by Software Product Lines (SPL, 30{\%}). The selected papers mentioned fourteen industrial domains, of which four stand out: aerospace and defense, telecommunications, electronics and IT services. The application of systematic reuse was reported in 78{\%} of the papers. Regarding the research type, 50{\%} use evaluation research as the investigation method. Finally, 13 papers (27{\%}) reported validity threats for the research method applied. Conclusions: The literature analyzed presents a lack of empirical data, making it difficult to evaluate the effective transfer of benefits to the industry. This work did not find any relationship between the reported benefits and the reuse strategy applied by the industry or the industry domain. Although the most reported research method was industrial case studies (25 works), half of these works (12) did not report threats to validity. {\textcopyright} 2018 Elsevier B.V.},
  annote   = {cited By 0},
  doi      = {10.1016/j.infsof.2018.06.003},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049539211{\&}doi=10.1016{\%}2Fj.infsof.2018.06.003{\&}partnerID=40{\&}md5=78f2c4abb85946cc56a3b5b29e716faf},
}

@Article{Alami201662,
  author   = {Alami, Adam},
  title    = {{Why Do Information Technology Projects Fail?}},
  journal  = {Procedia Computer Science},
  year     = {2016},
  volume   = {100},
  pages    = {62--71},
  issn     = {1877-0509},
  abstract = {Abstract With developing technological possibilities, {\{}IT{\}} projects are becoming increasingly ambitious in both goals and scale. Although technology itself is enabling easy management of project execution, failure can still occur, particularly with respect to an ample number of unique projects. It is argued here that the ampleness and uniqueness of projects provide criteria for such projects to be treated differently from smaller-scale enterprises of the same type. Gaps can be identified in the literature with regards to exact definitions of project success and failure. It is proposed that three main issues can impact a project's ecosystem and determine its failure, namely, uncertainty, volatility, and unknowns. Based on these aspects, future project performance can be estimated and correspondingly managed. At the same time, retrospective assessment of success or failure may be made rigorous based on exact definitions. Two case studies of major technological projects are presented and discussed here as examples of theory application. },
  annote   = {International Conference on {\{}ENTERprise{\}} Information Systems/International Conference on Project MANagement/International Conference on Health and Social Care Information Systems and Technologies, CENTERIS/ProjMAN / {\{}HCist{\}} 2016},
  doi      = {https://doi.org/10.1016/j.procs.2016.09.124},
  keywords = {IT project failure,LAUSD,UK e-borders,project management},
  url      = {http://www.sciencedirect.com/science/article/pii/S1877050916322918},
}

@Article{Fantinato2008373,
  author   = {Fantinato, M and {De Toledo}, M B F and {De Souza Gimenes}, I M},
  title    = {{WS-contract establishment with QOS:}},
  journal  = {International Journal of Cooperative Information Systems},
  year     = {2008},
  volume   = {17},
  number   = {3},
  pages    = {373--407},
  abstract = {Electronic contracts describe inter-organizational business processes in terms of supply and consumption of electronic services (commonly Web services). The establishment of e-contracts in a particular business domain usually involves a set of well-defined common and variable properties. These properties are not fully exploited by the existing e-contract establishment approaches. Feature modeling is a software engineering technique that has been widely used for capturing and managing commonalities and variabilities of product families in the context of software product line. This paper presents a feature-based approach to support Web services e-contract (WS-contract) establishment. The approach aims at improving the information structure and reuse of WS-contracts, including the QoS attributes. Features are used to represent possible WS-contract elements in order to drive WS-contract template instantiation, thus acting as a configuration space manager. A toolkit named FeatureContract was developed to automatically support the proposed approach. A case study was carried out within the telecom context to show the approach feasibility. {\textcopyright} 2008 World Scientific Publishing Company.},
  annote   = {cited By 19},
  doi      = {10.1142/S0218843008001889},
  url      = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-50949123633{\&}doi=10.1142{\%}2FS0218843008001889{\&}partnerID=40{\&}md5=5b4ae3194ddd2d62e7c64661813e9a4f},
}

@Article{BETTINI2017419,
  author   = {Bettini, Lorenzo and Damiani, Ferruccio},
  title    = {{Xtraitj: Traits for the Java platform}},
  journal  = {Journal of Systems and Software},
  year     = {2017},
  volume   = {131},
  pages    = {419--441},
  issn     = {0164-1212},
  abstract = {Traits were proposed as a mechanism for fine-grained code reuse to overcome many limitations of class-based inheritance. A trait is a set of methods that is independent from any class hierarchy and can be flexibly used to build other traits or classes by means of a suite of composition operations. In this paper we present the new version of Xtraitj, a trait-based programming language that features complete compatibility and interoperability with the Java platform. Xtraitj is implemented in Xtext and Xbase, and it provides a full Eclipse IDE that supports an incremental adoption of traits in existing Java projects. The new version of Xtraitj allows traits to be accessed from any Java project or library, even if the original Xtraitj source code is not available, since traits can be accessed in their byte-code format. This allows developers to create Xtraitj libraries that can be provided in their binary only format. We detail the technique we used to achieve such an implementation; this technique can be reused in other languages implemented in Xtext for the Java platform. We formalize our traits by means of flattening semantics and we provide some performance benchmarks that show that the runtime overhead introduced by our traits is acceptable.},
  doi      = {https://doi.org/10.1016/j.jss.2016.07.035},
  keywords = {Eclipse,IDE,Implementation,Java,Trait},
  url      = {http://www.sciencedirect.com/science/article/pii/S0164121216301297},
}

@Article{Mosser20131035,
  author   = {Mosser, S{\'{e}}bastien and Blay-Fornarino, Mireille},
  title    = {{“Adore”, a logical meta-model supporting business process evolution}},
  journal  = {Science of Computer Programming},
  year     = {2013},
  volume   = {78},
  number   = {8},
  pages    = {1035--1054},
  issn     = {0167-6423},
  abstract = {The Service Oriented Architecture (Soa) paradigm supports the assembly of atomic services to create applications that implement complex business processes. Since “real-life” processes can be very complex, composition mechanisms inspired by the Separation of Concerns paradigm (e.g. features, aspects) are good candidates to support the definition and the upcoming evolutions of large systems. We propose Adore, “an Activity meta-moDel supOrting oRchestration Evolution” to address this issue. The Adore meta-model allows process designers to express in the same formalism business processes and fragments of processes. Such fragments define additional activities that aim to be integrated into other processes and adequately support their evolution. The underlying logical foundations of Adore allow the definition of interference detection rules as logical predicate, as well as the definition of consistency properties on Adore models. Consequently, the Adore framework supports process designers while they design and then apply evolutions on large processes, managing the detection of interferences among fragments and ensuring that the composed processes are consistent and do not depend on the order of the composition. },
  annote   = {Special section on software evolution, adaptability, and maintenance {\&} Special section on the Brazilian Symposium on Programming Languages},
  doi      = {https://doi.org/10.1016/j.scico.2012.06.009},
  keywords = {Business processes,Logical composition,SOA,Sep. of concerns},
  url      = {http://www.sciencedirect.com/science/article/pii/S0167642312001220},
}

@InProceedings{Murugesupillai:2011:PMS:2019136.2019149,
  author        = {Murugesupillai, Esan and Mohabbati, Bardia and Ga{\v{s}}evi{\'{c}}, Dragan},
  title         = {{A Preliminary Mapping Study of Approaches Bridging Software Product Lines and Service-oriented Architectures}},
  booktitle     = {SPLC},
  year          = {2011},
  series        = {SPLC '11},
  pages         = {11:1----11:8},
  address       = {New York, NY, USA},
  publisher     = {ACM},
  __markedentry = {[mac:3]},
  doi           = {10.1145/2019136.2019149},
  isbn          = {978-1-4503-0789-5},
  keywords      = {service-oriented architecture,service-oriented product line,software product line,software variability,variability management},
  url           = {http://0-doi.acm.org.fama.us.es/10.1145/2019136.2019149},
}

@InProceedings{Wnuk:2016:SMS:2915970.2915985,
  author        = {Wnuk, Krzysztof and Kollu, Ravichandra Kumar},
  title         = {{A Systematic Mapping Study on Requirements Scoping}},
  booktitle     = {International Conference on Evaluation and Assessment in Software Engineering},
  year          = {2016},
  series        = {EASE '16},
  pages         = {32:1----32:11},
  address       = {New York, NY, USA},
  publisher     = {ACM},
  __markedentry = {[mac:3]},
  doi           = {10.1145/2915970.2915985},
  isbn          = {978-1-4503-3691-8},
  keywords      = {requirements scoping, snowballing, systematic mapping study},
  url           = {http://0-doi.acm.org.fama.us.es/10.1145/2915970.2915985},
}

@InProceedings{Myllarniemi:2012:SCL:2362536.2362546,
  author        = {Myll{\"{a}}rniemi, Varvana and Raatikainen, Mikko and M{\"{a}}nnist{\"{o}}, Tomi},
  title         = {{A Systematically Conducted Literature Review: Quality Attribute Variability in Software Product Lines}},
  booktitle     = {SPLC},
  year          = {2012},
  series        = {SPLC '12},
  pages         = {41--45},
  address       = {New York, NY, USA},
  publisher     = {ACM},
  __markedentry = {[mac:3]},
  doi           = {10.1145/2362536.2362546},
  isbn          = {978-1-4503-1094-9},
  keywords      = {quality attribute, systematic literature review, variability, rank2},
  url           = {http://0-doi.acm.org.fama.us.es/10.1145/2362536.2362546},
}

@InProceedings{Marimuthu:2017:SSS:3106195.3106212,
  author        = {Marimuthu, C and Chandrasekaran, K},
  title         = {{Systematic Studies in Software Product Lines: A Tertiary Study}},
  booktitle     = {International Systems and Software Product Line Conference},
  year          = {2017},
  series        = {SPLC '17},
  pages         = {143--152},
  address       = {New York, NY, USA},
  publisher     = {ACM},
  __markedentry = {[mac:3]},
  doi           = {10.1145/3106195.3106212},
  isbn          = {978-1-4503-5221-5},
  keywords      = {software product line,systematic review,tertiary study},
  url           = {http://doi.acm.org/10.1145/3106195.3106212},
}

@InProceedings{Kastner2007,
  author    = {Kastner, Christian and Apel, Sven and Batory, Don},
  title     = {{A Case Study Implementing Features Using AspectJ}},
  booktitle = {SPLC},
  year      = {2007},
  pages     = {223--232},
  month     = {sep},
  publisher = {IEEE},
  doi       = {10.1109/SPLINE.2007.12},
  isbn      = {0-7695-2888-0},
  url       = {http://ieeexplore.ieee.org/document/4339271/},
}

@InProceedings{Nagamine:2016:CSA:2934466.2934489,
  author        = {Nagamine, Motoi and Nakajima, Tsuyoshi and Kuno, Noriyoshi},
  title         = {{A Case Study of Applying Software Product Line Engineering to the Air Conditioner Domain}},
  booktitle     = {International Systems and Software Product Line Conference},
  year          = {2016},
  series        = {SPLC '16},
  pages         = {220--226},
  address       = {New York, NY, USA},
  publisher     = {ACM},
  doi           = {10.1145/2934466.2934489},
  isbn          = {978-1-4503-4050-2},
  keywords      = {Case Study,SPL,case study,embedded system,software product line},
  mendeley-tags = {Case Study},
  url           = {http://0-doi.acm.org.fama.us.es/10.1145/2934466.2934489},
}

@InProceedings{Nohrer:2012:CST:2362536.2362543,
  author    = {N{\"{o}}hrer, Alexander and Biere, Armin and Egyed, Alexander},
  title     = {{A Comparison of Strategies for Tolerating Inconsistencies During Decision-making}},
  booktitle = {SPLC},
  year      = {2012},
  series    = {SPLC '12},
  pages     = {11--20},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/2362536.2362543},
  isbn      = {978-1-4503-1094-9},
  keywords  = {formal reasoning,inconsistencies,user guidance},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/2362536.2362543},
}

@InProceedings{Noir:2016:DPE:2934466.2946045,
  author    = {Noir, J{\'{e}}rome Le and Madel{\'{e}}nat, S{\'{e}}bastien and Gailliard, Gr{\'{e}}gory and Labreuche, Christophe and Acher, Mathieu and Barais, Olivier and Constant, Olivier},
  title     = {{A Decision-making Process for Exploring Architectural Variants in Systems Engineering}},
  booktitle = {International Systems and Software Product Line Conference},
  year      = {2016},
  series    = {SPLC '16},
  pages     = {277--286},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/2934466.2946045},
  isbn      = {978-1-4503-4050-2},
  keywords  = {architecture,decision-making,design exploration,model-driven engineering,multi-criteria decision analysis,systems engineering},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/2934466.2946045},
}

@InProceedings{Kapfhammer:2003:FTA:940071.940086,
  author    = {Kapfhammer, Gregory M and Soffa, Mary Lou},
  title     = {{A Family of Test Adequacy Criteria for Database-driven Applications}},
  booktitle = {SIGSOFT International Symposium on Foundations of Software Engineering},
  year      = {2003},
  series    = {ESEC/FSE-11},
  pages     = {98--107},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/940071.940086},
  isbn      = {1-58113-743-5},
  keywords  = {database-driven applications,test adequacy criteria},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/940071.940086},
}

@InProceedings{Duboc:2007:FCA:1287624.1287679,
  author    = {Duboc, Leticia and Rosenblum, David and Wicks, Tony},
  title     = {{A Framework for Characterization and Analysis of Software System Scalability}},
  booktitle = {SIGSOFT International Symposium on Foundations of Software Engineering},
  year      = {2007},
  series    = {ESEC-FSE '07},
  pages     = {375--384},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/1287624.1287679},
  isbn      = {978-1-59593-811-4},
  keywords  = {design,microeconomics,requirements,scalability},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/1287624.1287679},
}

@InProceedings{Zhang:2016:MMP:2934466.2934469,
  author    = {Zhang, Yi and Guo, Jianmei and Blais, Eric and Czarnecki, Krzysztof and Yu, Huiqun},
  title     = {{A Mathematical Model of Performance-relevant Feature Interactions}},
  booktitle = {International Systems and Software Product Line Conference},
  year      = {2016},
  series    = {SPLC '16},
  pages     = {25--34},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/2934466.2934469},
  isbn      = {978-1-4503-4050-2},
  keywords  = {boolean functions,feature interactions,fourier transform,performance},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/2934466.2934469},
}

@InProceedings{Dillon:2014:MAP:2648511.2648550,
  author    = {Dillon, Michael and Rivera, Jorge and Darbin, Rowland},
  title     = {{A Methodical Approach to Product Line Adoption}},
  booktitle = {SPLC},
  year      = {2014},
  series    = {SPLC '14},
  pages     = {340--349},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/2648511.2648550},
  isbn      = {978-1-4503-2740-4},
  keywords  = {bill-of-features,feature constraints hierarchical product lines,feature modeling,feature profiles,product baselines,product configurator,product line adoption,product line engineering,product line governance,product portfolio,second generation product line engineering,software product lines,variation points},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/2648511.2648550},
}

@InProceedings{Belarbi:2018:MFE:3236405.3236426,
  author    = {Belarbi, Maouaheb},
  title     = {{A Methodological Framework to Enable the Generation of Code from DSML in SPL}},
  booktitle = {International Systems and Software Product Line Conference},
  year      = {2018},
  series    = {SPLC '18},
  pages     = {64--71},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/3236405.3236426},
  isbn      = {978-1-4503-5945-0},
  keywords  = {DSML,SPL,methodology,software factory,variability},
  url       = {http://doi.acm.org/10.1145/3236405.3236426},
}

@InProceedings{Hulkko:2005:MCS:1062455.1062545,
  author    = {Hulkko, Hanna and Abrahamsson, Pekka},
  title     = {{A Multiple Case Study on the Impact of Pair Programming on Product Quality}},
  booktitle = {International Conference on Software Engineering},
  year      = {2005},
  series    = {ICSE '05},
  pages     = {495--504},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/1062455.1062545},
  isbn      = {1-58113-963-2},
  keywords  = {agile software development, empirical software engineering, extreme programming, productivity, software quality},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/1062455.1062545},
}

@InProceedings{Rabiser:2016:PAM:2934466.2934487,
  author    = {Rabiser, Daniela and Gr{\"{u}}nbacher, Paul and Pr{\"{a}}hofer, Herbert and Angerer, Florian},
  title     = {{A Prototype-based Approach for Managing Clones in Clone-and-own Product Lines}},
  booktitle = {International Systems and Software Product Line Conference},
  year      = {2016},
  series    = {SPLC '16},
  pages     = {35--44},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/2934466.2934487},
  isbn      = {978-1-4503-4050-2},
  keywords  = {cloning,co-evolution,feature modeling,industrial systems},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/2934466.2934487},
}

@InProceedings{Dieumegard:2014:SPL:2648511.2648534,
  author    = {Dieumegard, Arnaud and Toom, Andres and Pantel, Marc},
  title     = {{A Software Product Line Approach for Semantic Specification of Block Libraries in Dataflow Languages}},
  booktitle = {SPLC},
  year      = {2014},
  series    = {SPLC '14},
  pages     = {217--226},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/2648511.2648534},
  isbn      = {978-1-4503-2740-4},
  keywords  = {Why3,Xcos,automated code generation,feature modelling,formal specification,model driven engineering,scicos,simulink,software qualification},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/2648511.2648534},
}

@InProceedings{Boehm2004,
  author    = {Boehm, B. and Brown, A.W. and Madachy, R. and {Ye Yang}},
  title     = {{A software product line life cycle cost estimation model}},
  booktitle = {International Symposium on Empirical Software Engineering, 2004. ISESE '04.},
  year      = {2004},
  pages     = {156--164},
  publisher = {IEEE},
  doi       = {10.1109/ISESE.2004.1334903},
  isbn      = {0-7695-2165-7},
  url       = {http://ieeexplore.ieee.org/document/1334903/},
}

@InProceedings{Abotsi:2011:SPL:2019136.2019171,
  author    = {Abotsi, Komi S and Kurniadi, S Tonny and Alsawalqah, Hamad I and Lee, Danhyung},
  title     = {{A Software Product Line-based Self-healing Strategy for Web-based Applications}},
  booktitle = {SPLC},
  year      = {2011},
  series    = {SPLC '11},
  pages     = {31:1----31:8},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/2019136.2019171},
  isbn      = {978-1-4503-0789-5},
  keywords  = {context-based adaptation,knowledge modeling,product line engineering,self-healing},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/2019136.2019171},
}

@InProceedings{Rabiser:2018:SCI:3233027.3233028,
  author    = {Rabiser, Rick and Schmid, Klaus and Becker, Martin and Botterweck, Goetz and Galster, Matthias and Groher, Iris and Weyns, Danny},
  title     = {{A Study and Comparison of Industrial vs. Academic Software Product Line Research Published at SPLC}},
  booktitle = {International Systems and Software Product Line Conference},
  year      = {2018},
  series    = {SPLC '18},
  pages     = {14--24},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/3233027.3233028},
  isbn      = {978-1-4503-6464-5},
  keywords  = {SPLC,academia,industry,software product lines},
  url       = {http://doi.acm.org/10.1145/3233027.3233028},
}

@InProceedings{Lee:2012:SSP:2362536.2362545,
  author    = {Lee, Jihyun and Kang, Sungwon and Lee, Danhyung},
  title     = {{A Survey on Software Product Line Testing}},
  booktitle = {SPLC},
  year      = {2012},
  series    = {SPLC '12},
  pages     = {31--40},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/2362536.2362545},
  isbn      = {978-1-4503-1094-9},
  keywords  = {software product line engineering,software product line testing,software testing},
  url       = {http://doi.acm.org/10.1145/2362536.2362545},
}

@InProceedings{Villela:2014:SSV:2648511.2648527,
  author    = {Villela, Karina and Silva, Adeline and Vale, Tassio and de Almeida, Eduardo Santana},
  title     = {{A Survey on Software Variability Management Approaches}},
  booktitle = {SPLC},
  year      = {2014},
  series    = {SPLC '14},
  pages     = {147--156},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/2648511.2648527},
  isbn      = {978-1-4503-2740-4},
  keywords  = {product line,state-of-the-practice,survey,variability,variability management},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/2648511.2648527},
}

@InProceedings{Helvensteijn:2012:ADM:2364412.2364449,
  author    = {Helvensteijn, Michiel},
  title     = {{Abstract Delta Modeling: My Research Plan}},
  booktitle = {SPLC},
  year      = {2012},
  series    = {SPLC '12},
  pages     = {217--224},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/2364412.2364449},
  isbn      = {978-1-4503-1095-6},
  keywords  = {PhD thesis,delta modeling,development workflow,dynamic product lines,modal logic,product lines,type systems},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/2364412.2364449},
}

@InProceedings{Munoz:2017:AEE:3109729.3109744,
  author    = {Munoz, Daniel-Jesus},
  title     = {{Achieving Energy Efficiency Using a Software Product Line Approach}},
  booktitle = {International Systems and Software Product Line Conference},
  year      = {2017},
  series    = {SPLC '17},
  pages     = {131--138},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/3109729.3109744},
  isbn      = {978-1-4503-5119-5},
  keywords  = {Clafer,Energy Efficiency,Metrics,Optimisation,Repository,Software Product Line,Variability},
  url       = {http://doi.acm.org/10.1145/3109729.3109744},
}

@InProceedings{Derakhshanmanesh:2012:AFR:2364412.2364414,
  author    = {Derakhshanmanesh, Mahdi and Fox, Joachim and Ebert, J{\"{u}}rgen},
  title     = {{Adopting Feature-centric Reuse of Requirements Assets: An Industrial Experience Report}},
  booktitle = {SPLC},
  year      = {2012},
  series    = {SPLC '12},
  pages     = {2--9},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/2364412.2364414},
  isbn      = {978-1-4503-1095-6},
  keywords  = {features,requirements,reuse,software product lines},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/2364412.2364414},
}

@Conference{Liebig2010105,
  author    = {Liebig, J and Apel, S and Lengauer, C and K{\"{a}}stner, C and Schulze, M},
  title     = {{An analysis of the variability in forty preprocessor-based software product lines}},
  booktitle = {International Conference on Software Engineering},
  year      = {2010},
  volume    = {1},
  pages     = {105--114},
  abstract  = {Over 30 years ago, the preprocessor cpp was developed to extend the programming language C by lightweight metaprogramming capabilities. Despite its error-proneness and low abstraction level, the preprocessor is still widely used in present-day software projects to implement variable software. However, not much is known about how cpp is employed to implement variability. To address this issue, we have analyzed forty open-source software projects written in C. Specifically, we answer the following questions: How does program size influence variability? How complex are extensions made via cpp's variability mechanisms? At which level of granularity are extensions applied? Which types of extension occur? These questions revive earlier discussions on program comprehension and refactoring in the context of the preprocessor. To provide answers, we introduce several metrics measuring the variability, complexity, granularity, and types of extension applied by preprocessor directives. Based on the collected data, we suggest alternative implementation techniques. Our data set is a rich source for rethinking language design and tool support. {\textcopyright} 2010 ACM.},
  annote    = {cited By 117},
  doi       = {10.1145/1806799.1806819},
  file      = {:Users/mac/Library/Application Support/Mendeley Desktop/Downloaded/Liebig et al. - 2010 - An analysis of the variability in forty preprocessor-based software product lines(2).pdf:pdf},
  keywords  = {Abstraction level; Data sets; Empirical studies; I,Linguistics; Software engineering,Program processors},
  url       = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954730541{\&}doi=10.1145{\%}2F1806799.1806819{\&}partnerID=40{\&}md5=8f67119512582057938b5763abfea2e3},
}

@InProceedings{Grechanik:2010:EIL:1852786.1852801,
  author    = {Grechanik, Mark and McMillan, Collin and DeFerrari, Luca and Comi, Marco and Crespi, Stefano and Poshyvanyk, Denys and Fu, Chen and Xie, Qing and Ghezzi, Carlo},
  title     = {{An Empirical Investigation into a Large-scale Java Open Source Code Repository}},
  booktitle = {International Symposium on Empirical Software Engineering and Measurement},
  year      = {2010},
  series    = {ESEM '10},
  pages     = {11:1----11:10},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/1852786.1852801},
  isbn      = {978-1-4503-0039-1},
  keywords  = {empirical study,large-scale software,mining software repositories,open source,patterns,practice,software repository},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/1852786.1852801},
}

@InProceedings{El-Sharkawy:2017:ESC:3106195.3106208,
  author    = {El-Sharkawy, Sascha and Krafczyk, Adam and Schmid, Klaus},
  title     = {{An Empirical Study of Configuration Mismatches in Linux}},
  booktitle = {International Systems and Software Product Line Conference},
  year      = {2017},
  series    = {SPLC '17},
  pages     = {19--28},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/3106195.3106208},
  isbn      = {978-1-4503-5221-5},
  keywords  = {Kconfig,Linux,Software product lines,configuration mismatches,empirical software engineering,static analysis,variability modeling},
  url       = {http://doi.acm.org/10.1145/3106195.3106208},
}

@InProceedings{Meinicke:2014:OAT:2647908.2655972,
  author    = {Meinicke, Jens and Th{\"{u}}m, Thomas and Schr{\"{o}}ter, Reimar and Benduhn, Fabian and Saake, Gunter},
  title     = {{An Overview on Analysis Tools for Software Product Lines}},
  booktitle = {SPLC},
  year      = {2014},
  series    = {SPLC '14},
  pages     = {94--101},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/2647908.2655972},
  isbn      = {978-1-4503-2739-8},
  keywords  = {code metrics,model checking,non-functional properties,sampling,software product lines,static analysis,testing,theorem proving,tool support,type checking},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/2647908.2655972},
}

@InProceedings{Kruger:2018:ACS:3233027.3236403,
  author    = {Kr{\"{u}}ger, Jacob and Fenske, Wolfram and Th{\"{u}}m, Thomas and Aporius, Dirk and Saake, Gunter and Leich, Thomas},
  title     = {{Apo-games: A Case Study for Reverse Engineering Variability from Cloned Java Variants}},
  booktitle = {International Systems and Software Product Line Conference},
  year      = {2018},
  series    = {SPLC '18},
  pages     = {251--256},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/3233027.3236403},
  isbn      = {978-1-4503-6464-5},
  keywords  = {case study,data set,extractive approach,feature location,reverse engineering,software-product-line engineering},
  url       = {http://doi.acm.org/10.1145/3233027.3236403},
}

@InProceedings{Kuhn:2016:AOC:2934466.2934470,
  author    = {K{\"{u}}hn, Thomas and Cazzola, Walter},
  title     = {{Apples and Oranges: Comparing Top-down and Bottom-up Language Product Lines}},
  booktitle = {International Systems and Software Product Line Conference},
  year      = {2016},
  series    = {SPLC '16},
  pages     = {50--59},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/2934466.2934470},
  isbn      = {978-1-4503-4050-2},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/2934466.2934470},
}

@InProceedings{terBeek:2015:APL:2791060.2791100,
  author    = {ter Beek, Maurice H and Fantechi, Alessandro and Gnesi, Stefania},
  title     = {{Applying the Product Lines Paradigm to the Quantitative Analysis of Collective Adaptive Systems}},
  booktitle = {SPLC},
  year      = {2015},
  series    = {SPLC '15},
  pages     = {321--326},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/2791060.2791100},
  isbn      = {978-1-4503-3613-0},
  keywords  = {ClaferMOO,collective adaptive systems,multi-objective optimization,quantitative analysis,quantitative modeling,variability analysis},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/2791060.2791100},
}

@InProceedings{Schultis:2014:ACI:2635868.2635876,
  author        = {Schultis, Klaus-Benedikt and Elsner, Christoph and Lohmann, Daniel},
  title         = {{Architecture Challenges for Internal Software Ecosystems: A Large-scale Industry Case Study}},
  booktitle     = {SIGSOFT International Symposium on Foundations of Software Engineering},
  year          = {2014},
  series        = {FSE 2014},
  pages         = {542--552},
  address       = {New York, NY, USA},
  publisher     = {ACM},
  doi           = {10.1145/2635868.2635876},
  isbn          = {978-1-4503-3056-5},
  keywords      = {Software ecosystem,case study,collaboration,decentralized software engineering,software architecture,software product line},
  mendeley-tags = {case study},
  url           = {http://0-doi.acm.org.fama.us.es/10.1145/2635868.2635876},
}

@InProceedings{Noda2008,
  author    = {Noda, Natsuko and Kishi, Tomoji},
  title     = {{Aspect-Oriented Modeling for Variability Management}},
  booktitle = {SPLC},
  year      = {2008},
  pages     = {213--222},
  month     = {sep},
  publisher = {IEEE},
  doi       = {10.1109/SPLC.2008.44},
  isbn      = {978-0-7695-3303-2},
  url       = {http://ieeexplore.ieee.org/document/4626855/},
}

@InProceedings{Filho:2015:APL:2791060.2791099,
  author    = {Filho, Jo{\~{a}}o Bosco Ferreira and Allier, Simon and Barais, Olivier and Acher, Mathieu and Baudry, Benoit},
  title     = {{Assessing Product Line Derivation Operators Applied to Java Source Code: An Empirical Study}},
  booktitle = {SPLC},
  year      = {2015},
  series    = {SPLC '15},
  pages     = {36--45},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/2791060.2791099},
  isbn      = {978-1-4503-3613-0},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/2791060.2791099},
}

@InProceedings{White2008,
  author    = {White, J. and Schmidt, D.C. and Benavides, D. and Trinidad, P. and Ruiz?Cort?s, A.},
  title     = {{Automated Diagnosis of Product-Line Configuration Errors in Feature Models}},
  booktitle = {SPLC},
  year      = {2008},
  pages     = {225--234},
  month     = {sep},
  publisher = {IEEE},
  doi       = {10.1109/SPLC.2008.16},
  isbn      = {978-0-7695-3303-2},
  url       = {http://ieeexplore.ieee.org/document/4626856/},
}

@InProceedings{Santos2008,
  author    = {Santos, Andr{\'{e}} L. and Koskimies, Kai and Lopes, Ant{\'{o}}nia},
  title     = {{Automated Domain-Specific Modeling Languages for Generating Framework-Based Applications}},
  booktitle = {SPLC},
  year      = {2008},
  pages     = {149--158},
  month     = {sep},
  publisher = {IEEE},
  doi       = {10.1109/SPLC.2008.17},
  file      = {:Users/mac/Library/Application Support/Mendeley Desktop/Downloaded/Santos, Koskimies, Lopes - 2008 - Automated Domain-Specific Modeling Languages for Generating Framework-Based Applications.pdf:pdf},
  isbn      = {978-0-7695-3303-2},
  url       = {http://ieeexplore.ieee.org/document/4626849/},
}

@InProceedings{Soltani:2012:APF:2362536.2362548,
  author    = {Soltani, Samaneh and Asadi, Mohsen and Ga{\v{s}}evi{\'{c}}, Dragan and Hatala, Marek and Bagheri, Ebrahim},
  title     = {{Automated Planning for Feature Model Configuration Based on Functional and Non-functional Requirements}},
  booktitle = {SPLC},
  year      = {2012},
  series    = {SPLC '12},
  pages     = {56--65},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/2362536.2362548},
  isbn      = {978-1-4503-1094-9},
  keywords  = {artificial intelligence,configuration,feature model,planning techniques,software product line engineering},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/2362536.2362548},
}

@InProceedings{Arcaini:2017:ARV:3106195.3106206,
  author    = {Arcaini, Paolo and Gargantini, Angelo and Vavassori, Paolo},
  title     = {{Automated Repairing of Variability Models}},
  booktitle = {International Systems and Software Product Line Conference},
  year      = {2017},
  series    = {SPLC '17},
  pages     = {9--18},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/3106195.3106206},
  isbn      = {978-1-4503-5221-5},
  url       = {http://doi.acm.org/10.1145/3106195.3106206},
}

@InProceedings{Patel:2015:ATS:2791060.2791072,
  author    = {Patel, Sachin and Shah, Vipul},
  title     = {{Automated Testing of Software-as-a-service Configurations Using a Variability Language}},
  booktitle = {SPLC},
  year      = {2015},
  series    = {SPLC '15},
  pages     = {253--262},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/2791060.2791072},
  isbn      = {978-1-4503-3613-0},
  keywords  = {enterprise software testing,model based testing,software-as-a-service,test automation,variability specification},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/2791060.2791072},
}

@InProceedings{Tawhid2011a,
  author    = {Tawhid, Rasha and Petriu, Dorina C.},
  title     = {{Automatic Derivation of a Product Performance Model from a Software Product Line Model}},
  booktitle = {SPLC},
  year      = {2011},
  pages     = {80--89},
  month     = {aug},
  publisher = {IEEE},
  doi       = {10.1109/SPLC.2011.27},
  isbn      = {978-1-4577-1029-2},
  url       = {http://ieeexplore.ieee.org/document/6030049/},
}

@InProceedings{Scholz:2011:ADF:2019136.2019144,
  author    = {Scholz, Wolfgang and Th{\"{u}}m, Thomas and Apel, Sven and Lengauer, Christian},
  title     = {{Automatic Detection of Feature Interactions Using the Java Modeling Language: An Experience Report}},
  booktitle = {SPLC},
  year      = {2011},
  series    = {SPLC '11},
  pages     = {7:1----7:8},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/2019136.2019144},
  isbn      = {978-1-4503-0789-5},
  keywords  = {FeatureHouse,JML,feature interaction,software product lines},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/2019136.2019144},
}

@InProceedings{Rumpe:2015:BCS:2791060.2791077,
  author    = {Rumpe, Bernhard and Schulze, Christoph and von Wenckstern, Michael and Ringert, Jan Oliver and Manhart, Peter},
  title     = {{Behavioral Compatibility of Simulink Models for Product Line Maintenance and Evolution}},
  booktitle = {SPLC},
  year      = {2015},
  series    = {SPLC '15},
  pages     = {141--150},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/2791060.2791077},
  isbn      = {978-1-4503-3613-0},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/2791060.2791077},
}

@InProceedings{Martinez:2015:BAS:2791060.2791086,
  author    = {Martinez, Jabier and Ziadi, Tewfik and Bissyand{\'{e}}, Tegawend{\'{e}} F and Klein, Jacques and {Le Traon}, Yves},
  title     = {{Bottom-up Adoption of Software Product Lines: A Generic and Extensible Approach}},
  booktitle = {SPLC},
  year      = {2015},
  series    = {SPLC '15},
  pages     = {101--110},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/2791060.2791086},
  isbn      = {978-1-4503-3613-0},
  keywords  = {mining existing assets,reverse engineering,software product line engineering},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/2791060.2791086},
}

@InProceedings{Pohjalainen2011,
  author    = {Pohjalainen, Pietu},
  title     = {{Bottom-up Modeling for a Software Product Line: An Experience Report on Agile Modeling of Governmental Mobile Networks}},
  booktitle = {SPLC},
  year      = {2011},
  pages     = {323--332},
  month     = {aug},
  publisher = {IEEE},
  doi       = {10.1109/SPLC.2011.48},
  isbn      = {978-1-4577-1029-2},
  url       = {http://ieeexplore.ieee.org/document/6030075/},
}

@InProceedings{Dumitrescu:2013:BGP:2491627.2491655,
  author    = {Dumitrescu, Cosmin and Mazo, Raul and Salinesi, Camille and Dauron, Alain},
  title     = {{Bridging the Gap Between Product Lines and Systems Engineering: An Experience in Variability Management for Automotive Model Based Systems Engineering}},
  booktitle = {SPLC},
  year      = {2013},
  series    = {SPLC '13},
  pages     = {254--263},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/2491627.2491655},
  isbn      = {978-1-4503-1968-3},
  keywords  = {systems engineering,variability management},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/2491627.2491655},
}

@InProceedings{Kozuka:2011:BPL:2019136.2019152,
  author    = {Kozuka, Nobuaki and Ishida, Yuzo},
  title     = {{Building a Product Line Architecture for Variant-rich Enterprise Applications Using a Data-oriented Approach}},
  booktitle = {SPLC},
  year      = {2011},
  series    = {SPLC '11},
  pages     = {14:1----14:6},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/2019136.2019152},
  isbn      = {978-1-4503-0789-5},
  keywords  = {core asset development,data intensiveness,data oriented approach,enterprise applications,product line architecture,quality attributes,relational database management system},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/2019136.2019152},
}

@InProceedings{Kim2007,
  author    = {Kim, Kangtae and Kim, Hyungrok and Kim, Woomok},
  title     = {{Building Software Product Line from the Legacy Systems "Experience in the Digital Audio and Video Domain"}},
  booktitle = {SPLC},
  year      = {2007},
  pages     = {171--180},
  month     = {sep},
  publisher = {IEEE},
  doi       = {10.1109/SPLINE.2007.27},
  isbn      = {0-7695-2888-0},
  url       = {http://ieeexplore.ieee.org/document/4339266/},
}

@InProceedings{Juergens:2010:CDS:1810295.1810308,
  author    = {Juergens, Elmar and Deissenboeck, Florian and Feilkas, Martin and Hummel, Benjamin and Schaetz, Bernhard and Wagner, Stefan and Domann, Christoph and Streit, Jonathan},
  title     = {{Can Clone Detection Support Quality Assessments of Requirements Specifications?}},
  booktitle = {International Conference on Software Engineering},
  year      = {2010},
  series    = {ICSE '10},
  pages     = {79--88},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/1810295.1810308},
  isbn      = {978-1-60558-719-6},
  keywords  = {clone detection,redundancy,requirements specification},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/1810295.1810308},
}

@InProceedings{Pinzger:2008:DNP:1453101.1453105,
  author    = {Pinzger, Martin and Nagappan, Nachiappan and Murphy, Brendan},
  title     = {{Can Developer-module Networks Predict Failures?}},
  booktitle = {SIGSOFT International Symposium on Foundations of Software Engineering},
  year      = {2008},
  series    = {SIGSOFT '08/FSE-16},
  pages     = {2--12},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/1453101.1453105},
  isbn      = {978-1-59593-995-1},
  keywords  = {developer contribution network,failure prediction,network centrality measures,social network analysis},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/1453101.1453105},
}

@InProceedings{Kato:2013:CSA:2491627.2491636,
  author    = {Kato, Tadahisa and Kawakami, Masumi and Myojin, Tomoyuki and Ogawa, Hideto and Hirono, Koji and Hasegawa, Takashi},
  title     = {{Case Study of Applying SPLE to Development of Network Switch Products}},
  booktitle = {SPLC},
  year      = {2013},
  series    = {SPLC '13},
  pages     = {198--207},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/2491627.2491636},
  isbn      = {978-1-4503-1968-3},
  keywords  = {document integration,software integration,software maintenance,software reuse,test automation},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/2491627.2491636},
}

@InProceedings{Lind:2010:CRS:1852786.1852821,
  author    = {Lind, Kenneth and Heldal, Rogardt},
  title     = {{Categorization of Real-time Software Components for Code Size Estimation}},
  booktitle = {International Symposium on Empirical Software Engineering and Measurement},
  year      = {2010},
  series    = {ESEM '10},
  pages     = {26:1----26:10},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/1852786.1852821},
  isbn      = {978-1-4503-0039-1},
  keywords  = {COSMIC function points,UML components,categorization,functional size measurement,software code size,system architecture},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/1852786.1852821},
}

@InProceedings{Seidl:2017:CSO:3106195.3106203,
  author    = {Seidl, Christoph and Berger, Thorsten and Elsner, Christoph and Schultis, Klaus-Benedikt},
  title     = {{Challenges and Solutions for Opening Small and Medium-Scale Industrial Software Platforms}},
  booktitle = {International Systems and Software Product Line Conference},
  year      = {2017},
  series    = {SPLC '17},
  pages     = {153--162},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/3106195.3106203},
  isbn      = {978-1-4503-5221-5},
  url       = {http://doi.acm.org/10.1145/3106195.3106203},
}

@InProceedings{Li:2011:CDP:1985793.1985894,
  author    = {Li, Paul Luo and Kivett, Ryan and Zhan, Zhiyuan and Jeon, Sung-eok and Nagappan, Nachiappan and Murphy, Brendan and Ko, Andrew J},
  title     = {{Characterizing the Differences Between Pre- and Post- Release Versions of Software}},
  booktitle = {International Conference on Software Engineering},
  year      = {2011},
  series    = {ICSE '11},
  pages     = {716--725},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/1985793.1985894},
  isbn      = {978-1-4503-0445-0},
  keywords  = {beta,customer experience improvement program,reliability analysis component (rac),usage,windows,windows error reporting (wer)},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/1985793.1985894},
}

@InProceedings{Steff:2012:CRC:2372251.2372261,
  author    = {Steff, Maximilian and Russo, Barbara},
  title     = {{Characterizing the Roles of Classes and Their Fault-proneness Through Change Metrics}},
  booktitle = {International Symposium on Empirical Software Engineering and Measurement},
  year      = {2012},
  series    = {ESEM '12},
  pages     = {59--68},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/2372251.2372261},
  isbn      = {978-1-4503-1056-7},
  keywords  = {fault-proneness,product metrics,software architectures},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/2372251.2372261},
}

@InProceedings{Hamza:2018:CTC:3233027.3233036,
  author    = {Hamza, Mostafa and Walker, Robert J and Elaasar, Maged},
  title     = {{CIAhelper: Towards Change Impact Analysis in Delta-oriented Software Product Lines}},
  booktitle = {International Systems and Software Product Line Conference},
  year      = {2018},
  series    = {SPLC '18},
  pages     = {31--42},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/3233027.3233036},
  isbn      = {978-1-4503-6464-5},
  keywords  = {change impact analysis,code assets,delta-oriented programming,feature model,variability model},
  url       = {http://doi.acm.org/10.1145/3233027.3233036},
}

@InProceedings{Zhang:2012:CVM:2364412.2364428,
  author    = {Zhang, Bo and Becker, Martin},
  title     = {{Code-based Variability Model Extraction for Software Product Line Improvement}},
  booktitle = {SPLC},
  year      = {2012},
  series    = {SPLC '12},
  pages     = {91--98},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/2364412.2364428},
  isbn      = {978-1-4503-1095-6},
  keywords  = {conditional compilation,software product line maintenance,variability model},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/2364412.2364428},
}

@InProceedings{Hellebrand:2014:CVM:2648511.2648542,
  author    = {Hellebrand, Robert and Silva, Adeline and Becker, Martin and Zhang, Bo and Sierszecki, Krzysztof and Savolainen, Juha},
  title     = {{Coevolution of Variability Models and Code: An Industrial Case Study}},
  booktitle = {SPLC},
  year      = {2014},
  series    = {SPLC '14},
  pages     = {274--283},
  address   = {New York, NY, USA},
  publisher = {ACM},
  annote    = {es cao de estudio},
  doi       = {10.1145/2648511.2648542},
  isbn      = {978-1-4503-2740-4},
  keywords  = {coevolution,feature models,metrics,product line evolution},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/2648511.2648542},
}

@InProceedings{Passos:2013:CVM:2491627.2491628,
  author    = {Passos, Leonardo and Guo, Jianmei and Teixeira, Leopoldo and Czarnecki, Krzysztof and W$\backslash$kasowski, Andrzej and Borba, Paulo},
  title     = {{Coevolution of Variability Models and Related Artifacts: A Case Study from the Linux Kernel}},
  booktitle = {SPLC},
  year      = {2013},
  series    = {SPLC '13},
  pages     = {91--100},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/2491627.2491628},
  isbn      = {978-1-4503-1968-3},
  keywords  = {Linux,catalog,evolution,patterns,variability},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/2491627.2491628},
}

@InProceedings{terBeek:2013:CDP:2499777.2500722,
  author    = {ter Beek, Maurice H and Lafuente, Alberto Lluch and Petrocchi, Marinella},
  title     = {{Combining Declarative and Procedural Views in the Specification and Analysis of Product Families}},
  booktitle = {SPLC},
  year      = {2013},
  series    = {SPLC '13 Workshops},
  pages     = {10--17},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/2499777.2500722},
  isbn      = {978-1-4503-2325-3},
  keywords  = {Maude,behavioural analyses,concurrent constraint programming,process algebra,product families,variability},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/2499777.2500722},
}

@InProceedings{Martini:2013:CFS:2491627.2491642,
  author    = {Martini, Antonio and Pareto, Lars and Bosch, Jan},
  title     = {{Communication Factors for Speed and Reuse in Large-scale Agile Software Development}},
  booktitle = {SPLC},
  year      = {2013},
  series    = {SPLC '13},
  pages     = {42--51},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/2491627.2491642},
  isbn      = {978-1-4503-1968-3},
  keywords  = {agile software development,communication,development speed,embedded systems,factors,software process improvement (SPI),software reuse,speed},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/2491627.2491642},
}

@InProceedings{Cardone:2001:CFL:381473.381503,
  author    = {Cardone, Richard and Lin, Calvin},
  title     = {{Comparing Frameworks and Layered Refinement}},
  booktitle = {International Conference on Software Engineering},
  year      = {2001},
  series    = {ICSE '01},
  pages     = {285--294},
  address   = {Washington, DC, USA},
  publisher = {IEEE Computer Society},
  isbn      = {0-7695-1050-7},
  keywords  = {frameworks,layers,mixins,parametric polymorphism},
  url       = {http://0-dl.acm.org.fama.us.es/citation.cfm?id=381473.381503},
}

@InProceedings{Olaechea:2014:CEA:2648511.2648521,
  author    = {Olaechea, Rafael and Rayside, Derek and Guo, Jianmei and Czarnecki, Krzysztof},
  title     = {{Comparison of Exact and Approximate Multi-objective Optimization for Software Product Lines}},
  booktitle = {SPLC},
  year      = {2014},
  series    = {SPLC '14},
  pages     = {92--101},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/2648511.2648521},
  isbn      = {978-1-4503-2740-4},
  keywords  = {multi-objective optimization,software product lines},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/2648511.2648521},
}

@InProceedings{Reinhartz-Berger:2014:COV:2648511.2648516,
  author    = {Reinhartz-Berger, Iris and Figl, Kathrin},
  title     = {{Comprehensibility of Orthogonal Variability Modeling Languages: The Cases of CVL and OVM}},
  booktitle = {SPLC},
  year      = {2014},
  series    = {SPLC '14},
  pages     = {42--51},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/2648511.2648516},
  isbn      = {978-1-4503-2740-4},
  keywords  = {CVL,OVM,empirical study,model comprehension,variability analysis},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/2648511.2648516},
}

@InProceedings{Quinton:2014:CCE:2648511.2648524,
  author    = {Quinton, Cl{\'{e}}ment and Pleuss, Andreas and Berre, Daniel Le and Duchien, Laurence and Botterweck, Goetz},
  title     = {{Consistency Checking for the Evolution of Cardinality-based Feature Models}},
  booktitle = {SPLC},
  year      = {2014},
  series    = {SPLC '14},
  pages     = {122--131},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/2648511.2648524},
  isbn      = {978-1-4503-2740-4},
  keywords  = {cardinality,consistency,edit,feature model},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/2648511.2648524},
}

@InProceedings{Alferez2011a,
  author    = {Alferez, German H. and Pelechano, Vicente},
  title     = {{Context-Aware Autonomous Web Services in Software Product Lines}},
  booktitle = {SPLC},
  year      = {2011},
  pages     = {100--109},
  month     = {aug},
  publisher = {IEEE},
  doi       = {10.1109/SPLC.2011.21},
  file      = {:Users/mac/Library/Application Support/Mendeley Desktop/Downloaded/Alferez, Pelechano - 2011 - Context-Aware Autonomous Web Services in Software Product Lines.pdf:pdf},
  isbn      = {978-1-4577-1029-2},
  url       = {http://ieeexplore.ieee.org/document/6030051/},
}

@InProceedings{Saller:2013:CDM:2499777.2500716,
  author    = {Saller, Karsten and Lochau, Malte and Reimund, Ingo},
  title     = {{Context-aware DSPLs: Model-based Runtime Adaptation for Resource-constrained Systems}},
  booktitle = {SPLC},
  year      = {2013},
  series    = {SPLC '13 Workshops},
  pages     = {106--113},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/2499777.2500716},
  isbn      = {978-1-4503-2325-3},
  keywords  = {DSPL,adaptive systems,contexts,feature models,state space reduction},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/2499777.2500716},
}

@InProceedings{Cordy:2014:CGA:2635868.2635919,
  author    = {Cordy, Maxime and Heymans, Patrick and Legay, Axel and Schobbens, Pierre-Yves and Dawagne, Bruno and Leucker, Martin},
  title     = {{Counterexample Guided Abstraction Refinement of Product-line Behavioural Models}},
  booktitle = {SIGSOFT International Symposium on Foundations of Software Engineering},
  year      = {2014},
  series    = {FSE 2014},
  pages     = {190--201},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/2635868.2635919},
  isbn      = {978-1-4503-3056-5},
  keywords  = {Abstraction,CEGAR,Model Checking,Software Product Lines},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/2635868.2635919},
}

@InProceedings{Zimmermann:2009:CDP:1595696.1595713,
  author    = {Zimmermann, Thomas and Nagappan, Nachiappan and Gall, Harald and Giger, Emanuel and Murphy, Brendan},
  title     = {{Cross-project Defect Prediction: A Large Scale Experiment on Data vs. Domain vs. Process}},
  booktitle = {SIGSOFT International Symposium on Foundations of Software Engineering},
  year      = {2009},
  series    = {ESEC/FSE '09},
  pages     = {91--100},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/1595696.1595713},
  isbn      = {978-1-60558-001-2},
  keywords  = {churn,cross-project,decision trees,defect prediction,logistic regression,prediction quality},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/1595696.1595713},
}

@InProceedings{Lettner:2013:CVM:2499777.2500713,
  author    = {Lettner, Daniela and Petruzelka, Michael and Rabiser, Rick and Angerer, Florian and Pr{\"{a}}hofer, Herbert and Gr{\"{u}}nbacher, Paul},
  title     = {{Custom-developed vs. Model-based Configuration Tools: Experiences from an Industrial Automation Ecosystem}},
  booktitle = {SPLC},
  year      = {2013},
  series    = {SPLC '13 Workshops},
  pages     = {52--58},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/2499777.2500713},
  isbn      = {978-1-4503-2325-3},
  keywords  = {configuration,model-based product lines,software ecosystem},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/2499777.2500713},
}

@InProceedings{Domis:2014:CDA:2648511.2648547,
  author    = {Domis, Dominik and Sehestedt, Stephan and Gamer, Thomas and Aleksy, Markus and Koziolek, Heiko},
  title     = {{Customizing Domain Analysis for Assessing the Reuse Potential of Industrial Software Systems: Experience Report}},
  booktitle = {SPLC},
  year      = {2014},
  series    = {SPLC '14},
  pages     = {310--319},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/2648511.2648547},
  isbn      = {978-1-4503-2740-4},
  keywords  = {domain analysis,software product lines,software reuse},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/2648511.2648547},
}

@InProceedings{Yue:2015:CSP:2791060.2791067,
  author    = {Yue, Tao and Ali, Shaukat and Selic, Bran},
  title     = {{Cyber-physical System Product Line Engineering: Comprehensive Domain Analysis and Experience Report}},
  booktitle = {SPLC},
  year      = {2015},
  series    = {SPLC '15},
  pages     = {338--347},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/2791060.2791067},
  isbn      = {978-1-4503-3613-0},
  keywords  = {cyber physical system (CPS),domain analysis,model based system engineering,product line engineering (PLE),requirements engineering},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/2791060.2791067},
}

@InProceedings{Mei:2009:DFT:1595696.1595720,
  author    = {Mei, Lijun and Chan, W K and Tse, T H},
  title     = {{Data Flow Testing of Service Choreography}},
  booktitle = {SIGSOFT International Symposium on Foundations of Software Engineering},
  year      = {2009},
  series    = {ESEC/FSE '09},
  pages     = {151--160},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/1595696.1595720},
  isbn      = {978-1-60558-001-2},
  keywords  = {choreography,data flow testing,orchestration,service composition,software testing,web services},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/1595696.1595720},
}

@InProceedings{Mei:2008:DFT:1368088.1368139,
  author    = {Mei, Lijun and Chan, W K and Tse, T H},
  title     = {{Data Flow Testing of Service-oriented Workflow Applications}},
  booktitle = {International Conference on Software Engineering},
  year      = {2008},
  series    = {ICSE '08},
  pages     = {371--380},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/1368088.1368139},
  isbn      = {978-1-60558-079-1},
  keywords  = {rewriting rules,service-orientation,soa,testing,workflow testing,ws-bpel,xml,xml document model,xpath},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/1368088.1368139},
}

@InProceedings{Kim:2011:DND:1985793.1985859,
  author    = {Kim, Sunghun and Zhang, Hongyu and Wu, Rongxin and Gong, Liang},
  title     = {{Dealing with Noise in Defect Prediction}},
  booktitle = {International Conference on Software Engineering},
  year      = {2011},
  series    = {ICSE '11},
  pages     = {481--490},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/1985793.1985859},
  isbn      = {978-1-4503-0445-0},
  keywords  = {buggy changes,buggy files,data quality,defect prediction,noise resistance},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/1985793.1985859},
}

@InProceedings{Caneill:2014:DLH:2652524.2652528,
  author    = {Caneill, Matthieu and Zacchiroli, Stefano},
  title     = {{Debsources: Live and Historical Views on Macro-level Software Evolution}},
  booktitle = {International Symposium on Empirical Software Engineering and Measurement},
  year      = {2014},
  series    = {ESEM '14},
  pages     = {28:1----28:10},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/2652524.2652528},
  isbn      = {978-1-4503-2774-9},
  keywords  = {debian,free software,open source,software evolution,source code},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/2652524.2652528},
}

@InProceedings{Weiss2008,
  author    = {Weiss, David M. and Li, J. Jenny and Slye, H. and Dinh-Trong, T. and Sun, Hongyu},
  title     = {{Decision-Model-Based Code Generation for SPLE}},
  booktitle = {SPLC},
  year      = {2008},
  pages     = {129--138},
  month     = {sep},
  publisher = {IEEE},
  doi       = {10.1109/SPLC.2008.42},
  isbn      = {978-0-7695-3303-2},
  url       = {http://ieeexplore.ieee.org/document/4626847/},
}

@InProceedings{Savolainen:2012:DPM:2362536.2362567,
  author    = {Savolainen, Juha and Mannion, Mike and Kuusela, Juha},
  title     = {{Developing Platforms for Multiple Software Product Lines}},
  booktitle = {SPLC},
  year      = {2012},
  series    = {SPLC '12},
  pages     = {220--228},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/2362536.2362567},
  isbn      = {978-1-4503-1094-9},
  keywords  = {industrial experience,multiple product lines,software reuse},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/2362536.2362567},
}

@InProceedings{Ferreira:2016:IIO:2934466.2934467,
  author    = {Ferreira, Gabriel and Malik, Momin and K{\"{a}}stner, Christian and Pfeffer, J{\"{u}}rgen and Apel, Sven},
  title     = {{Do {\#}Ifdefs Influence the Occurrence of Vulnerabilities? An Empirical Study of the Linux Kernel}},
  booktitle = {International Systems and Software Product Line Conference},
  year      = {2016},
  series    = {SPLC '16},
  pages     = {65--73},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/2934466.2934467},
  isbn      = {978-1-4503-4050-2},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/2934466.2934467},
}

@InProceedings{Hofman:2012:DSF:2362536.2362568,
  author    = {Hofman, Peter and Stenzel, Tobias and Pohley, Thomas and Kircher, Michael and Bermann, Andreas},
  title     = {{Domain Specific Feature Modeling for Software Product Lines}},
  booktitle = {SPLC},
  year      = {2012},
  series    = {SPLC '12},
  pages     = {229--238},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/2362536.2362568},
  isbn      = {978-1-4503-1094-9},
  keywords  = {Scrum,agile,commonality analysis,domain specific feature model,feature dependency diagram,feature modeling,product line,variability analysis},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/2362536.2362568},
}

@InProceedings{Gomaa:2011:DSA:2019136.2019176,
  author    = {Gomaa, Hassan and Hashimoto, Koji},
  title     = {{Dynamic Software Adaptation for Service-oriented Product Lines}},
  booktitle = {SPLC},
  year      = {2011},
  series    = {SPLC '11},
  pages     = {35:1----35:8},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/2019136.2019176},
  isbn      = {978-1-4503-0789-5},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/2019136.2019176},
}

@InProceedings{Ternava:2017:ECC:3106195.3106209,
  author    = {T{\"{e}}rnava, Xhevahire and Collet, Philippe},
  title     = {{Early Consistency Checking Between Specification and Implementation Variabilities}},
  booktitle = {International Systems and Software Product Line Conference},
  year      = {2017},
  series    = {SPLC '17},
  pages     = {29--38},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/3106195.3106209},
  isbn      = {978-1-4503-5221-5},
  url       = {http://doi.acm.org/10.1145/3106195.3106209},
}

@InProceedings{Miranskyy:2014:ETC:2652524.2652586,
  author    = {Miranskyy, Andriy and Caglayan, Bora and Bener, Ayse and Cialini, Enzo},
  title     = {{Effect of Temporal Collaboration Network, Maintenance Activity, and Experience on Defect Exposure}},
  booktitle = {International Symposium on Empirical Software Engineering and Measurement},
  year      = {2014},
  series    = {ESEM '14},
  pages     = {27:1----27:8},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/2652524.2652586},
  isbn      = {978-1-4503-2774-9},
  keywords  = {collaboration network,linear regression model,prediction quality},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/2652524.2652586},
}

@InProceedings{Mockus:2006:EES:1159733.1159767,
  author    = {Mockus, Audris},
  title     = {{Empirical Estimates of Software Availability of Deployed Systems}},
  booktitle = {International Symposium on Empirical Software Engineering},
  year      = {2006},
  series    = {ISESE '06},
  pages     = {222--231},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/1159733.1159767},
  isbn      = {1-59593-218-6},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/1159733.1159767},
}

@InProceedings{Martini:2012:EIS:2362536.2362554,
  author    = {Martini, Antonio and Pareto, Lars and Bosch, Jan},
  title     = {{Enablers and Inhibitors for Speed with Reuse}},
  booktitle = {SPLC},
  year      = {2012},
  series    = {SPLC '12},
  pages     = {116--125},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/2362536.2362554},
  isbn      = {978-1-4503-1094-9},
  keywords  = {agile software development,embedded systems,enablers,inhibitors,software process improvement (SPI),software reuse,speed},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/2362536.2362554},
}

@InProceedings{Chastek2011,
  author    = {Chastek, Gary and Donohoe, Patrick and McGregor, John D. and Muthig, Dirk},
  title     = {{Engineering a Production Method for a Software Product Line}},
  booktitle = {SPLC},
  year      = {2011},
  pages     = {277--286},
  month     = {aug},
  publisher = {IEEE},
  doi       = {10.1109/SPLC.2011.46},
  file      = {:Users/mac/Library/Application Support/Mendeley Desktop/Downloaded/Chastek et al. - 2011 - Engineering a Production Method for a Software Product Line.pdf:pdf},
  isbn      = {978-1-4577-1029-2},
  url       = {http://ieeexplore.ieee.org/document/6030070/},
}

@InProceedings{Canfora:2006:EAT:1159733.1159788,
  author    = {Canfora, Gerardo and Cimitile, Aniello and Garcia, Felix and Piattini, Mario and Visaggio, Corrado Aaron},
  title     = {{Evaluating Advantages of Test Driven Development: A Controlled Experiment with Professionals}},
  booktitle = {International Symposium on Empirical Software Engineering},
  year      = {2006},
  series    = {ISESE '06},
  pages     = {364--371},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/1159733.1159788},
  isbn      = {1-59593-218-6},
  keywords  = {empirical software engineering,process quality,test driven development},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/1159733.1159788},
}

@InProceedings{Echeverria:2016:EBS:2961111.2962635,
  author    = {Echeverri{\'{a}}, J and P{\'{e}}rez, Francisca and Abellanas, Andr{\'{e}}s and Panach, Jose Ignacio and Cetina, Carlos and Pastor, O and Echeverr$\backslash$'$\backslash$ia, Jorge and P{\'{e}}rez, Francisca and Abellanas, Andr{\'{e}}s and Panach, Jose Ignacio and Cetina, Carlos and Pastor, {\'{O}}scar},
  title     = {{Evaluating Bug-Fixing in Software Product Lines: An Industrial Case Study}},
  booktitle = {International Symposium on Empirical Software Engineering and Measurement},
  year      = {2016},
  volume    = {08-09-Sept},
  series    = {ESEM '16},
  pages     = {24:1----24:6},
  address   = {New York, NY, USA},
  publisher = {ACM},
  abstract  = {[Background] Bug-fixing could be complex in industrial practice since thousands of products share features in their configuration. Despite the importance and complexity of bug-fixing, there is still a lack of empirical data about the difficulties found in industrial Software Product Lines (SPLs). [Aims] This paper aims to evaluate engineers' performance fixing errors and propagating the fixes to other configured products in the context of an industrial SPL. [Method] We designed and conducted an empirical study to collect data with regard to bug-fixing tasks within the context of a Induction Hob SPL in the BSH group, the largest manufacturer of home appliances in Europe. [Results] We found that effectiveness, efficiency and satisfaction got reached good values. Through interviews we also found difficulties related to unused features, cloning features unintentionally, detecting modified features, and propagating the fix when the source of the bug is the interaction between features. [Conclusions] The identified difficulties are relevant to know how to better apply SPLs in industry in the future. {\textcopyright} 2016 ACM.},
  annote    = {From Duplicate 2 (Evaluating Bug-Fixing in Software Product Lines: An Industrial Case Study - Echeverri{\'{a}}, J; P{\'{e}}rez, F; Abellanas, A; Panach, J I; Cetina, C; Pastor, O) cited By 0},
  doi       = {10.1145/2961111.2962635},
  isbn      = {978-1-4503-4427-2},
  keywords  = {Computer software,Domestic appliances,Empirical studies,Induction hobs,Industrial cas,Software Product Line,Software design,Software e,Usability,Variability Modeling},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/2961111.2962635 https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991579756{\&}doi=10.1145{\%}2F2961111.2962635{\&}partnerID=40{\&}md5=aa98b9304b197c6ff3666e22413e0087},
}

@InProceedings{Echeverria2016,
  author    = {Echeverria, Jorge and P{\'{e}}rez, Francisca and Abellanas, Andr{\'{e}}s and Panach, Jose Ignacio and Cetina, Carlos and Pastor, {\'{O}}scar},
  title     = {{Evaluating Bug-Fixing in Software Product Lines: An Industrial Case Study}},
  booktitle = {International Symposium on Empirical Software Engineering and Measurement},
  year      = {2016},
  series    = {ESEM '16},
  pages     = {24:1----24:6},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/2961111.2962635},
  isbn      = {978-1-4503-4427-2},
  keywords  = {Software Product Line,Usability,Variability Modeling},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/2961111.2962635},
}

@InProceedings{Gustavsson2008,
  author    = {Gustavsson, H?kan and Axelsson, Jakob},
  title     = {{Evaluating Flexibility in Embedded Automotive Product Lines Using Real Options}},
  booktitle = {SPLC},
  year      = {2008},
  pages     = {235--242},
  month     = {sep},
  publisher = {IEEE},
  doi       = {10.1109/SPLC.2008.9},
  file      = {:Users/mac/Library/Application Support/Mendeley Desktop/Downloaded/Gustavsson, Axelsson - 2008 - Evaluating Flexibility in Embedded Automotive Product Lines Using Real Options.pdf:pdf},
  isbn      = {978-0-7695-3303-2},
  url       = {http://ieeexplore.ieee.org/document/4626857/},
}

@InProceedings{Bhat:2006:EET:1159733.1159787,
  author    = {Bhat, Thirumalesh and Nagappan, Nachiappan},
  title     = {{Evaluating the Efficacy of Test-driven Development: Industrial Case Studies}},
  booktitle = {International Symposium on Empirical Software Engineering},
  year      = {2006},
  series    = {ISESE '06},
  pages     = {356--363},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/1159733.1159787},
  isbn      = {1-59593-218-6},
  keywords  = {software quality,test-driven development},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/1159733.1159787},
}

@InProceedings{Barcellos:2010:ESM:1852786.1852822,
  author    = {Barcellos, Monalessa Perini and Rocha, Ana Regina and {de Almeida Falbo}, Ricardo},
  title     = {{Evaluating the Suitability of a Measurement Repository for Statistical Process Control}},
  booktitle = {International Symposium on Empirical Software Engineering and Measurement},
  year      = {2010},
  series    = {ESEM '10},
  pages     = {27:1----27:10},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/1852786.1852822},
  isbn      = {978-1-4503-0039-1},
  keywords  = {high maturity,measurement,statistical process control},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/1852786.1852822},
}

@InProceedings{Smiley:2015:EIA:2791060.2791106,
  author    = {Smiley, Karen and Schmidt, Werner and Dagnino, Aldo},
  title     = {{Evolving an Industrial Analytics Product Line Architecture}},
  booktitle = {SPLC},
  year      = {2015},
  series    = {SPLC '15},
  pages     = {263--272},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/2791060.2791106},
  isbn      = {978-1-4503-3613-0},
  keywords  = {asset health,extensibility,industrial analytics,interoperability,knowledge,performance,reusability,software product line},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/2791060.2791106},
}

@InProceedings{Figueiredo2008,
  author    = {Figueiredo, Eduardo and Cacho, Nelio},
  title     = {{Evolving software product lines with aspects}},
  booktitle = {International Conference on Software Engineering},
  year      = {2008},
  pages     = {261},
  address   = {New York, New York, USA},
  publisher = {ACM Press},
  doi       = {10.1145/1368088.1368124},
  isbn      = {9781605580791},
  url       = {http://portal.acm.org/citation.cfm?doid=1368088.1368124},
}

@InProceedings{Figueiredo:2008:ESP:1368088.1368124,
  author    = {Figueiredo, Eduardo and Cacho, Nelio and Sant'Anna, Claudio and Monteiro, Mario and Kulesza, Uira and Garcia, Alessandro and Soares, S{\'{e}}rgio and Ferrari, Fabiano and Khan, Safoora and {Castor Filho}, Fernando and Dantas, Francisco},
  title     = {{Evolving Software Product Lines with Aspects: An Empirical Study on Design Stability}},
  booktitle = {International Conference on Software Engineering},
  year      = {2008},
  series    = {ICSE '08},
  pages     = {261--270},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/1368088.1368124},
  isbn      = {978-1-60558-079-1},
  keywords  = {aspect-oriented programming,empirical evaluation,software product lines},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/1368088.1368124},
}

@InProceedings{Li:2006:ERI:1134285.1134343,
  author    = {Li, Paul Luo and Herbsleb, James and Shaw, Mary and Robinson, Brian},
  title     = {{Experiences and Results from Initiating Field Defect Prediction and Product Test Prioritization Efforts at ABB Inc.}},
  booktitle = {International Conference on Software Engineering},
  year      = {2006},
  series    = {ICSE '06},
  pages     = {413--422},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/1134285.1134343},
  isbn      = {1-59593-375-1},
  keywords  = {deployment and usage metrics,software and hardware configuration metrics,software reliability modeling,system test prioritization},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/1134285.1134343},
}

@InProceedings{Koziolek:2013:EIS:2491627.2491641,
  author    = {Koziolek, Heiko and Goldschmidt, Thomas and de Gooijer, Thijmen and Domis, Dominik and Sehestedt, Stephan},
  title     = {{Experiences from Identifying Software Reuse Opportunities by Domain Analysis}},
  booktitle = {SPLC},
  year      = {2013},
  series    = {SPLC '13},
  pages     = {208--217},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/2491627.2491641},
  isbn      = {978-1-4503-1968-3},
  keywords  = {business case,domain analysis,software product lines},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/2491627.2491641},
}

@InProceedings{Lillack:2016:ERM:2934466.2962733,
  author    = {Lillack, Max and Berger, Thorsten and Hebig, Regina},
  title     = {{Experiences from Reengineering and Modularizing a Legacy Software Generator with a Projectional Language Workbench}},
  booktitle = {International Systems and Software Product Line Conference},
  year      = {2016},
  series    = {SPLC '16},
  pages     = {346--353},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/2934466.2962733},
  isbn      = {978-1-4503-4050-2},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/2934466.2962733},
}

@InProceedings{Kodama:2014:ECC:2648511.2648540,
  author    = {Kodama, Ryuichiro and Shimabukuro, Jun and Takagi, Yoshimitsu and Koizumi, Shinobu and Tano, Shun'ichi},
  title     = {{Experiences with Commonality Control Procedures to Develop Clinical Instrument System}},
  booktitle = {SPLC},
  year      = {2014},
  series    = {SPLC '14},
  pages     = {254--263},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/2648511.2648540},
  isbn      = {978-1-4503-2740-4},
  keywords  = {architectures,cost estimation,domain analysis,software product lines},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/2648511.2648540},
}

@InProceedings{Alves2008,
  author    = {Alves, Vander and C?mara, Tarc?sio and Alves, Carina},
  title     = {{Experiences with Mobile Games Product Line Development at Meantime}},
  booktitle = {SPLC},
  year      = {2008},
  pages     = {287--296},
  month     = {sep},
  publisher = {IEEE},
  doi       = {10.1109/SPLC.2008.40},
  file      = {:Users/mac/Library/Application Support/Mendeley Desktop/Downloaded/Alves, Cmara, Alves - 2008 - Experiences with Mobile Games Product Line Development at Meantime.pdf:pdf},
  isbn      = {978-0-7695-3303-2},
  url       = {http://ieeexplore.ieee.org/document/4626862/},
}

@InProceedings{Quilty2011,
  author    = {Quilty, Gerard and Cinneide, Mel ?.},
  title     = {{Experiences with Software Product Line Development in Risk Management Software}},
  booktitle = {SPLC},
  year      = {2011},
  pages     = {251--260},
  month     = {aug},
  publisher = {IEEE},
  doi       = {10.1109/SPLC.2011.30},
  file      = {:Users/mac/Library/Application Support/Mendeley Desktop/Downloaded/Quilty, Cinneide - 2011 - Experiences with Software Product Line Development in Risk Management Software.pdf:pdf},
  isbn      = {978-1-4577-1029-2},
  url       = {http://ieeexplore.ieee.org/document/6030067/},
}

@InProceedings{Washizaki:2006:EQE:1134285.1134363,
  author    = {Washizaki, Hironori and Kobayashi, Yasuhide and Watanabe, Hiroyuki and Nakajima, Eiji and Hagiwara, Yuji and Hiranabe, Kenji and Fukuda, Kazuya},
  title     = {{Experiments on Quality Evaluation of Embedded Software in Japan Robot Software Design Contest}},
  booktitle = {International Conference on Software Engineering},
  year      = {2006},
  series    = {ICSE '06},
  pages     = {551--560},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/1134285.1134363},
  isbn      = {1-59593-375-1},
  keywords  = {embedded software development,robot contest,software design,software model,software quality},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/1134285.1134363},
}

@InProceedings{Cavalcante:2012:ESP:2364412.2364442,
  author    = {Cavalcante, Everton and Almeida, Andr{\'{e}} and Batista, Thais and Cacho, N{\'{e}}lio and Lopes, Frederico and Delicato, Flavia C and Sena, Thiago and Pires, Paulo F},
  title     = {{Exploiting Software Product Lines to Develop Cloud Computing Applications}},
  booktitle = {SPLC},
  year      = {2012},
  series    = {SPLC '12},
  pages     = {179--187},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/2364412.2364442},
  isbn      = {978-1-4503-1095-6},
  keywords  = {cloud computing,cloud platforms,health watcher system,services,software product lines},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/2364412.2364442},
}

@InProceedings{Sousa:2016:EFM:2934466.2934475,
  author    = {Sousa, Gustavo and Rudametkin, Walter and Duchien, Laurence},
  title     = {{Extending Feature Models with Relative Cardinalities}},
  booktitle = {International Systems and Software Product Line Conference},
  year      = {2016},
  series    = {SPLC '16},
  pages     = {79--88},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/2934466.2934475},
  isbn      = {978-1-4503-4050-2},
  keywords  = {cardinality,constraints,feature model},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/2934466.2934475},
}

@InProceedings{Sierszecki:2014:EVM:2648511.2648548,
  author    = {Sierszecki, Krzysztof and Steffens, Michaela and Hojrup, Helene H and Savolainen, Juha and Beuche, Danilo},
  title     = {{Extending Variability Management to the Next Level}},
  booktitle = {SPLC},
  year      = {2014},
  series    = {SPLC '14},
  pages     = {320--329},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/2648511.2648548},
  isbn      = {978-1-4503-2740-4},
  keywords  = {industrial experience,product specifications,variability management},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/2648511.2648548},
}

@InProceedings{Sree-Kumar:2018:ESP:3233027.3233029,
  author    = {Sree-Kumar, Anjali and Planas, Elena and Claris{\'{o}}, Robert},
  title     = {{Extracting Software Product Line Feature Models from Natural Language Specifications}},
  booktitle = {International Systems and Software Product Line Conference},
  year      = {2018},
  series    = {SPLC '18},
  pages     = {43--53},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/3233027.3233029},
  isbn      = {978-1-4503-6464-5},
  keywords  = {NLTK,feature model extraction,natural language processing,requirements engineering,software product line},
  url       = {http://doi.acm.org/10.1145/3233027.3233029},
}

@InProceedings{Kanda:2013:EPE:2491627.2491637,
  author    = {Kanda, Tetsuya and Ishio, Takashi and Inoue, Katsuro},
  title     = {{Extraction of Product Evolution Tree from Source Code of Product Variants}},
  booktitle = {SPLC},
  year      = {2013},
  series    = {SPLC '13},
  pages     = {141--150},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/2491627.2491637},
  isbn      = {978-1-4503-1968-3},
  keywords  = {software evolution,software product line,visualization},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/2491627.2491637},
}

@InProceedings{Cataldo:2011:FLI:1985793.1985816,
  author    = {Cataldo, Marcelo and Herbsleb, James D},
  title     = {{Factors Leading to Integration Failures in Global Feature-oriented Development: An Empirical Analysis}},
  booktitle = {International Conference on Software Engineering},
  year      = {2011},
  series    = {ICSE '11},
  pages     = {161--170},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/1985793.1985816},
  isbn      = {978-1-4503-0445-0},
  keywords  = {cross-feature interaction,feature-oriented development,global software development},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/1985793.1985816},
}

@InProceedings{Holthusen:2014:FMM:2647908.2655965,
  author    = {Holthusen, S{\"{o}}nke and Wille, David and Legat, Christoph and Beddig, Simon and Schaefer, Ina and Vogel-Heuser, Birgit},
  title     = {{Family Model Mining for Function Block Diagrams in Automation Software}},
  booktitle = {SPLC},
  year      = {2014},
  series    = {SPLC '14},
  pages     = {36--43},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/2647908.2655965},
  isbn      = {978-1-4503-2739-8},
  keywords  = {automation software,family mining,re-engineering,software engineering},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/2647908.2655965},
}

@InProceedings{Li:2018:FVE:3236405.3236427,
  author    = {Li, Yang},
  title     = {{Feature and Variability Extraction from Natural Language Software Requirements Specifications}},
  booktitle = {International Systems and Software Product Line Conference},
  year      = {2018},
  series    = {SPLC '18},
  pages     = {72--78},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/3236405.3236427},
  isbn      = {978-1-4503-5945-0},
  keywords  = {feature identification,requirement documents,reverse engineering,software product lines,variability extraction},
  url       = {http://doi.acm.org/10.1145/3236405.3236427},
}

@InProceedings{Martinez:2018:FLB:3233027.3236402,
  author    = {Martinez, Jabier and Ordo{\~{n}}ez, Nicolas and T{\"{e}}rnava, Xhevahire and Ziadi, Tewfik and Aponte, Jairo and Figueiredo, Eduardo and Valente, Marco Tulio},
  title     = {{Feature Location Benchmark with argoUML SPL}},
  booktitle = {International Systems and Software Product Line Conference},
  year      = {2018},
  series    = {SPLC '18},
  pages     = {257--263},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/3233027.3236402},
  isbn      = {978-1-4503-6464-5},
  keywords  = {argoUML,benchmark,extractive software product line adoption,feature location,reverse-engineering,software product lines},
  url       = {http://doi.acm.org/10.1145/3233027.3236402},
}

@InProceedings{Trujillo:2007:FOM:1248820.1248838,
  author    = {Trujillo, Salvador and Batory, Don and Diaz, Oscar},
  title     = {{Feature Oriented Model Driven Development: A Case Study for Portlets}},
  booktitle = {International Conference on Software Engineering},
  year      = {2007},
  series    = {ICSE '07},
  pages     = {44--53},
  address   = {Washington, DC, USA},
  publisher = {IEEE Computer Society},
  doi       = {10.1109/ICSE.2007.36},
  isbn      = {0-7695-2828-7},
  url       = {http://0-dx.doi.org.fama.us.es/10.1109/ICSE.2007.36},
}

@InProceedings{Wende2011,
  author    = {Wende, Christian and Assmann, Uwe and Zivkovic, Srdjan and Kuhn, Harald},
  title     = {{Feature-Based Customisation of Tool Environments for Model-Driven Software Development}},
  booktitle = {SPLC},
  year      = {2011},
  pages     = {45--54},
  month     = {aug},
  publisher = {IEEE},
  doi       = {10.1109/SPLC.2011.29},
  isbn      = {978-1-4577-1029-2},
  url       = {http://ieeexplore.ieee.org/document/6030045/},
}

@InProceedings{Nobauer:2018:FRE:3233027.3233051,
  author    = {N{\"{o}}bauer, Markus and Groher, Iris and Seyff, Norbert},
  title     = {{Feature-based Reuse in the ERP Domain: An Industrial Case Study}},
  booktitle = {International Systems and Software Product Line Conference},
  year      = {2018},
  series    = {SPLC '18},
  pages     = {170--178},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/3233027.3233051},
  isbn      = {978-1-4503-6464-5},
  keywords  = {ERP systems,case study,feature,reuse,variability},
  url       = {http://doi.acm.org/10.1145/3233027.3233051},
}

@InProceedings{Li:2017:FLF:3106195.3106216,
  author    = {Li, Yi and Zhu, Chenguang and Rubin, Julia and Chechik, Marsha},
  title     = {{FHistorian: Locating Features in Version Histories}},
  booktitle = {International Systems and Software Product Line Conference},
  year      = {2017},
  series    = {SPLC '17},
  pages     = {49--58},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/3106195.3106216},
  isbn      = {978-1-4503-5221-5},
  keywords  = {Feature location,feature relationship,version history},
  url       = {http://doi.acm.org/10.1145/3106195.3106216},
}

@InProceedings{Kruger:2017:FLF:3109729.3109736,
  author    = {Kr{\"{u}}ger, Jacob and Nell, Louis and Fenske, Wolfram and Saake, Gunter and Leich, Thomas},
  title     = {{Finding Lost Features in Cloned Systems}},
  booktitle = {International Systems and Software Product Line Conference},
  year      = {2017},
  series    = {SPLC '17},
  pages     = {65--72},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/3109729.3109736},
  isbn      = {978-1-4503-5119-5},
  keywords  = {Software product line,code clone detection,extractive approach,feature location,legacy system,reverse engineering},
  url       = {http://doi.acm.org/10.1145/3109729.3109736},
}

@InProceedings{Verlage:2005:FYP:1062455.1062551,
  author    = {Verlage, Martin and Kiesgen, Thomas},
  title     = {{Five Years of Product Line Engineering in a Small Company}},
  booktitle = {International Conference on Software Engineering},
  year      = {2005},
  series    = {ICSE '05},
  pages     = {534--543},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/1062455.1062551},
  isbn      = {1-58113-963-2},
  keywords  = {SME,experience report,product line engineering,project management},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/1062455.1062551},
}

@InProceedings{Simidchieva:2014:GCV:2648511.2648533,
  author    = {Simidchieva, Borislava I and Osterweil, Leon J},
  title     = {{Generation, Composition, and Verification of Families of Human-intensive Systems}},
  booktitle = {SPLC},
  year      = {2014},
  series    = {SPLC '14},
  pages     = {207--216},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/2648511.2648533},
  isbn      = {978-1-4503-2740-4},
  keywords  = {process families,software product lines,system variation},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/2648511.2648533},
}

@InProceedings{Kuiter:2018:GRC:3233027.3233050,
  author    = {Kuiter, Elias and Kr{\"{u}}ger, Jacob and Krieter, Sebastian and Leich, Thomas and Saake, Gunter},
  title     = {{Getting Rid of Clone-and-own: Moving to a Software Product Line for Temperature Monitoring}},
  booktitle = {International Systems and Software Product Line Conference},
  year      = {2018},
  series    = {SPLC '18},
  pages     = {179--189},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/3233027.3233050},
  isbn      = {978-1-4503-6464-5},
  keywords  = {case study,extraction,feature modeling,software product line},
  url       = {http://doi.acm.org/10.1145/3233027.3233050},
}

@InProceedings{Kastner:2008:GSP:1368088.1368131,
  author    = {K{\"{a}}stner, Christian and Apel, Sven and Kuhlemann, Martin},
  title     = {{Granularity in Software Product Lines}},
  booktitle = {International Conference on Software Engineering},
  year      = {2008},
  series    = {ICSE '08},
  pages     = {311--320},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/1368088.1368131},
  isbn      = {978-1-60558-079-1},
  keywords  = {feature refactoring,ide,software product lines,virtual separation of concerns},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/1368088.1368131},
}

@InProceedings{Krueger2008,
  author    = {Krueger, Charles W. and Churchett, Dale and Buhrdorf, Ross},
  title     = {{HomeAway's Transition to Software Product Line Practice: Engineering and Business Results in 60 Days}},
  booktitle = {SPLC},
  year      = {2008},
  pages     = {297--306},
  month     = {sep},
  publisher = {IEEE},
  doi       = {10.1109/SPLC.2008.36},
  isbn      = {978-0-7695-3303-2},
  url       = {http://ieeexplore.ieee.org/document/4626863/},
}

@InProceedings{Plakidas:2016:SEE:2934466.2934488,
  author    = {Plakidas, Konstantinos and Stevanetic, Srdjan and Schall, Daniel and Ionescu, Tudor B and Zdun, Uwe},
  title     = {{How Do Software Ecosystems Evolve? A Quantitative Assessment of the R Ecosystem.}},
  booktitle = {International Systems and Software Product Line Conference},
  year      = {2016},
  series    = {SPLC '16},
  pages     = {89--98},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/2934466.2934488},
  isbn      = {978-1-4503-4050-2},
  keywords  = {R,empirical study,predictive model,software ecosystems},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/2934466.2934488},
}

@InProceedings{Thurimella2008,
  author    = {Thurimella, A.K. and Bruegge, B. and Creighton, O.},
  title     = {{Identifying and Exploiting the Similarities between Rationale Management and Variability Management}},
  booktitle = {SPLC},
  year      = {2008},
  pages     = {99--108},
  month     = {sep},
  publisher = {IEEE},
  doi       = {10.1109/SPLC.2008.14},
  isbn      = {978-0-7695-3303-2},
  url       = {http://ieeexplore.ieee.org/document/4626844/},
}

@InProceedings{Patzke:2012:IIP:2362536.2362569,
  author    = {Patzke, Thomas and Becker, Martin and Steffens, Michaela and Sierszecki, Krzysztof and Savolainen, Juha Erik and Fogdal, Thomas},
  title     = {{Identifying Improvement Potential in Evolving Product Line Infrastructures: 3 Case Studies}},
  booktitle = {SPLC},
  year      = {2012},
  series    = {SPLC '12},
  pages     = {239--248},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/2362536.2362569},
  isbn      = {978-1-4503-1094-9},
  keywords  = {PuLSE-E,goal-based product line measurement,industrial case study,product line code evolution,variability code smells},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/2362536.2362569},
}

@InProceedings{Leitner:2012:IDR:2364412.2364446,
  author    = {Leitner, Andrea and Wei{\ss}, Reinhold and Kreiner, Christian and Ebner, Wolfgang},
  title     = {{Improving Domain Representation with Multi-paradigm Modeling}},
  booktitle = {SPLC},
  year      = {2012},
  series    = {SPLC '12},
  pages     = {201--208},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/2364412.2364446},
  isbn      = {978-1-4503-1095-6},
  keywords  = {domain-specific modeling,feature-oriented domain modeling,multi-paradigm modeling,software product lines},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/2364412.2364446},
}

@InProceedings{Robinson:2010:IIA:1852786.1852814,
  author    = {Robinson, Brian and Francis, Patrick},
  title     = {{Improving Industrial Adoption of Software Engineering Research: A Comparison of Open and Closed Source Software}},
  booktitle = {International Symposium on Empirical Software Engineering and Measurement},
  year      = {2010},
  series    = {ESEM '10},
  pages     = {21:1----21:10},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/1852786.1852814},
  isbn      = {978-1-4503-0039-1},
  keywords  = {empirical studies,industrial adoption,software comparison},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/1852786.1852814},
}

@InProceedings{Hoole:2016:IVD:2915970.2915994,
  author    = {Hoole, Alexander M and Traore, Issa and Delaitre, Aurelien and de Oliveira, Charles},
  title     = {{Improving Vulnerability Detection Measurement: [Test Suites and Software Security Assurance]}},
  booktitle = {International Conference on Evaluation and Assessment in Software Engineering},
  year      = {2016},
  series    = {EASE '16},
  pages     = {27:1----27:10},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/2915970.2915994},
  isbn      = {978-1-4503-3691-8},
  keywords  = {dynamic analysis,security metrics,static analysis,test suites,vulnerability,weakness},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/2915970.2915994},
}

@InProceedings{Braga:2012:ICF:2362536.2362570,
  author    = {Braga, Rosana T V and {Trindade Jr.}, Onofre and Branco, Kalinka R L J Castelo and Lee, Jaejoon},
  title     = {{Incorporating Certification in Feature Modelling of an Unmanned Aerial Vehicle Product Line}},
  booktitle = {SPLC},
  year      = {2012},
  series    = {SPLC '12},
  pages     = {249--258},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/2362536.2362570},
  isbn      = {978-1-4503-1094-9},
  keywords  = {critical software development,feature modelling,software certification,software product lines},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/2362536.2362570},
}

@InProceedings{Pettersson:2005:IEB:1081706.1081758,
  author    = {Pettersson, Ulf and Jarzabek, Stan},
  title     = {{Industrial Experience with Building a Web Portal Product Line Using a Lightweight, Reactive Approach}},
  booktitle = {SIGSOFT International Symposium on Foundations of Software Engineering},
  year      = {2005},
  series    = {ESEC/FSE-13},
  pages     = {326--335},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/1081706.1081758},
  isbn      = {1-59593-014-0},
  keywords  = {maintenance,program synthesis,reuse,software product lines,static meta-programming,web engineering},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/1081706.1081758},
}

@InProceedings{Nobauer:2014:IVC:2648511.2648544,
  author    = {N{\"{o}}bauer, Markus and Seyff, Norbert and Groher, Iris},
  title     = {{Inferring Variability from Customized Standard Software Products}},
  booktitle = {SPLC},
  year      = {2014},
  series    = {SPLC '14},
  pages     = {284--293},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/2648511.2648544},
  isbn      = {978-1-4503-2740-4},
  keywords  = {ERP systems,reuse,standard software product customizations,variability inference},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/2648511.2648544},
}

@InProceedings{Domis:2015:IVS:2791060.2791088,
  author    = {Domis, Dominik and Adler, Rasmus and Becker, Martin},
  title     = {{Integrating Variability and Safety Analysis Models Using Commercial UML-based Tools}},
  booktitle = {SPLC},
  year      = {2015},
  series    = {SPLC '15},
  pages     = {225--234},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/2791060.2791088},
  isbn      = {978-1-4503-3613-0},
  keywords  = {fault tree analysis,feature model,functional-safety,safety analysis,safety engineering,software product line engineering,tool support,variability,variant management},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/2791060.2791088},
}

@InProceedings{Kehrbusch:2016:ISA:2934466.2934468,
  author    = {Kehrbusch, Philipp and Richenhagen, Johannes and Rumpe, Bernhard and Schlo$\backslash$sser, Axel and Schulze, Christoph},
  title     = {{Interface-based Similarity Analysis of Software Components for the Automotive Industry}},
  booktitle = {International Systems and Software Product Line Conference},
  year      = {2016},
  series    = {SPLC '16},
  pages     = {99--108},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/2934466.2934468},
  isbn      = {978-1-4503-4050-2},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/2934466.2934468},
}

@InProceedings{Lavazza:2010:IEC:1852786.1852820,
  author    = {Lavazza, Luigi and Robiolo, Gabriela},
  title     = {{Introducing the Evaluation of Complexity in Functional Size Measurement: A UML-based Approach}},
  booktitle = {International Symposium on Empirical Software Engineering and Measurement},
  year      = {2010},
  series    = {ESEM '10},
  pages     = {25:1----25:9},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/1852786.1852820},
  isbn      = {978-1-4503-0039-1},
  keywords  = {COSMIC function points,UML-based measurement,effort estimation,function points,functional complexity measurement,functional size measurement,use case-based measurement},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/1852786.1852820},
}

@InProceedings{Schmid:2005:IPA:1062455.1062552,
  author    = {Schmid, Klaus and John, Isabel and Kolb, Ronny and Meier, Gerald},
  title     = {{Introducing the puLSE Approach to an Embedded System Population at Testo AG}},
  booktitle = {International Conference on Software Engineering},
  year      = {2005},
  series    = {ICSE '05},
  pages     = {544--552},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/1062455.1062552},
  isbn      = {1-58113-963-2},
  keywords  = {product line introduction,software product line,systematic software reuse,technology transfer},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/1062455.1062552},
}

@InProceedings{Layman:2008:IIF:1414004.1414038,
  author    = {Layman, Lucas and Kudrjavets, Gunnar and Nagappan, Nachiappan},
  title     = {{Iterative Identification of Fault-prone Binaries Using In-process Metrics}},
  booktitle = {International Symposium on Empirical Software Engineering and Measurement},
  year      = {2008},
  series    = {ESEM '08},
  pages     = {206--212},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/1414004.1414038},
  isbn      = {978-1-59593-971-5},
  keywords  = {code churn,fault prediction,regression,statistical models},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/1414004.1414038},
}

@InProceedings{Kroher:2018:KOI:3236405.3236410,
  author    = {Kr{\"{o}}her, Christian and El-Sharkawy, Sascha and Schmid, Klaus},
  title     = {{KernelHaven: An Open Infrastructure for Product Line Analysis}},
  booktitle = {International Systems and Software Product Line Conference},
  year      = {2018},
  series    = {SPLC '18},
  pages     = {5--10},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/3236405.3236410},
  isbn      = {978-1-4503-5945-0},
  keywords  = {empirical software engineering,software product line analysis,static analysis,variability extraction},
  url       = {http://doi.acm.org/10.1145/3236405.3236410},
}

@InProceedings{Salay:2014:LMT:2568225.2568267,
  author    = {Salay, Rick and Famelis, Michalis and Rubin, Julia and {Di Sandro}, Alessio and Chechik, Marsha},
  title     = {{Lifting Model Transformations to Product Lines}},
  booktitle = {International Conference on Software Engineering},
  year      = {2014},
  series    = {ICSE 2014},
  pages     = {117--128},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/2568225.2568267},
  isbn      = {978-1-4503-2756-5},
  keywords  = {Model Driven Engineering,Model Transformations,Software Product Lines},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/2568225.2568267},
}

@InProceedings{Rubin:2013:MCV:2491627.2491644,
  author    = {Rubin, Julia and Czarnecki, Krzysztof and Chechik, Marsha},
  title     = {{Managing Cloned Variants: A Framework and Experience}},
  booktitle = {SPLC},
  year      = {2013},
  series    = {SPLC '13},
  pages     = {101--110},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/2491627.2491644},
  isbn      = {978-1-4503-1968-3},
  keywords  = {cloned product variants,industrial case studies,legacy software product lines},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/2491627.2491644},
}

@InProceedings{Kim:2006:MBF:1181775.1181781,
  author    = {Kim, Sunghun and Pan, Kai and {Whitehead Jr.}, E E James},
  title     = {{Memories of Bug Fixes}},
  booktitle = {SIGSOFT International Symposium on Foundations of Software Engineering},
  year      = {2006},
  series    = {SIGSOFT '06/FSE-14},
  pages     = {35--45},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/1181775.1181781},
  isbn      = {1-59593-468-5},
  keywords  = {bug,bug finding tool,fault,fix,patterns,prediction},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/1181775.1181781},
}

@InProceedings{Giger:2012:MBP:2372251.2372285,
  author    = {Giger, Emanuel and D'Ambros, Marco and Pinzger, Martin and Gall, Harald C},
  title     = {{Method-level Bug Prediction}},
  booktitle = {International Symposium on Empirical Software Engineering and Measurement},
  year      = {2012},
  series    = {ESEM '12},
  pages     = {171--180},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/2372251.2372285},
  isbn      = {978-1-4503-1056-7},
  keywords  = {code metrics,fine-grained source code changes,method-level bug prediction},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/2372251.2372285},
}

@Conference{Nadi2014140,
  author    = {Nadi, S and Berger, T and K{\"{a}}stner, C and Czarnecki, K},
  title     = {{Mining configuration constraints: Static analyses and empirical results}},
  booktitle = {International Conference on Software Engineering},
  year      = {2014},
  number    = {CONFCODENUMBER},
  pages     = {140--151},
  abstract  = {Highly-configurable systems allow users to tailor the software to their specific needs. Not all combinations of configuration options are valid though, and constraints arise for technical or non-technical reasons. Explicitly describing these constraints in a variability model allows reasoning about the supported configurations. To automate creating variability models, we need to identify the origin of such configuration constraints. We propose an approach which uses build-time errors and a novel feature-effect heuristic to automatically extract configuration constraints from C code. We conduct an empirical study on four highly-configurable open-source systems with existing variability models having three objectives in mind: evaluate the accuracy of our approach, determine the recoverability of existing variability-model constraints using our analysis, and classify the sources of variability-model constraints. We find that both our extraction heuristics are highly accurate (93{\%} and 77{\%} respectively), and that we can recover 19{\%} of the existing variability-models using our approach. However, we find that many of the remaining constraints require expert knowledge or more expensive analyses. We argue that our approach, tooling, and experimental results support researchers and practitioners working on variability model re-engineering, evolution, and consistency-checking techniques. {\textcopyright} 2014 ACM.},
  annote    = {cited By 34},
  doi       = {10.1145/2568225.2568283},
  keywords  = {C (programming language); Hierarchical systems; Op,Configuration constraints; Configuration options;,Static analysis},
  url       = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994165595{\&}doi=10.1145{\%}2F2568225.2568283{\&}partnerID=40{\&}md5=5c2a0a81c01b1c1a6e7b881320bf850a},
}

@InProceedings{Nagappan:2006:MMP:1134285.1134349,
  author    = {Nagappan, Nachiappan and Ball, Thomas and Zeller, Andreas},
  title     = {{Mining Metrics to Predict Component Failures}},
  booktitle = {International Conference on Software Engineering},
  year      = {2006},
  series    = {ICSE '06},
  pages     = {452--461},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/1134285.1134349},
  isbn      = {1-59593-375-1},
  keywords  = {bug database,complexity metrics,empirical study,principal component analysis,regression model},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/1134285.1134349},
}

@InProceedings{Classen:2010:MCL:1806799.1806850,
  author    = {Classen, Andreas and Heymans, Patrick and Schobbens, Pierre-Yves and Legay, Axel and Raskin, Jean-Fran{\c{c}}ois},
  title     = {{Model Checking Lots of Systems: Efficient Verification of Temporal Properties in Software Product Lines}},
  booktitle = {International Conference on Software Engineering},
  year      = {2010},
  series    = {ICSE '10},
  pages     = {335--344},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/1806799.1806850},
  isbn      = {978-1-60558-719-6},
  keywords  = {features,software product lines,specification},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/1806799.1806850},
}

@InProceedings{Lavazza:2008:MFS:1414004.1414021,
  author    = {Lavazza, Luigi A and del Bianco, Vieri and Garavaglia, Carla},
  title     = {{Model-based Functional Size Measurement}},
  booktitle = {International Symposium on Empirical Software Engineering and Measurement},
  year      = {2008},
  series    = {ESEM '08},
  pages     = {100--109},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/1414004.1414021},
  isbn      = {978-1-59593-971-5},
  keywords  = {function point analysis,functional size measurement,requirements modeling,uml},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/1414004.1414021},
}

@InProceedings{Bilic:2018:MPL:3236405.3237200,
  author    = {Bilic, Damir and Sundmark, Daniel and Afzal, Wasif and Wallin, Peter and Causevic, Adnan and Amlinger, Christoffer},
  title     = {{Model-based Product Line Engineering in an Industrial Automotive Context: An Exploratory Case Study}},
  booktitle = {International Systems and Software Product Line Conference},
  year      = {2018},
  series    = {SPLC '18},
  pages     = {56--63},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/3236405.3237200},
  isbn      = {978-1-4503-5945-0},
  keywords  = {model-based systems engineering,orthogonal variability modeling,system product lines,variability management},
  url       = {http://doi.acm.org/10.1145/3236405.3237200},
}

@InProceedings{Gaeta:2015:MAS:2791060.2791104,
  author    = {Gaeta, Jes{\'{u}}s Padilla and Czarnecki, Krzysztof},
  title     = {{Modeling Aerospace Systems Product Lines in SysML}},
  booktitle = {SPLC},
  year      = {2015},
  series    = {SPLC '15},
  pages     = {293--302},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/2791060.2791104},
  isbn      = {978-1-4503-3613-0},
  keywords  = {SysML,UML,embedded software,model-based engineering,product-line engineering,systems engineering,variability modeling},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/2791060.2791104},
}

@InProceedings{Ripon:2012:MAP:2364412.2364417,
  author    = {Ripon, Shamim and Azad, Keya and Hossain, Sk Jahir and Hassan, Mehidee},
  title     = {{Modeling and Analysis of Product-line Variants}},
  booktitle = {SPLC},
  year      = {2012},
  series    = {SPLC '12},
  pages     = {26--31},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/2364412.2364417},
  isbn      = {978-1-4503-1095-6},
  keywords  = {alloy,feature model,first-order logic,product line,variants},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/2364412.2364417},
}

@InProceedings{Mansoor:2018:MTF:3236024.3275534,
  author    = {Mansoor, Niloofar and Saddler, Jonathan A and Silva, Bruno and Bagheri, Hamid and Cohen, Myra B and Farritor, Shane},
  title     = {{Modeling and Testing a Family of Surgical Robots: An Experience Report}},
  booktitle = {SIGSOFT International Symposium on Foundations of Software Engineering},
  year      = {2018},
  series    = {ESEC/FSE 2018},
  pages     = {785--790},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/3236024.3275534},
  isbn      = {978-1-4503-5573-5},
  keywords  = {Alloy,cyber-physical systems,software product lines,testing and analysis},
  url       = {http://doi.acm.org/10.1145/3236024.3275534},
}

@InProceedings{Luthmann:2017:MTP:3106195.3106204,
  author    = {Luthmann, Lars and Stephan, Andreas and B{\"{u}}rdek, Johannes and Lochau, Malte},
  title     = {{Modeling and Testing Product Lines with Unbounded Parametric Real-Time Constraints}},
  booktitle = {International Systems and Software Product Line Conference},
  year      = {2017},
  series    = {SPLC '17},
  pages     = {104--113},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/3106195.3106204},
  isbn      = {978-1-4503-5221-5},
  keywords  = {Model-based Testing,Real-Time Systems,Software Product Lines,Timed Automata},
  url       = {http://doi.acm.org/10.1145/3106195.3106204},
}

@InProceedings{Basile:2018:MAF:3236405.3236408,
  author    = {Basile, Davide and ter Beek, Maurice H and Gnesi, Stefania},
  title     = {{Modelling and Analysis with Featured Modal Contract Automata}},
  booktitle = {International Systems and Software Product Line Conference},
  year      = {2018},
  series    = {SPLC '18},
  pages     = {11--16},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/3236405.3236408},
  isbn      = {978-1-4503-5945-0},
  keywords  = {contract automata,orchestration,service product line,variability},
  url       = {http://doi.acm.org/10.1145/3236405.3236408},
}

@InProceedings{Xue:2018:MIP:3180155.3180257,
  author    = {Xue, Yinxing and Li, Yan-Fu},
  title     = {{Multi-objective Integer Programming Approaches for Solving Optimal Feature Selection Problem: A New Perspective on Multi-objective Optimization Problems in SBSE}},
  booktitle = {International Conference on Software Engineering},
  year      = {2018},
  series    = {ICSE '18},
  pages     = {1231--1242},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/3180155.3180257},
  isbn      = {978-1-4503-5638-1},
  keywords  = {multi-objective integer programming (MOIP),multi-objective optimization (MOO),optimal feature selection problem},
  url       = {http://doi.acm.org/10.1145/3180155.3180257},
}

@InProceedings{Henard:2013:MTG:2491627.2491635,
  author    = {Henard, Christopher and Papadakis, Mike and Perrouin, Gilles and Klein, Jacques and Traon, Yves Le},
  title     = {{Multi-objective Test Generation for Software Product Lines}},
  booktitle = {SPLC},
  year      = {2013},
  series    = {SPLC '13},
  pages     = {62--71},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/2491627.2491635},
  isbn      = {978-1-4503-1968-3},
  keywords  = {feature models,genetic algorithms,multi-objective optimization,software product lines,test generation},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/2491627.2491635},
}

@InProceedings{Wang:2014:MTP:2648511.2648515,
  author    = {Wang, Shuai and Buchmann, David and Ali, Shaukat and Gotlieb, Arnaud and Pradhan, Dipesh and Liaaen, Marius},
  title     = {{Multi-objective Test Prioritization in Software Product Line Testing: An Industrial Case Study}},
  booktitle = {SPLC},
  year      = {2014},
  series    = {SPLC '14},
  pages     = {32--41},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/2648511.2648515},
  isbn      = {978-1-4503-2740-4},
  keywords  = {multi-objective optimization,search algorithms,software product lines,test prioritization},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/2648511.2648515},
}

@InProceedings{Pereira:2018:NTF:3233027.3233039,
  author    = {Pereira, Juliana Alves and Schulze, Sandro and Figueiredo, Eduardo and Saake, Gunter},
  title     = {{N-dimensional Tensor Factorization for Self-configuration of Software Product Lines at Runtime}},
  booktitle = {International Systems and Software Product Line Conference},
  year      = {2018},
  series    = {SPLC '18},
  pages     = {87--97},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/3233027.3233039},
  isbn      = {978-1-4503-6464-5},
  keywords  = {recommender systems,runtime decision-making,self-configuration,software product lines},
  url       = {http://doi.acm.org/10.1145/3233027.3233039},
}

@Conference{Nguyen200956,
  author    = {Nguyen, Q L},
  title     = {{Non-functional requirements analysis modeling for software product lines}},
  booktitle = {International Conference on Software Engineering},
  year      = {2009},
  pages     = {56--61},
  address   = {Vancouver, BC},
  abstract  = {In most IT projects, software developers usually pay attention to functional requirements that satisfy business needs of the system. Non-functional requirements (NFR) such as performance, usability, security, etc. are usually handled ad-hoc during the system testing phase, when it is late and costly to fix problems. Due to the importance and criticality of NFR, I study the problem of modeling NFR for Software Product Lines (SPL), which adds yet an additional dimension of complexity. This paper will survey the software engineering literature, in search of a systematic way to analyze and design NFR, from the perspectives of the concept of commonality and variability of SPL and the characteristics of NFR. Finally, I will propose a methodology based on the extension of Product Line UML-Based Software Engineering (PLUS) techniques, for a unified and automated method to model NFR throughout all phases of SPL engineering.},
  annote    = {cited By 16; Conference of 2009 ICSE Workshop on Modeling in Software Engineering, MiSE 2009 ; Conference Date: 16 May 2009 Through 24 May 2009; Conference Code:79624},
  doi       = {10.1109/MISE.2009.5069898},
  keywords  = {Automated methods; Business needs; Commonality and,Computer software,Engineering},
  url       = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-77949791166{\&}doi=10.1109{\%}2FMISE.2009.5069898{\&}partnerID=40{\&}md5=98028da34e0fb02a410a06f64a4dd8dd},
}

@InProceedings{Sayyad2013,
  author    = {Sayyad, Abdel Salam and Menzies, Tim and Ammar, Hany},
  title     = {{On the value of user preferences in search-based software engineering: A case study in software product lines}},
  booktitle = {International Conference on Software Engineering},
  year      = {2013},
  pages     = {492--501},
  month     = {may},
  publisher = {IEEE},
  doi       = {10.1109/ICSE.2013.6606595},
  isbn      = {978-1-4673-3076-3},
  url       = {http://ieeexplore.ieee.org/document/6606595/},
}

@InProceedings{Weckesser:2018:ORD:3233027.3233030,
  author    = {Weckesser, Markus and Kluge, Roland and Pfannem{\"{u}}ller, Martin and Matth{\'{e}}, Michael and Sch{\"{u}}rr, Andy and Becker, Christian},
  title     = {{Optimal Reconfiguration of Dynamic Software Product Lines Based on Performance-influence Models}},
  booktitle = {International Systems and Software Product Line Conference},
  year      = {2018},
  series    = {SPLC '18},
  pages     = {98--109},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/3233027.3233030},
  isbn      = {978-1-4503-6464-5},
  keywords  = {dynamic software product lines,machine learning,performance-influence models},
  url       = {http://doi.acm.org/10.1145/3233027.3233030},
}

@InProceedings{Loesch2007151,
  author    = {Loesch, Felix and Ploedereder, Erhard},
  title     = {{Optimization of variability in software product lines}},
  booktitle = {SPLC},
  year      = {2007},
  pages     = {151--160},
  month     = {sep},
  publisher = {IEEE},
  abstract  = {The widespread use of the product line approach allows companies to realize significant improvements in time-tomarket, cost, productivity, and quality. However, a fundamental problem in software product line engineering is that a product line of industrial size can easily incorporate several thousand variable features. The complexity caused by this amount of variability makes variability management and product derivation tasks extremely difficult. To address this problem, we present a new method to optimize the variability provided in a software product line. Our method constructs a visualization that provides a classification of the usage of variable features in real products derived from the product line. We show how this classification can be used to derive restructuring strategies for simplifying the variability. The effectiveness of our work is demonstrated by presenting a case study of optimizing the variability in a large industrial software product line. {\textcopyright} 2007 IEEE.},
  annote    = {From Duplicate 1 (Optimization of variability in software product lines - Loesch, F; Ploedereder, E) cited By 38},
  doi       = {10.1109/SPLINE.2007.4339264},
  isbn      = {0-7695-2888-0},
  keywords  = {Case studies,Fundamental problems,Industrial si,Network architecture,Production engineering,Software design},
  url       = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-47949091380{\&}doi=10.1109{\%}2FSPLINE.2007.4339264{\&}partnerID=40{\&}md5=cfc71076ae973e95aacf58b1ce1fad57 http://ieeexplore.ieee.org/document/4339264/},
}

@InProceedings{Mockus:2010:OVE:1882291.1882311,
  author    = {Mockus, Audris},
  title     = {{Organizational Volatility and Its Effects on Software Defects}},
  booktitle = {SIGSOFT International Symposium on Foundations of Software Engineering},
  year      = {2010},
  series    = {FSE '10},
  pages     = {117--126},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/1882291.1882311},
  isbn      = {978-1-60558-791-2},
  keywords  = {organizational volatility,software defects},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/1882291.1882311},
}

@InProceedings{Myllarniemi:2013:PVS:2491627.2491631,
  author    = {Myll{\"{a}}rniemi, Varvana and Savolainen, Juha and M{\"{a}}nnist{\"{o}}, Tomi},
  title     = {{Performance Variability in Software Product Lines: A Case Study in the Telecommunication Domain}},
  booktitle = {SPLC},
  year      = {2013},
  series    = {SPLC '13},
  pages     = {32--41},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/2491627.2491631},
  isbn      = {978-1-4503-1968-3},
  keywords  = {architecture,case study,software product line,variability},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/2491627.2491631},
}

@InProceedings{Yang:2008:PDS:1414004.1414016,
  author    = {Yang, Ye and He, Mei and Li, Mingshu and Wang, Qing and Boehm, Barry},
  title     = {{Phase Distribution of Software Development Effort}},
  booktitle = {International Symposium on Empirical Software Engineering and Measurement},
  year      = {2008},
  series    = {ESEM '08},
  pages     = {61--69},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/1414004.1414016},
  isbn      = {978-1-59593-971-5},
  keywords  = {cost estimation,development type,effort allocation,effort distribution,estimation accuracy,phase distribution},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/1414004.1414016},
}

@InProceedings{Thum:2014:PST:2648511.2648530,
  author    = {Th{\"{u}}m, Thomas and Meinicke, Jens and Benduhn, Fabian and Hentschel, Martin and von Rhein, Alexander and Saake, Gunter},
  title     = {{Potential Synergies of Theorem Proving and Model Checking for Software Product Lines}},
  booktitle = {SPLC},
  year      = {2014},
  series    = {SPLC '14},
  pages     = {177--186},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/2648511.2648530},
  isbn      = {978-1-4503-2740-4},
  keywords  = {design by contract,family-based verification,feature-based specification,feature-oriented contracts,model checking,software product lines,theorem proving,variability encoding},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/2648511.2648530},
}

@InProceedings{Marijan:2013:PPT:2491627.2491646,
  author    = {Marijan, Dusica and Gotlieb, Arnaud and Sen, Sagar and Hervieu, Aymeric},
  title     = {{Practical Pairwise Testing for Software Product Lines}},
  booktitle = {SPLC},
  year      = {2013},
  series    = {SPLC '13},
  pages     = {227--235},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/2491627.2491646},
  isbn      = {978-1-4503-1968-3},
  keywords  = {feature modelling,software product lines,variability management},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/2491627.2491646},
}

@InProceedings{Schroter:2006:PCF:1159733.1159739,
  author    = {Schr{\"{o}}ter, Adrian and Zimmermann, Thomas and Zeller, Andreas},
  title     = {{Predicting Component Failures at Design Time}},
  booktitle = {International Symposium on Empirical Software Engineering},
  year      = {2006},
  series    = {ISESE '06},
  pages     = {18--27},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/1159733.1159739},
  isbn      = {1-59593-218-6},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/1159733.1159739},
}

@InProceedings{Wang:2012:PDN:2372251.2372287,
  author    = {Wang, Jue and Zhang, Hongyu},
  title     = {{Predicting Defect Numbers Based on Defect State Transition Models}},
  booktitle = {International Symposium on Empirical Software Engineering and Measurement},
  year      = {2012},
  series    = {ESEM '12},
  pages     = {191--200},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/2372251.2372287},
  isbn      = {978-1-4503-1056-7},
  keywords  = {defect numbers,defect prediction,defect state transitions,defect-fixing performance,markov models},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/2372251.2372287},
}

@InProceedings{Zimmermann:2008:PDU:1368088.1368161,
  author    = {Zimmermann, Thomas and Nagappan, Nachiappan},
  title     = {{Predicting Defects Using Network Analysis on Dependency Graphs}},
  booktitle = {International Conference on Software Engineering},
  year      = {2008},
  series    = {ICSE '08},
  pages     = {531--540},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/1368088.1368161},
  isbn      = {978-1-60558-079-1},
  keywords  = {defect prediction,dependency graph,network analysis,windows server 2003},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/1368088.1368161},
}

@InProceedings{Meneely:2008:PFD:1453101.1453106,
  author    = {Meneely, Andrew and Williams, Laurie and Snipes, Will and Osborne, Jason},
  title     = {{Predicting Failures with Developer Networks and Social Network Analysis}},
  booktitle = {SIGSOFT International Symposium on Foundations of Software Engineering},
  year      = {2008},
  series    = {SIGSOFT '08/FSE-16},
  pages     = {13--23},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/1453101.1453106},
  isbn      = {978-1-59593-995-1},
  keywords  = {developer network,failure prediction,logistic regression,negative binomial regression,social network analysis},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/1453101.1453106},
}

@InProceedings{Arisholm:2006:PFC:1159733.1159738,
  author    = {Arisholm, Erik and Briand, Lionel C},
  title     = {{Predicting Fault-prone Components in a Java Legacy System}},
  booktitle = {International Symposium on Empirical Software Engineering},
  year      = {2006},
  series    = {ISESE '06},
  pages     = {8--17},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/1159733.1159738},
  isbn      = {1-59593-218-6},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/1159733.1159738},
}

@InProceedings{Ganesan,
  author    = {Ganesan, D. and Muthig, D. and Yoshimura, K.},
  title     = {{Predicting Return-on-Investment for Product Line Generations}},
  booktitle = {SPLC},
  pages     = {13--22},
  publisher = {IEEE},
  doi       = {10.1109/SPLINE.2006.1691573},
  isbn      = {0-7695-2599-7},
  url       = {http://ieeexplore.ieee.org/document/1691573/},
}

@InProceedings{Stein:2014:PFM:2648511.2648525,
  author    = {Stein, Jacob and Nunes, Ingrid and Cirilo, Elder},
  title     = {{Preference-based Feature Model Configuration with Multiple Stakeholders}},
  booktitle = {SPLC},
  year      = {2014},
  series    = {SPLC '14},
  pages     = {132--141},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/2648511.2648525},
  isbn      = {978-1-4503-2740-4},
  keywords  = {feature model configuration,preferences,social choice},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/2648511.2648525},
}

@InProceedings{Holl:2011:PLB:2019136.2019184,
  author    = {Holl, Gerald},
  title     = {{Product Line Bundles to Support Product Derivation in Multi Product Lines}},
  booktitle = {SPLC},
  year      = {2011},
  series    = {SPLC '11},
  pages     = {41:1----41:6},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/2019136.2019184},
  isbn      = {978-1-4503-0789-5},
  keywords  = {multi product lines,product derivation,tool support},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/2019136.2019184},
}

@InProceedings{Voelter2007,
  author    = {Voelter, Markus and Groher, Iris},
  title     = {{Product Line Implementation using Aspect-Oriented and Model-Driven Software Development}},
  booktitle = {SPLC},
  year      = {2007},
  pages     = {233--242},
  month     = {sep},
  publisher = {IEEE},
  doi       = {10.1109/SPLINE.2007.23},
  isbn      = {0-7695-2888-0},
  url       = {http://ieeexplore.ieee.org/document/4339272/},
}

@InProceedings{Gillain:2012:PPS:2362536.2362559,
  author    = {Gillain, Joseph and Faulkner, Stephane and Heymans, Patrick and Jureta, Ivan and Snoeck, Monique},
  title     = {{Product Portfolio Scope Optimization Based on Features and Goals}},
  booktitle = {SPLC},
  year      = {2012},
  series    = {SPLC '12},
  pages     = {161--170},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/2362536.2362559},
  isbn      = {978-1-4503-1094-9},
  keywords  = {product portfolio,release planning,scoping optimization,software product line},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/2362536.2362559},
}

@InProceedings{Thum:2016:PME:2934466.2934471,
  author    = {Th{\"{u}}m, Thomas and Ribeiro, M{\'{a}}rcio and Schr{\"{o}}ter, Reimar and Siegmund, Janet and Dalton, Francisco},
  title     = {{Product-line Maintenance with Emergent Contract Interfaces}},
  booktitle = {International Systems and Software Product Line Conference},
  year      = {2016},
  series    = {SPLC '16},
  pages     = {134--143},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/2934466.2934471},
  isbn      = {978-1-4503-4050-2},
  keywords  = {design by contract,evolution,maintenance,preprocessor variability,software product lines,weakest precondition},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/2934466.2934471},
}

@InProceedings{Couto:2017:PGG:3106195.3106214,
  author    = {Couto, Marco and Borba, Paulo and Cunha, J{\'{a}}come and Fernandes, Jo{\~{a}}o Paulo and Pereira, Rui and Saraiva, Jo{\~{a}}o},
  title     = {{Products Go Green: Worst-Case Energy Consumption in Software Product Lines}},
  booktitle = {International Systems and Software Product Line Conference},
  year      = {2017},
  series    = {SPLC '17},
  pages     = {84--93},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/3106195.3106214},
  isbn      = {978-1-4503-5221-5},
  url       = {http://doi.acm.org/10.1145/3106195.3106214},
}

@InProceedings{Myllarniemi:2016:PPV:2934466.2934474,
  author    = {Myll{\"{a}}rniemi, Varvana and Raatikainen, Mikko and Savolainen, Juha and M{\"{a}}nnist{\"{o}}, Tomi},
  title     = {{Purposeful Performance Variability in Software Product Lines: A Comparison of Two Case Studies}},
  booktitle = {International Systems and Software Product Line Conference},
  year      = {2016},
  series    = {SPLC '16},
  pages     = {144--153},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/2934466.2934474},
  isbn      = {978-1-4503-4050-2},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/2934466.2934474},
}

@InProceedings{Wu:2008:QAF:1414004.1414037,
  author    = {Wu, Shujian and Wang, Qing and Yang, Ye},
  title     = {{Quantitative Analysis of Faults and Failures with Multiple Releases of Softpm}},
  booktitle = {International Symposium on Empirical Software Engineering and Measurement},
  year      = {2008},
  series    = {ESEM '08},
  pages     = {198--205},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/1414004.1414037},
  isbn      = {978-1-59593-971-5},
  keywords  = {empirical studies,metrics,software faults and failures},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/1414004.1414037},
}

@InProceedings{Linsbauer:2013:RTF:2491627.2491630,
  author    = {Linsbauer, Lukas and Lopez-Herrejon, E Roberto and Egyed, Alexander},
  title     = {{Recovering Traceability Between Features and Code in Product Variants}},
  booktitle = {SPLC},
  year      = {2013},
  series    = {SPLC '13},
  pages     = {131--140},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/2491627.2491630},
  isbn      = {978-1-4503-1968-3},
  keywords  = {features,product variants,traceability},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/2491627.2491630},
}

@InProceedings{Kim:2017:RJS:3106195.3106201,
  author    = {Kim, Jongwook and Batory, Don and Dig, Danny},
  title     = {{Refactoring Java Software Product Lines}},
  booktitle = {International Systems and Software Product Line Conference},
  year      = {2017},
  series    = {SPLC '17},
  pages     = {59--68},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/3106195.3106201},
  isbn      = {978-1-4503-5221-5},
  keywords  = {refactoring,software product lines},
  url       = {http://doi.acm.org/10.1145/3106195.3106201},
}

@InProceedings{Morasca:2008:RAD:1414004.1414035,
  author    = {Morasca, Sandro},
  title     = {{Refining the Axiomatic Definition of Internal Software Attributes}},
  booktitle = {International Symposium on Empirical Software Engineering and Measurement},
  year      = {2008},
  series    = {ESEM '08},
  pages     = {188--197},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/1414004.1414035},
  isbn      = {978-1-59593-971-5},
  keywords  = {cohesion,complexity,coupling,internal software attributes,size},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/1414004.1414035},
}

@InProceedings{Krafczyk:2018:REC:3236405.3237202,
  author    = {Krafczyk, Adam and El-Sharkawy, Sascha and Schmid, Klaus},
  title     = {{Reverse Engineering Code Dependencies: Converting Integer-based Variability to Propositional Logic}},
  booktitle = {International Systems and Software Product Line Conference},
  year      = {2018},
  series    = {SPLC '18},
  pages     = {34--41},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/3236405.3237202},
  isbn      = {978-1-4503-5945-0},
  keywords  = {integer-based expressions,propositional logic,reverse engineering,satisfiability,software product lines,variability management},
  url       = {http://doi.acm.org/10.1145/3236405.3237202},
}

@InProceedings{Li:2018:REV:3233027.3233033,
  author    = {Li, Yang and Schulze, Sandro and Saake, Gunter},
  title     = {{Reverse Engineering Variability from Requirement Documents Based on Probabilistic Relevance and Word Embedding}},
  booktitle = {International Systems and Software Product Line Conference},
  year      = {2018},
  series    = {SPLC '18},
  pages     = {121--131},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/3233027.3233033},
  isbn      = {978-1-4503-6464-5},
  keywords  = {feature identification,requirement documents,reverse engineering,software product lines,variability extraction},
  url       = {http://doi.acm.org/10.1145/3233027.3233033},
}

@InProceedings{Santos:2016:RJS:2934466.2934486,
  author    = {Santos, Alcemir Rodrigues and {do Carmo Machado}, Ivan and de Almeida, Eduardo Santana},
  title     = {{RiPLE-HC: Javascript Systems Meets Spl Composition}},
  booktitle = {International Systems and Software Product Line Conference},
  year      = {2016},
  series    = {SPLC '16},
  pages     = {154--163},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/2934466.2934486},
  isbn      = {978-1-4503-4050-2},
  keywords  = {eclipse plugin,feature composition,featureIDE,software product line engineering,web systems domain},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/2934466.2934486},
}

@InProceedings{Liang:2015:SAL:2791060.2791070,
  author    = {Liang, Jia Hui and Ganesh, Vijay and Czarnecki, Krzysztof and Raman, Venkatesh},
  title     = {{SAT-based Analysis of Large Real-world Feature Models is Easy}},
  booktitle = {SPLC},
  year      = {2015},
  series    = {SPLC '15},
  pages     = {91--100},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/2791060.2791070},
  isbn      = {978-1-4503-3613-0},
  keywords  = {SAT-based analysis,feature model},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/2791060.2791070},
}

@InProceedings{Bartholdt:2012:SEE:2362536.2362573,
  author    = {Bartholdt, J{\"{o}}rg and Becker, Detlef},
  title     = {{Scope Extension of an Existing Product Line}},
  booktitle = {SPLC},
  year      = {2012},
  series    = {SPLC '12},
  pages     = {275--282},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/2362536.2362573},
  isbn      = {978-1-4503-1094-9},
  keywords  = {C/V analysis,governance,hierarchical product-line,scope extension},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/2362536.2362573},
}

@InProceedings{Harman:2014:SBS:2648511.2648513,
  author    = {Harman, M and Jia, Y and Krinke, J and Langdon, W B and Petke, J and Zhang, Y},
  title     = {{Search Based Software Engineering for Software Product Line Engineering: A Survey and Directions for Future Work}},
  booktitle = {SPLC},
  year      = {2014},
  series    = {SPLC '14},
  pages     = {5--18},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/2648511.2648513},
  isbn      = {978-1-4503-2740-4},
  keywords  = {SBSE,SPL,genetic programming,program synthesis},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/2648511.2648513},
}

@InProceedings{Markiegi:2017:SPL:3106195.3106210,
  author    = {Markiegi, Urtzi and Arrieta, Aitor and Sagardui, Goiuria and Etxeberria, Leire},
  title     = {{Search-based Product Line Fault Detection Allocating Test Cases Iteratively}},
  booktitle = {International Systems and Software Product Line Conference},
  year      = {2017},
  series    = {SPLC '17},
  pages     = {123--132},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/3106195.3106210},
  isbn      = {978-1-4503-5221-5},
  keywords  = {Fault Detection,Product Line Testing,Search-based Software Engineering},
  url       = {http://doi.acm.org/10.1145/3106195.3106210},
}

@InProceedings{Arrieta:2016:STC:2934466.2946046,
  author    = {Arrieta, Aitor and Wang, Shuai and Sagardui, Goiuria and Etxeberria, Leire},
  title     = {{Search-based Test Case Selection of Cyber-physical System Product Lines for Simulation-based Validation}},
  booktitle = {International Systems and Software Product Line Conference},
  year      = {2016},
  series    = {SPLC '16},
  pages     = {297--306},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/2934466.2946046},
  isbn      = {978-1-4503-4050-2},
  keywords  = {cyber-physical system product lines,search-based software engineering,test case selection},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/2934466.2946046},
}

@InProceedings{Bashari:2017:SSM:3106195.3106215,
  author    = {Bashari, Mahdi and Bagheri, Ebrahim and Du, Weichang},
  title     = {{Self-healing in Service Mashups Through Feature Adaptation}},
  booktitle = {International Systems and Software Product Line Conference},
  year      = {2017},
  series    = {SPLC '17},
  pages     = {94--103},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/3106195.3106215},
  isbn      = {978-1-4503-5221-5},
  url       = {http://doi.acm.org/10.1145/3106195.3106215},
}

@InProceedings{Al-Hajjaji:2014:SPS:2648511.2648532,
  author    = {Al-Hajjaji, Mustafa and Th{\"{u}}m, Thomas and Meinicke, Jens and Lochau, Malte and Saake, Gunter},
  title     = {{Similarity-based Prioritization in Software Product-line Testing}},
  booktitle = {SPLC},
  year      = {2014},
  series    = {SPLC '14},
  pages     = {197--206},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/2648511.2648532},
  isbn      = {978-1-4503-2740-4},
  keywords  = {combinatorial interaction testing,prioritization,product-line testing,software product lines},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/2648511.2648532},
}

@InProceedings{Wang:2013:SFS:2491627.2491640,
  author    = {Wang, Bo and Passos, Leonardo and Xiong, Yingfei and Czarnecki, Krzysztof and Zhao, Haiyan and Zhang, Wei},
  title     = {{SmartFixer: Fixing Software Configurations Based on Dynamic Priorities}},
  booktitle = {SPLC},
  year      = {2013},
  series    = {SPLC '13},
  pages     = {82--90},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/2491627.2491640},
  isbn      = {978-1-4503-1968-3},
  keywords  = {configuration error,fixing,priority},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/2491627.2491640},
}

@InProceedings{Meneely:2011:SDN:1985793.1985832,
  author    = {Meneely, Andrew and Williams, Laurie},
  title     = {{Socio-technical Developer Networks: Should We Trust Our Measurements?}},
  booktitle = {International Conference on Software Engineering},
  year      = {2011},
  series    = {ICSE '11},
  pages     = {281--290},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/1985793.1985832},
  isbn      = {978-1-4503-0445-0},
  keywords  = {developer network,developers,social network analysis},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/1985793.1985832},
}

@InProceedings{terBeek:2014:SPL:2647908.2655970,
  author    = {ter Beek, Maurice H and de Vink, Erik P},
  title     = {{Software Product Line Analysis with mCRL2}},
  booktitle = {SPLC},
  year      = {2014},
  series    = {SPLC '14},
  pages     = {78--85},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/2647908.2655970},
  isbn      = {978-1-4503-2739-8},
  keywords  = {behavioral analysis,model checking,modular verification,product families,variability},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/2647908.2655970},
}

@InProceedings{Martinez:2018:SPL:3233027.3233038,
  author    = {Martinez, Jabier and T{\"{e}}rnava, Xhevahire and Ziadi, Tewfik},
  title     = {{Software Product Line Extraction from Variability-rich Systems: The Robocode Case Study}},
  booktitle = {International Systems and Software Product Line Conference},
  year      = {2018},
  series    = {SPLC '18},
  pages     = {132--142},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/3233027.3233038},
  isbn      = {978-1-4503-6464-5},
  keywords  = {education,extractive software product line adoption,reverse-engineering,robocode,software product lines},
  url       = {http://doi.acm.org/10.1145/3233027.3233038},
}

@InProceedings{Nagappan:2005:SAT:1062455.1062558,
  author    = {Nagappan, Nachiappan and Ball, Thomas},
  title     = {{Static Analysis Tools As Early Indicators of Pre-release Defect Density}},
  booktitle = {International Conference on Software Engineering},
  year      = {2005},
  series    = {ICSE '05},
  pages     = {580--586},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/1062455.1062558},
  isbn      = {1-58113-963-2},
  keywords  = {defect density,fault-proneness,static analysis tools,statistical methods},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/1062455.1062558},
}

@InProceedings{Meneely:2010:SEA:1852786.1852798,
  author    = {Meneely, Andrew and Williams, Laurie},
  title     = {{Strengthening the Empirical Analysis of the Relationship Between Linus' Law and Software Security}},
  booktitle = {International Symposium on Empirical Software Engineering and Measurement},
  year      = {2010},
  series    = {ESEM '10},
  pages     = {9:1----9:10},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/1852786.1852798},
  isbn      = {978-1-4503-0039-1},
  keywords  = {contribution network,developer network,metric,vulnerability},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/1852786.1852798},
}

@InProceedings{Braunschweig:2012:SVP:2372251.2372286,
  author    = {Braunschweig, Brandt and Dhage, Neha and Viera, Maria Jose and Seaman, Carolyn and Sampath, Sreedevi and Koru, Gunes A},
  title     = {{Studying Volatility Predictors in Open Source Software}},
  booktitle = {International Symposium on Empirical Software Engineering and Measurement},
  year      = {2012},
  series    = {ESEM '12},
  pages     = {181--190},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/2372251.2372286},
  isbn      = {978-1-4503-1056-7},
  keywords  = {empirical study,focus group study,open source software,volatility predictors},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/2372251.2372286},
}

@Conference{Michalik2011187,
  author    = {Michalik, B and Weyns, D and Bouck{\'{e}}, N and Helleboogh, A},
  title     = {{Supporting online updates of software product lines: A controlled experiment}},
  booktitle = {International Symposium on Empirical Software Engineering and Measurement},
  year      = {2011},
  pages     = {187--196},
  abstract  = {The evolution of Software Product Lines (SPL) is challenging because stakeholders have to deal with both regular evolution and the co-existence of different products. Our focus of product evolution is on the tasks integrators have to perform to update deployed SPL products with minimal interruption of services. In case of Egemin, our industrial partner, the updates of SPL products is further hampered as a consequence of outdated and imprecise architectural knowledge of deployed products. To facilitate the updates of products, we have developed the architecture-centric approach which comprises two complementary parts: an update viewpoint and a supporting tool. In this paper we present an evaluation of the architecturecentric approach. The approach is compared with the Egemin's current update approach in a controlled experiment. In the experiment 17 professionals were asked to perform 68 updates of logistic systems. The results obtained from the experiment show that the architecture-centric approach significantly improves the correctness of updates and reduces the interruption of services during updates of Egemin's SPL products. {\textcopyright} 2011 IEEE.},
  annote    = {cited By 1},
  keywords  = {Architectural knowledge; Co-existence; Controlled,Experiments; Software architecture; Software desi,Network architecture},
  url       = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84858714628{\&}partnerID=40{\&}md5=79f700f574b04a0c311d7c2e21861c87},
}

@InProceedings{Classen2011,
  author    = {Classen, Andreas and Heymans, Patrick and Schobbens, Pierre-Yves and Legay, Axel},
  title     = {{Symbolic model checking of software product lines}},
  booktitle = {International conference on Software engineering},
  year      = {2011},
  pages     = {321},
  address   = {New York, New York, USA},
  publisher = {ACM Press},
  doi       = {10.1145/1985793.1985838},
  isbn      = {9781450304450},
  url       = {http://portal.acm.org/citation.cfm?doid=1985793.1985838},
}

@InProceedings{Becan:2015:SAF:2791060.2791068,
  author    = {B{\'{e}}can, Guillaume and Behjati, Razieh and Gotlieb, Arnaud and Acher, Mathieu},
  title     = {{Synthesis of Attributed Feature Models from Product Descriptions}},
  booktitle = {SPLC},
  year      = {2015},
  series    = {SPLC '15},
  pages     = {1--10},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/2791060.2791068},
  isbn      = {978-1-4503-3613-0},
  keywords  = {attributed feature models,product descriptions},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/2791060.2791068},
}

@InProceedings{Yu:2014:TTD:2648511.2648531,
  author    = {Yu, Wenjing and Zhang, Wei and Zhao, Haiyan and Jin, Zhi},
  title     = {{TDL: A Transformation Description Language from Feature Model to Use Case for Automated Use Case Derivation}},
  booktitle = {SPLC},
  year      = {2014},
  series    = {SPLC '14},
  pages     = {187--196},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/2648511.2648531},
  isbn      = {978-1-4503-2740-4},
  keywords  = {feature model,software product line,transformation,use case},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/2648511.2648531},
}

@InProceedings{Seidl:2014:TVE:2647908.2655961,
  author    = {Seidl, Christoph and Domachowska, Irena},
  title     = {{Teaching Variability Engineering to Cognitive Psychologists}},
  booktitle = {SPLC},
  year      = {2014},
  series    = {SPLC '14},
  pages     = {16--23},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/2647908.2655961},
  isbn      = {978-1-4503-2739-8},
  keywords  = {cognitive psychology,feature model,teaching,variability engineering},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/2647908.2655961},
}

@InProceedings{Fogdal:2016:TYP:2934466.2934491,
  author    = {Fogdal, Thomas and Scherrebeck, Helene and Kuusela, Juha and Becker, Martin and Zhang, Bo},
  title     = {{Ten Years of Product Line Engineering at Danfoss: Lessons Learned and Way Ahead}},
  booktitle = {International Systems and Software Product Line Conference},
  year      = {2016},
  series    = {SPLC '16},
  pages     = {252--261},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/2934466.2934491},
  isbn      = {978-1-4503-4050-2},
  keywords  = {industrial experiences,product line adoption,product line evaluation},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/2934466.2934491},
}

@InProceedings{Arrieta:2015:TCA:2791060.2791095,
  author    = {Arrieta, Aitor and Sagardui, Goiuria and Etxeberria, Leire},
  title     = {{Test Control Algorithms for the Validation of Cyber-physical Systems Product Lines}},
  booktitle = {SPLC},
  year      = {2015},
  series    = {SPLC '15},
  pages     = {273--282},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/2791060.2791095},
  isbn      = {978-1-4503-3613-0},
  keywords  = {cyber-physical systems product lines,product line engineering,testing,validation},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/2791060.2791095},
}

@InProceedings{Markiegi:2017:TOH:3109729.3109745,
  author    = {Markiegi, Urtzi},
  title     = {{Test Optimisation for Highly-Configurable Cyber-Physical Systems}},
  booktitle = {International Systems and Software Product Line Conference},
  year      = {2017},
  series    = {SPLC '17},
  pages     = {139--144},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/3109729.3109745},
  isbn      = {978-1-4503-5119-5},
  keywords  = {Cyber-Physical Systems,Fault Detection,Highly-Configurable Systems,Product Line Testing,Search-Based Software Engineering,Software Engineering},
  url       = {http://doi.acm.org/10.1145/3109729.3109745},
}

@InProceedings{Denger:2006:TIR:1159733.1159762,
  author    = {Denger, Christian and Kolb, Ronny},
  title     = {{Testing and Inspecting Reusable Product Line Components: First Empirical Results}},
  booktitle = {International Symposium on Empirical Software Engineering},
  year      = {2006},
  series    = {ISESE '06},
  pages     = {184--193},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/1159733.1159762},
  isbn      = {1-59593-218-6},
  keywords  = {controlled experiment,functional testing,inspection,quality assurance,reusable components,software product line},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/1159733.1159762},
}

@InProceedings{Nagappan:2008:IOS:1368088.1368160,
  author    = {Nagappan, Nachiappan and Murphy, Brendan and Basili, Victor},
  title     = {{The Influence of Organizational Structure on Software Quality: An Empirical Case Study}},
  booktitle = {International Conference on Software Engineering},
  year      = {2008},
  series    = {ICSE '08},
  pages     = {521--530},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/1368088.1368160},
  isbn      = {978-1-60558-079-1},
  keywords  = {code churn,developers,empirical studies,failures,organizational structure,software mining},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/1368088.1368160},
}

@InProceedings{Bachmann:2010:MLB:1882291.1882308,
  author    = {Bachmann, Adrian and Bird, Christian and Rahman, Foyzur and Devanbu, Premkumar and Bernstein, Abraham},
  title     = {{The Missing Links: Bugs and Bug-fix Commits}},
  booktitle = {SIGSOFT International Symposium on Foundations of Software Engineering},
  year      = {2010},
  series    = {FSE '10},
  pages     = {97--106},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/1882291.1882308},
  isbn      = {978-1-60558-791-2},
  keywords  = {apache,bias,case study,manual annotation,tool},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/1882291.1882308},
}

@InProceedings{Gregg:2015:MYM:2791060.2791065,
  author    = {Gregg, Susan P and Scharadin, Rick and Clements, Paul},
  title     = {{The More You Do, the More You Save: The Superlinear Cost Avoidance Effect of Systems Product Line Engineering}},
  booktitle = {SPLC},
  year      = {2015},
  series    = {SPLC '15},
  pages     = {303--310},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/2791060.2791065},
  isbn      = {978-1-4503-3613-0},
  keywords  = {AEGIS,feature modeling,product configurator,product derivation,product line economics,product line engineering,product line measurement,second generation product line engineering,systems and software product lines,variation points},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/2791060.2791065},
}

@InProceedings{Berger:2014:CCE:2648511.2648549,
  author    = {Berger, Thorsten and St$\backslash$uanciulescu, $\backslash$cStefan and $\backslash$Og$\backslash$aard, Ommund and Haugen, $\backslash$Oystein and Larsen, Bo and W$\backslash$kasowski, Andrzej},
  title     = {{To Connect or Not to Connect: Experiences from Modeling Topological Variability}},
  booktitle = {SPLC},
  year      = {2014},
  series    = {SPLC '14},
  pages     = {330--339},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/2648511.2648549},
  isbn      = {978-1-4503-2740-4},
  keywords  = {class diagrams,configuration,experience report,software product lines,topology,variability modeling},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/2648511.2648549},
}

@InProceedings{Guo:2018:PPI:3180155.3180163,
  author    = {Guo, Jianmei and Shi, Kai},
  title     = {{To Preserve or Not to Preserve Invalid Solutions in Search-based Software Engineering: A Case Study in Software Product Lines}},
  booktitle = {International Conference on Software Engineering},
  year      = {2018},
  series    = {ICSE '18},
  pages     = {1027--1038},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/3180155.3180163},
  isbn      = {978-1-4503-5638-1},
  keywords  = {constraint solving,multi-objective evolutionary algorithms,search-based software engineering,software product lines,validity},
  url       = {http://doi.acm.org/10.1145/3180155.3180163},
}

@InProceedings{Vasilevskiy:2016:TRP:2934466.2934484,
  author    = {Vasilevskiy, Anatoly and Chauvel, Franck and Haugen, $\backslash$Oystein},
  title     = {{Toward Robust Product Realisation in Software Product Lines}},
  booktitle = {International Systems and Software Product Line Conference},
  year      = {2016},
  series    = {SPLC '16},
  pages     = {184--193},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/2934466.2934484},
  isbn      = {978-1-4503-4050-2},
  keywords  = {automated planning,bvr,fragment substitution,model,product derivation,product line,realisation,variation point},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/2934466.2934484},
}

@InProceedings{Gnesi:2012:TEA:2364412.2364424,
  author    = {Gnesi, Stefania and Petrocchi, Marinella},
  title     = {{Towards an Executable Algebra for Product Lines}},
  booktitle = {SPLC},
  year      = {2012},
  series    = {SPLC '12},
  pages     = {66--73},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/2364412.2364424},
  isbn      = {978-1-4503-1095-6},
  keywords  = {Maude,automated analysis,product families,variability},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/2364412.2364424},
}

@InProceedings{Kruger:2018:TAT:3233027.3233040,
  author    = {Kr{\"{u}}ger, Jacob and Al-Hajjaji, Mustafa and Schulze, Sandro and Saake, Gunter and Leich, Thomas},
  title     = {{Towards Automated Test Refactoring for Software Product Lines}},
  booktitle = {International Systems and Software Product Line Conference},
  year      = {2018},
  series    = {SPLC '18},
  pages     = {143--148},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/3233027.3233040},
  isbn      = {978-1-4503-6464-5},
  keywords  = {extractive approach,legacy system,maintenance,migration,software product line,testing},
  url       = {http://doi.acm.org/10.1145/3233027.3233040},
}

@InProceedings{Marcen:2017:TFL:3109729.3109734,
  author    = {Marc{\'{e}}n, Ana C and Font, Jaime and Pastor, {\'{O}}scar and Cetina, Carlos},
  title     = {{Towards Feature Location in Models Through a Learning to Rank Approach}},
  booktitle = {International Systems and Software Product Line Conference},
  year      = {2017},
  series    = {SPLC '17},
  pages     = {57--64},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/3109729.3109734},
  isbn      = {978-1-4503-5119-5},
  url       = {http://doi.acm.org/10.1145/3109729.3109734},
}

@InProceedings{Lackner:2014:TAS:2647908.2655968,
  author    = {Lackner, Hartmut and Schmidt, Martin},
  title     = {{Towards the Assessment of Software Product Line Tests: A Mutation System for Variable Systems}},
  booktitle = {SPLC},
  year      = {2014},
  series    = {SPLC '14},
  pages     = {62--69},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/2647908.2655968},
  isbn      = {978-1-4503-2739-8},
  keywords  = {mutation analysis,software product lines,software testing,test quality},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/2647908.2655968},
}

@InProceedings{Mohalik:2012:TSP:2362536.2362562,
  author    = {Mohalik, Swarup and Ramesh, S and Millo, Jean-Vivien and Krishna, Shankara Narayanan and Narwane, Ganesh Khandu},
  title     = {{Tracing SPLs Precisely and Efficiently}},
  booktitle = {SPLC},
  year      = {2012},
  series    = {SPLC '12},
  pages     = {186--195},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/2362536.2362562},
  isbn      = {978-1-4503-1094-9},
  keywords  = {QSAT,feature model,formal methods,software product line},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/2362536.2362562},
}

@InProceedings{Mizuno:2007:TEE:1287624.1287683,
  author    = {Mizuno, Osamu and Kikuno, Tohru},
  title     = {{Training on Errors Experiment to Detect Fault-prone Software Modules by Spam Filter}},
  booktitle = {SIGSOFT International Symposium on Foundations of Software Engineering},
  year      = {2007},
  series    = {ESEC-FSE '07},
  pages     = {405--414},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/1287624.1287683},
  isbn      = {978-1-59593-811-4},
  keywords  = {fault-prone modules,spam filter,text mining},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/1287624.1287683},
}

@InProceedings{Hamza:2017:UES:3109729.3109739,
  author    = {Hamza, Mostafa and Walker, Robert J and Elaasar, Maged},
  title     = {{Unanticipated Evolution in Software Product Lines Versus Independent Products: A Case Study}},
  booktitle = {International Systems and Software Product Line Conference},
  year      = {2017},
  series    = {SPLC '17},
  pages     = {97--104},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/3109729.3109739},
  isbn      = {978-1-4503-5119-5},
  keywords  = {Software product lines,case study,comparative study,delta-oriented programming,retrospective study,separate products,unanticipated evolution},
  url       = {http://doi.acm.org/10.1145/3109729.3109739},
}

@InProceedings{Aquino:2010:UEM:1852786.1852826,
  author    = {Aquino, Nathalie and Vanderdonckt, Jean and Condori-Fern{\'{a}}ndez, Nelly and Dieste, {\'{O}}scar and Pastor, {\'{O}}scar},
  title     = {{Usability Evaluation of Multi-device/Platform User Interfaces Generated by Model-driven Engineering}},
  booktitle = {International Symposium on Empirical Software Engineering and Measurement},
  year      = {2010},
  series    = {ESEM '10},
  pages     = {30:1----30:10},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/1852786.1852826},
  isbn      = {978-1-4503-0039-1},
  keywords  = {effectiveness,efficiency,interaction with small and large screens,model-driven engineering,multi-device interface,multi-platform interface,satisfaction,usability evaluation},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/1852786.1852826},
}

@InProceedings{Nagappan:2005:URC:1062455.1062514,
  author    = {Nagappan, Nachiappan and Ball, Thomas},
  title     = {{Use of Relative Code Churn Measures to Predict System Defect Density}},
  booktitle = {International Conference on Software Engineering},
  year      = {2005},
  series    = {ICSE '05},
  pages     = {284--292},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/1062455.1062514},
  isbn      = {1-58113-963-2},
  keywords  = {defect density,fault-proneness,multiple regression,principal component analysis,relative code churn},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/1062455.1062514},
}

@InProceedings{Feigenspan2011,
  author    = {Feigenspan, J. and Schulze, M. and Papendieck, M. and Kastner, C. and Dachselt, R. and Koppen, V. and Frisch, M.},
  title     = {{Using background colors to support program comprehension in software product lines}},
  booktitle = {Conference on Evaluation {\&} Assessment in Software Engineering (EASE 2011)},
  year      = {2011},
  pages     = {66--75},
  publisher = {IET},
  doi       = {10.1049/ic.2011.0008},
  isbn      = {978-1-84919-509-6},
  url       = {http://digital-library.theiet.org/content/conferences/10.1049/ic.2011.0008},
}

@InProceedings{Zhang:2011:UKS:2019136.2019172,
  author    = {Zhang, Guoheng and Ye, Huilin and Lin, Yuqing},
  title     = {{Using Knowledge-based Systems to Manage Quality Attributes in Software Product Lines}},
  booktitle = {SPLC},
  year      = {2011},
  series    = {SPLC '11},
  pages     = {32:1----32:7},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/2019136.2019172},
  isbn      = {978-1-4503-0789-5},
  keywords  = {feature model,non-functional requirements,product configuration,quality attributes,software product line},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/2019136.2019172},
}

@InProceedings{Tizzei:2017:UMS:3106195.3106224,
  author    = {Tizzei, Leonardo P and Nery, Marcelo and Segura, Vin$\backslash$'$\backslash$icius C V B and Cerqueira, Renato F G},
  title     = {{Using Microservices and Software Product Line Engineering to Support Reuse of Evolving Multi-tenant SaaS}},
  booktitle = {International Systems and Software Product Line Conference},
  year      = {2017},
  series    = {SPLC '17},
  pages     = {205--214},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/3106195.3106224},
  isbn      = {978-1-4503-5221-5},
  keywords  = {Microservices,Multi-tenancy,Service-oriented Architectures,Software Evolution,Software Reuse},
  url       = {http://doi.acm.org/10.1145/3106195.3106224},
}

@InProceedings{Trask,
  author    = {Trask, B. and Roman, A. and Paniscotti, D. and Bhanot, V.},
  title     = {{Using Model-Driven Engineering to Complement Software Product Line Engineering in Developing Software Defined Radio Components and Applications}},
  booktitle = {SPLC},
  pages     = {192--202},
  publisher = {IEEE},
  doi       = {10.1109/SPLINE.2006.1691591},
  isbn      = {0-7695-2599-7},
  url       = {http://ieeexplore.ieee.org/document/1691591/},
}

@InProceedings{Chavarriaga:2015:UMF:2791060.2791091,
  author    = {Chavarriaga, Jaime and Rangel, Carlos and Noguera, Carlos and Casallas, Rubby and Jonckers, Viviane},
  title     = {{Using Multiple Feature Models to Specify Configuration Options for Electrical Transformers: An Experience Report}},
  booktitle = {SPLC},
  year      = {2015},
  series    = {SPLC '15},
  pages     = {216--224},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/2791060.2791091},
  isbn      = {978-1-4503-3613-0},
  keywords  = {electrical transformers,product configuration,variability models},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/2791060.2791091},
}

@InProceedings{Zhang:2013:VEE:2491627.2491645,
  author    = {Zhang, Bo and Becker, Martin and Patzke, Thomas and Sierszecki, Krzysztof and Savolainen, Juha Erik},
  title     = {{Variability Evolution and Erosion in Industrial Product Lines: A Case Study}},
  booktitle = {SPLC},
  year      = {2013},
  series    = {SPLC '13},
  pages     = {168--177},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/2491627.2491645},
  isbn      = {978-1-4503-1968-3},
  keywords  = {industrial case study,product line evolution,static code analysis,variability erosion},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/2491627.2491645},
}

@InProceedings{Wille:2017:VMT:3106195.3106202,
  author    = {Wille, David and Wehling, Kenny and Seidl, Christoph and Pluchator, Martin and Schaefer, Ina},
  title     = {{Variability Mining of Technical Architectures}},
  booktitle = {International Systems and Software Product Line Conference},
  year      = {2017},
  series    = {SPLC '17},
  pages     = {39--48},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/3106195.3106202},
  isbn      = {978-1-4503-5221-5},
  keywords  = {enterprise architecture,technical architecture,variability mining},
  url       = {http://doi.acm.org/10.1145/3106195.3106202},
}

@InProceedings{Berger:2012:VMW:2364412.2364452,
  author    = {Berger, Thorsten},
  title     = {{Variability Modeling in the Wild}},
  booktitle = {SPLC},
  year      = {2012},
  series    = {SPLC '12},
  pages     = {233--241},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/2364412.2364452},
  isbn      = {978-1-4503-1095-6},
  keywords  = {empirical software engineering,software ecosystems,software product lines,variability modeling},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/2364412.2364452},
}

@InProceedings{Guana:2011:VQE:2019136.2019158,
  author    = {Guana, Victor and Correal, Dario},
  title     = {{Variability Quality Evaluation on Component-based Software Product Lines}},
  booktitle = {SPLC},
  year      = {2011},
  series    = {SPLC '11},
  pages     = {19:1----19:8},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/2019136.2019158},
  isbn      = {978-1-4503-0789-5},
  keywords  = {domain specific modeling,model composition,model-driven software product line,quality attribute,sensitivity point},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/2019136.2019158},
}

@InProceedings{Montalvillo:2017:VPC:3109729.3109737,
  author    = {Montalvillo, Leticia and D$\backslash$'$\backslash$iaz, Oscar and Azanza, Maider},
  title     = {{Visualizing Product Customization Efforts for Spotting SPL Reuse Opportunities}},
  booktitle = {International Systems and Software Product Line Conference},
  year      = {2017},
  series    = {SPLC '17},
  pages     = {73--80},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/3109729.3109737},
  isbn      = {978-1-4503-5119-5},
  url       = {http://doi.acm.org/10.1145/3109729.3109737},
}

@InProceedings{Berger:2015:FQS:2791060.2791108,
  author    = {Berger, Thorsten and Lettner, Daniela and Rubin, Julia and Gr{\"{u}}nbacher, Paul and Silva, Adeline and Becker, Martin and Chechik, Marsha and Czarnecki, Krzysztof},
  title     = {{What is a Feature?: A Qualitative Study of Features in Industrial Software Product Lines}},
  booktitle = {SPLC},
  year      = {2015},
  series    = {SPLC '15},
  pages     = {16--25},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/2791060.2791108},
  isbn      = {978-1-4503-3613-0},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/2791060.2791108},
}

@InProceedings{Kim:2007:WIF:1287624.1287633,
  author    = {Kim, Sunghun and Ernst, Michael D},
  title     = {{Which Warnings Should I Fix First?}},
  booktitle = {SIGSOFT International Symposium on Foundations of Software Engineering},
  year      = {2007},
  series    = {ESEC-FSE '07},
  pages     = {45--54},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/1287624.1287633},
  isbn      = {978-1-59593-811-4},
  keywords  = {bug,bug-finding tool,fault,fix,patterns,prediction},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/1287624.1287633},
}

@InProceedings{Umarji:2008:WPA:1414004.1414027,
  author    = {Umarji, Medha and Seaman, Carolyn},
  title     = {{Why Do Programmers Avoid Metrics?}},
  booktitle = {International Symposium on Empirical Software Engineering and Measurement},
  year      = {2008},
  series    = {ESEM '08},
  pages     = {129--138},
  address   = {New York, NY, USA},
  publisher = {ACM},
  doi       = {10.1145/1414004.1414027},
  isbn      = {978-1-59593-971-5},
  keywords  = {code review,in-process metrics,phase containment,quality,software metrics,static analysis,unit testing},
  url       = {http://0-doi.acm.org.fama.us.es/10.1145/1414004.1414027},
}

@Comment{jabref-meta: databaseType:bibtex;}
